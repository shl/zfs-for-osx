<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [zfs-discuss] read errors
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=%5Bzfs-discuss%5D%20read%20errors&In-Reply-To=">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000914.html">
   <LINK REL="Next"  HREF="000917.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[zfs-discuss] read errors</H1>
    <B>Brett Koonce</B> 
    <A HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=%5Bzfs-discuss%5D%20read%20errors&In-Reply-To="
       TITLE="[zfs-discuss] read errors">koonce at universe42.com
       </A><BR>
    <I>Wed Oct  1 10:13:05 PDT 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="000914.html">[zfs-discuss] can't delete from full volume
</A></li>
        <LI>Next message: <A HREF="000917.html">[zfs-discuss] read errors
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#915">[ date ]</a>
              <a href="thread.html#915">[ thread ]</a>
              <a href="subject.html#915">[ subject ]</a>
              <a href="author.html#915">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>I built a little raidz array with 4 500GB hard drives in a Sans  
Digital TRU-4 usb box.  I copied over some data (~700GB), ran a zfs  
scrub and got this:

$ zpool status
   pool: clover
  state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
	attempt was made to correct the error.  Applications are unaffected.
action: Determine if the device needs to be replaced, and clear the  
errors
	using 'zpool clear' or replace the device with 'zpool replace'.
    see: <A HREF="http://www.sun.com/msg/ZFS-8000-9P">http://www.sun.com/msg/ZFS-8000-9P</A>
  scrub: scrub in progress, 28.97% done, 12h15m to go
config:

	NAME         STATE     READ WRITE CKSUM
	clover       ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk1s2  ONLINE       0     0     0
	    disk2s2  ONLINE       0     0     0
	    disk3s2  ONLINE       0     0     0
	    disk4s2  ONLINE       9     0     0

I decided to clear the errors and went along my merry way.  So a  
couple days ago I decided to scrub things again, and got the exact  
same results (9 read errors on disk4 near the 30% mark).

So, my questions:

I was under the impression that zfs would attempt to repair the  
errors.  Or rather, is the fact that I got the same numbers twice a  
weird coincidence?  (i.e. are they the same 9 errors?)

My understanding (correct me if I'm wrong) is that zfs will only  
attempt to fix things on a COW operation.  In that case, if I was to  
find the file in question, duplicate/replace it from my other backups,  
and delete the original, would this theoretically get around the bad  
sectors?  Is there a way to get that information out of zfs?  The  
system.log only mentions the errors happened.

I guess I should just pony up the fifty bucks for a new drive.  But  
I'd like to know how this works.  Thanks in advance,
-Brett
</PRE>



<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000914.html">[zfs-discuss] can't delete from full volume
</A></li>
	<LI>Next message: <A HREF="000917.html">[zfs-discuss] read errors
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#915">[ date ]</a>
              <a href="thread.html#915">[ thread ]</a>
              <a href="subject.html#915">[ subject ]</a>
              <a href="author.html#915">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">More information about the zfs-discuss
mailing list</a><br>
</body></html>
