<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [zfs-discuss] HELP PLEASE!!! Re: Now zpool status is giving me a	kernel panic
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=Re%3A%20%5Bzfs-discuss%5D%20HELP%20PLEASE%21%21%21%20Re%3A%20Now%20zpool%20status%20is%20giving%20me%20a%0A%09kernel%20panic&In-Reply-To=%3C762437f0810191355j53bd826av2e1b683c4545f6cf%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000998.html">
   <LINK REL="Next"  HREF="001004.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[zfs-discuss] HELP PLEASE!!! Re: Now zpool status is giving me a	kernel panic</H1>
    <B>Mario Camou</B> 
    <A HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=Re%3A%20%5Bzfs-discuss%5D%20HELP%20PLEASE%21%21%21%20Re%3A%20Now%20zpool%20status%20is%20giving%20me%20a%0A%09kernel%20panic&In-Reply-To=%3C762437f0810191355j53bd826av2e1b683c4545f6cf%40mail.gmail.com%3E"
       TITLE="[zfs-discuss] HELP PLEASE!!! Re: Now zpool status is giving me a	kernel panic">mcamou at tecnoguru.com
       </A><BR>
    <I>Sun Oct 19 13:55:27 PDT 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="000998.html">[zfs-discuss] Now zpool status is giving me a kernel panic
</A></li>
        <LI>Next message: <A HREF="001004.html">[zfs-discuss] HELP PLEASE!!! Re: Now zpool status is giving me a kernel panic
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1003">[ date ]</a>
              <a href="thread.html#1003">[ thread ]</a>
              <a href="subject.html#1003">[ subject ]</a>
              <a href="author.html#1003">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>OK, so this shit is getting worse and worse.

I started with the degraded pool. Since I didn't have a 1TB disk on hand, I
removed one of the disks from the mirror and imported the pool with both
sets in degraded status (the raidz with one disk missing -- to get zpool
status to work -- and the mirror also with one disk missing). I created a
new ZFS pool on the disk that I removed from the mirror and duplicated the
data from the old pool to the new one using rsync. So far so good.

Then I destroyed the old pool and added the 1TB disk as a mirror device. It
started resilvering and meanwhile I started using the disk.

At one point &quot;zpool status&quot; started saying it had a large amount of data
errors. I did a &quot;zpool status -v&quot; to see on which files.

And I *AGAIN* got a kernel panic.

Rebooted in single-user mode. Imported the pool. There are 12414 data errors
in it. &quot;zpool status -v&quot;...kernel panic in the exact same address as before.

What's the use of the -v option if all you get from it is a panic? I would
classify this as a critical error.

At this point I just want to get rid of ZFS altogether and return to
something akin to my old setup, using Linux on the server and this time
using Linux's kernel mirroring facilities. Forget about pools, forget about
a single device. I just want to get my data back.

Problem: since these are USB drives, I don't know how to determine which of
the drives currently contains the data and which one is getting filled.

HELP!!!!!!

I'm thinking of getting the Linux ZFS implementation up and running just to
see if I can get my data back. I don't like it because it's a UserFS
implementation and I've previously had problems with UserFS, but if I only
use it to check the mirror status, perhaps resilver and get my data back,
things just MIGHT be OK.

AARRGGHHHHH!!!!

Any ideas as to how I can get my data back?

On Sat, Oct 18, 2008 at 10:40 PM, Mario Camou &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">mcamou at tecnoguru.com</A>&gt; wrote:

&gt;<i> Hmmmmm....curiouser and curiouser.
</I>&gt;<i>
</I>&gt;<i> I disconnected one of the disks in the raidz, booted into single user and
</I>&gt;<i> did a zpool import. This time zpool status did not crash and gave me a
</I>&gt;<i> correct pool status (i.e., the disk has a FAULTED state and the raidz1 and
</I>&gt;<i> the pool itself are DEGRADED). So now that I have access to the system we
</I>&gt;<i> come back to my previous question. How do I completely remove the raidz from
</I>&gt;<i> the pool? The mirror is enough to hold the current data so it *SHOULD* be
</I>&gt;<i> somehow possible to move all the data into the mirror.
</I>&gt;<i>
</I>&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">mario at tumbolia</A> ~ 1 % zpool status
</I>&gt;<i>   pool: phlogiston
</I>&gt;<i>  state: DEGRADED
</I>&gt;<i> status: One or more devices could not be used because the label is missing
</I>&gt;<i> or
</I>&gt;<i>         invalid.  Sufficient replicas exist for the pool to continue
</I>&gt;<i>         functioning in a degraded state.
</I>&gt;<i> action: Replace the device using 'zpool replace'.
</I>&gt;<i>    see: <A HREF="http://www.sun.com/msg/ZFS-8000-4J">http://www.sun.com/msg/ZFS-8000-4J</A>
</I>&gt;<i>  scrub: resilver completed with 0 errors on Sat Oct 18 22:32:59 2008
</I>&gt;<i> config:
</I>&gt;<i>
</I>&gt;<i>         NAME                      STATE     READ WRITE CKSUM
</I>&gt;<i>         phlogiston                DEGRADED     0     0     0
</I>&gt;<i>           mirror                  ONLINE       0     0     0
</I>&gt;<i>             disk3s2               ONLINE       0     0     0
</I>&gt;<i>             disk4s2               ONLINE       0     0     0
</I>&gt;<i>           raidz1                  DEGRADED     0     0     0
</I>&gt;<i>             disk1s2               ONLINE       0     0     0
</I>&gt;<i>             16293508939705988688  FAULTED      0     0     0  was
</I>&gt;<i> /dev/disk1s2
</I>&gt;<i>             disk2s2               ONLINE       0     0     0
</I>&gt;<i>
</I>&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">mario at tumbolia</A> ~ 2 % df -H
</I>&gt;<i> Filesystem     Size   Used  Avail Capacity  Mounted on
</I>&gt;<i> /dev/disk0s2   120G    80G    39G    68%    /
</I>&gt;<i> devfs          117k   117k     0B   100%    /dev
</I>&gt;<i> fdesc          1.0k   1.0k     0B   100%    /dev
</I>&gt;<i> phlogiston     2.0T   966G   1.0T    50%    /Volumes/phlogiston
</I>&gt;<i>
</I>&gt;<i> According to the df's I did before creating the raidz, I have about 983G in
</I>&gt;<i> the mirror.
</I>&gt;<i>
</I>&gt;<i> On Sat, Oct 18, 2008 at 10:25 PM, Mario Camou &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">mcamou at tecnoguru.com</A>&gt;wrote:
</I>&gt;<i>
</I>&gt;&gt;<i> If the problem is with the raidz, is there any way of telling ZFS not to
</I>&gt;&gt;<i> look for it and just make do with the mirror? The data I have all fits in
</I>&gt;&gt;<i> the mirror and I haven't (yet) written anything into the disks since adding
</I>&gt;&gt;<i> the raidz, so I imagine it's possible that the data is still just in the
</I>&gt;&gt;<i> mirror and the raidz is still empty. One can hope. The problem right now is
</I>&gt;&gt;<i> that if I do an import without connecting the raidz drives it (obviously)
</I>&gt;&gt;<i> fails because the raidz isn't present.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> AARRRGGGHHHH!!!!
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;<i>
</I>

-- 
The impossible has, on occasion, let me down
                                                       --R.U. Sirius
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <A HREF="http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081019/77228f86/attachment.html">http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081019/77228f86/attachment.html</A> 
</PRE>




<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000998.html">[zfs-discuss] Now zpool status is giving me a kernel panic
</A></li>
	<LI>Next message: <A HREF="001004.html">[zfs-discuss] HELP PLEASE!!! Re: Now zpool status is giving me a kernel panic
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1003">[ date ]</a>
              <a href="thread.html#1003">[ thread ]</a>
              <a href="subject.html#1003">[ subject ]</a>
              <a href="author.html#1003">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">More information about the zfs-discuss
mailing list</a><br>
</body></html>
