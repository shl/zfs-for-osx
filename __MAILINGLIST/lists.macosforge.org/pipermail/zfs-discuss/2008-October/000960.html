<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [zfs-discuss] data loss with ZFS as scratch disk
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=%5Bzfs-discuss%5D%20data%20loss%20with%20ZFS%20as%20scratch%20disk&In-Reply-To=D61A2215-F2E9-4865-8A37-53CC2CCB66F7%40sogeeky.net">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000981.html">
   <LINK REL="Next"  HREF="000964.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[zfs-discuss] data loss with ZFS as scratch disk</H1>
    <B>Joergen Geerds</B> 
    <A HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=%5Bzfs-discuss%5D%20data%20loss%20with%20ZFS%20as%20scratch%20disk&In-Reply-To=D61A2215-F2E9-4865-8A37-53CC2CCB66F7%40sogeeky.net"
       TITLE="[zfs-discuss] data loss with ZFS as scratch disk">geerds at bago.net
       </A><BR>
    <I>Sun Oct 12 11:16:36 PDT 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="000981.html">[zfs-discuss] hungry for new bits...
</A></li>
        <LI>Next message: <A HREF="000964.html">[zfs-discuss] Faulted Drives,	fixed by export/import... but what causes it?
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#960">[ date ]</a>
              <a href="thread.html#960">[ thread ]</a>
              <a href="subject.html#960">[ subject ]</a>
              <a href="author.html#960">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Any word if this issue was ever fixed/addressed or at least looked at  
recently?

Joergen Geerds
23-34 30th Drive
Astoria, NY 11102
+1.646.479.0977
<A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">mail at nypano.com</A>
<A HREF="http://luminous-newyork.com">http://luminous-newyork.com</A>



On Aug 4, 2008, at 23:09 , Mr. Zorg wrote:

&gt;<i> I know there were issues with mmap coherency at one point, but that  
</I>&gt;<i> was supposedly fixed in zfs-111...  Needs another look perhaps, N&#246;el?
</I>&gt;<i>
</I>&gt;<i> On Aug 4, 2008, at 7:26 PM, Joergen Geerds &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">geerds at bago.net</A>&gt; wrote:
</I>&gt;<i>
</I>&gt;&gt;<i> Joost from PTgui told me that he's using mmap, mflush and munmap.  
</I>&gt;&gt;<i> This may be the reason why the problem only appears with PTGui.
</I>&gt;&gt;<i> I hope this helps a bit.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Joergen Geerds
</I>&gt;&gt;<i> <A HREF="http://luminous-newyork.com">http://luminous-newyork.com</A>
</I>&gt;&gt;<i> <A HREF="http://newyorkpanorama.com">http://newyorkpanorama.com</A>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> On Aug 4, 2008, at 15:12 , Joergen Geerds wrote:
</I>&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> I did another test, with 3 250GB disks, and created a clean,  
</I>&gt;&gt;&gt;<i> working raidz set that reported 460GB space:
</I>&gt;&gt;&gt;<i> pool: bigraid
</I>&gt;&gt;&gt;<i> state: ONLINE
</I>&gt;&gt;&gt;<i> scrub: none requested
</I>&gt;&gt;&gt;<i> config:
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> 	NAME         STATE     READ WRITE CKSUM
</I>&gt;&gt;&gt;<i> 	bigraid      ONLINE       0     0     0
</I>&gt;&gt;&gt;<i> 	  raidz1     ONLINE       0     0     0
</I>&gt;&gt;&gt;<i> 	    disk3s2  ONLINE       0     0     0
</I>&gt;&gt;&gt;<i> 	    disk2s2  ONLINE       0     0     0
</I>&gt;&gt;&gt;<i> 	    disk1s2  ONLINE       0     0     0
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> errors: No known data errors
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> and yet, PTgui (using the ZFS as temp folder) still has some  
</I>&gt;&gt;&gt;<i> serious issues with data corruption when writing the PSB from it's  
</I>&gt;&gt;&gt;<i> temp files:
</I>&gt;&gt;&gt;<i> <A HREF="http://nypano.com/clients/ptgui8b6-scratchdisk-error-2.jpg">http://nypano.com/clients/ptgui8b6-scratchdisk-error-2.jpg</A>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> photoshop, on the other hand, has no issues using it for temp files.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> any ideas? are there any non-kosher ways to read/write files to a  
</I>&gt;&gt;&gt;<i> ZFS that could destroy data along the way?
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Joergen Geerds
</I>&gt;&gt;&gt;<i> <A HREF="http://luminous-newyork.com">http://luminous-newyork.com</A>
</I>&gt;&gt;&gt;<i> <A HREF="http://newyorkpanorama.com">http://newyorkpanorama.com</A>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> On Aug 4, 2008, at 13:43 , Mr. Zorg wrote:
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> That would be good, yes. It is very much like raid5. You CAN use  
</I>&gt;&gt;&gt;&gt;<i> two disks, it's just pointless.
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> On Aug 4, 2008, at 10:35 AM, Joergen Geerds &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">geerds at bago.net</A>&gt;  
</I>&gt;&gt;&gt;&gt;<i> wrote:
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> I added the history of the pool on the bottom. after the first  
</I>&gt;&gt;&gt;&gt;&gt;<i> setup gave me only 230GB, I properly destroyed the pool, and  
</I>&gt;&gt;&gt;&gt;&gt;<i> redid it with one disk, and then adding the second to create a  
</I>&gt;&gt;&gt;&gt;&gt;<i> raid (i do assume that it would do some sort of stripping). if  
</I>&gt;&gt;&gt;&gt;&gt;<i> this is not the proper way to go, then the FAQ needs some work  
</I>&gt;&gt;&gt;&gt;&gt;<i> (i.e. &quot;you can't build a raid from 2 disks&quot;). maybe the FAQ  
</I>&gt;&gt;&gt;&gt;&gt;<i> should say that the raidz is something like a RAID5, not a  
</I>&gt;&gt;&gt;&gt;&gt;<i> RAID0, and therefore needs 3 disks minimum.
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> can anyone shed a bit more light on this?
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> Joergen Geerds
</I>&gt;&gt;&gt;&gt;&gt;<i> <A HREF="http://luminous-newyork.com">http://luminous-newyork.com</A>
</I>&gt;&gt;&gt;&gt;&gt;<i> <A HREF="http://newyorkpanorama.com">http://newyorkpanorama.com</A>
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> On Aug 4, 2008, at 12:03 , Mr. Zorg wrote:
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> 230GB is correct. In a raidz configuration you only get n-1  
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> disks worth of storage since one disk is reserved for parity.  
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> It really doesn't make sense to use less than three disks for  
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> raidz, with just two you may as well use a mirror configuration.
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> It is possible that the corruption is a result of you adding  
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> disk2s2 to the pool a second time. It may have been using it  
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> both as a data disk and a parity disk resulting in scrambled  
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> data. Of course, it should not have allowed that.
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> I don't have any spare disks, can anyone else verify this  
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> behavior?
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> On Aug 4, 2008, at 8:09 AM, Joergen Geerds &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">geerds at bago.net</A>&gt;  
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i> wrote:
</I>&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> I installed the latest build of ZFS (July 16, 2008), and  
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> partinioned my external FW800 case (2 sata disks) as a ZFS  
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> raid pool.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> First bug:
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> On my first attempt, I did follow the &quot;Getting started&quot; steps:
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> zpool create bigraid raidz disk1s2 disk2s2
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> This resulted in a raidz pool that was only reporting 230GB  
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> instead of 460GB to the OS (but zpool iostat reported 460GB in  
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> size)
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> So I redid the setup, created the pool with just one disk, and  
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> then &quot;zpool add bigraid raidz  disk2s2&quot;
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> This gave me a pool with 460 GB. It would be great if you  
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> could look into this.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> Second bug:
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> photoshop seems to love the additional speed of the ZFS  
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> scratch disk, it felt really faster than usual (no hard data  
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> to back it up) and no problems.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> So I decided to do a quick test with PTgui 8b6 (using the ZFS  
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> bigraid as PTgui temp folder, but reading/writing all input/ 
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> output from the internal HFS+). The output is a layered PSB  
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> file of about 1GB in this case. The PSB was shredded.
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> See <A HREF="http://nypano.com/clients/ptgui8b6-scratchdisk-error.jpg">http://nypano.com/clients/ptgui8b6-scratchdisk-error.jpg</A>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> I was able to verify that it is indeed the ZFS scratchdisk  
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> that is the culprit (moving the ptgui temp folder to the  
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> internal disk results in normal output, but PTgui 7.8 with the  
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> ZFS as temp folder shreds the PSB also).
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> Joost from PTgui claims that system &amp; ZFS is at fault, not  
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> PTgui. Any ideas what went wrong?
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> System info:
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> Imac 2.16GHz, 10.5.4, 3GB, 500GB internal HFS+, 2x250GB ext  
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> FW800
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> pool: bigraid
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> state: ONLINE
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> scrub: none requested
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> config:
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> 	NAME        STATE     READ WRITE CKSUM
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> 	bigraid     ONLINE       0     0     0
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> 	  disk1s2   ONLINE       0     0     0
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> 	  disk2s2   ONLINE       0     0     0
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> errors: No known data errors
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> History for 'bigraid':
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> 2008-08-03.18:42:02 zpool create bigraid /dev/disk1s2
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> 2008-08-03.18:46:03 zpool upgrade bigraid 8
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> 2008-08-03.18:47:10 zpool add bigraid disk2s2
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> Joergen Geerds
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> <A HREF="http://luminous-newyork.com">http://luminous-newyork.com</A>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> <A HREF="http://newyorkpanorama.com">http://newyorkpanorama.com</A>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> zfs-discuss mailing list
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">zfs-discuss at lists.macosforge.org</A>
</I>&gt;&gt;&gt;&gt;&gt;&gt;&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss</A>
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;&gt;&gt;&gt;<i> zfs-discuss mailing list
</I>&gt;&gt;&gt;&gt;&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">zfs-discuss at lists.macosforge.org</A>
</I>&gt;&gt;&gt;&gt;&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss</A>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;&gt;<i> zfs-discuss mailing list
</I>&gt;&gt;&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">zfs-discuss at lists.macosforge.org</A>
</I>&gt;&gt;&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss</A>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> /div&gt;
</I>&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;<i> zfs-discuss mailing list
</I>&gt;&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">zfs-discuss at lists.macosforge.org</A>
</I>&gt;&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss</A>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;<i> zfs-discuss mailing list
</I>&gt;&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">zfs-discuss at lists.macosforge.org</A>
</I>&gt;&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss</A>
</I>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <A HREF="http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081012/1dff9741/attachment-0001.html">http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081012/1dff9741/attachment-0001.html</A> 
</PRE>




















<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000981.html">[zfs-discuss] hungry for new bits...
</A></li>
	<LI>Next message: <A HREF="000964.html">[zfs-discuss] Faulted Drives,	fixed by export/import... but what causes it?
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#960">[ date ]</a>
              <a href="thread.html#960">[ thread ]</a>
              <a href="subject.html#960">[ subject ]</a>
              <a href="author.html#960">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">More information about the zfs-discuss
mailing list</a><br>
</body></html>
