<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [zfs-discuss] invalid vdev configuration after crash
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=%5Bzfs-discuss%5D%20invalid%20vdev%20configuration%20after%20crash&In-Reply-To=8DFCB52A-338E-432E-866D-28DCA1A65D75%40macnews.de">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000322.html">
   <LINK REL="Next"  HREF="000330.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[zfs-discuss] invalid vdev configuration after crash</H1>
    <B>Kane Dijkman</B> 
    <A HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=%5Bzfs-discuss%5D%20invalid%20vdev%20configuration%20after%20crash&In-Reply-To=8DFCB52A-338E-432E-866D-28DCA1A65D75%40macnews.de"
       TITLE="[zfs-discuss] invalid vdev configuration after crash">kane at inius.com
       </A><BR>
    <I>Thu Mar 13 14:35:21 PDT 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="000322.html">[zfs-discuss] invalid vdev configuration after crash
</A></li>
        <LI>Next message: <A HREF="000330.html">[zfs-discuss] Filename conventions
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#323">[ date ]</a>
              <a href="thread.html#323">[ thread ]</a>
              <a href="subject.html#323">[ subject ]</a>
              <a href="author.html#323">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Ralf,

Thanks for the info. This is really good to know. As the name of the  
pool, zfsMirror, indicates its a mirror, so nothing was lost or at  
risk. I've blown the old one away and am creating a new one so I can  
copy things back over. But this is really good info to know should  
this happen again.

Thanks,
Kane


On Mar 13, 2008, at 11:31 AM, Ralf Bertling wrote:

&gt;<i> Hi list, hi Kane
</I>&gt;<i> I had similar problems during my early experiments with ZFS, but  
</I>&gt;<i> managed to get all my data out the pool (however, I was unable to  
</I>&gt;<i> fully recover it. No&#235;l attributed the problems to one of the disks  
</I>&gt;<i> not having an GUID partition scheme, but I am not sure that was  
</I>&gt;<i> really the cause of the problem.
</I>&gt;<i>
</I>&gt;<i> You get the weird number because disk10 is not recognized by zfs as  
</I>&gt;<i> being the disk correctly, i.e. the disk/pool identifiers do not match.
</I>&gt;<i> What worked for me was disconnecting the disk, bringing the pool up  
</I>&gt;<i> in a degraded state.
</I>&gt;<i> Unfortunately, I was unable to add the disk later without getting a  
</I>&gt;<i> kenel panic on the next zpool status, so I just copied all data from  
</I>&gt;<i> the broken pool and started over.
</I>&gt;<i> AFTER rescuing all data, you might try to copy / wipe the first/last  
</I>&gt;<i> MB of the disk using dd. That way, zfs would probably recognize the  
</I>&gt;<i> identification a broken and rewrite it from the other disks. Maybe  
</I>&gt;<i> No&#235;l can recall my case and give you further instructions.
</I>&gt;<i> Hope this helps,
</I>&gt;<i> 	ralf
</I>&gt;<i>
</I>&gt;<i> Am 12.03.2008 um 17:51 schrieb <A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">zfs-discuss-request at lists.macosforge.org</A> 
</I>&gt;<i> :
</I>&gt;<i>
</I>&gt;&gt;<i> As of this morning when I sent this (before leaving for work and  
</I>&gt;&gt;<i> after the system was up for 8 hours) I had the following
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Lumiere:~ kane$ zpool import -af
</I>&gt;&gt;<i> cannot import 'zfsMirror': invalid vdev configuration
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> as the only message.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> I just got home and tried again and got the following
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Lumiere:~ kane$ zpool import
</I>&gt;&gt;<i> pool: zfsMirror
</I>&gt;&gt;<i>   id: 5643149568963753982
</I>&gt;&gt;<i> state: FAULTED
</I>&gt;&gt;<i> action: The pool cannot be imported due to damaged devices or data.
</I>&gt;&gt;<i> config:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> 	zfsMirror                 UNAVAIL  insufficient replicas
</I>&gt;&gt;<i> 	  raidz1                  UNAVAIL  corrupted data
</I>&gt;&gt;<i> 	    disk7s2               ONLINE
</I>&gt;&gt;<i> 	    disk8s2               ONLINE
</I>&gt;&gt;<i> 	    disk12s2              ONLINE
</I>&gt;&gt;<i> 	    10120111723985263653  ONLINE
</I>&gt;&gt;<i> 	    disk6s2               ONLINE
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> So the system is currently offline and not working.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> I have no idea what 10120111723985263653 is. When things were  
</I>&gt;&gt;<i> working this was just a normal disk?s? name.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Looking at my list of drives with diskutil list it appears that the  
</I>&gt;&gt;<i> following is this weird numbered drive
</I>&gt;&gt;<i> /dev/disk10
</I>&gt;&gt;<i>  #:                       TYPE NAME                    SIZE        
</I>&gt;&gt;<i> IDENTIFIER
</I>&gt;&gt;<i>  0:      GUID_partition_scheme                        *279.5 Gi    
</I>&gt;&gt;<i> disk10
</I>&gt;&gt;<i>  1:                        EFI                         200.0 Mi    
</I>&gt;&gt;<i> disk10s1
</I>&gt;&gt;<i>  2:                        ZFS zfsMirror               279.1 Gi    
</I>&gt;&gt;<i> disk10s2
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> The pool only ever had these 5 disks in it.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> When I created the pool I used the slice names
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Is there a way to recover from this fubarred state?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Also, a little more info that may or may not be important. All my  
</I>&gt;&gt;<i> drives are in an external firewire mega case. One of the the drives  
</I>&gt;&gt;<i> for a normal apple raid had died so I took the case apart, removed  
</I>&gt;&gt;<i> that drive and then moved another drive into its place (so it was  
</I>&gt;&gt;<i> connected elsewhere in the firewire ide chain). It was one of the  
</I>&gt;&gt;<i> drives from this pool. But after that was done I booted up and the  
</I>&gt;&gt;<i> pool showed up and everything worked as normal without any issues  
</I>&gt;&gt;<i> that I am aware of. It wasnt until a few restarts later that I had  
</I>&gt;&gt;<i> the crash that has me in the current situation.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Everything on this pool is mirrored on another raid, so I can  
</I>&gt;&gt;<i> easily just destroy and recreate it. But I want to make sure there  
</I>&gt;&gt;<i> is no other useful info I can give you before I do.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Thanks,
</I>&gt;&gt;<i> Kane
</I>&gt;<i>
</I>&gt;<i> _______________________________________________
</I>&gt;<i> zfs-discuss mailing list
</I>&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">zfs-discuss at lists.macosforge.org</A>
</I>&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">http://lists.macosforge.org/mailman/listinfo/zfs-discuss</A>
</I>

---------------------------------------------------------------------------------------------
&quot;It said Windows 2000 or better on the box, so I bought a Mac&quot;

</PRE>










<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000322.html">[zfs-discuss] invalid vdev configuration after crash
</A></li>
	<LI>Next message: <A HREF="000330.html">[zfs-discuss] Filename conventions
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#323">[ date ]</a>
              <a href="thread.html#323">[ thread ]</a>
              <a href="subject.html#323">[ subject ]</a>
              <a href="author.html#323">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">More information about the zfs-discuss
mailing list</a><br>
</body></html>
