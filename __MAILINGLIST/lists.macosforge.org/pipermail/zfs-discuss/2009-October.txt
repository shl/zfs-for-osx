From j.averous at sourcemac.com  Wed Oct  7 00:17:00 2009
From: j.averous at sourcemac.com (=?iso-8859-1?Q?Julien-Pierre_Av=E9rous?=)
Date: Wed, 7 Oct 2009 09:17:00 +0200
Subject: [zfs-discuss] Panic in AVL (build 154 aka 10a286)
Message-ID: <7C911D5D-69EB-4FFA-B8E3-BC9447BA4311@sourcemac.com>

Hi there !

I know, I should forget ZFS for the moment, until Apple resolve its  
problems with legals (or technicals (or...)) issues. But I love too  
much ZFS ;)

So I have installed the build 154 (released in the Mac OS  Build  
10a286) in Mac OS X Server Snow Leopard X.6.1.

I have a WD Drive in USB2 of 2 TB. I have formatted my drive,  
configured the main partition as a ZFS entity, created a ZFS Pool on  
this entity and finally created a filesystem on this pool.

This seem work fine, until I try to copy (with cp -pR) a big amount of  
file (between 3 to 6 MB each) : after a little amount of file copied  
(less than 3000, not on the same directory), my server is killed with  
a call to panic, generating this trace :

= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
========================================================================
panic(cpu 0 caller 0x32f09ab3): "avl_find() succeeded inside avl_add 
()"@/SourceCache/zfs/zfs-154/src/common/avl/avl.c:642
Backtrace (CPU 0), Frame : Return Address (4 potential args on stack)
0x3a5e3c68 : 0x21acfa (0x5ce650 0x3a5e3c9c 0x223156 0x0)
0x3a5e3cb8 : 0x32f09ab3 (0x32f6c850 0x3a5e3d84 0x3a5e3cdc 0x32ef65b0)
0x3a5e3ce8 : 0x32e8e34c (0x3a98294c 0x3a5e3d84 0x6 0x0)
0x3a5e3de8 : 0x2f4082 (0x3a5e3e0c 0x5474ba 0x41a6000 0x4fb5c6)
0x3a5e3e48 : 0x4cdad7 (0x5b45c54 0x0 0x80000 0x0)
0x3a5e3ed8 : 0x24fcfa (0x5b45c54 0x0 0x80000 0x0)
0x3a5e3f38 : 0x25040c (0x36bdc94 0x80000 0x0 0x1000)
0x3a5e3f68 : 0x274bd5 (0x36bdc94 0x80000 0x0 0x1000)
0x3a5e3fc8 : 0x29c68c (0x84985c 0x0 0x10 0x45af764)
       Kernel Extensions in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0x32e87000->0x32fcdfff
             dependency: com.apple.iokit.IOStorageFamily(1.6)@0xb6c000

BSD process name corresponding to current thread: kernel_task

Mac OS version:
10B504

Kernel version:
Darwin Kernel Version 10.0.0: Fri Jul 31 22:47:34 PDT 2009;  
root:xnu-1456.1.25~1/RELEASE_I386
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
= 
========================================================================

Have you an idea why this happen ?


From jasonbelec at rogers.com  Wed Oct  7 13:21:14 2009
From: jasonbelec at rogers.com (Jason Belec)
Date: Wed, 7 Oct 2009 16:21:14 -0400
Subject: [zfs-discuss] Panic in AVL (build 154 aka 10a286)
In-Reply-To: <7C911D5D-69EB-4FFA-B8E3-BC9447BA4311@sourcemac.com>
References: <7C911D5D-69EB-4FFA-B8E3-BC9447BA4311@sourcemac.com>
Message-ID: <93D48B5C-6202-42FB-8702-D93B58211450@rogers.com>

I had an issue similar, resolved by telling the 'spindle drives' not  
to spin down. I read that on some blog, can't remember where but seems  
to be a solution for me...

I think your goals are worth continuing with. But I'm a masochist.

--

Jason

?We can't solve problems by using the same kind of thinking we used  
when we created them.? ~ Albert Einstein








On 2009-10-07, at 3:17 AM, Julien-Pierre Av?rous wrote:

> Hi there !
>
> I know, I should forget ZFS for the moment, until Apple resolve its  
> problems with legals (or technicals (or...)) issues. But I love too  
> much ZFS ;)
>
> So I have installed the build 154 (released in the Mac OS  Build  
> 10a286) in Mac OS X Server Snow Leopard X.6.1.
>
> I have a WD Drive in USB2 of 2 TB. I have formatted my drive,  
> configured the main partition as a ZFS entity, created a ZFS Pool on  
> this entity and finally created a filesystem on this pool.
>
> This seem work fine, until I try to copy (with cp -pR) a big amount  
> of file (between 3 to 6 MB each) : after a little amount of file  
> copied (less than 3000, not on the same directory), my server is  
> killed with a call to panic, generating this trace :

> EDIT...
>
> BSD process name corresponding to current thread: kernel_task
>
> Mac OS version:
> 10B504
>
> Kernel version:
> Darwin Kernel Version 10.0.0: Fri Jul 31 22:47:34 PDT 2009;  
> root:xnu-1456.1.25~1/RELEASE_I386
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> = 
> ======================================================================
>
> Have you an idea why this happen ?
>
> _______________________________________________

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091007/91abcc34/attachment.html>

From j.averous at sourcemac.com  Thu Oct  8 01:13:18 2009
From: j.averous at sourcemac.com (=?windows-1252?Q?Julien-Pierre_Av=E9rous?=)
Date: Thu, 8 Oct 2009 10:13:18 +0200
Subject: [zfs-discuss] Panic in AVL (build 154 aka 10a286)
In-Reply-To: <93D48B5C-6202-42FB-8702-D93B58211450@rogers.com>
References: <7C911D5D-69EB-4FFA-B8E3-BC9447BA4311@sourcemac.com>
	<93D48B5C-6202-42FB-8702-D93B58211450@rogers.com>
Message-ID: <23908CBA-2A6C-4523-AFF6-AC00A16F9775@sourcemac.com>

Thank you for this tip. I will searching on this.

On 7 oct. 2009, at 22:21, Jason Belec wrote:

> I had an issue similar, resolved by telling the 'spindle drives' not  
> to spin down. I read that on some blog, can't remember where but  
> seems to be a solution for me...
>
> I think your goals are worth continuing with. But I'm a masochist.
>
> --
>
> Jason
>
> ?We can't solve problems by using the same kind of thinking we used  
> when we created them.? ~ Albert Einstein
>
>
>
>
>
>
>
>
> On 2009-10-07, at 3:17 AM, Julien-Pierre Av?rous wrote:
>
>> Hi there !
>>
>> I know, I should forget ZFS for the moment, until Apple resolve its  
>> problems with legals (or technicals (or...)) issues. But I love too  
>> much ZFS ;)
>>
>> So I have installed the build 154 (released in the Mac OS  Build  
>> 10a286) in Mac OS X Server Snow Leopard X.6.1.
>>
>> I have a WD Drive in USB2 of 2 TB. I have formatted my drive,  
>> configured the main partition as a ZFS entity, created a ZFS Pool  
>> on this entity and finally created a filesystem on this pool.
>>
>> This seem work fine, until I try to copy (with cp -pR) a big amount  
>> of file (between 3 to 6 MB each) : after a little amount of file  
>> copied (less than 3000, not on the same directory), my server is  
>> killed with a call to panic, generating this trace :
>
>> EDIT...
>>
>> BSD process name corresponding to current thread: kernel_task
>>
>> Mac OS version:
>> 10B504
>>
>> Kernel version:
>> Darwin Kernel Version 10.0.0: Fri Jul 31 22:47:34 PDT 2009;  
>> root:xnu-1456.1.25~1/RELEASE_I386
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> = 
>> =====================================================================
>>
>> Have you an idea why this happen ?
>>
>> _______________________________________________
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091008/aa65410e/attachment.html>

From lists at chipmunk-app.com  Thu Oct  8 01:44:42 2009
From: lists at chipmunk-app.com (Ruotger Skupin)
Date: Thu, 8 Oct 2009 10:44:42 +0200
Subject: [zfs-discuss] Panic in AVL (build 154 aka 10a286)
In-Reply-To: <7C911D5D-69EB-4FFA-B8E3-BC9447BA4311@sourcemac.com>
References: <7C911D5D-69EB-4FFA-B8E3-BC9447BA4311@sourcemac.com>
Message-ID: <422DC05D-48D0-47B0-8708-3AF38F1D279E@chipmunk-app.com>


Am 07.10.2009 um 09:17 schrieb Julien-Pierre Av?rous:

> Hi there !
>
> I know, I should forget ZFS for the moment, until Apple resolve its  
> problems with legals (or technicals (or...)) issues. But I love too  
> much ZFS ;)
>
> So I have installed the build 154 (released in the Mac OS  Build  
> 10a286) in Mac OS X Server Snow Leopard X.6.1.
>
> I have a WD Drive in USB2 of 2 TB. I have formatted my drive,  
> configured the main partition as a ZFS entity, created a ZFS Pool on  
> this entity and finally created a filesystem on this pool.
>
> This seem work fine, until I try to copy (with cp -pR) a big amount  
> of file (between 3 to 6 MB each) : after a little amount of file  
> copied (less than 3000, not on the same directory), my server is  
> killed with a call to panic, generating this trace :

Hi Julien-Pierre,

I don't want to discourage you but in my own experiments (with 119) I  
had the problem, that heavy load would produce a kernel panic. Virtual  
Box (fun fact: also from Sun) would trigger that reliably. There  
seemed to be general agreement on older list postings that USB is a  
bad choice for the mac version of ZFS.  You might want to search the  
archives.

On a side note: I have never before stumbled upon a file system people  
get so passionate about than ZFS. Call us ZFS fanboys if you want, I  
don't care. So it seems like a natural choice for Apple. Please go on  
with it Apple!

Ruotger


From lists at chipmunk-app.com  Thu Oct  8 01:50:25 2009
From: lists at chipmunk-app.com (Ruotger Skupin)
Date: Thu, 8 Oct 2009 10:50:25 +0200
Subject: [zfs-discuss] Another ZFS is missing on 10.6 reason?
Message-ID: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>

Hi,

this is deep in "I have no idea"-land so bear with me...

Does anyone know, if ZFS is 64bit compatible? I mean that in a  
compiler/64bit kernel/AMD64-code way. Is there a 64bit version of  
solaris? There doesn't seem to be a 64bit download of OpenSolaris:

http://opensolaris.org/os/downloads/

Would be a more than good reason for Apple to throw ZFS out of 10.6  
(imho).

Ruotger


From alex.blewitt at gmail.com  Thu Oct  8 01:52:06 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Thu, 8 Oct 2009 09:52:06 +0100
Subject: [zfs-discuss] Another ZFS is missing on 10.6 reason?
In-Reply-To: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>
References: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>
Message-ID: <BE409EEC-5B73-4D80-B464-8E6B4C9D4A2B@gmail.com>

On 8 Oct 2009, at 09:50, Ruotger Skupin wrote:

> Hi,
>
> this is deep in "I have no idea"-land so bear with me...
>
> Does anyone know, if ZFS is 64bit compatible? I mean that in a  
> compiler/64bit kernel/AMD64-code way. Is there a 64bit version of  
> solaris? There doesn't seem to be a 64bit download of OpenSolaris:

A 128-bit filesystem limited to running on 32-bits? I doubt it :-)

From hukl at h3q.com  Thu Oct  8 01:52:58 2009
From: hukl at h3q.com (John-Paul Bader)
Date: Thu, 8 Oct 2009 10:52:58 +0200
Subject: [zfs-discuss] Another ZFS is missing on 10.6 reason?
In-Reply-To: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>
References: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>
Message-ID: <0CA49A8B-4540-487A-8017-32960AF31AAC@h3q.com>

FreeBSD runs happily on amd64 with zfs root/boot filesystem.


On 08.10.2009, at 10:50, Ruotger Skupin wrote:

> Hi,
>
> this is deep in "I have no idea"-land so bear with me...
>
> Does anyone know, if ZFS is 64bit compatible? I mean that in a  
> compiler/64bit kernel/AMD64-code way. Is there a 64bit version of  
> solaris? There doesn't seem to be a 64bit download of OpenSolaris:
>
> http://opensolaris.org/os/downloads/
>
> Would be a more than good reason for Apple to throw ZFS out of 10.6  
> (imho).
>
> Ruotger
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From mman at martinman.net  Thu Oct  8 02:03:50 2009
From: mman at martinman.net (Martin Man)
Date: Thu, 8 Oct 2009 11:03:50 +0200
Subject: [zfs-discuss] Another ZFS is missing on 10.6 reason?
In-Reply-To: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>
References: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>
Message-ID: <90F99347-ED0B-4E89-A06D-B735F9474398@martinman.net>

Hi all,

OpenSolaris and Solaris are 64bit native for ages....

HTH,
Martin

On Oct 8, 2009, at 10:50, Ruotger Skupin wrote:

> Hi,
>
> this is deep in "I have no idea"-land so bear with me...
>
> Does anyone know, if ZFS is 64bit compatible? I mean that in a  
> compiler/64bit kernel/AMD64-code way. Is there a 64bit version of  
> solaris? There doesn't seem to be a 64bit download of OpenSolaris:
>
> http://opensolaris.org/os/downloads/
>
> Would be a more than good reason for Apple to throw ZFS out of 10.6  
> (imho).
>
> Ruotger
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From lists at loveturtle.net  Thu Oct  8 05:11:49 2009
From: lists at loveturtle.net (Dillon Kass)
Date: Thu, 08 Oct 2009 08:11:49 -0400
Subject: [zfs-discuss] Another ZFS is missing on 10.6 reason?
In-Reply-To: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>
References: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>
Message-ID: <4ACDD705.6000403@loveturtle.net>

10.6 has a 32bit kernel, ZFS would be running 32bit.
OpenSolaris has 32bit and 64bit on the same disc, it picks the right one 
at boot. Sun recommends 64bit for ZFS. FreeBSD recommends 64bit for ZFS.

Mac OS is way behind the curve on the 64bit transition, everyone else is 
done. Windows, Linux, FreeBSD, Solaris, all have full 64bit versions and 
have for years.

On 10/8/09 4:50 AM, Ruotger Skupin wrote:
> Hi,
>
> this is deep in "I have no idea"-land so bear with me...
>
> Does anyone know, if ZFS is 64bit compatible? I mean that in a 
> compiler/64bit kernel/AMD64-code way. Is there a 64bit version of 
> solaris? There doesn't seem to be a 64bit download of OpenSolaris:
>
> http://opensolaris.org/os/downloads/
>
> Would be a more than good reason for Apple to throw ZFS out of 10.6 
> (imho).
>
> Ruotger
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ralph.scheuer at gmx.net  Thu Oct  8 05:21:36 2009
From: ralph.scheuer at gmx.net (Ralph Scheuer)
Date: Thu, 8 Oct 2009 14:21:36 +0200
Subject: [zfs-discuss] Another ZFS is missing on 10.6 reason?
In-Reply-To: <4ACDD705.6000403@loveturtle.net>
References: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>
	<4ACDD705.6000403@loveturtle.net>
Message-ID: <855BD995-43EF-4272-A1BF-70226367A8C1@gmx.net>

Hi,

Am 08.10.2009 um 14:11 schrieb Dillon Kass:

> 10.6 has a 32bit kernel, ZFS would be running 32bit.

Would it?

> OpenSolaris has 32bit and 64bit on the same disc, it picks the  
> right one at boot. Sun recommends 64bit for ZFS. FreeBSD recommends  
> 64bit for ZFS.
>
> Mac OS is way behind the curve on the 64bit transition, everyone  
> else is done. Windows, Linux, FreeBSD, Solaris, all have full 64bit  
> versions and have for years.

Could it be that your knowledge regarding Snow Leopard and its kernel 
(s) is a bit "behind the curve" here? And, pretty please, although I  
know this does not belong here, don't try to tell a tech-savvy  
audience that the Windows 64-bit transition is done. Regarding mass  
adoption and software offerings, it has not even really started.

SCNR.

Ralph

From hukl at h3q.com  Thu Oct  8 05:36:47 2009
From: hukl at h3q.com (John-Paul Bader)
Date: Thu, 8 Oct 2009 14:36:47 +0200
Subject: [zfs-discuss] Another ZFS is missing on 10.6 reason?
In-Reply-To: <855BD995-43EF-4272-A1BF-70226367A8C1@gmx.net>
References: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>
	<4ACDD705.6000403@loveturtle.net>
	<855BD995-43EF-4272-A1BF-70226367A8C1@gmx.net>
Message-ID: <44ED1D17-4949-4554-92BC-8BA45AA63C43@h3q.com>

Hey again,

ZFS is indeed recommended to run on 64 Bit but it should run fine on  
32 Bit with a bit of tuning to get good performance and stability.  
Snow Leopard is booting the 32 Bit Kernel, even on 64 Bit machine.  
Probably because there are too many 32 Bit Kernel extensions / drivers  
that are not available in 64 Bit. AFAIK Snow Leopard only boots 64 Bit  
Kernel on XServes.

So if you're booting a 32 Bit Kernel, ZFS would have to run in 32 Bit  
as well as it is a Kernel Extension. If you're booting the 64 Bit  
Kernel you would also use the 64 Bit ZS Kernel Extension.

I'd guess that Apple shipped 32 and 64 bit variants of ZFS in the Snow  
Leopard betas.

I don't know if there are some aspects of OS X kernel architecture  
that would allow variations from the thesis above but using 64 bit  
extensions on a 32 bit kernel doesn't seem very likely.

Kind regards, John

On 08.10.2009, at 14:21, Ralph Scheuer wrote:

> Hi,
>
> Am 08.10.2009 um 14:11 schrieb Dillon Kass:
>
>> 10.6 has a 32bit kernel, ZFS would be running 32bit.
>
> Would it?
>
>> OpenSolaris has 32bit and 64bit on the same disc, it picks the  
>> right one at boot. Sun recommends 64bit for ZFS. FreeBSD recommends  
>> 64bit for ZFS.
>>
>> Mac OS is way behind the curve on the 64bit transition, everyone  
>> else is done. Windows, Linux, FreeBSD, Solaris, all have full 64bit  
>> versions and have for years.
>
> Could it be that your knowledge regarding Snow Leopard and its kernel 
> (s) is a bit "behind the curve" here? And, pretty please, although I  
> know this does not belong here, don't try to tell a tech-savvy  
> audience that the Windows 64-bit transition is done. Regarding mass  
> adoption and software offerings, it has not even really started.
>
> SCNR.
>
> Ralph
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From alexandre.dufour at gmail.com  Thu Oct  8 05:46:35 2009
From: alexandre.dufour at gmail.com (Alexandre Dufour)
Date: Thu, 8 Oct 2009 14:46:35 +0200
Subject: [zfs-discuss] Another ZFS is missing on 10.6 reason?
In-Reply-To: <44ED1D17-4949-4554-92BC-8BA45AA63C43@h3q.com>
References: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>
	<4ACDD705.6000403@loveturtle.net>
	<855BD995-43EF-4272-A1BF-70226367A8C1@gmx.net>
	<44ED1D17-4949-4554-92BC-8BA45AA63C43@h3q.com>
Message-ID: <FFF1A511-7804-4160-B846-337321BA3B74@gmail.com>

Hi,

> Hey again,
>
> ZFS is indeed recommended to run on 64 Bit but it should run fine on  
> 32 Bit with a bit of tuning to get good performance and stability.  
> Snow Leopard is booting the 32 Bit Kernel, even on 64 Bit machine.  
> Probably because there are too many 32 Bit Kernel extensions /  
> drivers that are not available in 64 Bit. AFAIK Snow Leopard only  
> boots 64 Bit Kernel on XServes.

This is not entirely correct. You can boot up any 10.6 client kernel  
in 64 bit mode by sinking the '6' and '4' keys when switching on until  
the grey Apple (or whatever icon you set up there) shows up. This is  
however quite experimental, since your assumption on why the default  
kernel mode is 32 bit definitely holds. Hopefully by 10.6.2 more  
extensions will be fully 64 bit.

all the best,

Alexandre

From lists at loveturtle.net  Thu Oct  8 05:56:22 2009
From: lists at loveturtle.net (Dillon Kass)
Date: Thu, 08 Oct 2009 08:56:22 -0400
Subject: [zfs-discuss] Fwd: Re:  Another ZFS is missing on 10.6 reason?
Message-ID: <4ACDE176.2000207@loveturtle.net>



-------- Original Message --------
Subject: 	Re: [zfs-discuss] Another ZFS is missing on 10.6 reason?
Date: 	Thu, 8 Oct 2009 14:46:35 +0200
From: 	Alexandre Dufour <alexandre.dufour at gmail.com>
To: 	zfs-discuss at lists.macosforge.org



>This is not entirely correct. You can boot up any 10.6 client kernel
>in 64 bit mode by sinking the '6' and '4' keys when switching

not any, only recent Macs. There was a release note thinggy that said which
systems were and weren't supported, i don't remember off the top of my head.
but for example my new shiny 17" unibody macbook pro has no problem booting
the 64bit kernel but my macpro1,1 will not, even though it has a nice 64bit cpu.

you can also just add arch=x86_64 to boot-args with nvram and have a
permanent 64bit booting.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091008/1506f64c/attachment-0001.html>

From dillon at terranova.net  Thu Oct  8 06:17:37 2009
From: dillon at terranova.net (Dillon Kass)
Date: Thu, 08 Oct 2009 09:17:37 -0400
Subject: [zfs-discuss] Another ZFS is missing on 10.6 reason?
In-Reply-To: <855BD995-43EF-4272-A1BF-70226367A8C1@gmx.net>
References: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>	<4ACDD705.6000403@loveturtle.net>
	<855BD995-43EF-4272-A1BF-70226367A8C1@gmx.net>
Message-ID: <4ACDE671.2090406@terranova.net>

On 10/8/09 8:21 AM, Ralph Scheuer wrote:
> Hi,
>
> Am 08.10.2009 um 14:11 schrieb Dillon Kass:
>
>> 10.6 has a 32bit kernel, ZFS would be running 32bit.
>
> Would it?
Yes, It would. You can't load a 32bit module with a 64bit kernel and 
vice versa.
>
>> OpenSolaris has 32bit and 64bit on the same disc, it picks the right 
>> one at boot. Sun recommends 64bit for ZFS. FreeBSD recommends 64bit 
>> for ZFS.
>>
>> Mac OS is way behind the curve on the 64bit transition, everyone else 
>> is done. Windows, Linux, FreeBSD, Solaris, all have full 64bit 
>> versions and have for years.
>
> Could it be that your knowledge regarding Snow Leopard and its 
> kernel(s) is a bit "behind the curve" here? 
No. Can you elaborate?
If you mean that 10.6 has a 64bit kernel that you can run on some 
machines then I'm aware of that, I've been using it since I got my new 
mbp last month. You could have just said that.

> And, pretty please, although I know this does not belong here, don't 
> try to tell a tech-savvy audience that the Windows 64-bit transition 
> is done. Regarding mass adoption and software offerings, it has not 
> even really started.
Mass adoption and software offerings are a different story. I'm talking 
about the operating system as a package. Though I don't really 
understand what you mean it hasn't even really started? being that a 
large number of people are running 64bit vista or win7 with full 
hardware support and running all the normal software they would run I'd 
say it's more than started. Maybe when XP64 came out it hadn't "even 
really started" but it's 2009 now.

By this logic I guess MacOSX isn't even a real operating system because 
of mass adoption and software offerings? I'll bet you there's more 64bit 
windows systems in the wild than there are macs running 10.5 and 10.6 
combined.

From hanche at math.ntnu.no  Thu Oct  8 06:52:04 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Thu, 08 Oct 2009 09:52:04 -0400 (EDT)
Subject: [zfs-discuss] Another ZFS is missing on 10.6 reason?
In-Reply-To: <44ED1D17-4949-4554-92BC-8BA45AA63C43@h3q.com>
References: <4ACDD705.6000403@loveturtle.net>
	<855BD995-43EF-4272-A1BF-70226367A8C1@gmx.net>
	<44ED1D17-4949-4554-92BC-8BA45AA63C43@h3q.com>
Message-ID: <20091008.095204.182723159.hanche@math.ntnu.no>

+ John-Paul Bader <hukl at h3q.com>:

> ZFS is indeed recommended to run on 64 Bit but it should run fine on
> 32 Bit with a bit of tuning to get good performance and stability.

FWIW, I have run ZFS both on 32 bit and 64 bit FreeBSD. Even with
tuning, my limited experience said it was vastly more stable running
on 64 bit. Eh, make that vastly less unstable ... It was still usable
on 32 bit, but I learned do little tricks like using kill -STOP once
in a while on processes that wanted to copy many gigabytes of data,
just to allow the ZFS susbsystem to catch up. (Follow, of course, by
kill -CONT once disk activity ceased.)

- Harald

From ralph.scheuer at gmx.net  Thu Oct  8 06:59:50 2009
From: ralph.scheuer at gmx.net (Ralph Scheuer)
Date: Thu, 8 Oct 2009 15:59:50 +0200
Subject: [zfs-discuss] Another ZFS is missing on 10.6 reason?
In-Reply-To: <4ACDE671.2090406@terranova.net>
References: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>	<4ACDD705.6000403@loveturtle.net>
	<855BD995-43EF-4272-A1BF-70226367A8C1@gmx.net>
	<4ACDE671.2090406@terranova.net>
Message-ID: <27CB4C90-2041-41B6-9A1F-28053836CEF8@gmx.net>


Am 08.10.2009 um 15:17 schrieb Dillon Kass:

> On 10/8/09 8:21 AM, Ralph Scheuer wrote:
>> Hi,
>>
>> Am 08.10.2009 um 14:11 schrieb Dillon Kass:
>>
>>> 10.6 has a 32bit kernel, ZFS would be running 32bit.
>>
>> Would it?
> Yes, It would. You can't load a 32bit module with a 64bit kernel  
> and vice versa.

I know. Nevertheless, as others have already pointed out, Snow  
Leopard comes with a 64 bit kernel as well, and if your machine  
supports it and your other drivers are ready for that, you can run  
the 64 bit version without a hitch by changing the startup arguments.  
So what's your problem?
>
> By this logic I guess MacOSX isn't even a real operating system  
> because of mass adoption and software offerings? I'll bet you  
> there's more 64bit windows systems in the wild than there are macs  
> running 10.5 and 10.6 combined.

Please, take this topic to another forum.

Ralph
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091008/a75c201e/attachment.html>

From lists at loveturtle.net  Thu Oct  8 07:05:40 2009
From: lists at loveturtle.net (Dillon Kass)
Date: Thu, 08 Oct 2009 10:05:40 -0400
Subject: [zfs-discuss] Another ZFS is missing on 10.6 reason?
In-Reply-To: <20091008.095204.182723159.hanche@math.ntnu.no>
References: <4ACDD705.6000403@loveturtle.net>	<855BD995-43EF-4272-A1BF-70226367A8C1@gmx.net>	<44ED1D17-4949-4554-92BC-8BA45AA63C43@h3q.com>
	<20091008.095204.182723159.hanche@math.ntnu.no>
Message-ID: <4ACDF1B4.9090602@loveturtle.net>

If you haven't already you should check out RELENG_7 or one of the 8.0 
betas. The newer ZFS code (not in 7.2-R) is really nice.

We had this customer that had a flood and lost some hardware and we 
rigged up this old 800mhz(ish) 32bit p3 with like 14 80gb disks with
7-STABLE and with a little kmem size tuning it worked flawlessly for a 
few months until they could replace it with something more modern.

I'm currently running FreeBSD on my dell mini9 netbook for work with ZFS 
root and it's 32bit and works great. but yeah, there's still a few rough 
edges.

On 10/8/09 9:52 AM, Harald Hanche-Olsen wrote:
> + John-Paul Bader<hukl at h3q.com>:
>
>    
>> ZFS is indeed recommended to run on 64 Bit but it should run fine on
>> 32 Bit with a bit of tuning to get good performance and stability.
>>      
> FWIW, I have run ZFS both on 32 bit and 64 bit FreeBSD. Even with
> tuning, my limited experience said it was vastly more stable running
> on 64 bit. Eh, make that vastly less unstable ... It was still usable
> on 32 bit, but I learned do little tricks like using kill -STOP once
> in a while on processes that wanted to copy many gigabytes of data,
> just to allow the ZFS susbsystem to catch up. (Follow, of course, by
> kill -CONT once disk activity ceased.)
>
> - Harald
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>    


From dillon at terranova.net  Thu Oct  8 07:10:12 2009
From: dillon at terranova.net (Dillon Kass)
Date: Thu, 08 Oct 2009 10:10:12 -0400
Subject: [zfs-discuss] Another ZFS is missing on 10.6 reason?
In-Reply-To: <27CB4C90-2041-41B6-9A1F-28053836CEF8@gmx.net>
References: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>	<4ACDD705.6000403@loveturtle.net>	<855BD995-43EF-4272-A1BF-70226367A8C1@gmx.net>	<4ACDE671.2090406@terranova.net>
	<27CB4C90-2041-41B6-9A1F-28053836CEF8@gmx.net>
Message-ID: <4ACDF2C4.5010209@terranova.net>

On 10/8/09 9:59 AM, Ralph Scheuer wrote:
>
> Am 08.10.2009 um 15:17 schrieb Dillon Kass:
>
>> On 10/8/09 8:21 AM, Ralph Scheuer wrote:
>> Yes, It would. You can't load a 32bit module with a 64bit kernel and 
>> vice versa.
>
> I know. Nevertheless, as others have already pointed out, Snow Leopard 
> comes with a 64 bit kernel as well, and if your machine supports it 
> and your other drivers are ready for that, you can run the 64 bit 
> version without a hitch by changing the startup arguments. So what's 
> your problem?
Huh? Someone suggested that ZFS was dropped on 10.6 because it's not 
64bit, I pointed out that any default install of 10.6 would be 32bit and 
ZFS would be running 32bit (excluding current gen xservs). Then you 
started arguing with me. So what's your problem?
>>
>> By this logic I guess MacOSX isn't even a real operating system 
>> because of mass adoption and software offerings? I'll bet you there's 
>> more 64bit windows systems in the wild than there are macs running 
>> 10.5 and 10.6 combined.
>
> Please, take this topic to another forum.
This is your topic, I was just sarcastically entertaining it. Please 
take this topic to another forum.

:-)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091008/61adf23e/attachment.html>

From hukl at h3q.com  Thu Oct  8 07:21:11 2009
From: hukl at h3q.com (John-Paul Bader)
Date: Thu, 8 Oct 2009 16:21:11 +0200
Subject: [zfs-discuss] Another ZFS is missing on 10.6 reason?
In-Reply-To: <4ACDF2C4.5010209@terranova.net>
References: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>	<4ACDD705.6000403@loveturtle.net>	<855BD995-43EF-4272-A1BF-70226367A8C1@gmx.net>	<4ACDE671.2090406@terranova.net>
	<27CB4C90-2041-41B6-9A1F-28053836CEF8@gmx.net>
	<4ACDF2C4.5010209@terranova.net>
Message-ID: <F6688E17-2402-49DE-955C-937AC3DB20FA@h3q.com>

Yeah so to conclude the bittiness of the Kernel is not why Apple  
dropped ZFS. Maybe they dropped it as soon as Oracle bought Sun due to  
weird concerns or maybe and more likely - It just wasn't ready yet.  
Think of all the custom HFS+ features they had to port to ZFS for  
example. Maybe they're even developing their own next generation file  
system - who knows.

Instead of wondering why its not included and why there are no updated  
on macosforge we should either port ZFS "ourselves", continually  
demand answers from Apple, rant about HFS+ and how lame and horrible  
it is for worlds most advanced operating system and share information.

This is all much more useful than starting a (f)lamewar on a mailing  
list thats almost dead anyway ;)

Kind regards,

John

On 08.10.2009, at 16:10, Dillon Kass wrote:

> On 10/8/09 9:59 AM, Ralph Scheuer wrote:
>>
>> Am 08.10.2009 um 15:17 schrieb Dillon Kass:
>>
>>> On 10/8/09 8:21 AM, Ralph Scheuer wrote:
>>> Yes, It would. You can't load a 32bit module with a 64bit kernel  
>>> and vice versa.
>>
>> I know. Nevertheless, as others have already pointed out, Snow  
>> Leopard comes with a 64 bit kernel as well, and if your machine  
>> supports it and your other drivers are ready for that, you can run  
>> the 64 bit version without a hitch by changing the startup  
>> arguments. So what's your problem?
> Huh? Someone suggested that ZFS was dropped on 10.6 because it's not  
> 64bit, I pointed out that any default install of 10.6 would be 32bit  
> and ZFS would be running 32bit (excluding current gen xservs). Then  
> you started arguing with me. So what's your problem?
>>>
>>> By this logic I guess MacOSX isn't even a real operating system  
>>> because of mass adoption and software offerings? I'll bet you  
>>> there's more 64bit windows systems in the wild than there are macs  
>>> running 10.5 and 10.6 combined.
>>
>> Please, take this topic to another forum.
> This is your topic, I was just sarcastically entertaining it. Please  
> take this topic to another forum.
>
> :-)
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From james at themacplace.co.uk  Thu Oct  8 15:06:58 2009
From: james at themacplace.co.uk (James Relph)
Date: Thu, 8 Oct 2009 23:06:58 +0100
Subject: [zfs-discuss] Another ZFS is missing on 10.6 reason?
In-Reply-To: <F6688E17-2402-49DE-955C-937AC3DB20FA@h3q.com>
References: <622F926A-81C9-405D-B30F-2DC68CD0AD65@chipmunk-app.com>	<4ACDD705.6000403@loveturtle.net>	<855BD995-43EF-4272-A1BF-70226367A8C1@gmx.net>	<4ACDE671.2090406@terranova.net>
	<27CB4C90-2041-41B6-9A1F-28053836CEF8@gmx.net>
	<4ACDF2C4.5010209@terranova.net>
	<F6688E17-2402-49DE-955C-937AC3DB20FA@h3q.com>
Message-ID: <980C8E64-D18D-4659-8266-9286DB7A6494@themacplace.co.uk>

> Maybe they dropped it as soon as Oracle bought Sun due to weird  
> concerns or maybe and more likely - It just wasn't ready yet.

Why can't they just come out and say that though?  I can't see a good  
reason for them not to just come out and say "there's licensing  
issues" or "it wasn't ready for primetime".  It's not like it's a big  
secret special Apple project, it's already in use on Solaris and  
FreeBSD.

> we should either port ZFS "ourselves",

The problem being of course is whether the people who *can* continue  
ZFS on OS X are willing to put the time into it when Apple could come  
out in 3 months time and say "you've wasted your time, here's an up to  
date version that we're releasing now that problem x has been  
solved"?  I'm not up on coding filesystems but if someone wants to  
kick it off I can provide server space and other resources.

> continually demand answers from Apple


That's roughly equivalent to demanding diamonds from a grapefruit and  
equally unlikely to produce a positive result.

James

Website:		www.themacplace.co.uk
Blog:		www.themacplace.co.uk/blog.html


From lists at loveturtle.net  Thu Oct  8 18:49:11 2009
From: lists at loveturtle.net (Dillon Kass)
Date: Thu, 08 Oct 2009 21:49:11 -0400
Subject: [zfs-discuss] must watch.
Message-ID: <4ACE9697.8050901@loveturtle.net>

https://slx.sun.com/1179275620

From lists at chipmunk-app.com  Fri Oct  9 12:28:47 2009
From: lists at chipmunk-app.com (Ruotger Skupin)
Date: Fri, 9 Oct 2009 21:28:47 +0200
Subject: [zfs-discuss] must watch.
In-Reply-To: <4ACE9697.8050901@loveturtle.net>
References: <4ACE9697.8050901@loveturtle.net>
Message-ID: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>

Thanks for the link!

There is a possible technical answer to a question everone asks  
themselves on that list starting at about minute 22!

Ruotger

Am 09.10.2009 um 03:49 schrieb Dillon Kass:

> https://slx.sun.com/1179275620
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dbg at apple.com  Fri Oct  9 16:33:55 2009
From: dbg at apple.com (Dominic Giampaolo)
Date: Fri, 9 Oct 2009 16:33:55 -0700
Subject: [zfs-discuss] must watch.
In-Reply-To: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
References: <4ACE9697.8050901@loveturtle.net>
	<03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
Message-ID: <77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>

> There is a possible technical answer to a question everone asks  
> themselves on that list starting at about minute 22!
>
Without implying anything about why ZFS was not shipped with
SnowLeopard, the discussion that starts out at minute 22
exposes some continued mis-understanding by Mr. Bonwick and
crew.  I have a great deal of respect for Jeff but I am a
bit surprised that he's not aware of these issues.

At around 23:45 or so he says that "laptops never experience
problems with flush-track-cache because they have a battery
and thus never lose power".   While it is true that laptops
have batteries, machines still crash and/or get force rebooted.
When a reboot happens the bus is reset and what the drive will
do with dirty data sitting in its cache is undefined at that
point.  Regardless of what the specs say, some drives toss any
unwritten data while others will keep it and eventually write
it after the reset.

To be fair, we guarantee that all internal drives Apple ships
will honor flush track cache so you're safe on a Mac laptop.
However if you're running Solaris on some other brand of laptop
you may or may not be out of luck.

Last, I do not believe that the crash protection scheme used
by ZFS can ever work reliably on drives that drop the flush
track cache request.  The only approach that is guaranteed to
work is to keep enough data in a log that when you remount the
drive, you can replay more data than the drive could have kept
cached.  This is effectively what the journal on HFS has done
since Leopard.


--dominic


From alex.blewitt at gmail.com  Sat Oct 10 00:54:18 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Sat, 10 Oct 2009 08:54:18 +0100
Subject: [zfs-discuss] Panic in AVL (build 154 aka 10a286)
In-Reply-To: <7C911D5D-69EB-4FFA-B8E3-BC9447BA4311@sourcemac.com>
References: <7C911D5D-69EB-4FFA-B8E3-BC9447BA4311@sourcemac.com>
Message-ID: <F279214B-CF82-4E59-9E9E-E1BA5A124417@gmail.com>


On 7 Oct 2009, at 08:17, Julien-Pierre Av?rous wrote:

> I know, I should forget ZFS for the moment, until Apple resolve its  
> problems with legals (or technicals (or...)) issues. But I love too  
> much ZFS ;)
>
> So I have installed the build 154 (released in the Mac OS  Build  
> 10a286) in Mac OS X Server Snow Leopard X.6.1.

Worth noting that build 154 was never released to the public ... the  
last published version was 119. Your mileage may vary quite a lot :-)

> I have a WD Drive in USB2 of 2 TB. I have formatted my drive,  
> configured the main partition as a ZFS entity, created a ZFS Pool on  
> this entity and finally created a filesystem on this pool.

Don't do it! USB sucks. USB2 sucks faster.

http://alblue.blogspot.com/2007/12/mac-data-loss-on-leopard-usb-drives.html

> This seem work fine, until I try to copy (with cp -pR) a big amount  
> of file (between 3 to 6 MB each) : after a little amount of file  
> copied (less than 3000, not on the same directory), my server is  
> killed with a call to panic, generating this trace :

There are problems with the USB drive and filling the write cache. The  
USB device is reporting (in response to an fsync) that eveything is  
written, when it isn't. This ends up confusing ZFS and you end up with  
weird errors.

Best go out and buy yourself a decent FireWire drive and then use the  
USB as a /tmp drive.

Alex

From kacperw at online.no  Sat Oct 10 06:24:30 2009
From: kacperw at online.no (Kacper Wysocki)
Date: Sat, 10 Oct 2009 15:24:30 +0200
Subject: [zfs-discuss] Panic in AVL (build 154 aka 10a286)
In-Reply-To: <F279214B-CF82-4E59-9E9E-E1BA5A124417@gmail.com>
References: <7C911D5D-69EB-4FFA-B8E3-BC9447BA4311@sourcemac.com>
	<F279214B-CF82-4E59-9E9E-E1BA5A124417@gmail.com>
Message-ID: <367a23780910100624s387e4322lcb81bdad4c3a3dcc@mail.gmail.com>

2009/10/10 Alex Blewitt <alex.blewitt at gmail.com>:
> On 7 Oct 2009, at 08:17, Julien-Pierre Av?rous wrote:
[snip the-future-of-zfs woes]
>> I have a WD Drive in USB2 of 2 TB. I have formatted my drive, configured
>> the main partition as a ZFS entity, created a ZFS Pool on this entity and
>> finally created a filesystem on this pool.
>
> Don't do it! USB sucks. USB2 sucks faster.
>
> http://alblue.blogspot.com/2007/12/mac-data-loss-on-leopard-usb-drives.html
>
>> This seem work fine, until I try to copy (with cp -pR) a big amount of
>> file (between 3 to 6 MB each) : after a little amount of file copied (less
>> than 3000, not on the same directory), my server is killed with a call to
>> panic, generating this trace :
>
> There are problems with the USB drive and filling the write cache. The USB
> device is reporting (in response to an fsync) that eveything is written,
> when it isn't. This ends up confusing ZFS and you end up with weird errors.
>
> Best go out and buy yourself a decent FireWire drive and then use the USB as
> a /tmp drive.

Oh no, you shouldn't use it for /tmp either... The ubiquity of USB is
pretty much the only thing USB has going for it. USB is still
excellent for sneakernet & cold boots.

But my God fire wire.

-- 
http://kacper.doesntexist.org
http://windows.dontexist.com
Employ no technique to gain supreme enlightment.
- Mar pa Chos kyi blos gros

From james at themacplace.co.uk  Sat Oct 10 09:59:07 2009
From: james at themacplace.co.uk (James Relph)
Date: Sat, 10 Oct 2009 17:59:07 +0100
Subject: [zfs-discuss] must watch.
In-Reply-To: <77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
References: <4ACE9697.8050901@loveturtle.net>
	<03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
Message-ID: <8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>

> Last, I do not believe that the crash protection scheme used
> by ZFS can ever work reliably on drives that drop the flush
> track cache request.  The only approach that is guaranteed to
> work is to keep enough data in a log that when you remount the
> drive, you can replay more data than the drive could have kept
> cached.  This is effectively what the journal on HFS has done
> since Leopard.

I'm just a filesystem user rather than having any particular knowledge  
of file system design, but I would be interested to know whether  
that's something that you think is fixable with ZFS or whether the  
design makes that very hard or impossible to do?

Thanks,

James

Website:		www.themacplace.co.uk
Blog:		www.themacplace.co.uk/blog.html

From hanche at math.ntnu.no  Sat Oct 10 11:53:35 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Sat, 10 Oct 2009 14:53:35 -0400 (EDT)
Subject: [zfs-discuss] must watch.
In-Reply-To: <8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
Message-ID: <20091010.145335.37374416.hanche@math.ntnu.no>

+ James Relph <james at themacplace.co.uk>:

> > Last, I do not believe that the crash protection scheme used
> > by ZFS can ever work reliably on drives that drop the flush
> > track cache request.  The only approach that is guaranteed to
> > work is to keep enough data in a log that when you remount the
> > drive, you can replay more data than the drive could have kept
> > cached.  This is effectively what the journal on HFS has done
> > since Leopard.
> 
> I'm just a filesystem user rather than having any particular knowledge
> of file system design, but I would be interested to know whether
> that's something that you think is fixable with ZFS or whether the
> design makes that very hard or impossible to do?

If you watch the video that started this thread, you'll see that this
is one of the upcoming features of ZFS. Basically, they plan to keep a
backlog of ?berblocks so that, if the latest one is no good, you can
go back and use an earlier one. You'll lose recently written data, of
course, but your entire pool won't be gone the way it can happen now.

- Harald

From toby at telegraphics.com.au  Sat Oct 10 13:00:17 2009
From: toby at telegraphics.com.au (Toby Thain)
Date: Sat, 10 Oct 2009 16:00:17 -0400
Subject: [zfs-discuss] must watch.
In-Reply-To: <8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
References: <4ACE9697.8050901@loveturtle.net>
	<03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
Message-ID: <814AA6BA-7712-425B-B437-248A03EE097F@telegraphics.com.au>


On 10-Oct-09, at 12:59 PM, James Relph wrote:

>> Last, I do not believe that the crash protection scheme used
>> by ZFS can ever work reliably on drives that drop the flush
>> track cache request.  The only approach that is guaranteed to
>> work is to keep enough data in a log that when you remount the
>> drive, you can replay more data than the drive could have kept
>> cached.  This is effectively what the journal on HFS has done
>> since Leopard.
>
> I'm just a filesystem user rather than having any particular  
> knowledge of file system design, but I would be interested to know  
> whether that's something that you think is fixable with ZFS or  
> whether the design makes that very hard or impossible to do?

I'm not an expert like Dominic, but I believe without a synchronous  
flush/barrier, you lose the means to force I/O ordering, which is  
what typical transactional filesystems and DBMS rely upon for integrity.

(That's why Apple cares enough to make it a hardware requirement.  
They get blamed if the assumption doesn't hold. One more point in  
favour of integrated systems.)

--Toby

>
> Thanks,
>
> James
>
> Website:		www.themacplace.co.uk
> Blog:		www.themacplace.co.uk/blog.html
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From james at themacplace.co.uk  Sat Oct 10 13:06:26 2009
From: james at themacplace.co.uk (James Relph)
Date: Sat, 10 Oct 2009 21:06:26 +0100
Subject: [zfs-discuss] must watch.
In-Reply-To: <20091010.145335.37374416.hanche@math.ntnu.no>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
	<20091010.145335.37374416.hanche@math.ntnu.no>
Message-ID: <9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>

> If you watch the video that started this thread, you'll see that this
> is one of the upcoming features of ZFS. Basically, they plan to keep a
> backlog of ?berblocks so that, if the latest one is no good, you can
> go back and use an earlier one. You'll lose recently written data, of
> course, but your entire pool won't be gone the way it can happen now.

Indeed, which would be reassuring, except Dominic said:

"Last, I do not believe that the crash protection scheme used
by ZFS can ever work reliably on drives that drop the flush
track cache request.  The only approach that is guaranteed to
work is to keep enough data in a log that when you remount the
drive, you can replay more data than the drive could have kept
cached."

Obviously Dominic knows a *lot* more about filesystems than I do, so  
if he thinks that the proposed upcoming feature *won't* solve that  
problem then it's both interesting and would possibly explain the  
second thoughts by Apple.  Someone on the mac-osx-server list (I  
initially mistakenly posted my response on the wrong list) mentioned  
the ZIL, but from what I understand I don't think that solves the  
problem either.

James

Website:		www.themacplace.co.uk
Blog:		www.themacplace.co.uk/blog.html

From dbg at apple.com  Sat Oct 10 15:22:03 2009
From: dbg at apple.com (Dominic Giampaolo)
Date: Sat, 10 Oct 2009 15:22:03 -0700
Subject: [zfs-discuss] must watch.
In-Reply-To: <9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
	<20091010.145335.37374416.hanche@math.ntnu.no>
	<9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>
Message-ID: <DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>

>> If you watch the video that started this thread, you'll see that this
>> is one of the upcoming features of ZFS. Basically, they plan to  
>> keep a
>> backlog of ?berblocks so that, if the latest one is no good, you can
>> go back and use an earlier one. You'll lose recently written data, of
>> course, but your entire pool won't be gone the way it can happen now.
>
> Indeed, which would be reassuring, except Dominic said:
>
> "Last, I do not believe that the crash protection scheme used
> by ZFS can ever work reliably on drives that drop the flush
>
If the ZFS guys keep a pool of uberblocks then they
will be able to recover.  Of course I'm not sure how
you make it automatic... that is, unless you walked
the entire fs you wouldn't know if some of the on-disk
structures were corrupt and walking the entire fs is
not practical (i.e. it's the moral equivalent of fsck).

If you don't walk the entire fs then you run the risk
of making changes to the file system, discovering it
is corrupt and then having to revert to a prior version
in one of the saved uberblocks (which you hopefully did
not lose by that point).  Then you would lose the changes
you made recently.

Unless I'm missing something this seems sub-optimal to
me.


--dominic


From richard.elling at gmail.com  Sat Oct 10 16:29:25 2009
From: richard.elling at gmail.com (Richard Elling)
Date: Sat, 10 Oct 2009 16:29:25 -0700
Subject: [zfs-discuss] must watch.
In-Reply-To: <DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
	<20091010.145335.37374416.hanche@math.ntnu.no>
	<9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
Message-ID: <F6954FBD-3763-4808-89DC-A2B297BBD442@gmail.com>

On Oct 10, 2009, at 3:22 PM, Dominic Giampaolo wrote:
>>> If you watch the video that started this thread, you'll see that  
>>> this
>>> is one of the upcoming features of ZFS. Basically, they plan to  
>>> keep a
>>> backlog of ?berblocks so that, if the latest one is no good, you can
>>> go back and use an earlier one. You'll lose recently written data,  
>>> of
>>> course, but your entire pool won't be gone the way it can happen  
>>> now.
>>
>> Indeed, which would be reassuring, except Dominic said:
>>
>> "Last, I do not believe that the crash protection scheme used
>> by ZFS can ever work reliably on drives that drop the flush
>>
> If the ZFS guys keep a pool of uberblocks then they
> will be able to recover.  Of course I'm not sure how
> you make it automatic... that is, unless you walked
> the entire fs you wouldn't know if some of the on-disk
> structures were corrupt and walking the entire fs is
> not practical (i.e. it's the moral equivalent of fsck).

There is a circular queue of 128 uberblocks. For current code,
you can figure on an uberblock update nominally every 30
seconds -- more often if a sync occurs (snapshots cause syncs).

> If you don't walk the entire fs then you run the risk
> of making changes to the file system, discovering it
> is corrupt and then having to revert to a prior version
> in one of the saved uberblocks (which you hopefully did
> not lose by that point).  Then you would lose the changes
> you made recently.

The risk is that as you become full, some data gets freed
and then overwritten. Not likely on a COW file system until
it gets close to full, but still theoretically possible. The check
is done via scrub, which can take quite some time on TB-sized
file systems.  So the current, manual process is to find a previous
uberblock (easy) and then scrub from that uberblock.  If the
scrub passes, then roll back to the previous uberblock.

> Unless I'm missing something this seems sub-optimal to
> me.

I agree. Perhaps it would be a good idea to intentionally not
reuse recently freed blocks, enforced by some policy?  This
would remove the desire to do the scrub and the rollback
is nearly instantaneous.
  -- richard


From james-zfsosx at jrv.org  Sat Oct 10 16:31:43 2009
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Sat, 10 Oct 2009 18:31:43 -0500
Subject: [zfs-discuss] must watch.
In-Reply-To: <DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>	<20091010.145335.37374416.hanche@math.ntnu.no>	<9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
Message-ID: <4AD1195F.408@jrv.org>

Dominic Giampaolo wrote:
> If the ZFS guys keep a pool of uberblocks then they
> will be able to recover.

ZFS never updates anything in place, not even the superblock/uberblock. 
There is an array of 128 uberblocks (replicated four times) written
circularly.  At mount time ZFS chooses the uberblock with the highest
txtag - the most recently written slot - and uses it if it has a good
checksum.

The change being made is not to retain old uberblocks - ZFS already
keeps the most recent 128 - but rather defer reallocation of freed disk
blocks longer so that references by and through those older uberblocks
are less likely to have been overwritten.  The change automatically uses
an older uberblock if the most recent cannot be used (if the pool
metadata is corrupt, for example).

Fundamentally no filesytem can withstand having block writes being
arbitrarily reordered and indefinitely deferred, not even log-based
filesystems such as NTFS or HFS.  Fortunately write deferrals are not
infinite in the real world and a long enough log can survive the bad
behavior of a disk.  ZFS need merely defer block reallocation as long as
these logs to get a similar benefit.


From kacperw at online.no  Sun Oct 11 05:33:40 2009
From: kacperw at online.no (Kacper Wysocki)
Date: Sun, 11 Oct 2009 14:33:40 +0200
Subject: [zfs-discuss] Disk speeds
In-Reply-To: <DFFD49D7-C71D-4862-9C39-097D65D27669@ironsoftware.de>
References: <BACEEE95-0D86-4508-BE18-80116D00DE54@avoidant.org>
	<612C550B-0C41-426F-8487-87711F08B84C@ironsoftware.de>
	<098EA756-3D36-485B-87A0-309A5D8F5B6D@avoidant.org>
	<DFFD49D7-C71D-4862-9C39-097D65D27669@ironsoftware.de>
Message-ID: <367a23780910110533n783dc417if97fb00b957c4466@mail.gmail.com>

On Mon, Sep 21, 2009 at 3:21 PM, Christian Kendi <ksh at ironsoftware.de> wrote:
> El 21/09/2009, a las 8:35, sammy ominsky escribi?:
>> This is with 2000 8MB files:
>> zefat:zfscachetest sambo$ time (find . | cpio -C 32256 -o > /dev/null)
>> 32768355 blocks
>> real    4m57.183s
>> user    0m3.919s
>> sys     0m33.403s
>>
>> and your kext:
>> zefat:zfscachetest sambo$ time (find . | cpio -C 32256 -o > /dev/null)
>> 32768355 blocks
>> real    2m45.003s
>> user    0m2.926s
>> sys     0m20.218s
>
> This now looks like a working zfs :)

Hey all,
thought I'd try the Kendi improvements for myself. Stock G5 with two
7200rpm seagate 1.5TB SATAs in RAIDZ1 on OSX 10.5:

I made 2000 8MB files with contents from /dev/urandom in a single
folder in a newly created ZFS filesystem.

- regular zfs.kext -
nurgle:scratch kwy$ time (find . | cpio -C 32256 -o > /dev/null)
real    5m58.890s
user    0m31.158s
sys     1m27.168s

- 2nd run -
nurgle:scratch kwy$ time (find . | cpio -C 32256 -o > /dev/null)
real    5m58.092s
user    0m31.161s
sys     1m27.054s

- kendi zfs.kext -
nurgle:scratch kwy$ time (find . | cpio -C 32256 -o > /dev/null)
real    4m35.392s
user    0m31.147s
sys     2m0.470s

- 2nd run -
nurgle:scratch kwy$ time (find . | cpio -C 32256 -o > /dev/null)
real    4m20.960s
user    0m31.053s
sys     1m55.890s

so, thanks for that one!

Still, the mac can not handle mplayer playback of a xvid movie while
copying gigs from firewire 800. CPU gets overloaded, movie gets too
choppy! :-(

cheers,
  Kacper
-- 
http://kacper.doesntexist.org
http://windows.dontexist.com
Employ no technique to gain supreme enlightment.
- Mar pa Chos kyi blos gros

From james at themacplace.co.uk  Sun Oct 11 05:50:37 2009
From: james at themacplace.co.uk (James Relph)
Date: Sun, 11 Oct 2009 13:50:37 +0100
Subject: [zfs-discuss] must watch.
In-Reply-To: <F6954FBD-3763-4808-89DC-A2B297BBD442@gmail.com>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
	<20091010.145335.37374416.hanche@math.ntnu.no>
	<9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
	<F6954FBD-3763-4808-89DC-A2B297BBD442@gmail.com>
Message-ID: <2DB7E73D-BA41-4147-B062-8289A552C360@themacplace.co.uk>

> I agree. Perhaps it would be a good idea to intentionally not
> reuse recently freed blocks, enforced by some policy?  This
> would remove the desire to do the scrub and the rollback
> is nearly instantaneous.
> -- richard

Presumably that's what this change then achieves if (as mentioned by  
James R. Van Artsdalen) the new build does defer the reallocation of  
freed space?  The slides from the presentation are available here:

http://www.snia.org/events/storage-developer2009/presentations/monday/JeffBonwick_zfs-What_Next-SDC09.pdf

On page 10 they specifically say "Rollback made reliable by deferred  
block reallocation", so it looks like that is what they're up to, and  
it sounds like that may give you a similar level of recoverability  
from damaged pools to that provided with HFS/NTFS volumes by using  
journals.

This may have been something Apple were waiting for before officially  
"sanctioning" ZFS by including it in a release.  Of course that  
doesn't necessarily explain why the MacOSforge builds haven't been  
updated.  Perhaps we'll get a new build when the recoverability from  
pool corruption is up to scratch, just clutching at straws really but  
there's a few filesystem features I got used to using on test systems  
and I'm missing them when building production environments!

James

Website:		www.themacplace.co.uk
Blog:		www.themacplace.co.uk/blog.html
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091011/927e8942/attachment.html>

From dbg at apple.com  Sun Oct 11 06:25:37 2009
From: dbg at apple.com (Dominic Giampaolo)
Date: Sun, 11 Oct 2009 06:25:37 -0700
Subject: [zfs-discuss] must watch.
In-Reply-To: <4AD1195F.408@jrv.org>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
	<20091010.145335.37374416.hanche@math.ntnu.no>
	<9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
	<4AD1195F.408@jrv.org>
Message-ID: <B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>


On Oct 10, 2009, at 4:31 PM, James R. Van Artsdalen wrote:

> Dominic Giampaolo wrote:
>> If the ZFS guys keep a pool of uberblocks then they
>> will be able to recover.
>
> ZFS never updates anything in place, not even the superblock/ 
> uberblock.
> There is an array of 128 uberblocks (replicated four times) written
> circularly.  At mount time ZFS chooses the uberblock with the highest
> txtag - the most recently written slot - and uses it if it has a good
> checksum.
>
I understand this.

What I don't understand is how you know whether there is
damage to the most recent txtag without actually walking
the entire file system?  Even if the uberblock checksum is
ok, there may be damage elsewhere in the pool.

If you just start using the pool represented by latest
valid txtag and then later find damage, what do you do?
Which txtag do you roll back to?  There's nothing that says
the previous txtag is any more valid than the one you used.
Further, how do you apply the changes you made since you
mounted to the previous pool?  Further still, if it takes
you a while to discover the damage and you make a lot of
changes, you may have recycled through those 128 uberblocks
already.

On drives that do not honor flush-track-cache, unless you
do a complete file system walk at mount time I don't see how
you can know if a particular txtag is valid or not.  Please
educate me if I'm missing something.


> Fundamentally no filesytem can withstand having block writes being
> arbitrarily reordered and indefinitely deferred, not even log-based
> filesystems such as NTFS or HFS.  Fortunately write deferrals are not
>
Actually I disagree: if your log contains more data than
the drive could have buffered then you're safe.  That's why
we let the active journal grow very large on HFS (on a big
drive it may be 10's of megabytes or more).  If a drive has
a 16 meg cache and you've got 20 megs worth of data to
replay then you will undo any damage the drive could have
done.


--dominic


From james-zfsosx at jrv.org  Sun Oct 11 09:02:01 2009
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Sun, 11 Oct 2009 11:02:01 -0500
Subject: [zfs-discuss] must watch.
In-Reply-To: <B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
	<20091010.145335.37374416.hanche@math.ntnu.no>
	<9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
	<4AD1195F.408@jrv.org>
	<B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>
Message-ID: <4AD20179.3040803@jrv.org>

Dominic Giampaolo wrote:
> What I don't understand is how you know whether there is
> damage to the most recent txtag without actually walking
> the entire file system?

I don't know what approach they're using.  But in practice it isn't
necessary to scan the entire filesystem - just scan the recently written
objects (in the same manner resilvering works after a mirror or RAID
component is reattached).

I think all ZFS objects contain the txtag in which that object was
"born".  In walking the filesystem objects born in the "distant past"
needn't be scanned.  Note that even very large database files are
efficiently handled since the data blocks need be scanned only if the
block containing the pointers was born "recently".

Drawing the line between "distant past" and "recently" may be a little
arbitrary, but ZFS could easily record the number of writes and
wall-clock time represented by each new transaction group and ZFS could
scan backwards 60 seconds or 2 GB in writes, etc, then choose the most
recent valid uberblock.  This might require a pool version bump since I
don't think uberblocks contain I/O write statistics.

We're only going to do this on unclean shutdowns so it is no great price
to delay before allowing access to the pool.  Even if you chose to check
the most recent transaction group in its entirety on every boot as a
sanity check that's probably only 5 seconds or so at most.

(one complication is the ZIL - I'd have to think about that one for a while)

Please note I'm not saying it's possible for ZFS to overcome an
arbitrarily pathological drive.  I'm only saying that drives in the real
world that can be safely used with mature log-based filesystems should
be safe with ZFS also (with ZFS changes!).  The strong integrity
mechanisms in ZFS also make it possible to detect any damage, which
isn't true of NTFS or HFS, so if redundancy is used recovery is more likely.

> if your log contains more data than
> the drive could have buffered then you're safe.

There's no reason for a drive to defer writes in order - a write to a
distant part of a disk might be deferred for a long time if there are
other writes that can be completed without a seek.  It's not "could have
buffered" but rather the number of committed writes since the deferred
write.

> If a drive has
> a 16 meg cache and you've got 20 megs worth of data to
> replay then you will undo any damage the drive could have
> done.

I have a drive with 256 MB cache.  I'm told there's one with 1 GB of
cache...  I would not depend on existing log filesystems having a big
enough log to deal with write fence errors in these drives.

From developer at mvasilakis.com  Sun Oct 11 10:35:56 2009
From: developer at mvasilakis.com (mano vasilakis)
Date: Sun, 11 Oct 2009 13:35:56 -0400
Subject: [zfs-discuss] Pool gone after Kernel Panic
Message-ID: <2bfa7a9b0910111035k55565f2coa1a948e13fed1946@mail.gmail.com>

I am having a problem with an external firewire enclosure and data
corruption. It seems ok for abit using Time Machine but the corruption
eventually gets so bad that even Disk Warrior cant recover the drive.
Initial copy to the drive is ok. DW shows no corruption. But after a few
days its messed up so bad that DW takes a week to fix the 2+ TB on the
volume. After a few weeks the drive becomes unrecoverable.

So I chose to try out a simple single drive ZFS partition to see if I could
avoid the data errors that eventually take the drive out. I use rsync to
copy the data over and then check the status of the pool using zpool status.
I get a message that there are 1700+ files with corruption and to use the -v
option to get details. Problem is when I choose zpool status -v I get a
kernel panic. When I reboot the ZFS drive is there but the pool is gone.
Setup instructions I used:
1. diskutil list
2. diskutil partitiondisk /dev/disk# GPTFormat ZFS %noformat% 100%
3. sudo zpool create Backup /dev/disk#s#
4. zfs set compression=on Backup
5. sudo chown -R me:staff /Volumes/Backup
6. zfs get compression Backup
7. zpool upgrade
8. zpool status

Everything is good at this point. Files copy over just fine. Except for the
few files that I mentioned that are corrupt. When I try to get more info
using zpool list -v the machine panics and I loose my pool, Backup. After
reboot zpool list states there are no pools.
diskutil list shows disk4s2 but no zfs commands bring up the old pool.

Is this normal behavior to loose the pool after a kernel panic? Is there a
command I can use to revive the pool? I'm comfortable with command line but
I'm real new to ZFS. (obviously)
Any help would be appreciated.
Mano
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091011/e533f9d7/attachment.html>

From dbg at apple.com  Sun Oct 11 11:39:10 2009
From: dbg at apple.com (Dominic Giampaolo)
Date: Sun, 11 Oct 2009 11:39:10 -0700
Subject: [zfs-discuss] must watch.
In-Reply-To: <4AD20179.3040803@jrv.org>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
	<20091010.145335.37374416.hanche@math.ntnu.no>
	<9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
	<4AD1195F.408@jrv.org>
	<B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>
	<4AD20179.3040803@jrv.org>
Message-ID: <BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>


On Oct 11, 2009, at 9:02 AM, James R. Van Artsdalen wrote:

> Dominic Giampaolo wrote:
>> What I don't understand is how you know whether there is
>> damage to the most recent txtag without actually walking
>> the entire file system?
>
> I don't know what approach they're using.  But in practice it isn't
> necessary to scan the entire filesystem - just scan the recently  
> written
> objects (in the same manner resilvering works after a mirror or RAID
> component is reattached).
>
I did not think there was a way to iterate over
just the objects in a transaction group.  I guess
you're saying that they start at the root and walk
just the items referenced in "recent" transaction
groups to insure they're valid.

Yeah, I suppose that would work and would avoid a
full file system walk.


--dominic


From alex.blewitt at gmail.com  Sun Oct 11 12:22:24 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Sun, 11 Oct 2009 20:22:24 +0100
Subject: [zfs-discuss] Pool gone after Kernel Panic
In-Reply-To: <2bfa7a9b0910111035k55565f2coa1a948e13fed1946@mail.gmail.com>
References: <2bfa7a9b0910111035k55565f2coa1a948e13fed1946@mail.gmail.com>
Message-ID: <9A610B49-404C-4218-B369-337B7F4D2E04@gmail.com>

Your disk is hosed. Please stop using it. ZFS is telling you this  
where other file systems are blindly ignoring it.

I believe that there were reported problems automatically mounting a  
ZFS drive on Snow Leopard with some of the bits. I'm using Mac OSX  
10.5 with the 119 build, and provided that you follow the instructions  
(which it looks like you used) then it should mount on startup. Others  
have put in a cron type job that mounts the disk after the system has  
been running for a bit with zpool import Backup, or zpool import -f  
Backup.

But honestly, a backup with that many disk errors isn't much of a  
backup. zpool scrub will tell you how bad the problem is; but you  
clearly know that already.

Alex

On Oct 11, 2009, at 18:35, mano vasilakis wrote:

> I am having a problem with an external firewire enclosure and data  
> corruption. It seems ok for abit using Time Machine but the  
> corruption eventually gets so bad that even Disk Warrior cant  
> recover the drive. Initial copy to the drive is ok. DW shows no  
> corruption. But after a few days its messed up so bad that DW takes  
> a week to fix the 2+ TB on the volume. After a few weeks the drive  
> becomes unrecoverable.
>
> So I chose to try out a simple single drive ZFS partition to see if  
> I could avoid the data errors that eventually take the drive out. I  
> use rsync to copy the data over and then check the status of the  
> pool using zpool status. I get a message that there are 1700+ files  
> with corruption and to use the -v option to get details. Problem is  
> when I choose zpool status -v I get a kernel panic. When I reboot  
> the ZFS drive is there but the pool is gone.
>
> Setup instructions I used:
> 1. diskutil list
> 2. diskutil partitiondisk /dev/disk# GPTFormat ZFS %noformat% 100%
> 3. sudo zpool create Backup /dev/disk#s#
> 4. zfs set compression=on Backup
> 5. sudo chown -R me:staff /Volumes/Backup
> 6. zfs get compression Backup
> 7. zpool upgrade
> 8. zpool status
>
> Everything is good at this point. Files copy over just fine. Except  
> for the few files that I mentioned that are corrupt. When I try to  
> get more info using zpool list -v the machine panics and I loose my  
> pool, Backup. After reboot zpool list states there are no pools.
> diskutil list shows disk4s2 but no zfs commands bring up the old pool.
>
> Is this normal behavior to loose the pool after a kernel panic? Is  
> there a command I can use to revive the pool? I'm comfortable with  
> command line but I'm real new to ZFS. (obviously)
> Any help would be appreciated.
> Mano
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From developer at mvasilakis.com  Sun Oct 11 12:45:06 2009
From: developer at mvasilakis.com (mano vasilakis)
Date: Sun, 11 Oct 2009 15:45:06 -0400
Subject: [zfs-discuss] Pool gone after Kernel Panic
In-Reply-To: <9A610B49-404C-4218-B369-337B7F4D2E04@gmail.com>
References: <2bfa7a9b0910111035k55565f2coa1a948e13fed1946@mail.gmail.com>
	<9A610B49-404C-4218-B369-337B7F4D2E04@gmail.com>
Message-ID: <2bfa7a9b0910111245p7208c32g84764473063dee02@mail.gmail.com>

I doubt the disk is hosed. I think rather the enclosure may be the issue.
When I looked and saw quite a few people having issues with Time Machine and
firewire backup volumes I figured it was possible that TM was corrupting
something. And with 10.6 so new well...

I'll deal with the manufacturer and see if they can't help me. (Long story
there) If not I guess I need to buy a new enclosure.
Thanks.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091011/fe827196/attachment.html>

From alex.blewitt at gmail.com  Sun Oct 11 13:14:01 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Sun, 11 Oct 2009 21:14:01 +0100
Subject: [zfs-discuss] Pool gone after Kernel Panic
In-Reply-To: <2bfa7a9b0910111245p7208c32g84764473063dee02@mail.gmail.com>
References: <2bfa7a9b0910111035k55565f2coa1a948e13fed1946@mail.gmail.com>
	<9A610B49-404C-4218-B369-337B7F4D2E04@gmail.com>
	<2bfa7a9b0910111245p7208c32g84764473063dee02@mail.gmail.com>
Message-ID: <9C3BC70B-2F9B-4AFE-9619-4345EFA66B43@gmail.com>

On Oct 11, 2009, at 20:45, mano vasilakis wrote:

> I doubt the disk is hosed. I think rather the enclosure may be the  
> issue. When I looked and saw quite a few people having issues with  
> Time Machine and firewire backup volumes I figured it was possible  
> that TM was corrupting something. And with 10.6 so new well...
>
> I'll deal with the manufacturer and see if they can't help me. (Long  
> story there) If not I guess I need to buy a new enclosure.
> Thanks.

Certainly something is suspect :-) There were, back in the dim and  
distant past, problems with Oxford FireWire chipsets but those flavour  
of problems were consigned to the 10.1/10.2 days. I've not seen any  
problems with FireWire drives since then. I did see problems with  
cheapo external USB drives. So it's possible that it's the controller/ 
enclosure, but unless you've got a way of testing the drive in  
different enclosures to find out, it's a bit difficult to know.

There's a small possibility that the make/model of your firewire drive/ 
enclosure might help others (Apple?) on this list, though they might  
not want to ask for it in case it gives the game away. If you run  
system_profiler | pbcopy, you'll get a diagnostic of what's on your  
system. Search for FireWire:, which will be the start of your firewire  
drives, and then you can cut out the rest of the stuff in a mail, e.g.  
mine as below. You'll probably want to sanity check it to confirm it  
doesn't have any personally identifiable information in it (it will  
have the GUID of the drive which if you're paranoid, you might want to  
obfuscate) but otherwise it'll be a text file which is similar to the  
data in System Profiler. (You could probably get the info from the  
graphical system profiler and doing a 'copy' on the appropriate bit  
too ... click on 'About this Mac' and then 'More Info' to launch the  
GUI one)

Alex


FireWire:

     FireWire Bus:

       Maximum Speed: Up to 400 Mb/sec

         Rugged FW/USB:

           Manufacturer: LaCie
           Model: 0x0
           GUID: **********
           Maximum Speed: Up to 800 Mb/sec
           Connection Speed: Up to 400 Mb/sec
           Sub-units:
             Rugged FW/USB Unit:
               Unit Software Version: 0x10483
               Unit Spec ID: 0x609E
               Firmware Revision: 0x110
               Product Revision Level:
               Sub-units:
                 Rugged FW/USB SBP-LUN:
                   Capacity: 465.76 GB
                   Removable Media: Yes
                   BSD Name: disk2
                   Mac OS 9 Drivers: No
                   Partition Map Type: GPT (GUID Partition Table)
                   S.M.A.R.T. status: Not Supported
                   Volumes:
                     Data:
                       Capacity: 465.44 GB
                       Writable: Yes
                       File System: ZFS
                       BSD Name: disk2s2
                       Mount Point:
                   Volumes:
                     disk2s2:
                       Capacity: 465.44 GB
                       Writable: Yes
                       File System: ZFS


From james at themacplace.co.uk  Sun Oct 11 14:24:31 2009
From: james at themacplace.co.uk (James Relph)
Date: Sun, 11 Oct 2009 22:24:31 +0100
Subject: [zfs-discuss] must watch.
In-Reply-To: <BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
	<20091010.145335.37374416.hanche@math.ntnu.no>
	<9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
	<4AD1195F.408@jrv.org>
	<B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>
	<4AD20179.3040803@jrv.org>
	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>
Message-ID: <B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>

> I did not think there was a way to iterate over
> just the objects in a transaction group.  I guess
> you're saying that they start at the root and walk
> just the items referenced in "recent" transaction
> groups to insure they're valid.
>
> Yeah, I suppose that would work and would avoid a
> full file system walk.

Sounds like that's what they're doing, from 25:30 in the video Jeff  
Bonwick says that they can step back to the last uberblock and do a  
scrub of the recent transactions (which only takes a few seconds), and  
if that fails, step back to the previous uberblock etc. until it  
checks out.  According to the slides that's a working system and  
they're just "finalizing user experience", wikipedia seems to suggest  
that that's due Q4 2009, although there's no source for that.

James

Website:		www.themacplace.co.uk
Blog:		www.themacplace.co.uk/blog.html


From kona8lend at gmail.com  Sun Oct 11 15:18:45 2009
From: kona8lend at gmail.com (Kona Blend)
Date: Sun, 11 Oct 2009 18:18:45 -0400
Subject: [zfs-discuss] must watch.
In-Reply-To: <B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
	<20091010.145335.37374416.hanche@math.ntnu.no>
	<9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
	<4AD1195F.408@jrv.org>
	<B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>
	<4AD20179.3040803@jrv.org>
	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>
	<B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>
Message-ID: <B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>

That's my take on it too; the most recent transaction (or maybe  
several) are sacrificed
for sanity and the system is then pegged at the most recent completed  
transaction point.

A small price to pay in order to support questionable buses/drives.

[me just hopes they're still on track to finish up all these goodies  
by end-of-year].

--kb

On 2009-10-11, at 5:24 PM, James Relph wrote:

>> I did not think there was a way to iterate over
>> just the objects in a transaction group.  I guess
>> you're saying that they start at the root and walk
>> just the items referenced in "recent" transaction
>> groups to insure they're valid.
>>
>> Yeah, I suppose that would work and would avoid a
>> full file system walk.
>
> Sounds like that's what they're doing, from 25:30 in the video Jeff  
> Bonwick says that they can step back to the last uberblock and do a  
> scrub of the recent transactions (which only takes a few seconds),  
> and if that fails, step back to the previous uberblock etc. until it  
> checks out.  According to the slides that's a working system and  
> they're just "finalizing user experience", wikipedia seems to  
> suggest that that's due Q4 2009, although there's no source for that.
>
> James
>
> Website:		www.themacplace.co.uk
> Blog:		www.themacplace.co.uk/blog.html
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From developer at mvasilakis.com  Sun Oct 11 16:11:36 2009
From: developer at mvasilakis.com (mano vasilakis)
Date: Sun, 11 Oct 2009 19:11:36 -0400
Subject: [zfs-discuss] Pool gone after Kernel Panic
In-Reply-To: <9C3BC70B-2F9B-4AFE-9619-4345EFA66B43@gmail.com>
References: <2bfa7a9b0910111035k55565f2coa1a948e13fed1946@mail.gmail.com>
	<9A610B49-404C-4218-B369-337B7F4D2E04@gmail.com>
	<2bfa7a9b0910111245p7208c32g84764473063dee02@mail.gmail.com>
	<9C3BC70B-2F9B-4AFE-9619-4345EFA66B43@gmail.com>
Message-ID: <2bfa7a9b0910111611x3dccc113l5dc12b5da1f1a73d@mail.gmail.com>

It's an older enclosure I picked up at OWC as on open box. I had to flash it
to get it to recognize over 2TB and I think that RAIDON may not have fully
developed the drivers.
Anyway that is why my suspicions are with the enclosure and not the drives.
I will order a SATA to USB cable and see if I see errors with these drives
individually but I'm fairly certain I blew my money on the enclosure. (once
in the enclosure the drives are viewable to the FW bus as one drive, that's
why I referenced it in my original post as one drive, to keep from confusing
anyone.)

Enclosure:
Stardom 7610 4 Bay Rack Mount SATA-II Removable HD Enclosure with
eSATA/FW800/USB 2.0

FireWire:

    FireWire Bus:

      Maximum Speed: Up to 800 Mb/sec

        Built-in Hub:

          Maximum Speed: Up to 800 Mb/sec
          Connection Speed: Up to 800 Mb/sec

        Oxford IDE Device0:

          Manufacturer: Oxford Semiconductor Ltd.
          Model: 0x0
          GUID: ***********
          Maximum Speed: Up to 800 Mb/sec
          Connection Speed: Up to 800 Mb/sec
          Sub-units:
            Oxford IDE Device0 Unit:
              Unit Software Version: 0x10483
              Unit Spec ID: 0x609E
              Firmware Revision: 0x100
              Product Revision Level: V.70
              Sub-units:
                Oxford IDE Device0 SBP-LUN:
                  Capacity: 4 TB (4,000,819,445,760 bytes)
                  Removable Media: Yes
                  BSD Name: disk4
                  Partition Map Type: GPT (GUID Partition Table)
                  S.M.A.R.T. status: Not Supported
                  Volumes:
                    ZFS File System:
                      Capacity: 4 TB (4,000,475,471,872 bytes)
                      Writable: Yes
                      File System: ZFS (Not Mountable)
                      BSD Name: disk4s2

On Sun, Oct 11, 2009 at 4:14 PM, Alex Blewitt <alex.blewitt at gmail.com>wrote:

> On Oct 11, 2009, at 20:45, mano vasilakis wrote:
>
>  I doubt the disk is hosed. I think rather the enclosure may be the issue.
>> When I looked and saw quite a few people having issues with Time Machine and
>> firewire backup volumes I figured it was possible that TM was corrupting
>> something. And with 10.6 so new well...
>>
>> I'll deal with the manufacturer and see if they can't help me. (Long story
>> there) If not I guess I need to buy a new enclosure.
>> Thanks.
>>
>
> Certainly something is suspect :-) There were, back in the dim and distant
> past, problems with Oxford FireWire chipsets but those flavour of problems
> were consigned to the 10.1/10.2 days. I've not seen any problems with
> FireWire drives since then. I did see problems with cheapo external USB
> drives. So it's possible that it's the controller/enclosure, but unless
> you've got a way of testing the drive in different enclosures to find out,
> it's a bit difficult to know.
>
> There's a small possibility that the make/model of your firewire
> drive/enclosure might help others (Apple?) on this list, though they might
> not want to ask for it in case it gives the game away. If you run
> system_profiler | pbcopy, you'll get a diagnostic of what's on your system.
> Search for FireWire:, which will be the start of your firewire drives, and
> then you can cut out the rest of the stuff in a mail, e.g. mine as below.
> You'll probably want to sanity check it to confirm it doesn't have any
> personally identifiable information in it (it will have the GUID of the
> drive which if you're paranoid, you might want to obfuscate) but otherwise
> it'll be a text file which is similar to the data in System Profiler. (You
> could probably get the info from the graphical system profiler and doing a
> 'copy' on the appropriate bit too ... click on 'About this Mac' and then
> 'More Info' to launch the GUI one)
>
> Alex
>
>
> FireWire:
>
>    FireWire Bus:
>
>      Maximum Speed: Up to 400 Mb/sec
>
>        Rugged FW/USB:
>
>          Manufacturer: LaCie
>          Model: 0x0
>          GUID: **********
>          Maximum Speed: Up to 800 Mb/sec
>          Connection Speed: Up to 400 Mb/sec
>          Sub-units:
>            Rugged FW/USB Unit:
>              Unit Software Version: 0x10483
>              Unit Spec ID: 0x609E
>              Firmware Revision: 0x110
>              Product Revision Level:
>              Sub-units:
>                Rugged FW/USB SBP-LUN:
>                  Capacity: 465.76 GB
>                  Removable Media: Yes
>                  BSD Name: disk2
>                  Mac OS 9 Drivers: No
>                  Partition Map Type: GPT (GUID Partition Table)
>                  S.M.A.R.T. status: Not Supported
>                  Volumes:
>                    Data:
>                      Capacity: 465.44 GB
>                      Writable: Yes
>                      File System: ZFS
>                      BSD Name: disk2s2
>                      Mount Point:
>                  Volumes:
>                    disk2s2:
>                      Capacity: 465.44 GB
>                      Writable: Yes
>                      File System: ZFS
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091011/6759ecf6/attachment.html>

From developer at mvasilakis.com  Mon Oct 12 12:38:38 2009
From: developer at mvasilakis.com (mano vasilakis)
Date: Mon, 12 Oct 2009 15:38:38 -0400
Subject: [zfs-discuss] Pool gone after Kernel Panic
In-Reply-To: <2bfa7a9b0910111611x3dccc113l5dc12b5da1f1a73d@mail.gmail.com>
References: <2bfa7a9b0910111035k55565f2coa1a948e13fed1946@mail.gmail.com>
	<9A610B49-404C-4218-B369-337B7F4D2E04@gmail.com>
	<2bfa7a9b0910111245p7208c32g84764473063dee02@mail.gmail.com>
	<9C3BC70B-2F9B-4AFE-9619-4345EFA66B43@gmail.com>
	<2bfa7a9b0910111611x3dccc113l5dc12b5da1f1a73d@mail.gmail.com>
Message-ID: <2bfa7a9b0910121238rc30a208s6a473e09e6bb7686@mail.gmail.com>

Ok when I attach the drive using: zpool import -f Backup
The drive becomes accessible but will not mount to desktop. Finder gives the
message that this version will not mount but the drive is accessible via
command line. (Is this because I did a zpool upgrade? I'm using v119 and I
built it myself using the instructions on the website.) I can even access
files and open them under finder, I just cant open the directories that
contain the files in Finder.

FWIW after a zpool scrub I got 100,000+ errors. Nice. No wonder I was
loosing my backups.

I am thinking of getting a DROBO enclosure to use in the next month or two.
Till then I want to play with ZFS to see what a worst case scenario looks
like. And to tell you the truth I am highly impressed that this file system
has found errors where they were not apparent to Disk Warrior until after a
week or two of use. I don't know much about the underpinnings of file
systems but one that self repairs pretty cool. (I guess I wouldn't be saying
that if I worked for a file system utility company.)

Thanks again for for your time.
Mano

On Sun, Oct 11, 2009 at 7:11 PM, mano vasilakis <developer at mvasilakis.com>wrote:

> It's an older enclosure I picked up at OWC as on open box. I had to flash
> it to get it to recognize over 2TB and I think that RAIDON may not have
> fully developed the drivers.
> Anyway that is why my suspicions are with the enclosure and not the drives.
> I will order a SATA to USB cable and see if I see errors with these drives
> individually but I'm fairly certain I blew my money on the enclosure. (once
> in the enclosure the drives are viewable to the FW bus as one drive, that's
> why I referenced it in my original post as one drive, to keep from confusing
> anyone.)
>
> Enclosure:
> Stardom 7610 4 Bay Rack Mount SATA-II Removable HD Enclosure with
> eSATA/FW800/USB 2.0
>
> FireWire:
>
>     FireWire Bus:
>
>       Maximum Speed: Up to 800 Mb/sec
>
>         Built-in Hub:
>
>           Maximum Speed: Up to 800 Mb/sec
>           Connection Speed: Up to 800 Mb/sec
>
>         Oxford IDE Device0:
>
>           Manufacturer: Oxford Semiconductor Ltd.
>           Model: 0x0
>           GUID: ***********
>           Maximum Speed: Up to 800 Mb/sec
>           Connection Speed: Up to 800 Mb/sec
>           Sub-units:
>             Oxford IDE Device0 Unit:
>               Unit Software Version: 0x10483
>               Unit Spec ID: 0x609E
>               Firmware Revision: 0x100
>               Product Revision Level: V.70
>               Sub-units:
>                 Oxford IDE Device0 SBP-LUN:
>                   Capacity: 4 TB (4,000,819,445,760 bytes)
>                   Removable Media: Yes
>                   BSD Name: disk4
>                   Partition Map Type: GPT (GUID Partition Table)
>                   S.M.A.R.T. status: Not Supported
>                   Volumes:
>                     ZFS File System:
>                       Capacity: 4 TB (4,000,475,471,872 bytes)
>                       Writable: Yes
>                       File System: ZFS (Not Mountable)
>                       BSD Name: disk4s2
>
>
> On Sun, Oct 11, 2009 at 4:14 PM, Alex Blewitt <alex.blewitt at gmail.com>wrote:
>
>> On Oct 11, 2009, at 20:45, mano vasilakis wrote:
>>
>>  I doubt the disk is hosed. I think rather the enclosure may be the issue.
>>> When I looked and saw quite a few people having issues with Time Machine and
>>> firewire backup volumes I figured it was possible that TM was corrupting
>>> something. And with 10.6 so new well...
>>>
>>> I'll deal with the manufacturer and see if they can't help me. (Long
>>> story there) If not I guess I need to buy a new enclosure.
>>> Thanks.
>>>
>>
>> Certainly something is suspect :-) There were, back in the dim and distant
>> past, problems with Oxford FireWire chipsets but those flavour of problems
>> were consigned to the 10.1/10.2 days. I've not seen any problems with
>> FireWire drives since then. I did see problems with cheapo external USB
>> drives. So it's possible that it's the controller/enclosure, but unless
>> you've got a way of testing the drive in different enclosures to find out,
>> it's a bit difficult to know.
>>
>> There's a small possibility that the make/model of your firewire
>> drive/enclosure might help others (Apple?) on this list, though they might
>> not want to ask for it in case it gives the game away. If you run
>> system_profiler | pbcopy, you'll get a diagnostic of what's on your system.
>> Search for FireWire:, which will be the start of your firewire drives, and
>> then you can cut out the rest of the stuff in a mail, e.g. mine as below.
>> You'll probably want to sanity check it to confirm it doesn't have any
>> personally identifiable information in it (it will have the GUID of the
>> drive which if you're paranoid, you might want to obfuscate) but otherwise
>> it'll be a text file which is similar to the data in System Profiler. (You
>> could probably get the info from the graphical system profiler and doing a
>> 'copy' on the appropriate bit too ... click on 'About this Mac' and then
>> 'More Info' to launch the GUI one)
>>
>> Alex
>>
>>
>> FireWire:
>>
>>    FireWire Bus:
>>
>>      Maximum Speed: Up to 400 Mb/sec
>>
>>        Rugged FW/USB:
>>
>>          Manufacturer: LaCie
>>          Model: 0x0
>>          GUID: **********
>>          Maximum Speed: Up to 800 Mb/sec
>>          Connection Speed: Up to 400 Mb/sec
>>          Sub-units:
>>            Rugged FW/USB Unit:
>>              Unit Software Version: 0x10483
>>              Unit Spec ID: 0x609E
>>              Firmware Revision: 0x110
>>              Product Revision Level:
>>              Sub-units:
>>                Rugged FW/USB SBP-LUN:
>>                  Capacity: 465.76 GB
>>                  Removable Media: Yes
>>                  BSD Name: disk2
>>                  Mac OS 9 Drivers: No
>>                  Partition Map Type: GPT (GUID Partition Table)
>>                  S.M.A.R.T. status: Not Supported
>>                  Volumes:
>>                    Data:
>>                      Capacity: 465.44 GB
>>                      Writable: Yes
>>                      File System: ZFS
>>                      BSD Name: disk2s2
>>                      Mount Point:
>>                  Volumes:
>>                    disk2s2:
>>                      Capacity: 465.44 GB
>>                      Writable: Yes
>>                      File System: ZFS
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091012/1fccf39b/attachment.html>

From hanche at math.ntnu.no  Mon Oct 12 12:53:43 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Mon, 12 Oct 2009 15:53:43 -0400 (EDT)
Subject: [zfs-discuss] Snapshots under os x
In-Reply-To: <648B81BF-89D0-486B-B863-0FB376717853@gmail.com>
References: <110703.23107.qm@web88105.mail.re2.yahoo.com>
	<20090929.185651.174684793.hanche@math.ntnu.no>
	<648B81BF-89D0-486B-B863-0FB376717853@gmail.com>
Message-ID: <20091012.155343.21763656.hanche@math.ntnu.no>

+ Alex Blewitt <alex.blewitt at gmail.com>:

> On 29 Sep 2009, at 23:56, Harald Hanche-Olsen <hanche at math.ntnu.no>
> wrote:
> 
> > Also, you cannot send and receive snapshots to and from pipes. You
> > need to store them as files.
> 
> You can mkfifo a virtual file, get one zfs send > fifo and then zfs
> recv < fifo if you don't have storage space to store an entire
> snapshot. I believe the writing process will block after some small
> buffer though so the processes need to be concurrently running.

Thanks for this tip. I now have a great opportunity to test it out, as
I have just aquired a shiny new firewire disk and wish to move a pile
of zfs filesystems with all their snapshots off a usb disk onto the
firewire one.

I have come up with this procedure:

Have two open terminal windows, one labelled the receiver window and
the other the sender window. Run a root shell in both. First, create a
fifo (mkfifo /tmp/zsnap). In the receiver window, run a loop, each
time running  zfs receive TARGET-FS </tmp/zsnap; sleep 1.

In the sender window, first run  zfs send FIRST-SNAPSHOT >/tmp/zsnap.
When it's done, run

# zfs unmount TARGET-FS
# zfs rollback TARGET-FS at snapshotname

(otherwise, TARGET-FS tends to get modified, and the next receive will
fail).

Then, in the sender window, run a loop, each time running

  zfs send -i previous-snapshot next-snapshot && sleep 2


HOWEVER, once in a while, the zfs send command dies at the start with
broken pipe, all without anything untoward happening on the receiver
end. I inserted the sleep commands because they seem to make the
problem less severe, but it still happens. So I actually ended up
making the sender loop more complex still, trying each zfs send
command until it succeeds, up to three times (with a 2 second sleep
between). This seems to work well. I'd post the actual code I use, but
my shell is es, so that would only confuse the readership.


Is the broken pipe problem a known problem in OS X, though?
It seems that fifos just aren't all that reliable.

- Harald

From gadams at fgm.com  Tue Oct 13 11:33:31 2009
From: gadams at fgm.com (Adams, Geoff)
Date: Tue, 13 Oct 2009 14:33:31 -0400
Subject: [zfs-discuss] must watch.
In-Reply-To: <B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
	<20091010.145335.37374416.hanche@math.ntnu.no>
	<9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
	<4AD1195F.408@jrv.org>
	<B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>
	<4AD20179.3040803@jrv.org>
	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>
	<B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>
	<B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>
Message-ID: <3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>

On 11 Oct 2009, at 6:18 PM, Kona Blend wrote:

> That's my take on it too; the most recent transaction (or maybe  
> several) are sacrificed
> for sanity and the system is then pegged at the most recent  
> completed transaction point.

Bill also noted in the talk that this rollback won't be done  
automatically, since it does imply data loss. The pool goes into some  
degraded state, and then the user has to deal with it. That's where  
the user experience bits that they're still working on come in.

Another question comes to mind, after reading Dominic's comments: Is  
there any test we (users) can do to determine whether a given disk  
drive, or a disk drive together with its bus bridge, can be trusted to  
honor the write barrier sync commands? Apple clearly tests all  
internal drives as part of their qualification, but I use a lot of  
third party equipment, too. I'd like to know I haven't bought crap.

- Geoff
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091013/5ad92f41/attachment.html>

From james at themacplace.co.uk  Tue Oct 13 15:09:29 2009
From: james at themacplace.co.uk (James Relph)
Date: Tue, 13 Oct 2009 23:09:29 +0100
Subject: [zfs-discuss] must watch.
In-Reply-To: <3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
	<20091010.145335.37374416.hanche@math.ntnu.no>
	<9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
	<4AD1195F.408@jrv.org>
	<B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>
	<4AD20179.3040803@jrv.org>
	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>
	<B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>
	<B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>
	<3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>
Message-ID: <BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>

> Another question comes to mind, after reading Dominic's comments: Is
> there any test we (users) can do to determine whether a given disk
> drive, or a disk drive together with its bus bridge, can be trusted to
> honor the write barrier sync commands? Apple clearly tests all
> internal drives as part of their qualification, but I use a lot of
> third party equipment, too. I'd like to know I haven't bought crap.
>

There are SAS/SATA analysers that you can hook up to devices to see  
what commands are being sent, so you could use one to check to see  
whether a USB/Firewire bus is passing the commands.  They are  
astonishingly expensive though.  I think that the only way you could  
tell if an actual drive was supporting the commands would be to do  
some pretty hairy digging around in the drive's firmware, so even if  
you'd used your $30k SATA analyser and found a bridge that did support  
the commands, you might then hook a drive up that didn't.

Apple and Sun are both mentioned in various places (and obviously  
Dominic confirmed that as far as Apple are concerned) as requiring all  
their drives to support the command, so that's one way to guarantee  
you get ones that do.  I would guess that there's probably  
manufacturers selling "enterprise" quality drives that actually will  
tell you or mention on the spec sheet that they support their  
commands, but certainly it's not a commonly advertised feature.

It's difficult, and after looking into it it does highlight the  
importance of a filesystem that can work around drives or bridges that  
don't honour the commands.  Obviously for a company in Sun's position,  
where the hardware is generally a higher spec, and the admins are more  
likely to be specifically trained, it's probably a lesser problem.   
You have to wonder if this *is* the reason Apple pulled ZFS, you get a  
lot of Apple admins who do the admin side as a second job to being a  
teacher/studio manager etc., and they're probably much more likely to  
hook up a USB/Firewire drive to an Xserve as storage than your average  
Solaris admin.  Perhaps Apple decided they needed a better level of  
pool recovery before they gave ZFS their stamp of approval by  
including it with the OS.  Of course that doesn't explain why we  
couldn't have a newer build on MacOSforge, but short of storming  
Cupertino with pitchforks and flaming torches I doubt we'll find out  
soon.

Saying that, if someone wants to get some flaming torches sorted, I  
could probably drum up a few pitchforks and maybe the odd scythe :-)

James

Website:		www.themacplace.co.uk
Blog:		www.themacplace.co.uk/blog.html
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091013/b3ac301c/attachment.html>

From caronni at gmail.com  Tue Oct 13 16:03:43 2009
From: caronni at gmail.com (Germano Caronni)
Date: Wed, 14 Oct 2009 01:03:43 +0200
Subject: [zfs-discuss] must watch.
In-Reply-To: <BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com> 
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com> <4AD1195F.408@jrv.org>
	<B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>
	<4AD20179.3040803@jrv.org> 
	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>
	<B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk> 
	<B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>
	<3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com> 
	<BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
Message-ID: <327b821f0910131603h63936d37qa6a153d4f83fb6a9@mail.gmail.com>

>
> Saying that, if someone wants to get some flaming torches sorted, I could
> probably drum up a few pitchforks and maybe the odd scythe :-)
>

Just to change the tone slightly, maybe a couple of beers or a few nice
cakes would help. 'Angry mob' is only a short-term motivator ;-)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091014/364fa838/attachment.html>

From hanche at math.ntnu.no  Tue Oct 13 16:24:14 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Tue, 13 Oct 2009 19:24:14 -0400 (EDT)
Subject: [zfs-discuss] must watch.
In-Reply-To: <BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
References: <B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>
	<3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>
	<BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
Message-ID: <20091013.192414.39268800.hanche@math.ntnu.no>

+ James Relph <james at themacplace.co.uk>:

> You have to wonder if this *is* the reason Apple pulled ZFS

It's as good at theory as any, except it doesn't explain (or doesn't
seem to me to explain) why Apple isn't more open about it. It's their
silence more than anything that makes us come up with all sorts of
sinister explanations.

- Harald

From bryanhenry at mac.com  Tue Oct 13 16:29:09 2009
From: bryanhenry at mac.com (Bryan Henry)
Date: Tue, 13 Oct 2009 19:29:09 -0400
Subject: [zfs-discuss] must watch.
In-Reply-To: <20091013.192414.39268800.hanche@math.ntnu.no>
References: <B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>
	<3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>
	<BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
	<20091013.192414.39268800.hanche@math.ntnu.no>
Message-ID: <077F94FC-7A72-4BEE-93F6-CE9D83081834@mac.com>

Apple's culture is an explanation for the silence. Apple has never  
been known to make statements about or release details of unreleased  
and unannounced products. Core to Apple corporate culture is secrecy -  
there's not really anything sinister about it, that's just Apple for ya.

- Bryan

Sent from my iPhone

On Oct 13, 2009, at 7:24 PM, Harald Hanche-Olsen <hanche at math.ntnu.no>  
wrote:

> + James Relph <james at themacplace.co.uk>:
>
>> You have to wonder if this *is* the reason Apple pulled ZFS
>
> It's as good at theory as any, except it doesn't explain (or doesn't
> seem to me to explain) why Apple isn't more open about it. It's their
> silence more than anything that makes us come up with all sorts of
> sinister explanations.
>
> - Harald
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From hanche at math.ntnu.no  Tue Oct 13 16:59:57 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Tue, 13 Oct 2009 19:59:57 -0400 (EDT)
Subject: [zfs-discuss] must watch.
In-Reply-To: <077F94FC-7A72-4BEE-93F6-CE9D83081834@mac.com>
References: <BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
	<20091013.192414.39268800.hanche@math.ntnu.no>
	<077F94FC-7A72-4BEE-93F6-CE9D83081834@mac.com>
Message-ID: <20091013.195957.96809598.hanche@math.ntnu.no>

+ Bryan Henry <bryanhenry at mac.com>:

> Apple's culture is an explanation for the silence. Apple has never
> been known to make statements about or release details of unreleased
> and unannounced products.

I hope you're right. But in the case of ZFS they did talk about it,
then all of a sudden they clammed up. If it weren't for the former,
the latter wouldn't worry me so much.

- Harald

From james-zfsosx at jrv.org  Tue Oct 13 17:36:39 2009
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Tue, 13 Oct 2009 19:36:39 -0500
Subject: [zfs-discuss] must watch.
In-Reply-To: <BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>	<20091010.145335.37374416.hanche@math.ntnu.no>	<9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>	<4AD1195F.408@jrv.org>	<B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>	<4AD20179.3040803@jrv.org>	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>	<B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>	<B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>	<3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>
	<BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
Message-ID: <4AD51D17.10100@jrv.org>

James Relph wrote:
> I think that the only way you could tell if an actual drive was
> supporting the commands would be to do some pretty hairy digging
> around in the drive's firmware, so even if you'd used your $30k SATA
> analyser and found a bridge that did support the commands, you might
> then hook a drive up that didn't.

Testing is not impractical.  First mark the sectors on a disk with a
unique per-sector pattern.  Then write code that does the following:

1. A long write starting at block 0.
2. A one-sector write at the end of the disk.
3. A disk flush / write fence.
4. A one-sector write not far after the end of step 1.
5. An X microsecond delay.
6. ACPI power off.

This would have to be written tightly, going right to the hardware, and
with some trial & error in step 5 to determine "X".  But I think it will
show whether the firmware is tempted to violate the write fence in #3. 
The hard part might be finding a system that actually turned off power
quickly when the ACPI power bit is flipped so you can catch it doing one
of #2 & #4 before the other: you might have to solder a FET into the
SATA power cable instead.

> Apple and Sun are both mentioned in various places (and obviously
> Dominic confirmed that as far as Apple are concerned) as requiring all
> their drives to support the command, so that's one way to guarantee
> you get ones that do.  I would guess that there's probably
> manufacturers selling "enterprise" quality drives that actually will
> tell you or mention on the spec sheet that they support their
> commands, but certainly it's not a commonly advertised feature.

Also, the disk vendors can't be trusted.  As you near the day when the
supply contract is to be awarded the sales rep who's losing will
suddenly walk in the door one day with new, much faster disk drive
firmware.  What's in that firmware?  Why didn't they have the
twice-as-fast firmware the week before?  The sales rep is paid to win
that contract, not give honest answers, so you're not going to be told
what's wrong with it, just how much better it is than the competitor's.

Were I put in charge of a group to do a new FS my first act would be to
go to a flea market in a cheap part of town and buy up every
mostly-working disk I could find, and then require the devs to use only
those (but as many as they wished).  If you want a fault-tolerant FS,
develop it on faulty disks.

From toby at telegraphics.com.au  Tue Oct 13 17:41:53 2009
From: toby at telegraphics.com.au (Toby Thain)
Date: Tue, 13 Oct 2009 20:41:53 -0400
Subject: [zfs-discuss] must watch.
In-Reply-To: <BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
	<20091010.145335.37374416.hanche@math.ntnu.no>
	<9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
	<4AD1195F.408@jrv.org>
	<B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>
	<4AD20179.3040803@jrv.org>
	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>
	<B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>
	<B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>
	<3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>
	<BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
Message-ID: <9E6A5A5C-E123-48B0-8C28-B73F689D99C1@telegraphics.com.au>


On 13-Oct-09, at 6:09 PM, James Relph wrote:

>> Another question comes to mind, after reading Dominic's comments: Is
>> there any test we (users) can do to determine whether a given disk
>> drive, or a disk drive together with its bus bridge, can be  
>> trusted to
>> honor the write barrier sync commands? Apple clearly tests all
>> internal drives as part of their qualification, but I use a lot of
>> third party equipment, too. I'd like to know I haven't bought crap.
>>
>
> There are SAS/SATA analysers that you can hook up to devices to see  
> what commands are being sent, so you could use one to check to see  
> whether a USB/Firewire bus is passing the commands.  They are  
> astonishingly expensive though.  I think that the only way you  
> could tell if an actual drive was supporting the commands would be  
> to do some pretty hairy digging around in the drive's firmware, so  
> even if you'd used your $30k SATA analyser and found a bridge that  
> did support the commands, you might then hook a drive up that didn't.
>
> Apple and Sun are both mentioned in various places (and obviously  
> Dominic confirmed that as far as Apple are concerned) as requiring  
> all their drives to support the command, so that's one way to  
> guarantee you get ones that do.  I would guess that there's  
> probably manufacturers selling "enterprise" quality drives that  
> actually will tell you or mention on the spec sheet that they  
> support their commands, but certainly it's not a commonly  
> advertised feature.
>
> It's difficult, and after looking into it it does highlight the  
> importance of a filesystem that can work around drives or bridges  
> that don't honour the commands.

It is not clear that any such filesystem exists.

> Obviously for a company in Sun's position, where the hardware is  
> generally a higher spec, and the admins are more likely to be  
> specifically trained, it's probably a lesser problem.  You have to  
> wonder if this *is* the reason Apple pulled ZFS, you get a lot of  
> Apple admins who do the admin side as a second job to being a  
> teacher/studio manager etc., and they're probably much more likely  
> to hook up a USB/Firewire drive to an Xserve as storage than your  
> average Solaris admin.

No, because the issue applies equally to journaling filesystems (like  
HFS+).

--Toby

>  Perhaps Apple decided they needed a better level of pool recovery  
> before they gave ZFS their stamp of approval by including it with  
> the OS.  Of course that doesn't explain why we couldn't have a  
> newer build on MacOSforge, but short of storming Cupertino with  
> pitchforks and flaming torches I doubt we'll find out soon.
>
> Saying that, if someone wants to get some flaming torches sorted, I  
> could probably drum up a few pitchforks and maybe the odd scythe :-)
>
> James
>
> Website:		www.themacplace.co.uk
> Blog:		www.themacplace.co.uk/blog.html
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091013/a20a490e/attachment.html>

From james at themacplace.co.uk  Wed Oct 14 01:08:08 2009
From: james at themacplace.co.uk (James Relph)
Date: Wed, 14 Oct 2009 09:08:08 +0100
Subject: [zfs-discuss] must watch.
In-Reply-To: <9E6A5A5C-E123-48B0-8C28-B73F689D99C1@telegraphics.com.au>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>
	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>
	<20091010.145335.37374416.hanche@math.ntnu.no>
	<9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
	<4AD1195F.408@jrv.org>
	<B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>
	<4AD20179.3040803@jrv.org>
	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>
	<B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>
	<B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>
	<3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>
	<BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
	<9E6A5A5C-E123-48B0-8C28-B73F689D99C1@telegraphics.com.au>
Message-ID: <CF0B63A6-A224-4410-A10B-D1BF32B7A8C5@themacplace.co.uk>

> No, because the issue applies equally to journaling filesystems  
> (like HFS+).

The issue is the same, but journaled filesystems (including HFS+) at  
least have a pretty reliable method of recovery from the problem by  
replaying the journal.  Currently the same problem with ZFS basically  
leaves you with an inconsistent pool and a recovery option that boils  
down to "I hope your backups have been working".

James

Website:		www.themacplace.co.uk
Blog:		www.themacplace.co.uk/blog.html

From james at themacplace.co.uk  Wed Oct 14 01:11:14 2009
From: james at themacplace.co.uk (James Relph)
Date: Wed, 14 Oct 2009 09:11:14 +0100
Subject: [zfs-discuss] must watch.
In-Reply-To: <327b821f0910131603h63936d37qa6a153d4f83fb6a9@mail.gmail.com>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
	<4AD1195F.408@jrv.org>
	<B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>
	<4AD20179.3040803@jrv.org>
	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>
	<B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>
	<B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>
	<3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>
	<BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
	<327b821f0910131603h63936d37qa6a153d4f83fb6a9@mail.gmail.com>
Message-ID: <B9B527F8-0BB8-4EFE-B246-E427A22688DA@themacplace.co.uk>

On 14 Oct 2009, at 00:03, Germano Caronni wrote:

> Saying that, if someone wants to get some flaming torches sorted, I  
> could probably drum up a few pitchforks and maybe the odd scythe :-)
>
> Just to change the tone slightly, maybe a couple of beers or a few  
> nice cakes would help. 'Angry mob' is only a short-term motivator ;-)

I've tried various options with anyone at Apple I think would listen,  
with tones ranging from being nice to pleading.  Unfortunately it's  
not worked too well, so I'm falling back on my backup plan of fire and  
farming implements I'm afraid ;-)

James

Website:		www.themacplace.co.uk
Blog:		www.themacplace.co.uk/blog.html

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091014/e19d2013/attachment.html>

From alex.blewitt at gmail.com  Wed Oct 14 01:32:15 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Wed, 14 Oct 2009 09:32:15 +0100
Subject: [zfs-discuss] must watch.
In-Reply-To: <B9B527F8-0BB8-4EFE-B246-E427A22688DA@themacplace.co.uk>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
	<4AD1195F.408@jrv.org>
	<B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>
	<4AD20179.3040803@jrv.org>
	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>
	<B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>
	<B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>
	<3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>
	<BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
	<327b821f0910131603h63936d37qa6a153d4f83fb6a9@mail.gmail.com>
	<B9B527F8-0BB8-4EFE-B246-E427A22688DA@themacplace.co.uk>
Message-ID: <E47970FA-5D6F-47FE-B1CB-AA93F8A81423@gmail.com>

On 14 Oct 2009, at 09:11, James Relph <james at themacplace.co.uk> wrote:

> I've tried various options with anyone at Apple I think would listen

I'm sure they're listening, it's whether they (can) do anything about  
it. Perhaps the reason why it shipped without ZFS was to allow your  
entire home directory to be wiped out easier when using the Guest  
account?

Minor arguments about my-filesystem-is-better-than-your-filesystem  
aside, I couldn't go back to a world without snapshots, regardless of  
the hypothetical benefits of restoring from a crash. But the ability  
to (incrementally) back up my data to (multiple) hosts on external  
hard drives which last longer than 17 months and 17 days is something  
I'm prepared to change server-side OS for if it's not possible to get  
it on my preferred OS.

Alex 

From james at themacplace.co.uk  Wed Oct 14 03:01:33 2009
From: james at themacplace.co.uk (James Relph)
Date: Wed, 14 Oct 2009 11:01:33 +0100
Subject: [zfs-discuss] must watch.
In-Reply-To: <E47970FA-5D6F-47FE-B1CB-AA93F8A81423@gmail.com>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
	<4AD1195F.408@jrv.org>
	<B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>
	<4AD20179.3040803@jrv.org>
	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>
	<B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>
	<B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>
	<3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>
	<BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
	<327b821f0910131603h63936d37qa6a153d4f83fb6a9@mail.gmail.com>
	<B9B527F8-0BB8-4EFE-B246-E427A22688DA@themacplace.co.uk>
	<E47970FA-5D6F-47FE-B1CB-AA93F8A81423@gmail.com>
Message-ID: <81C287EE-9F78-4751-B7D9-89521FA58875@themacplace.co.uk>

> Perhaps the reason why it shipped without ZFS was to allow your  
> entire home directory to be wiped out easier when using the Guest  
> account?

Ouch!

> Minor arguments about my-filesystem-is-better-than-your-filesystem  
> aside, I couldn't go back to a world without snapshots, regardless  
> of the hypothetical benefits of restoring from a crash. But the  
> ability to (incrementally) back up my data to (multiple) hosts on  
> external hard drives which last longer than 17 months and 17 days is  
> something I'm prepared to change server-side OS for if it's not  
> possible to get it on my preferred OS.

Indeed, it's never particularly easy to deal with features being  
removed, especially when other systems can still take advantage of them.

It is the secrecy that's the most irritating though.  This isn't an  
Apple specific project and things like potential licensing/patent  
problems are all out in the open, so what is the benefit of keeping it  
hushed up?  They could come out and say "it's not ready because of x"  
or "we're waiting for the NetApp problem or Oracle merger to be  
sorted" and it would at least answer the questions.  Even "it's gone,  
never coming back" would let everyone know where they stand.

The only thing that would make the secrecy make sense would be:

1.	They're developing their own next-gen filesystem.
2.	They wanted to officially release ZFS along with some hardware to  
take advantage of it, which isn't ready yet.

I'm not convinced that either of those are particularly likely though.

James

Website:		www.themacplace.co.uk
Blog:		www.themacplace.co.uk/blog.html


From james-zfsosx at jrv.org  Wed Oct 14 03:11:46 2009
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Wed, 14 Oct 2009 05:11:46 -0500
Subject: [zfs-discuss] must watch.
In-Reply-To: <CF0B63A6-A224-4410-A10B-D1BF32B7A8C5@themacplace.co.uk>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>	<77DF43AB-65CE-43BD-BA69-6B0F94BEF81B@apple.com>	<8E3B2409-63A9-4EE3-BC13-853A0FF9F032@themacplace.co.uk>	<20091010.145335.37374416.hanche@math.ntnu.no>	<9FCADA84-EC35-4217-953E-E02EF9CEBE68@themacplace.co.uk>	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>	<4AD1195F.408@jrv.org>	<B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>	<4AD20179.3040803@jrv.org>	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>	<B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>	<B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>	<3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>	<BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>	<9E6A5A5C-E123-48B0-8C28-B73F689D99C1@telegraphics.com.au>
	<CF0B63A6-A224-4410-A10B-D1BF32B7A8C5@themacplace.co.uk>
Message-ID: <4AD5A3E2.4000708@jrv.org>

James Relph wrote:
> The issue is the same, but journaled filesystems (including HFS+) at
> least have a pretty reliable method of recovery from the problem by
> replaying the journal.  Currently the same problem with ZFS basically
> leaves you with an inconsistent pool and a recovery option that boils
> down to "I hope your backups have been working".

The problems people see are not related to the journal / log etc but
rather to the transactional "never update in place" nature of ZFS. 
Journals and logs are Red Herrings.

HFS or NTFS filesystems generally don't move around the superblocks,
filesystem metadata or even the top levels of the filesystem: these
objects are usually updated in place.  When the system crashes the few
things that may have changed are generally easily repaired by the log,
or buried deeply enough in the filesystem that the damage is localized.

ZFS is different because it uses a different "superblock" every few
seconds (every transaction commit), and more importantly, the top levels
of the filesystem and some pool metadata are moved too.  After every tx
commit the uberblock is in a different place and some of its pointers
are to different places.

Moreover, blocks that were freed by this process are rapidly reclaimed. 
The uberblock itself is not reclaimed for another 127 commits - several
minutes - but the things it points to are.  In other words as soon as tx
group N is committed, blocks from N-1 that are no longer referenced are
reclaimed as free space.

What goes wrong when the write fence / cache flush doesn't happen:

As soon as the uberblock for tx group N is written everything from N-1
that is no longer referenced is marked free for reallocation, and these
newly-freed blocks often contain part of the top levels of the N-1 pool
/ filesystems and metadata.

If the uberblock for N is _not_ written to media when it was supposed to
be then ZFS may happily reuse the blocks from N-1 while the uberblock
for N-1 is still the most recent on media, instead of N as ZFS expects. 
In other words there might be a window where the most recent uberblock
on disk media (N-1) points to a toplevel directory block that is
overwritten with unrelated data - disaster.

That window closes once uberblock N hits media.  Unfortunately with no
write fence it might be a long time before that happens.  The uberblocks
are near the ends of the disk and likely to require a longseek, and
elevator sort algorithms are likely to put off writing them.  If ZFS is
being used within a Virtual Machine then the disk drive may behave as
though the disk cache/buffer was hundreds of megabytes in size and the
window of vulnerability might be many seconds long (even extending into
the N+1 group!)

The fix is to delay reallocation of N-1's newly-unreferenced blocks. 
This way if uberblock N is not written when it is supposed to be at
least N-1 continues to point to valid data longer, and the window of
vulnerability goes away.

How long that reallocation delay needs to be is an interesting question
and may require some tuning.

The uberblock rollback feature should be unneeded.  If it is ever needed
that means that delayed reallocation failed and must be increased.  Once
delayed reallocation is ironed out I doubt that uberblock rollback will
work well in the real world: anything too severely broken for a long
delayed reallocation probably won't leave much behind for uberblock
rollback.

From hanche at math.ntnu.no  Wed Oct 14 07:30:59 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Wed, 14 Oct 2009 10:30:59 -0400 (EDT)
Subject: [zfs-discuss] must watch.
In-Reply-To: <4AD5A3E2.4000708@jrv.org>
References: <9E6A5A5C-E123-48B0-8C28-B73F689D99C1@telegraphics.com.au>
	<CF0B63A6-A224-4410-A10B-D1BF32B7A8C5@themacplace.co.uk>
	<4AD5A3E2.4000708@jrv.org>
Message-ID: <20091014.103059.162578695.hanche@math.ntnu.no>

+ "James R. Van Artsdalen" <james-zfsosx at jrv.org>:

> [snip]
> The fix is to delay reallocation of N-1's newly-unreferenced blocks.
> [...]

Thank you for this very illuminating and clear lecture. In particular
it makes it abundantly clear how my characterization of the fix (way
back in this thread) was an oversimplification at best. I think I
understand the issue much better now.

- Harald

From toby at telegraphics.com.au  Wed Oct 14 07:47:34 2009
From: toby at telegraphics.com.au (Toby Thain)
Date: Wed, 14 Oct 2009 10:47:34 -0400
Subject: [zfs-discuss] must watch.
In-Reply-To: <81C287EE-9F78-4751-B7D9-89521FA58875@themacplace.co.uk>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
	<4AD1195F.408@jrv.org>
	<B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>
	<4AD20179.3040803@jrv.org>
	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>
	<B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>
	<B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>
	<3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>
	<BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
	<327b821f0910131603h63936d37qa6a153d4f83fb6a9@mail.gmail.com>
	<B9B527F8-0BB8-4EFE-B246-E427A22688DA@themacplace.co.uk>
	<E47970FA-5D6F-47FE-B1CB-AA93F8A81423@gmail.com>
	<81C287EE-9F78-4751-B7D9-89521FA58875@themacplace.co.uk>
Message-ID: <054879BE-0F48-4CD5-89AA-D34EF57FA054@telegraphics.com.au>


On 14-Oct-09, at 6:01 AM, James Relph wrote:

>> Perhaps the reason why it shipped without ZFS was to allow your  
>> entire home directory to be wiped out easier when using the Guest  
>> account?
>
> Ouch!
>
>> Minor arguments about my-filesystem-is-better-than-your-filesystem  
>> aside, I couldn't go back to a world without snapshots, regardless  
>> of the hypothetical benefits of restoring from a crash. But the  
>> ability to (incrementally) back up my data to (multiple) hosts on  
>> external hard drives which last longer than 17 months and 17 days  
>> is something I'm prepared to change server-side OS for if it's not  
>> possible to get it on my preferred OS.
>
> Indeed, it's never particularly easy to deal with features being  
> removed, especially when other systems can still take advantage of  
> them.
>
> It is the secrecy that's the most irritating though.  This isn't an  
> Apple specific project and things like potential licensing/patent  
> problems are all out in the open, so what is the benefit of keeping  
> it hushed up?  They could come out and say "it's not ready because  
> of x" or "we're waiting for the NetApp problem or Oracle merger to  
> be sorted" and it would at least answer the questions.  Even "it's  
> gone, never coming back" would let everyone know where they stand.

Not Steve's style, unfortunately.

>
> The only thing that would make the secrecy make sense would be:
>
> 1.	They're developing their own next-gen filesystem.

In my wild-ass opinion, this is likely, although Dominic would  
undoubtedly be the lead, and he'd be in stealth mode rather than  
posting on the list; or, perhaps it's a distraction ploy!

--Toby

> 2.	They wanted to officially release ZFS along with some hardware  
> to take advantage of it, which isn't ready yet.
>
> I'm not convinced that either of those are particularly likely though.
>
> James
>
> Website:		www.themacplace.co.uk
> Blog:		www.themacplace.co.uk/blog.html
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From jasonbelec at rogers.com  Wed Oct 14 07:49:34 2009
From: jasonbelec at rogers.com (Jason Belec)
Date: Wed, 14 Oct 2009 07:49:34 -0700 (PDT)
Subject: [zfs-discuss] must watch.
Message-ID: <535362.97494.qm@web88106.mail.re2.yahoo.com>

I would also like to thank those sharing such useful information. It is always appreciated when trying to understand somthing, having several views helps redirect ones thinking.

Jason

Sent from my iPhone

On 2009-10-14, at 10:30 AM, Harald Hanche-Olsen <hanche at math.ntnu.no> wrote:

+ "James R. Van Artsdalen" <james-zfsosx at jrv.org>:

[snip]
The fix is to delay reallocation of N-1's newly-unreferenced blocks.
[...]

Thank you for this very illuminating and clear lecture. In particular
it makes it abundantly clear how my characterization of the fix (way
back in this thread) was an oversimplification at best. I think I
understand the issue much better now.

- Harald
_______________________________________________
zfs-discuss mailing list
zfs-discuss at lists.macosforge.org
http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss



From aucl at me.com  Wed Oct 14 08:34:35 2009
From: aucl at me.com (Clemens Auer)
Date: Wed, 14 Oct 2009 19:34:35 +0400
Subject: [zfs-discuss] Has anyone had any luck with ZFS 119 or the
 10A286 binaries in SL 64-bit mode?
In-Reply-To: <DD807C46-9D6E-4FE4-B2B1-B8F3C278EE5E@kargapoltsevamoon.com>
References: <DD807C46-9D6E-4FE4-B2B1-B8F3C278EE5E@kargapoltsevamoon.com>
Message-ID: <4B629017-0B13-4C82-AF76-E5C561BBAAF5@me.com>

I am not sure if this answers the question, but in system profiler I  
read:

zfs:

   Version:	8.0
   Last Modified:	7/13/09 8:14 PM
   Kind:	Intel
   Architectures:	i386, x86_64
   64-Bit (Intel):	Yes
   Location:	/System/Library/Extensions/zfs.kext
   Kext Version:	8.0
   Load Address:	0x74036000
   Valid:	Yes
   Authentic:	Yes
   Dependencies:	Satisfied

It is 119 self compiled against Snow Leopard SDK.
And even if the binary supports i386 and x86_64 I believe it is  
running in 64bit mode.

Works fine, but the only pool I have is made from 2*1,5TB USB disks  
mirrored.


---
Clemens Auer
see also: www.aucl.net



On Sep 16, 2009, at 5:17 AM, Alvin Moon wrote:

> I'm using the 10A286 binaries in SL 32-bit mode with no real  
> problems (the only problem is that I am always reminded on boot-up  
> that SL can't read ZFS formatted disks -- despite which, SL fully  
> recognizes my pool)._______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091014/b094f9f8/attachment.html>

From richard.elling at gmail.com  Wed Oct 14 09:10:13 2009
From: richard.elling at gmail.com (Richard Elling)
Date: Wed, 14 Oct 2009 09:10:13 -0700
Subject: [zfs-discuss] must watch.
In-Reply-To: <535362.97494.qm@web88106.mail.re2.yahoo.com>
References: <535362.97494.qm@web88106.mail.re2.yahoo.com>
Message-ID: <23E65F29-BBD2-4E53-B33C-7636CAF6AAA7@gmail.com>

On Oct 14, 2009, at 7:49 AM, Jason Belec wrote:

> I would also like to thank those sharing such useful information. It  
> is always appreciated when trying to understand somthing, having  
> several views helps redirect ones thinking.

<shameless_plug>

I cover this, to some extent, in the ZFS tutorial I've prepared for the
USENIX LISA09 conference in Baltimore next month. USENIX is a
good organization, and I encourage everyone to attend, even if you
don't attend the ZFS tutorial :-)
http://www.usenix.org/events/lisa09/

  -- richard


From james at themacplace.co.uk  Wed Oct 14 10:06:56 2009
From: james at themacplace.co.uk (James Relph)
Date: Wed, 14 Oct 2009 18:06:56 +0100
Subject: [zfs-discuss] must watch.
In-Reply-To: <054879BE-0F48-4CD5-89AA-D34EF57FA054@telegraphics.com.au>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<DF65EB55-1B27-423F-9073-5D25CA32A7BA@apple.com>
	<4AD1195F.408@jrv.org>
	<B8A2872A-C8D0-400F-9885-21BBDF6722F7@apple.com>
	<4AD20179.3040803@jrv.org>
	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>
	<B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>
	<B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>
	<3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>
	<BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
	<327b821f0910131603h63936d37qa6a153d4f83fb6a9@mail.gmail.com>
	<B9B527F8-0BB8-4EFE-B246-E427A22688DA@themacplace.co.uk>
	<E47970FA-5D6F-47FE-B1CB-AA93F8A81423@gmail.com>
	<81C287EE-9F78-4751-B7D9-89521FA58875@themacplace.co.uk>
	<054879BE-0F48-4CD5-89AA-D34EF57FA054@telegraphics.com.au>
Message-ID: <335716ED-870B-4EE2-A817-2AC4CAD0062C@themacplace.co.uk>

> In my wild-ass opinion, this is likely, although Dominic would  
> undoubtedly be the lead, and he'd be in stealth mode rather than  
> posting on the list; or, perhaps it's a distraction ploy!

It would have been a strange decision to have ZFS in SL server up  
until June and then suddenly decide to build their own from scratch.   
I guess there would be advantages in building their own, especially in  
making it look to apps much more like HFS+ for compatibility, but I  
would have thought it was lots easier just to go with ZFS.  Apple have  
done pretty well with open source recently.

We could always ask Dominic directly, and if he *doesn't* respond,  
perhaps he is working on one.

Or not  :-)

James

Website:		www.themacplace.co.uk
Blog:		www.themacplace.co.uk/blog.html


From kona8lend at gmail.com  Wed Oct 14 11:45:10 2009
From: kona8lend at gmail.com (Kona Blend)
Date: Wed, 14 Oct 2009 14:45:10 -0400
Subject: [zfs-discuss] zfs-10a286 patch for x86_64 kernel
In-Reply-To: <DD807C46-9D6E-4FE4-B2B1-B8F3C278EE5E@kargapoltsevamoon.com>
References: <DD807C46-9D6E-4FE4-B2B1-B8F3C278EE5E@kargapoltsevamoon.com>
Message-ID: <FFC71CA9-3FCD-401E-A27B-52B948B0B9C4@gmail.com>

Hi, if anyone is interested in this (I've only done bare minimal testing
with a 1TB pool, reading existing pool created by zfs-119, and
creating/reading several 60GB files). Hope it helps....

file: zfs-10a286-x86_64.tar.bz2
size: 11604 bytes
md5: 0d8e1ea35ee53cde636b0354fdc7d6ee
download: http://www.mediafire.com/download.php?mrhtaytlztm

...included readme file:

*******************************************************************************
***
***  WARNING: READ BEFORE INSTALLING
***  WARNING: USE AT YOUR OWN RISK. NO WARRANTIES IMPLIED.
***
***  REQUIRED: Mac OS X 10.6.1
***  REQUIRED: x86_64 kernel (64-bit)
***  REQUIRED: zfs-10a286 binaries installed
***
***  If you are running 32-bit kernel, this patch is not required!
***
*******************************************************************************

This is a patch which exports 2 additional kernel (x86_64) symbols  
required
by zfs-10a286 (also known as zfs-154). The symbols are:

     _OSRuntimeFinalizeCPP
     _OSRuntimeInitializeCPP

To install: as root issue the following commands from the directory
containing this readme file:

     mkdir /BACKUP
     cp -R /System/Library/Extensions/System.kext/PlugIns/ 
Libkern.kext /BACKUP/.
     cp Libkern.kext/Libkern /System/Library/Extensions/System.kext/ 
PlugIns/Libkern.kext/.
     kextcache -z -system-prelinked-kernel -system-caches


MANIFEST
     Libkern.kext/ --> kernel exporter binary
     xnu.patch     --> patch for XNU sources to roll your own binary


GOING FORWARD
     If Apple updates Libkern.kext in the future your zfs.kext module  
will
     suddently stop loading. So a new binary will need to be built.
     The obvious workaround will be to boot 32-bit kernel.


--KonaBlend


From ryanwalklin at testtoast.com  Thu Oct 15 14:59:13 2009
From: ryanwalklin at testtoast.com (Ryan Walklin)
Date: Fri, 16 Oct 2009 10:59:13 +1300
Subject: [zfs-discuss] zfs-10a286 patch for x86_64 kernel
In-Reply-To: <mailman.25.1255615210.45897.zfs-discuss@lists.macosforge.org>
References: <mailman.25.1255615210.45897.zfs-discuss@lists.macosforge.org>
Message-ID: <D9BABEFF-B68E-4DB4-9944-00FE1F2C78C0@testtoast.com>


>
> This is a patch which exports 2 additional kernel (x86_64) symbols
> required
> by zfs-10a286 (also known as zfs-154). The symbols are:
>
>    _OSRuntimeFinalizeCPP
>    _OSRuntimeInitializeCPP
>
>

Great work KonaBlend, working fine on my 2TB RAID-Z under 10.6.1. Can  
finally run a 64-bit kernel!

-Ryan

From peter at bierman.com  Thu Oct 15 18:30:17 2009
From: peter at bierman.com (Peter Bierman)
Date: Thu, 15 Oct 2009 18:30:17 -0700
Subject: [zfs-discuss] must watch.
In-Reply-To: <81C287EE-9F78-4751-B7D9-89521FA58875@themacplace.co.uk>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>
	<B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>
	<B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>
	<3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>
	<BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
	<327b821f0910131603h63936d37qa6a153d4f83fb6a9@mail.gmail.com>
	<B9B527F8-0BB8-4EFE-B246-E427A22688DA@themacplace.co.uk>
	<E47970FA-5D6F-47FE-B1CB-AA93F8A81423@gmail.com>
	<81C287EE-9F78-4751-B7D9-89521FA58875@themacplace.co.uk>
Message-ID: <227944b50910151830j7bfdd5cbp5cfadcc372a576cb@mail.gmail.com>

On Wed, Oct 14, 2009 at 3:01 AM, James Relph <james at themacplace.co.uk> wrote:
> It is the secrecy that's the most irritating though. ?This isn't an Apple
> specific project and things like potential licensing/patent problems are all
> out in the open, so what is the benefit of keeping it hushed up?

First, Apple, with few very carefully controlled exceptions, does not
talk about future products. ZFS is not currently something they would
build a promotion around, so you aren't going to hear anything
official about it.

Second, Apple does not make public statements unless they are
promotional. Apple's employees occasionally skirt the edges of this,
but unless someone really wants to risk their career, no one in the
know is going to say why Apple doesn't do something.

Third, what benefit would it be to them to do so? Lots of people want
Apple to do things that benefit the person making the request. That's
silly. Apple, the corporation, only acts with self-interest. If you
want something official from Apple, make a _good_ argument about how
it will benefit Apple, and then some Apple employee can try to act on
that.

But you have to be brutally honest with yourself about #3. Sunshine
and good feelings won't cut it. Can you show a specific person or
group that would take specific action that would benefit Apple if they
clarified their ZFS plans?

-pmb

From james at themacplace.co.uk  Fri Oct 16 02:01:12 2009
From: james at themacplace.co.uk (James Relph)
Date: Fri, 16 Oct 2009 10:01:12 +0100
Subject: [zfs-discuss] must watch.
In-Reply-To: <227944b50910151830j7bfdd5cbp5cfadcc372a576cb@mail.gmail.com>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>
	<B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>
	<B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>
	<3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>
	<BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
	<327b821f0910131603h63936d37qa6a153d4f83fb6a9@mail.gmail.com>
	<B9B527F8-0BB8-4EFE-B246-E427A22688DA@themacplace.co.uk>
	<E47970FA-5D6F-47FE-B1CB-AA93F8A81423@gmail.com>
	<81C287EE-9F78-4751-B7D9-89521FA58875@themacplace.co.uk>
	<227944b50910151830j7bfdd5cbp5cfadcc372a576cb@mail.gmail.com>
Message-ID: <484B87AD-5F90-4B79-BBBF-9E122B200673@themacplace.co.uk>

> First, Apple, with few very carefully controlled exceptions, does not
> talk about future products. ZFS is not currently something they would
> build a promotion around, so you aren't going to hear anything
> official about it.

Except of course that ZFS isn't "their" project, it's already in use  
in production systems, it was released by Apple for Leopard and as  
part of Snow Leopard builds and they actually *did* promote it as part  
of Snow Leopard Server.  There is no need for the secrecy they'd have  
compared to a new iMac release for instance.

> But you have to be brutally honest with yourself about #3. Sunshine
> and good feelings won't cut it. Can you show a specific person or
> group that would take specific action that would benefit Apple if they
> clarified their ZFS plans?

Yes: Apple.  We've got a couple of FTSE clients who use Macs and this  
attitude quite seriously makes Apple look pretty amateurish compared  
to Dell, HP or Microsoft etc. when you're dealing with enterprise  
clients.  Large businesses don't like surprises and like to plan  
things, understandably, in advance.  If you're going to buy 500 new  
machines you don't want to do that a month before there's a  
significant price drop or a significant new technology.  You  
definitely, for instance, don't want to start testing a new filesystem  
which Apple say will be released with their latest server OS, only for  
that to disappear without a word from Apple.  Fortunately because  
we're used to things like this we do warn people beforehand not to  
count on a feature until it's officially released, but it doesn't look  
good.  Microsoft, HP and Dell are quite happy to go into businesses  
like this and discuss long-term roadmaps.  We may be used to Apple's  
secrecy, but speak to an enterprise CTO or asset manager about it and  
they literally find it quite hard to believe.

Apple obviously gets benefits from secrecy at the consumer level, but  
what benefit do they get when you're talking about a server technology  
that other systems are already using?  ZFS is more of a "large  
deployment" piece of tech yet Apple are treating it like the next  
update to the iPod nano.  As it is we're getting projects together for  
clients to go into planning for next year's budgets starting in  
April.  We don't know about Apple's plans for next-gen filesystems, so  
storage requirements are being met by Xsan in some cases, other SAN  
solutions or in some cases Solaris systems running ZFS shared out over  
iSCSI.  If we knew what Apple were up to with ZFS could have been a  
basis for some of those storage projects but as it is we're speccing  
alternate servers and systems *not from Apple* instead.  Even if Apple  
come out in 3 months and say "here it is, a first-rate enterprise  
level ZFS solution", then it's too late; the storage projects will  
already be halfway through budgeting and planning.

James

Website:		www.themacplace.co.uk
Blog:		www.themacplace.co.uk/blog.html
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091016/9ee694ed/attachment.html>

From rwalists at washdcmail.com  Fri Oct 16 05:18:20 2009
From: rwalists at washdcmail.com (rwalists at washdcmail.com)
Date: Fri, 16 Oct 2009 08:18:20 -0400
Subject: [zfs-discuss] must watch.
In-Reply-To: <227944b50910151830j7bfdd5cbp5cfadcc372a576cb@mail.gmail.com>
References: <03298DE7-2CF5-42F0-B54B-2CC13592CC62@chipmunk-app.com>
	<BA5F51FE-32E7-4E97-B517-CB29A141B3A5@apple.com>
	<B355A87B-3864-4832-A462-443D0BBC2D1D@themacplace.co.uk>
	<B83CDE6D-02BA-4D6D-9E01-486FBE0438B0@gmail.com>
	<3ABB62A6-3E16-49C9-9264-B7CF89069C7F@fgm.com>
	<BCEB2035-17C4-4B85-8E11-7FECC60EB5BB@themacplace.co.uk>
	<327b821f0910131603h63936d37qa6a153d4f83fb6a9@mail.gmail.com>
	<B9B527F8-0BB8-4EFE-B246-E427A22688DA@themacplace.co.uk>
	<E47970FA-5D6F-47FE-B1CB-AA93F8A81423@gmail.com>
	<81C287EE-9F78-4751-B7D9-89521FA58875@themacplace.co.uk>
	<227944b50910151830j7bfdd5cbp5cfadcc372a576cb@mail.gmail.com>
Message-ID: <2EE2905A-03CE-436E-AB23-0F8E1D517C2A@washdcmail.com>


On Oct 15, 2009, at 9:30 PM, Peter Bierman wrote:

> But you have to be brutally honest with yourself about #3. Sunshine
> and good feelings won't cut it. Can you show a specific person or
> group that would take specific action that would benefit Apple if they
> clarified their ZFS plans?

We have two data centers, each with (formerly) 5x XServe RAIDs serving  
purely as MySQL storage DAS boxes over fibre channel.  Right after the  
Promise Boxes came out we upgraded one to Promise SAS boxes, but we  
had been waiting for Snow Leopard and ZFS to consider what to do next.

When ZFS disappeared we got a Sun 7410 on try and buy and connected it  
to our XServes via NFS.  It was faster than the Promise boxes (for our  
internal MySQL benchmark scripts and sysbench) and so far it has  
sustained that speed advantage with 5 MySQL servers all accessing the  
same pool.  With that many servers (and Sun's discounting even to  
small customers...you should get a price quote) it is cheaper than the  
Promise boxes, faster and you get all the ZFS benefits for free.

While we were at it we got a Sun x2270 instead of XServes to try as  
MySQL heads.  Those were just as easy and cheaper after the discount  
(we got the x2270 with 48 GB RAM for less than we could get the XServe  
with 24 GB RAM).  That got us looking at VMWare and we started  
migrating things to VMs on ESXi (storage on the 7410) rather than  
hosting them on XServes.

So I'll admit we aren't a core market for Apple, and we aren't very  
big, but we formerly had 10x XServes with MySQL, 2-4 XServes for other  
things and 10x Apple DAS boxes which are now going to wind up 10-20x  
x2270s (MySQL and ESXi) and 2x 7410s.  We are very happy with it after  
five months.

I think your point is correct, but I do think in some cases Apple has  
hurt itself.  For some customers there are alternatives, and waiting  
for an unspecified (possibly infinite) time for Apple to match them  
isn't realistic.

--Ware

From developer at mvasilakis.com  Fri Oct 16 10:44:22 2009
From: developer at mvasilakis.com (mano vasilakis)
Date: Fri, 16 Oct 2009 13:44:22 -0400
Subject: [zfs-discuss] zfs-discuss Digest, Vol 22, Issue 18
In-Reply-To: <mailman.29.1255701604.15211.zfs-discuss@lists.macosforge.org>
References: <mailman.29.1255701604.15211.zfs-discuss@lists.macosforge.org>
Message-ID: <2bfa7a9b0910161044j30a2e69ak3f46e3f046c33df9@mail.gmail.com>

*------------------------------

Message: 2
Date: Thu, 15 Oct 2009 18:30:17 -0700
From: Peter Bierman <peter at bierman.com>
To: James Relph <james at themacplace.co.uk>
Cc: "zfs-discuss at lists.macosforge.org"
       <zfs-discuss at lists.macosforge.org>
Subject: Re: [zfs-discuss] must watch.
Message-ID:
       <227944b50910151830j7bfdd5cbp5cfadcc372a576cb at mail.gmail.com* *>
Content-Type: text/plain; charset=UTF-8

On Wed, Oct 14, 2009 at 3:01 AM, James Relph <james at themacplace.co.uk>
wrote:
> It is the secrecy that's the most irritating though. ?This isn't an Apple
> specific project and things like potential licensing/patent problems are
all
> out in the open, so what is the benefit of keeping it hushed up?

First, Apple, with few very carefully controlled exceptions, does not
talk about future products. ZFS is not currently something they would
build a promotion around, so you aren't going to hear anything
official about it.

Second, Apple does not make public statements unless they are
promotional. Apple's employees occasionally skirt the edges of this,
but unless someone really wants to risk their career, no one in the
know is going to say why Apple doesn't do something.

Third, what benefit would it be to them to do so? Lots of people want
Apple to do things that benefit the person making the request. That's
silly. Apple, the corporation, only acts with self-interest. If you
want something official from Apple, make a _good_ argument about how
it will benefit Apple, and then some Apple employee can try to act on
that.

But you have to be brutally honest with yourself about #3. Sunshine
and good feelings won't cut it. Can you show a specific person or
group that would take specific action that would benefit Apple if they
clarified their ZFS plans?

-pmb


------------------------------
*
That said why won't Apple at least update macosforge to the latest released
ZFS code released in the Snow Leopard Server Beta. (For those of us who do
not have a developer acct with beta access?)

Apple could very well benefit from the work that is done here to the code
they open source anyway. (Even if not directly. As witnessed with the
patch.)

I think most of the frustration is that there is no closure. We don't know
if Apple will release anything, we don't know if its worth the investment to
learn. We read the problems, some which have been addressed by later Solaris
ZFS builds and yet over here nothing.

Ok we didn't get it with SL. It didn't kill anyone so no harm, no foul. But
macosforge could have been update with the latest released beta build. A
statement could have been released, we had problems, its gone for now. Or
even better, hang on we'll throw it in a 10.6.x update and update the forge
as soon as we have what we want. (though stuff here doesn't need to be a
final build for us to play with. And heck Apple has a group of very willing
beta testers!)

Personally I waited to see ZFS delivered in SL before trying it because of
the bugs I read about with 119. I ended up trying it anyway to narrow down
an issue I was having with an enclosure and I fell in love with it. I mean
real time feedback on filesystem errors? Self repair? Snapshots? Dude! (And
that's not even scratching the surface.)

Anyway I can respect Apples policy/culture of secrecy. What is frustrating
is the secrecy when its really no necessary.

my .32? :)
Mano*
*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091016/44e2117e/attachment.html>

From nefar at otherware.org  Sun Oct 18 10:29:40 2009
From: nefar at otherware.org (Daniel)
Date: Sun, 18 Oct 2009 10:29:40 -0700
Subject: [zfs-discuss] zfs-10a286 ?
In-Reply-To: <3c72d6220910171207u3b6bfdebi78263d16ba457dab@mail.gmail.com>
References: <3c72d6220910171207u3b6bfdebi78263d16ba457dab@mail.gmail.com>
Message-ID: <3c72d6220910181029r3bc6b1e1t206c5439897f36d4@mail.gmail.com>

 hi, I see talk about zfs-10a286 source in the zfs-10a286 patch for x86_64
kernel thread. WHere can I download this code? Doesn't show up on the page.
Also, it would be really nice if someone could throw together what's
required to get ZFS working on Snow Leopard. I'm considering moving from my
FreeBSD server to osx, but as I have a lot of data on ZFS, the lack of ZFS
makes it cumbersome. -d

-- 
"America was founded by men who understood that the threat of domestic
tyranny is as great as any threat from abroad. If we want to be worthy of
their legacy, we must resist the rush toward ever-increasing state control
of our society. Otherwise, our own government will become a greater threat
to our freedoms than any foreign terrorist."
- Ron Paul, Texas Straight Talk, May 31, 2004
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091018/9f18eb07/attachment.html>

From alex.blewitt at gmail.com  Sun Oct 18 11:11:06 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Sun, 18 Oct 2009 19:11:06 +0100
Subject: [zfs-discuss] zfs-10a286 ?
In-Reply-To: <3c72d6220910181029r3bc6b1e1t206c5439897f36d4@mail.gmail.com>
References: <3c72d6220910171207u3b6bfdebi78263d16ba457dab@mail.gmail.com>
	<3c72d6220910181029r3bc6b1e1t206c5439897f36d4@mail.gmail.com>
Message-ID: <7AE613A5-62E2-40F7-8E3A-76D14377D3AD@gmail.com>

On 18 Oct 2009, at 18:29, Daniel <nefar at otherware.org> wrote:

>  hi, I see talk about zfs-10a286 source in the zfs-10a286 patch for  
> x86_64 kernel thread.
>
>  WHere can I download this code? Doesn't show up on the page.

It was never publicly released. It was only available under NDA in  
binary form; whoever leaked it and made it available has done so  
without Apple's consent. I don't support using it, even if Apple has  
publicly given up on thus project.

> Also, it would be really nice if someone could throw together what's  
> required to get ZFS working on Snow Leopard.

Word has it that the 119 bits work; I made a download available from  
my blog and some tutorials there. The instructions on the wiki still  
apply.

>  I'm considering moving from my FreeBSD server to osx, but as I have  
> a lot of data on ZFS, the lack of ZFS makes it cumbersome.

Don't move on to OSX Server - stay on FreeBSD. You're likely to get  
much better ongoing ZFS performance there: in addition, it wouldn't  
surprise me if the version that the 119 bits are capable of reading  
(8) is lower than the support in FreeBSD anyway, which means your pool  
may not even be loadable.

There is really very little to recommend OSX as a server over ant  
other OS mow (well, perhaps except Windows, but anyone who uses  
Windows as a server deserves whatever they get anyway). Many people I  
know have interpreted the lack of anything on this project as implying  
the death of ZFS on OSX to the extent that server installs are moving  
away from OSX and onto more reliable systems like FreeBSD. In any  
case, there's really nothing that OSX server can do that any other  
Unix box can do, and quite a lot it can't.

Alex
http://alblue.blogspot.com
@alblue 
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091018/fc034346/attachment.html>

From hanche at math.ntnu.no  Sun Oct 18 13:06:15 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Sun, 18 Oct 2009 16:06:15 -0400 (EDT)
Subject: [zfs-discuss] zfs-10a286 ?
In-Reply-To: <7AE613A5-62E2-40F7-8E3A-76D14377D3AD@gmail.com>
References: <3c72d6220910171207u3b6bfdebi78263d16ba457dab@mail.gmail.com>
	<3c72d6220910181029r3bc6b1e1t206c5439897f36d4@mail.gmail.com>
	<7AE613A5-62E2-40F7-8E3A-76D14377D3AD@gmail.com>
Message-ID: <20091018.160615.00168130.hanche@math.ntnu.no>

+ Alex Blewitt <alex.blewitt at gmail.com>:

> Don't move on to OSX Server - stay on FreeBSD. You're likely to get
> much better ongoing ZFS performance there: in addition, it wouldn't
> surprise me if the version that the 119 bits are capable of reading
> (8) is lower than the support in FreeBSD anyway, which means your pool
> may not even be loadable.

On FreeBSD 7.2:

#; zpool upgrade
This system is currently running ZFS version 6.

I think FreeBSD 8 (which is not yet released) runs ZFS version 13,
however. And there is ongoing work to move it up to ZFS version 19.

I agree with your assessment, though. Both freebsd and opensolaris
ought to be excellent choices for a server. Actually, I wonder if it
wouldn't be better for all parties, Apple included, if Apple stopped
trying to sell and support OS X on servers and instead worked to
develop software for other unix-based servers to support OS X users.

- Harald

From nefar at otherware.org  Sun Oct 18 18:07:44 2009
From: nefar at otherware.org (Daniel)
Date: Sun, 18 Oct 2009 18:07:44 -0700
Subject: [zfs-discuss] zfs-10a286 ?
In-Reply-To: <7AE613A5-62E2-40F7-8E3A-76D14377D3AD@gmail.com>
References: <3c72d6220910171207u3b6bfdebi78263d16ba457dab@mail.gmail.com>
	<3c72d6220910181029r3bc6b1e1t206c5439897f36d4@mail.gmail.com>
	<7AE613A5-62E2-40F7-8E3A-76D14377D3AD@gmail.com>
Message-ID: <3c72d6220910181807t54d768e9x4f735ed4aa8473e7@mail.gmail.com>

Am I getting the sense that people are giving up on ZFS  on osx?  FreeBSD as
a community has pulled it off without the backing of a company such as
Apple, so I'm curious of why the project can't be successful just on it's
own merit?


On Sun, Oct 18, 2009 at 11:11 AM, Alex Blewitt <alex.blewitt at gmail.com>wrote:

> On 18 Oct 2009, at 18:29, Daniel <nefar at otherware.org> wrote:
>
> hi, I see talk about zfs-10a286 source in the zfs-10a286 patch for x86_64
> kernel thread. WHere can I download this code? Doesn't show up on the page.
>
>
> It was never publicly released. It was only available under NDA in binary
> form; whoever leaked it and made it available has done so without Apple's
> consent. I don't support using it, even if Apple has publicly given up on
> thus project.
>
> Also, it would be really nice if someone could throw together what's
> required to get ZFS working on Snow Leopard.
>
>
> Word has it that the 119 bits work; I made a download available from my
> blog and some tutorials there. The instructions on the wiki still apply.
>
> I'm considering moving from my FreeBSD server to osx, but as I have a lot
> of data on ZFS, the lack of ZFS makes it cumbersome.
>
>
> Don't move on to OSX Server - stay on FreeBSD. You're likely to get much
> better ongoing ZFS performance there: in addition, it wouldn't surprise me
> if the version that the 119 bits are capable of reading (8) is lower than
> the support in FreeBSD anyway, which means your pool may not even be
> loadable.
> There is really very little to recommend OSX as a server over ant other OS
> mow (well, perhaps except Windows, but anyone who uses Windows as a server
> deserves whatever they get anyway). Many people I know have interpreted the
> lack of anything on this project as implying the death of ZFS on OSX to the
> extent that server installs are moving away from OSX and onto more reliable
> systems like FreeBSD. In any case, there's really nothing that OSX server
> can do that any other Unix box can do, and quite a lot it can't.
>
> Alex
> http://alblue.blogspot.com
> @alblue
>



-- 
"America was founded by men who understood that the threat of domestic
tyranny is as great as any threat from abroad. If we want to be worthy of
their legacy, we must resist the rush toward ever-increasing state control
of our society. Otherwise, our own government will become a greater threat
to our freedoms than any foreign terrorist."
- Ron Paul, Texas Straight Talk, May 31, 2004
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091018/1c661cf4/attachment.html>

From hanche at math.ntnu.no  Sun Oct 18 18:43:00 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Sun, 18 Oct 2009 21:43:00 -0400 (EDT)
Subject: [zfs-discuss] zfs-10a286 ?
In-Reply-To: <3c72d6220910181807t54d768e9x4f735ed4aa8473e7@mail.gmail.com>
References: <3c72d6220910181029r3bc6b1e1t206c5439897f36d4@mail.gmail.com>
	<7AE613A5-62E2-40F7-8E3A-76D14377D3AD@gmail.com>
	<3c72d6220910181807t54d768e9x4f735ed4aa8473e7@mail.gmail.com>
Message-ID: <20091018.214300.92726822.hanche@math.ntnu.no>

+ Daniel <nefar at otherware.org>:

> Am I getting the sense that people are giving up on ZFS on osx?

Nobody knows what to think, so naturally they jump in different
directions.

> FreeBSD as a community has pulled it off without the backing of a
> company such as Apple, so I'm curious of why the project can't be
> successful just on it's own merit?

I think the previous discussion has already answered this question,
but I'll try to summarize. First, no project has ever been successful
on its own merit alone. It always takes real people to do the work,
and they have to be motivated enough and have the means to do the
job. Second, it's probably easier to motivate people to develop for a
free OS than for a non-free one, especially if they aren't getting
paid. Third, FreeBSD has a well-functioning community of kernel
developers. There are people writing free software for the Mac (and
some of it is very good), but they write userland code mostly (or even
exclusively?). Fourth, hacking ZFS for OS X probably requires
knowledge of kernel internals that is hard to get if you don't work at
Apple. (Perhaps not impossible, just hard.) I think I'll stop there.

- Harald

From dorofeev at gmail.com  Sun Oct 18 19:08:45 2009
From: dorofeev at gmail.com (Andrei Dorofeev)
Date: Sun, 18 Oct 2009 19:08:45 -0700
Subject: [zfs-discuss] Can zfs-119 bits be safely used on Snow Leopard
	(32-bit)?
Message-ID: <a782ada90910181908h199d3884i30f2be0e7ddc81df@mail.gmail.com>

Hi all,

I was wondering how stable is the latest released zfs-119
build when running on Mac OS X v10.6.1?

Thanks,
-Andrei
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091018/b2f513fd/attachment-0001.html>

From hanche at math.ntnu.no  Sun Oct 18 19:31:12 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Sun, 18 Oct 2009 22:31:12 -0400 (EDT)
Subject: [zfs-discuss] Can zfs-119 bits be safely used on Snow Leopard
 (32-bit)?
In-Reply-To: <a782ada90910181908h199d3884i30f2be0e7ddc81df@mail.gmail.com>
References: <a782ada90910181908h199d3884i30f2be0e7ddc81df@mail.gmail.com>
Message-ID: <20091018.223112.150150048.hanche@math.ntnu.no>

+ Andrei Dorofeev <dorofeev at gmail.com>:

> I was wondering how stable is the latest released zfs-119
> build when running on Mac OS X v10.6.1?

I doubt that anybody really dares answer the question in the subject
line affirmatively, but nobody has reported any serious problems, and
several people have reported that it does work. (Personally, I haven't
upgraded yet since I only got my copy recently, and because I can't
afford risking any major disruption with my computer right now.)

That said, bear in mind that ZFS is far from trouble free even on
Leopard. Read the list archives for some of the things that can go
wrong. And always make sure you have a backup.

- Harald

From aucl at me.com  Sun Oct 18 20:23:05 2009
From: aucl at me.com (Clemens Auer)
Date: Mon, 19 Oct 2009 07:23:05 +0400
Subject: [zfs-discuss] Can zfs-119 bits be safely used on Snow
	Leopard	(32-bit)?
In-Reply-To: <a782ada90910181908h199d3884i30f2be0e7ddc81df@mail.gmail.com>
References: <a782ada90910181908h199d3884i30f2be0e7ddc81df@mail.gmail.com>
Message-ID: <C7DE26B1-FE57-4FE1-9CBF-2F66D6B43119@me.com>

Hey!

I am using zfs-119 on SL since the first day of SL. Since I have 2TB  
of data on the ZFS volume i had a motivation to get it work.
On day one there were not any descriptions how to do it, so i  
downloaded the zfs-119 source and compiled it for SL 32/64bit intel on  
my own.
I have it working in 64bit mode.
Actually i face only 3 problems which are all documented on the page  
as known bugs:
+ unplugging the disks without exporting the zpool reliable brings a  
screen of death (maybe thats why it was not included for stable use)
+ Finder shows file system as symbolic links
+ and emptying the trash is not really working, (have to do it on the  
command line)

So on big deal, but of course i would prefer to see ongoing support by  
apple. otherwise i will one day or another migrate to another solution.



---
Clemens Auer
see also: www.aucl.net



On Oct 19, 2009, at 6:08 AM, Andrei Dorofeev wrote:

> Hi all,
>
> I was wondering how stable is the latest released zfs-119
> build when running on Mac OS X v10.6.1?
>
> Thanks,
> -Andrei
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From aucl at me.com  Sun Oct 18 20:25:59 2009
From: aucl at me.com (Clemens Auer)
Date: Mon, 19 Oct 2009 07:25:59 +0400
Subject: [zfs-discuss] Can zfs-119 bits be safely used on Snow	Leopard
 (32-bit)?
In-Reply-To: <C7DE26B1-FE57-4FE1-9CBF-2F66D6B43119@me.com>
References: <a782ada90910181908h199d3884i30f2be0e7ddc81df@mail.gmail.com>
	<C7DE26B1-FE57-4FE1-9CBF-2F66D6B43119@me.com>
Message-ID: <1E9D23A5-3F89-45C1-BE6A-9D50653C3302@me.com>


On Oct 19, 2009, at 7:23 AM, Clemens Auer wrote:
> + unplugging the disks without exporting the zpool reliable brings a  
> screen of death (maybe thats why it was not included for stable use)
> + Finder shows file system as symbolic links
> + and emptying the trash is not really working, (have to do it on  
> the command line)


PS:
  If someone has a solution for some of this problems. please let me  
know.


From ryanwalklin at testtoast.com  Sun Oct 18 22:14:50 2009
From: ryanwalklin at testtoast.com (Ryan Walklin)
Date: Mon, 19 Oct 2009 18:14:50 +1300
Subject: [zfs-discuss] Can zfs-119 bits be safely used on Snow Leopard
	(32-bit)?
In-Reply-To: <mailman.6066.1255918128.77573.zfs-discuss@lists.macosforge.org>
References: <mailman.6066.1255918128.77573.zfs-discuss@lists.macosforge.org>
Message-ID: <D79E1D3D-F921-4515-BB9E-BEF9CD5C0578@testtoast.com>


> Hi all,
>
> I was wondering how stable is the latest released zfs-119
> build when running on Mac OS X v10.6.1?
>

Works for me, haven't had any problems on either 64 or 32 bit SL.  
(Volumes don't automount, but running mount -a in a  launchd script  
takes care of it.

Ryan

From lopez.on.the.lists at yellowspace.net  Tue Oct 20 03:22:33 2009
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Tue, 20 Oct 2009 12:22:33 +0200
Subject: [zfs-discuss] zfs-10a286 ?
In-Reply-To: <20091018.160615.00168130.hanche@math.ntnu.no>
References: <3c72d6220910171207u3b6bfdebi78263d16ba457dab@mail.gmail.com>
	<3c72d6220910181029r3bc6b1e1t206c5439897f36d4@mail.gmail.com>
	<7AE613A5-62E2-40F7-8E3A-76D14377D3AD@gmail.com>
	<20091018.160615.00168130.hanche@math.ntnu.no>
Message-ID: <13EAF01B-55D5-4AF9-AB77-949834499C20@yellowspace.net>


On 18.10.2009, at 22:06, Harald Hanche-Olsen wrote:

> + Alex Blewitt <alex.blewitt at gmail.com>:
>
>> Don't move on to OSX Server - stay on FreeBSD. You're likely to get
>> much better ongoing ZFS performance there: in addition, it wouldn't
>> surprise me if the version that the 119 bits are capable of reading
>> (8) is lower than the support in FreeBSD anyway, which means your  
>> pool
>> may not even be loadable.
>
> On FreeBSD 7.2:
>
> #; zpool upgrade
> This system is currently running ZFS version 6.
>
> I think FreeBSD 8 (which is not yet released) runs ZFS version 13,
> however. And there is ongoing work to move it up to ZFS version 19.

That is correct, FreeBSD 8 (due out soon) will have v13.
Also, it has just been labeled production ready (the advice
about ZFS being experimental has been removed).
However, ZFS version 13 has been also merged since some time
to FreeBSD 7-STABLE, so if you need it on the 7- branch
just csup/build your system to RELENG_7 and you'll be just fine.

> I agree with your assessment, though. Both freebsd and opensolaris
> ought to be excellent choices for a server. Actually, I wonder if it
> wouldn't be better for all parties, Apple included, if Apple stopped
> trying to sell and support OS X on servers and instead worked to
> develop software for other unix-based servers to support OS X users.

What I wonder about is, when Mac OS X will finally have a decent
file system? ZFS sure was worth a try (*probably* stopped by
the ORACLE/SUN merger licensing variables), but whatever
replacement will eventually arrive, imo it's really horribly overdue.
Especially after most of the system has been so elegantly optimized for
parallel multitasking.

HFS is really old and has some serious performance and reliability
drawbacks (the most famous being the Catalog File, which has to
be contended by any task updating the filesystem structure[1]).
Almost all spinning wheels I see on SL are related to
file system / volume handling.

I'm no Linu[xs] fan, but I think Linus Torvalds has a point with his
comments on HFS... [2]

Cheers,


Lorenzo

[1] http://en.wikipedia.org/wiki/Hierarchical_File_System#Problems
[2] http://www.smh.com.au/news/technology/torvalds-pans-apples-os-x/2008/02/05/1202090393959.html



From david299792 at googlemail.com  Tue Oct 20 04:24:36 2009
From: david299792 at googlemail.com (David Ritchie)
Date: Tue, 20 Oct 2009 12:24:36 +0100
Subject: [zfs-discuss] zfs-10a286 ?
In-Reply-To: <13EAF01B-55D5-4AF9-AB77-949834499C20@yellowspace.net>
References: <3c72d6220910171207u3b6bfdebi78263d16ba457dab@mail.gmail.com>
	<3c72d6220910181029r3bc6b1e1t206c5439897f36d4@mail.gmail.com>
	<7AE613A5-62E2-40F7-8E3A-76D14377D3AD@gmail.com>
	<20091018.160615.00168130.hanche@math.ntnu.no>
	<13EAF01B-55D5-4AF9-AB77-949834499C20@yellowspace.net>
Message-ID: <0764C062-1D67-457E-933B-5A5AA706FC2A@googlemail.com>

Hi all,

I've been following this list for some time and I, like many here I'm  
sure, was really hoping that ZFS would be much more usable in SL - at  
least with working Spotlight and .zfs snapshots directory. But alas.  
I've been using ZFS in Leopard for some time now, and on the whole it  
works very well and has been pretty stable since the latest release.

What I'd really like, in an ideal world, is of course to be able to  
boot and root from ZFS on my Mac(s) and have everything work as  
expected. Almost as good as that would be to have my home directory on  
ZFS. I haven't done that because spotlight doesn't work properly,  
which I use a lot.

At the moment I'm running most things on HFS and periodically rsyncing  
my home to the local ZFS pool and snapshotting. Some things - mainly  
VMWare vms - I run straight from the zpool.

I installed SL on my laptop to see what the ZFS support was like, and  
I've managed to get it working by downloading the 119 bits and putting  
in a launchd script to forcibly import the zpool, but this seems  
slightly worrying. Also, even though spotlight was working on the root  
zfs file system it all went a bit wrong - it just kept reindexing all  
the time - so I disabled it. On my laptop my home folder IS on ZFS,  
but that means no spotlight! I don't use the laptop that much though.

With all the uncertainty about the future of ZFS I'm starting to  
wonder about running a file server with Solaris/FreeBSD and storing  
stuff on that. I just set up OpenSolaris on a VM and compiled netatalk  
on it. I can mount the test AFP share just fine and it seems to work -  
I haven't tested performance at all. BUT spotlight doesn't work on it.

So now I'm wondering what to do:-

1. If I didn't need to run Mac OS X at all I'd probably just run  
Solaris or FreeBSD on my machine and be done with it, but I really DO  
want to use Mac OS X

2. I could set up a file server and access my home directory from it.  
What would performance be like? Can I get spotlight to work?

3. I could have a file server for backup, run my home directory and  
everything locally on HFS and just use the file server to access  
backups and snapshots - similar to what I'm doing now but with easier  
access to snapshots

4. I could carry on as I am, maybe not upgrade to SL yet. Not very  
convenient access to snapshots (unless I clone every snapshot)

5. I could use iSCSI to mount a ZFS backed disk image and store my  
home on that. That would give me spotlight and the reliability  
advantages of ZFS (scrubbing, checksums &c) but not very convenient  
access to snapshots - it would involve mounting a snapshot to look for  
stuff I presume

6. I could use a ZFS file server for Time machine backups and thus use  
TM instead of snapshots to get historical data (presumably it would  
use disk images). Currently I have another disk which TM is backing up  
to as well.


So on the whole it might be better, now, to migrate my main file store  
to another system and access it over GigE, but there just seem to be  
niggling issues with that. What are other peoples thoughts on this  
matter? I really like ZFS.


cheers,
-- David


On 20 Oct 2009, at 11:22, Lorenzo Perone wrote:

>
> On 18.10.2009, at 22:06, Harald Hanche-Olsen wrote:
>
>> + Alex Blewitt <alex.blewitt at gmail.com>:
>>
>>> Don't move on to OSX Server - stay on FreeBSD. You're likely to get
>>> much better ongoing ZFS performance there: in addition, it wouldn't
>>> surprise me if the version that the 119 bits are capable of reading
>>> (8) is lower than the support in FreeBSD anyway, which means your  
>>> pool
>>> may not even be loadable.
>>
>> On FreeBSD 7.2:
>>
>> #; zpool upgrade
>> This system is currently running ZFS version 6.
>>
>> I think FreeBSD 8 (which is not yet released) runs ZFS version 13,
>> however. And there is ongoing work to move it up to ZFS version 19.
>
> That is correct, FreeBSD 8 (due out soon) will have v13.
> Also, it has just been labeled production ready (the advice
> about ZFS being experimental has been removed).
> However, ZFS version 13 has been also merged since some time
> to FreeBSD 7-STABLE, so if you need it on the 7- branch
> just csup/build your system to RELENG_7 and you'll be just fine.
>
>> I agree with your assessment, though. Both freebsd and opensolaris
>> ought to be excellent choices for a server. Actually, I wonder if it
>> wouldn't be better for all parties, Apple included, if Apple stopped
>> trying to sell and support OS X on servers and instead worked to
>> develop software for other unix-based servers to support OS X users.
>
> What I wonder about is, when Mac OS X will finally have a decent
> file system? ZFS sure was worth a try (*probably* stopped by
> the ORACLE/SUN merger licensing variables), but whatever
> replacement will eventually arrive, imo it's really horribly overdue.
> Especially after most of the system has been so elegantly optimized  
> for
> parallel multitasking.
>
> HFS is really old and has some serious performance and reliability
> drawbacks (the most famous being the Catalog File, which has to
> be contended by any task updating the filesystem structure[1]).
> Almost all spinning wheels I see on SL are related to
> file system / volume handling.
>
> I'm no Linu[xs] fan, but I think Linus Torvalds has a point with his
> comments on HFS... [2]
>
> Cheers,
>
>
> Lorenzo
>
> [1] http://en.wikipedia.org/wiki/Hierarchical_File_System#Problems
> [2] http://www.smh.com.au/news/technology/torvalds-pans-apples-os-x/2008/02/05/1202090393959.html
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From lopez.on.the.lists at yellowspace.net  Tue Oct 20 04:57:47 2009
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Tue, 20 Oct 2009 13:57:47 +0200
Subject: [zfs-discuss] zfs-10a286 ?
In-Reply-To: <0764C062-1D67-457E-933B-5A5AA706FC2A@googlemail.com>
References: <3c72d6220910171207u3b6bfdebi78263d16ba457dab@mail.gmail.com>
	<3c72d6220910181029r3bc6b1e1t206c5439897f36d4@mail.gmail.com>
	<7AE613A5-62E2-40F7-8E3A-76D14377D3AD@gmail.com>
	<20091018.160615.00168130.hanche@math.ntnu.no>
	<13EAF01B-55D5-4AF9-AB77-949834499C20@yellowspace.net>
	<0764C062-1D67-457E-933B-5A5AA706FC2A@googlemail.com>
Message-ID: <1B0A04DD-5C61-4B26-9C9B-DF0F8637242C@yellowspace.net>


Hi David,

yep - we all here licked blood and now try hard to get some more...

On 20.10.2009, at 13:24, David Ritchie wrote:

> So now I'm wondering what to do:-
>
> 1. If I didn't need to run Mac OS X at all I'd probably just run  
> Solaris or FreeBSD on my machine and be done with it, but I really  
> DO want to use Mac OS X

same here... at least for the desktops

> 2. I could set up a file server and access my home directory from  
> it. What would performance be like? Can I get spotlight to work?

afaik, for spotlight to work on network shares, it has to be supported  
on the
server side (mac os x server does this since 10.5). afaik, netatalk does
not support spotlight yet. so this won't work unless you hack your local
spotlight to index network shares as well, if that's possible at all,  
since
I don't know if the kernel actually produces "file modified" events et  
al
for network shares (i think not)..

> 3. I could have a file server for backup, run my home directory and  
> everything locally on HFS and just use the file server to access  
> backups and snapshots - similar to what I'm doing now but with  
> easier access to snapshots

I'm doing it like this for a local fileserver: it rsyncs its stuff on  
a sparse
disk image which is located on a FreeBSD-backed ZFS pool and shared to  
the
Mac over NFS (a script mounts the sparse image from the shared volume,
makes the rsync, and detaches it again). The zfs dataset is  
snapshotted daily.
As you can probably imagine, performance **cks, 'cause it's really a  
layering
party - but still viable for nightly backups and way more reliable and  
compatible
than sharing the backup volume itself over NFS. to recover stuff I  
have to export
the .zfs snapshot to another NFS share and mount the image on it. Had  
the case a
few times,  it works without problems, just make sure you wrap it  
around a script
to make it more handy..

> 4. I could carry on as I am, maybe not upgrade to SL yet. Not very  
> convenient access to snapshots (unless I clone every snapshot)

Dunno about you, but despite it being fairly stable in my experience  
XFS-119 was
far from fast on my (RIP) zfs pools on mac os x...

> 5. I could use iSCSI to mount a ZFS backed disk image and store my  
> home on that. That would give me spotlight and the reliability  
> advantages of ZFS (scrubbing, checksums &c) but not very convenient  
> access to snapshots - it would involve mounting a snapshot to look  
> for stuff I presume

Since I was curious too, I tried this with a virtualbox running
nexenta and a sparse zvol on it shared over iscsi, formatted and mounted
on the mac.  Actually it surprised me: on a MacBook Pro 2.1GHz it still
copied files  with over 12MB/s sustained data rate - and that was from
virtualbox on the same machine to the mounted zvol and vice-versa.
So that sounds fine, it's just being dependant on an third party  
commercial
iSCSI initiator (i used the GlobalSAN one) that made me back up a
little... but I might be overcareful on that front - still I'd welcome
built-in OSX-iSCSI..

> 6. I could use a ZFS file server for Time machine backups and thus  
> use TM instead of snapshots to get historical data (presumably it  
> would use disk images). Currently I have another disk which TM is  
> backing up to as well.

I wouldn't do this - after all You're doing the same thing twice here ;)

> So on the whole it might be better, now, to migrate my main file  
> store to another system and access it over GigE, but there just seem  
> to be niggling issues with that. What are other peoples thoughts on  
> this matter? I really like ZFS.

For that matter, HAMMERfs (dragonflybsd) could even outperform it from  
some
points of view (snapshotting, replication and data retention  
policies)... ;)


Regards,

Lorenzo



From alex.blewitt at gmail.com  Wed Oct 21 01:46:46 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Wed, 21 Oct 2009 09:46:46 +0100
Subject: [zfs-discuss] New Mac Mini Server
Message-ID: <A55D6C31-5358-4BE2-ACB5-AA9DFA774A31@gmail.com>

Did anyone else see the announcement of the new Mac Mini server, and  
think "If only I could run ZFS on that..." ?

From james at themacplace.co.uk  Wed Oct 21 02:41:37 2009
From: james at themacplace.co.uk (James Relph)
Date: Wed, 21 Oct 2009 10:41:37 +0100
Subject: [zfs-discuss] New Mac Mini Server
In-Reply-To: <A55D6C31-5358-4BE2-ACB5-AA9DFA774A31@gmail.com>
References: <A55D6C31-5358-4BE2-ACB5-AA9DFA774A31@gmail.com>
Message-ID: <0A88B88B-DB1E-49EF-BCD0-255A6EB724E7@themacplace.co.uk>

> Did anyone else see the announcement of the new Mac Mini server, and  
> think "If only I could run ZFS on that..." ?

Yes.

...sigh...

Seriously this is driving me mad, it's the first time in about 4 years  
where I've looked at another OS and really, really wanted a feature  
that was missing from OS X.  Sure there's times when I want things  
like proper clustering of AFP, and a distributed filesystem, but  
they're generally the more infrequent requirements.  ZFS I could see  
being useful in *lots* of places.  I have this horrible rising dread  
that Apple are going to pop up in 12 months and say "See, you don't  
need ZFS, we've bolted snapshots onto the side of HFS+".

James

Website:		www.themacplace.co.uk
Blog:		www.themacplace.co.uk/blog.html


From hanche at math.ntnu.no  Wed Oct 21 04:10:59 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Wed, 21 Oct 2009 07:10:59 -0400 (EDT)
Subject: [zfs-discuss] New Mac Mini Server
In-Reply-To: <0A88B88B-DB1E-49EF-BCD0-255A6EB724E7@themacplace.co.uk>
References: <A55D6C31-5358-4BE2-ACB5-AA9DFA774A31@gmail.com>
	<0A88B88B-DB1E-49EF-BCD0-255A6EB724E7@themacplace.co.uk>
Message-ID: <20091021.071059.31186458.hanche@math.ntnu.no>

+ James Relph <james at themacplace.co.uk>:

> I have this horrible rising dread that Apple are going to pop up in
> 12 months and say "See, you don't need ZFS, we've bolted snapshots
> onto the side of HFS+".

Must stop reading this list in the morning.
I almost choked on my coffee.

- Harald

From peter at bierman.com  Wed Oct 21 04:33:51 2009
From: peter at bierman.com (Peter Bierman)
Date: Wed, 21 Oct 2009 04:33:51 -0700
Subject: [zfs-discuss] New Mac Mini Server
In-Reply-To: <20091021.071059.31186458.hanche@math.ntnu.no>
References: <A55D6C31-5358-4BE2-ACB5-AA9DFA774A31@gmail.com>
	<0A88B88B-DB1E-49EF-BCD0-255A6EB724E7@themacplace.co.uk>
	<20091021.071059.31186458.hanche@math.ntnu.no>
Message-ID: <227944b50910210433m72d4a8cwa53fa3322d76dc16@mail.gmail.com>

On Wed, Oct 21, 2009 at 4:10 AM, Harald Hanche-Olsen
<hanche at math.ntnu.no> wrote:
> + James Relph <james at themacplace.co.uk>:
>
>> I have this horrible rising dread that Apple are going to pop up in
>> 12 months and say "See, you don't need ZFS, we've bolted snapshots
>> onto the side of HFS+".
>
> Must stop reading this list in the morning.
> I almost choked on my coffee.


I know it's cliche, but everyone be sure to let Apple know your
desires via their feedback page:
http://www.apple.com/feedback/server.html

Apple's strategic decisions don't get much influence from mailing
lists, but feedback sent via those pages is tallied up and used to set
priorities.

-pmb

From david299792 at googlemail.com  Wed Oct 21 05:17:26 2009
From: david299792 at googlemail.com (David Ritchie)
Date: Wed, 21 Oct 2009 13:17:26 +0100
Subject: [zfs-discuss] zfs-10a286 ?
In-Reply-To: <869DD39E-3739-4908-8A50-11913FCBA5A0@amsi.org.au>
References: <3c72d6220910171207u3b6bfdebi78263d16ba457dab@mail.gmail.com>
	<3c72d6220910181029r3bc6b1e1t206c5439897f36d4@mail.gmail.com>
	<7AE613A5-62E2-40F7-8E3A-76D14377D3AD@gmail.com>
	<20091018.160615.00168130.hanche@math.ntnu.no>
	<13EAF01B-55D5-4AF9-AB77-949834499C20@yellowspace.net>
	<0764C062-1D67-457E-933B-5A5AA706FC2A@googlemail.com>
	<869DD39E-3739-4908-8A50-11913FCBA5A0@amsi.org.au>
Message-ID: <3A8DB67C-B2DE-43B0-9F6B-FE09D5294A2D@googlemail.com>


On 21 Oct 2009, at 06:25, Raoul Callaghan wrote:

>> 5. I could use iSCSI to mount a ZFS backed disk image and store my  
>> home on that. That would give me spotlight and the reliability  
>> advantages of ZFS (scrubbing, checksums &c) but not very convenient  
>> access to snapshots - it would involve mounting a snapshot to look  
>> for stuff I presume
>
>
> I set this up using OpenSolaris (0906) and successfully used a  
> MacPro to access the iscsi share.
> I wasn't wrapped with the 1MB/sec transfer speeds though!
>
> http://serverfault.com/questions/68837/opensolaris-iscsi-target-slow-read
> http://opensolaris.org/jive/thread.jspa;jsessionid=8AEB8CE2E1E0C35868C2DB1A3AA4E728?messageID=363859&#363859
> http://forums.sun.com/thread.jspa?threadID=5299072
> http://www.snsforums.com/index.php?showtopic=279
>
> I haven't tried Atto's initiator yet either, nor have I tried a  
> cross over cable to eliminate my L2 switch.
>
> Considering Apple's silence regarding ZFS, I've considered OS-0906  
> and use iscsi for my home setup.
> I guess I just have to wait for ZFS/iscsi to mature also.

I just installed OpenSolaris in VMWare to test iSCSI. I'm using the  
GlobalSAN free iSCSI initiator. I set up a zvol, set shareiscsi on.

It seems to work ok. I'm definitely getting better than 1MB/s - maybe  
30MB/s read and 10-13 write. When I do this the VMWare VM is running  
at just over 100% CPU use, so I assume it's maxing out 1 core (Mac  
Pro, quad 2.66GHz). CPU use on the client is low. Interestingly when I  
access the iSCSI target from the same machine (Mac Pro) I get the  
above 30/13MB/s read/write speed, but from my MacBook (running Snow  
Leopard) it's more like 17/7, which isn't so great. I don't know why  
that should be - is extra latency of going out to the gigabit ethernet  
slowing things down?

 From some of the links above (thanks) I can see that it could partly  
be due to this initiator and/or network gear.

Does anyone know what kind of speeds I can expect over GbE? 128MB/s  
would be nice! I'm guessing that since the VM is pegging the CPU  
there's something weird going on with virtualization and that if I ran  
it on real hardware it would be much faster? I'm wondering whether to  
build a server for this.

On the whole the iSCSI route seems a good way to go - it gives  
straightforward fully functional client access with ZFS backed  
storage. The only disadvantage being that accessing snapshots won't be  
as convenient, because I'd have to clone and mount the old zvols. I  
can use Time Machine for that I suppose.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091021/b8a14a0e/attachment.html>

From nathan.stocks at gmail.com  Wed Oct 21 07:39:26 2009
From: nathan.stocks at gmail.com (Nathan)
Date: Wed, 21 Oct 2009 08:39:26 -0600
Subject: [zfs-discuss] New Mac Mini Server
In-Reply-To: <227944b50910210433m72d4a8cwa53fa3322d76dc16@mail.gmail.com>
References: <A55D6C31-5358-4BE2-ACB5-AA9DFA774A31@gmail.com>
	<0A88B88B-DB1E-49EF-BCD0-255A6EB724E7@themacplace.co.uk>
	<20091021.071059.31186458.hanche@math.ntnu.no>
	<227944b50910210433m72d4a8cwa53fa3322d76dc16@mail.gmail.com>
Message-ID: <96c9d6a80910210739t8cd69f9o3f20b43f83d85814@mail.gmail.com>

On Wed, Oct 21, 2009 at 5:33 AM, Peter Bierman <peter at bierman.com> wrote:
> On Wed, Oct 21, 2009 at 4:10 AM, Harald Hanche-Olsen
> <hanche at math.ntnu.no> wrote:
>> + James Relph <james at themacplace.co.uk>:
>>
>>> I have this horrible rising dread that Apple are going to pop up in
>>> 12 months and say "See, you don't need ZFS, we've bolted snapshots
>>> onto the side of HFS+".
>>
>> Must stop reading this list in the morning.
>> I almost choked on my coffee.
>
>
> I know it's cliche, but everyone be sure to let Apple know your
> desires via their feedback page:
> http://www.apple.com/feedback/server.html
>
> Apple's strategic decisions don't get much influence from mailing
> lists, but feedback sent via those pages is tallied up and used to set
> priorities.
>
> -pmb

Thanks!  I don't think that's cliche at all -- I never knew about that
feedback page.  I went ahead and left feedback about how disappointed
I was at the absence of ZFS in Snow Leopard server and how I've bought
Solaris and FreeBSD boxes instead of additional Xserves as a result.

~ Nathan

From toby at telegraphics.com.au  Wed Oct 21 11:05:53 2009
From: toby at telegraphics.com.au (Toby Thain)
Date: Wed, 21 Oct 2009 14:05:53 -0400
Subject: [zfs-discuss] New Mac Mini Server
In-Reply-To: <0A88B88B-DB1E-49EF-BCD0-255A6EB724E7@themacplace.co.uk>
References: <A55D6C31-5358-4BE2-ACB5-AA9DFA774A31@gmail.com>
	<0A88B88B-DB1E-49EF-BCD0-255A6EB724E7@themacplace.co.uk>
Message-ID: <36A592A0-8696-49A0-8096-88A030E7B2A4@telegraphics.com.au>


On 21-Oct-09, at 5:41 AM, James Relph wrote:

>> Did anyone else see the announcement of the new Mac Mini server,  
>> and think "If only I could run ZFS on that..." ?
>
> Yes.
>
> ...sigh...
>
> Seriously this is driving me mad, it's the first time in about 4  
> years where I've looked at another OS and really, really wanted a  
> feature that was missing from OS X.  Sure there's times when I want  
> things like proper clustering of AFP, and a distributed filesystem,  
> but they're generally the more infrequent requirements.  ZFS I  
> could see being useful in *lots* of places.  I have this horrible  
> rising dread that Apple are going to pop up in 12 months and say  
> "See, you don't need ZFS, we've bolted snapshots onto the side of  
> HFS+".

It's hard to believe Apple doesn't see more value in ZFS than  
"snapshots". Apple engineers are on this list. If they are producing  
a ZFS wannabe, it will have to offer many of the benefits people look  
for in ZFS.

--Toby

>
> James
>
> Website:		www.themacplace.co.uk
> Blog:		www.themacplace.co.uk/blog.html
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From hukl at h3q.com  Fri Oct 23 04:59:27 2009
From: hukl at h3q.com (John-Paul Bader)
Date: Fri, 23 Oct 2009 13:59:27 +0200
Subject: [zfs-discuss] Apple is looking for a File System Engineer
Message-ID: <1684CDA5-B5D9-4123-ACD5-C03BD991A881@h3q.com>

Doesn't sound very ZFS'sy

http://jobs.apple.com/index.ajs?method=mExternal.showJob&RID=42559

~ John

From sebastian at pixelmilk.com  Fri Oct 23 05:13:14 2009
From: sebastian at pixelmilk.com (=?iso-8859-1?Q?Sebastian_D=F6ll?=)
Date: Fri, 23 Oct 2009 14:13:14 +0200
Subject: [zfs-discuss] Apple is looking for a File System Engineer
In-Reply-To: <1684CDA5-B5D9-4123-ACD5-C03BD991A881@h3q.com>
References: <1684CDA5-B5D9-4123-ACD5-C03BD991A881@h3q.com>
Message-ID: <B5694F68-7DE4-432E-9C06-87BF8C33CDFF@pixelmilk.com>

Hi,

I've read all the discussions on the mailing list lately,
but beside the deep desire to have ZFS on OSX,
I think it'll never happen. ZFS has some advantages
for sure and I use it on my servers (Solaris), but this isn't
enough to get Apple to integrate it deeply into
the complete os. They'll stay with HFS+ for a long time
and keep extending it with filesystem compression like in 10.6
and other features. So at this point I would state ZFS on OSX is dead.

Bye

Am 23.10.2009 um 13:59 schrieb John-Paul Bader:

> Doesn't sound very ZFS'sy
>
> http://jobs.apple.com/index.ajs?method=mExternal.showJob&RID=42559
>
> ~ John
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4818 bytes
Desc: not available
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/0bc7b9ac/attachment.bin>

From al at runlevel7.org  Fri Oct 23 07:37:38 2009
From: al at runlevel7.org (Al Gordon)
Date: Fri, 23 Oct 2009 10:37:38 -0400
Subject: [zfs-discuss] Apple is looking for a File System Engineer
In-Reply-To: <1684CDA5-B5D9-4123-ACD5-C03BD991A881@h3q.com>
References: <1684CDA5-B5D9-4123-ACD5-C03BD991A881@h3q.com>
Message-ID: <1256308658.3899.3.camel@agordon-desktop>

The job posting mentions both OS X and iPhone, which reminds me...

When are we going to have ZFS for iPhone?!

-  

  -- AL --

On Fri, 2009-10-23 at 13:59 +0200, John-Paul Bader wrote:
> Doesn't sound very ZFS'sy
> 
> http://jobs.apple.com/index.ajs?method=mExternal.showJob&RID=42559
> 
> ~ John
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From david299792 at googlemail.com  Fri Oct 23 09:10:00 2009
From: david299792 at googlemail.com (David Ritchie)
Date: Fri, 23 Oct 2009 17:10:00 +0100
Subject: [zfs-discuss] iSCSI on ZFS (was Re:  zfs-10a286 ?)
In-Reply-To: <3A8DB67C-B2DE-43B0-9F6B-FE09D5294A2D@googlemail.com>
References: <3c72d6220910171207u3b6bfdebi78263d16ba457dab@mail.gmail.com>
	<3c72d6220910181029r3bc6b1e1t206c5439897f36d4@mail.gmail.com>
	<7AE613A5-62E2-40F7-8E3A-76D14377D3AD@gmail.com>
	<20091018.160615.00168130.hanche@math.ntnu.no>
	<13EAF01B-55D5-4AF9-AB77-949834499C20@yellowspace.net>
	<0764C062-1D67-457E-933B-5A5AA706FC2A@googlemail.com>
	<869DD39E-3739-4908-8A50-11913FCBA5A0@amsi.org.au>
	<3A8DB67C-B2DE-43B0-9F6B-FE09D5294A2D@googlemail.com>
Message-ID: <4FEB9AC8-4EB0-4ED1-A4E7-FDBC8E9ABF69@googlemail.com>

Hi,

In case anyone's interested I've been doing some more experimenting  
with iSCSI.

I tried installing OpenSolaris on my MacBook using a spare hard drive  
in order to see what iSCSI performance would be like with the target  
running on real hardware. Turns out OpenSolaris runs pretty well on a  
MacBook.

I used the globalSAN initiator on my Mac Pro to connect to a zvol over  
GigE. In this way I managed to lower the write performance to <1MB/s -  
similar to the results Raoul Callaghan gets!!

This isn't great. It surprises me a bit, because going from a VMWare  
Solaris target on one machine to the globalSAN initiator on another  
machine got me much better performance than that. Why does running  
OpenSolaris on real hardware make it that much worse? Is there  
something much better about the network card in the Mac Pro? (maybe).  
I guess it would be good to try another initiator, although it seems  
to be quite expensive for the Atto one.

I got slightly better performance using the MS initiator in Windows XP  
in VMWare connecting to the Solaris on MacBook - about 2.4MB/s. Still  
not great.

I'm beginning to wonder whether it would be quicker to just mount a  
file share via AFP or SMB on the client machine (shared from a Solaris  
box) and put a disk image on it. By mounting the disk image and having  
my home folder in that I get spotlight coupled with ZFS goodness and  
reasonable speed. A quick test indicates that works pretty well  
(serving a sparse disk image via AFP from Mac Pro). I'd have to write  
some startup script to hook this all up on boot on the client, and  
point my home folder to the appropriate place, or mount the image  
over /Users.


-- David

From buffyg at mac.com  Fri Oct 23 09:35:01 2009
From: buffyg at mac.com (Bayard Bell)
Date: Fri, 23 Oct 2009 17:35:01 +0100
Subject: [zfs-discuss] ZFS for OS X, we hardly knew thee?
Message-ID: <AC090380-B57D-4229-9B8A-1977B5FB81CA@mac.com>

Just as I'm in the process of building my first zfs pool, using disk  
that arrived earlier this week, I was shocked to find the following  
text abruptly appearing on zfs.macosforge.org with today's date:

The ZFS project has been discontinued. The mailing list and repository  
will also be removed shortly.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/3eccd6a5/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 195 bytes
Desc: Signierter Teil der Nachricht
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/3eccd6a5/attachment.bin>

From byron at mac.com  Fri Oct 23 09:46:01 2009
From: byron at mac.com (Byron Servies)
Date: Fri, 23 Oct 2009 09:46:01 -0700
Subject: [zfs-discuss] ZFS for OS X, we hardly knew thee?
In-Reply-To: <AC090380-B57D-4229-9B8A-1977B5FB81CA@mac.com>
References: <AC090380-B57D-4229-9B8A-1977B5FB81CA@mac.com>
Message-ID: <38DC1DB8-020C-47FA-A341-9ED5095BA2A0@mac.com>

On Oct 23, 2009, at 9:35 AM, Bayard Bell wrote:

> Just as I'm in the process of building my first zfs pool, using disk  
> that arrived earlier this week, I was shocked to find the following  
> text abruptly appearing on zfs.macosforge.org with today's date:
>
> The ZFS project has been discontinued. The mailing list and  
> repository will also be removed shortly.

Yup.  The HFS long-hairs have won. Most junk will be grafted on to  
their crufty file system shortly.

Byron
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/ebe26769/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 2409 bytes
Desc: not available
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/ebe26769/attachment.bin>

From aucl at me.com  Fri Oct 23 09:47:37 2009
From: aucl at me.com (Clemens Auer)
Date: Fri, 23 Oct 2009 20:47:37 +0400
Subject: [zfs-discuss] ZFS for OS X, we hardly knew thee?
In-Reply-To: <AC090380-B57D-4229-9B8A-1977B5FB81CA@mac.com>
References: <AC090380-B57D-4229-9B8A-1977B5FB81CA@mac.com>
Message-ID: <F57BD733-9CEC-4DBF-9365-01805828BF37@me.com>

Well, that is at least some kind of "official" statement. Sad, very sad!
What would be nice of course would be some migration guides, or  
recommendations how to move on.

Keeping the disks and use them in some FreeBSD installation for  
example might work, but it is not what everyone really wanna.


On Oct 23, 2009, at 8:35 PM, Bayard Bell wrote:

> Just as I'm in the process of building my first zfs pool, using disk  
> that arrived earlier this week, I was shocked to find the following  
> text abruptly appearing on zfs.macosforge.org with today's date:
>
> The ZFS project has been discontinued. The mailing list and  
> repository will also be removed shortly.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/5f87b434/attachment.html>

From alexandre.dufour at gmail.com  Fri Oct 23 09:48:06 2009
From: alexandre.dufour at gmail.com (Alexandre Dufour)
Date: Fri, 23 Oct 2009 18:48:06 +0200
Subject: [zfs-discuss] Apple is looking for a File System Engineer
In-Reply-To: <1684CDA5-B5D9-4123-ACD5-C03BD991A881@h3q.com>
References: <1684CDA5-B5D9-4123-ACD5-C03BD991A881@h3q.com>
Message-ID: <F1AFB2CB-01C8-48EA-BB90-92F7F3EE4E99@gmail.com>


Le 23 oct. 2009 ? 13:59, John-Paul Bader a ?crit :
> Doesn't sound very ZFS'sy
>
> http://jobs.apple.com/index.ajs?method=mExternal.showJob&RID=42559
>
> ~ John


Le 23 oct. 2009 ? 18:35, Bayard Bell a ?crit :
> Just as I'm in the process of building my first zfs pool, using disk  
> that arrived earlier this week, I was shocked to find the following  
> text abruptly appearing on zfs.macosforge.org with today's date:
>
> The ZFS project has been discontinued. The mailing list and  
> repository will also be removed shortly.


Coincidence ?...

From lists at loveturtle.net  Fri Oct 23 09:50:16 2009
From: lists at loveturtle.net (Dillon Kass)
Date: Fri, 23 Oct 2009 12:50:16 -0400
Subject: [zfs-discuss] ZFS for OS X, we hardly knew thee?
In-Reply-To: <38DC1DB8-020C-47FA-A341-9ED5095BA2A0@mac.com>
References: <AC090380-B57D-4229-9B8A-1977B5FB81CA@mac.com>
	<38DC1DB8-020C-47FA-A341-9ED5095BA2A0@mac.com>
Message-ID: <4AE1DEC8.9010900@loveturtle.net>

About fucking time. There must be a six month waiting period for 
straight answers.

Byron Servies wrote:
> On Oct 23, 2009, at 9:35 AM, Bayard Bell wrote:
>
>> Just as I'm in the process of building my first zfs pool, using disk 
>> that arrived earlier this week, I was shocked to find the following 
>> text abruptly appearing on zfs.macosforge.org 
>> <http://zfs.macosforge.org> with today's date:
>>
>> The ZFS project has been discontinued. The mailing list and 
>> repository will also be removed shortly.
>
> Yup.  The HFS long-hairs have won. Most junk will be grafted on to 
> their crufty file system shortly.
>
> Byron
> ------------------------------------------------------------------------
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>   


From dorofeev at gmail.com  Fri Oct 23 09:52:04 2009
From: dorofeev at gmail.com (Andrei Dorofeev)
Date: Fri, 23 Oct 2009 09:52:04 -0700
Subject: [zfs-discuss] ZFS for OS X, we hardly knew thee?
In-Reply-To: <AC090380-B57D-4229-9B8A-1977B5FB81CA@mac.com>
References: <AC090380-B57D-4229-9B8A-1977B5FB81CA@mac.com>
Message-ID: <EF4F4442-91CF-45FD-A4D9-A9C9DB916EBD@gmail.com>

So now that Apple officially pulled the plug on this, what do folks  
think about moving this project to sourceforge? I did not read the  
license for the source code and the binaries carefully yet, but  
perhaps we can still at the very least redistribute them.

- Andrei

On Oct 23, 2009, at 9:35 AM, Bayard Bell <buffyg at mac.com> wrote:

> Just as I'm in the process of building my first zfs pool, using disk  
> that arrived earlier this week, I was shocked to find the following  
> text abruptly appearing on zfs.macosforge.org with today's date:
>
> The ZFS project has been discontinued. The mailing list and  
> repository will also be removed shortly.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/06d3c6d3/attachment.html>

From gadams at fgm.com  Fri Oct 23 09:52:15 2009
From: gadams at fgm.com (Adams, Geoff)
Date: Fri, 23 Oct 2009 12:52:15 -0400
Subject: [zfs-discuss] iSCSI on ZFS (was Re:  zfs-10a286 ?)
In-Reply-To: <4FEB9AC8-4EB0-4ED1-A4E7-FDBC8E9ABF69@googlemail.com>
References: <3c72d6220910171207u3b6bfdebi78263d16ba457dab@mail.gmail.com>
	<3c72d6220910181029r3bc6b1e1t206c5439897f36d4@mail.gmail.com>
	<7AE613A5-62E2-40F7-8E3A-76D14377D3AD@gmail.com>
	<20091018.160615.00168130.hanche@math.ntnu.no>
	<13EAF01B-55D5-4AF9-AB77-949834499C20@yellowspace.net>
	<0764C062-1D67-457E-933B-5A5AA706FC2A@googlemail.com>
	<869DD39E-3739-4908-8A50-11913FCBA5A0@amsi.org.au>
	<3A8DB67C-B2DE-43B0-9F6B-FE09D5294A2D@googlemail.com>
	<4FEB9AC8-4EB0-4ED1-A4E7-FDBC8E9ABF69@googlemail.com>
Message-ID: <4F67CD08-08AA-4C3B-97EB-583F546CD4EC@fgm.com>

On 23 Oct 2009, at 12:10 PM, David Ritchie wrote:

> I used the globalSAN initiator on my Mac Pro to connect to a zvol  
> over GigE. In this way I managed to lower the write performance to  
> <1MB/s - similar to the results Raoul Callaghan gets!!

That's quite surprising.

I actually just last night set up the globalSAN initiator on a  
PowerMac G5 (Mac OS 10.5.x), using the COMSTAR iSCSI target on  
OpenSolaris (build 125) running on a SunFire x4150. Over 100base-T  
(the GigE port on the G5 was lightning damaged), I see over 6 MB/s.  
Granted, this is nothing compared to the roughly 100 GB/s that I can  
see to that same zpool of cheap, large SATA disks locally, but 6 MB/s  
isn't too bad for 100base-T. I'll have to stick a GigE card in the  
client machine to see if it gets any better.

hfuhruhurr:~: dd if=/dev/zero of=/Volumes/Time\ Machine/gig bs=1m  
count=1024
1024+0 records in
1024+0 records out
1073741824 bytes transferred in 160.978045 secs (6670113 bytes/sec)

Not too bad for a first test, I think.

- Geoff
(I apologize for the Exchange email system here at the office that  
insists on rudely HTMLizing my outgoing mail.)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/56f94359/attachment.html>

From richard.elling at gmail.com  Fri Oct 23 09:55:00 2009
From: richard.elling at gmail.com (Richard Elling)
Date: Fri, 23 Oct 2009 09:55:00 -0700
Subject: [zfs-discuss] iSCSI on ZFS (was Re:  zfs-10a286 ?)
In-Reply-To: <4FEB9AC8-4EB0-4ED1-A4E7-FDBC8E9ABF69@googlemail.com>
References: <3c72d6220910171207u3b6bfdebi78263d16ba457dab@mail.gmail.com>
	<3c72d6220910181029r3bc6b1e1t206c5439897f36d4@mail.gmail.com>
	<7AE613A5-62E2-40F7-8E3A-76D14377D3AD@gmail.com>
	<20091018.160615.00168130.hanche@math.ntnu.no>
	<13EAF01B-55D5-4AF9-AB77-949834499C20@yellowspace.net>
	<0764C062-1D67-457E-933B-5A5AA706FC2A@googlemail.com>
	<869DD39E-3739-4908-8A50-11913FCBA5A0@amsi.org.au>
	<3A8DB67C-B2DE-43B0-9F6B-FE09D5294A2D@googlemail.com>
	<4FEB9AC8-4EB0-4ED1-A4E7-FDBC8E9ABF69@googlemail.com>
Message-ID: <5A179B4D-9CB4-4888-A5BF-F95697E2C914@gmail.com>

On Oct 23, 2009, at 9:10 AM, David Ritchie wrote:
> Hi,
>
> In case anyone's interested I've been doing some more experimenting  
> with iSCSI.
>
> I tried installing OpenSolaris on my MacBook using a spare hard  
> drive in order to see what iSCSI performance would be like with the  
> target running on real hardware. Turns out OpenSolaris runs pretty  
> well on a MacBook.
>
> I used the globalSAN initiator on my Mac Pro to connect to a zvol  
> over GigE. In this way I managed to lower the write performance to  
> <1MB/s - similar to the results Raoul Callaghan gets!!
>
> This isn't great. It surprises me a bit, because going from a VMWare  
> Solaris target on one machine to the globalSAN initiator on another  
> machine got me much better performance than that. Why does running  
> OpenSolaris on real hardware make it that much worse? Is there  
> something much better about the network card in the Mac Pro?  
> (maybe). I guess it would be good to try another initiator, although  
> it seems to be quite expensive for the Atto one.
>
> I got slightly better performance using the MS initiator in Windows  
> XP in VMWare connecting to the Solaris on MacBook - about 2.4MB/s.  
> Still not great.

Sounds like the Nagle algorithm is showing its ugly side. Check your  
initiators
to see if disabling the Nagle algorithm (TCP_NODELAY) helps.
http://en.wikipedia.org/wiki/Nagle%27s_algorithm
  -- richard

> I'm beginning to wonder whether it would be quicker to just mount a  
> file share via AFP or SMB on the client machine (shared from a  
> Solaris box) and put a disk image on it. By mounting the disk image  
> and having my home folder in that I get spotlight coupled with ZFS  
> goodness and reasonable speed. A quick test indicates that works  
> pretty well (serving a sparse disk image via AFP from Mac Pro). I'd  
> have to write some startup script to hook this all up on boot on the  
> client, and point my home folder to the appropriate place, or mount  
> the image over /Users.
>
>
> -- David
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From nancejk at phys.washington.edu  Fri Oct 23 09:58:32 2009
From: nancejk at phys.washington.edu (Jared Nance)
Date: Fri, 23 Oct 2009 09:58:32 -0700
Subject: [zfs-discuss] iSCSI on ZFS (was Re:  zfs-10a286 ?)
In-Reply-To: <4FEB9AC8-4EB0-4ED1-A4E7-FDBC8E9ABF69@googlemail.com>
References: <3c72d6220910171207u3b6bfdebi78263d16ba457dab@mail.gmail.com>
	<3c72d6220910181029r3bc6b1e1t206c5439897f36d4@mail.gmail.com>
	<7AE613A5-62E2-40F7-8E3A-76D14377D3AD@gmail.com>
	<20091018.160615.00168130.hanche@math.ntnu.no>
	<13EAF01B-55D5-4AF9-AB77-949834499C20@yellowspace.net>
	<0764C062-1D67-457E-933B-5A5AA706FC2A@googlemail.com>
	<869DD39E-3739-4908-8A50-11913FCBA5A0@amsi.org.au>
	<3A8DB67C-B2DE-43B0-9F6B-FE09D5294A2D@googlemail.com>
	<4FEB9AC8-4EB0-4ED1-A4E7-FDBC8E9ABF69@googlemail.com>
Message-ID: <9813C287-17AD-4FAC-950C-811B8B9C6B95@phys.washington.edu>

Hi David-
I experimented with OSX as a ZFS 'host' for some time before I  
eventually decided to go with an OS where ZFS is 'native', so to speak  
- started with OpenSolaris and now I run FreeBSD - and I must say I am  
quite thrilled with the ZFS implementation there.  However, I do use  
OSX as a ZFS 'client' - i.e. talking to my BSD box for backup purposes  
and so on.

My experience with the globalSAN initiator was similar.  I topped out  
at around 1MB/s and no amount of tuning seemed to get me anywhere  
positive.  My subsequent research turned up a lot of information about  
the ZIL, which is the intent log that ZFS keeps around so that it  
knows what it is supposed to be doing.  The consensus seems to be that  
iSCSI (at least the globalSAN implementation of it) makes a lot of  
tiny writes, which involves consulting the intent log many many  
times.  In other words, lots of going to disk to find out what it's  
supposed to be up to.  This is, as you might expect, slow.

The solution(s), as offered up on some of the opensolaris forums, are  
helpful to varying degrees.  You can a) disable ZIL altogether (very  
not recommended), b) put the ZIL on a very fast disk (should be  
fine).  If you want more information, you can search for ZIL, iSCSI,  
and OSX on the opensolaris forums.  Good luck.

Jared N


On Oct 23, 2009, at 9:10 AM, David Ritchie wrote:

> Hi,
>
> In case anyone's interested I've been doing some more experimenting  
> with iSCSI.
>
> I tried installing OpenSolaris on my MacBook using a spare hard  
> drive in order to see what iSCSI performance would be like with the  
> target running on real hardware. Turns out OpenSolaris runs pretty  
> well on a MacBook.
>
> I used the globalSAN initiator on my Mac Pro to connect to a zvol  
> over GigE. In this way I managed to lower the write performance to  
> <1MB/s - similar to the results Raoul Callaghan gets!!
>
> This isn't great. It surprises me a bit, because going from a VMWare  
> Solaris target on one machine to the globalSAN initiator on another  
> machine got me much better performance than that. Why does running  
> OpenSolaris on real hardware make it that much worse? Is there  
> something much better about the network card in the Mac Pro?  
> (maybe). I guess it would be good to try another initiator, although  
> it seems to be quite expensive for the Atto one.
>
> I got slightly better performance using the MS initiator in Windows  
> XP in VMWare connecting to the Solaris on MacBook - about 2.4MB/s.  
> Still not great.
>
> I'm beginning to wonder whether it would be quicker to just mount a  
> file share via AFP or SMB on the client machine (shared from a  
> Solaris box) and put a disk image on it. By mounting the disk image  
> and having my home folder in that I get spotlight coupled with ZFS  
> goodness and reasonable speed. A quick test indicates that works  
> pretty well (serving a sparse disk image via AFP from Mac Pro). I'd  
> have to write some startup script to hook this all up on boot on the  
> client, and point my home folder to the appropriate place, or mount  
> the image over /Users.
>
>
> -- David
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From caronni at gmail.com  Fri Oct 23 09:59:56 2009
From: caronni at gmail.com (Germano Caronni)
Date: Fri, 23 Oct 2009 18:59:56 +0200
Subject: [zfs-discuss] ZFS for OS X, we hardly knew thee?
In-Reply-To: <EF4F4442-91CF-45FD-A4D9-A9C9DB916EBD@gmail.com>
References: <AC090380-B57D-4229-9B8A-1977B5FB81CA@mac.com> 
	<EF4F4442-91CF-45FD-A4D9-A9C9DB916EBD@gmail.com>
Message-ID: <327b821f0910230959v13fb3295w18b654b4578a01dd@mail.gmail.com>

>
>
> The ZFS project has been discontinued. The mailing list and repository will
> also be removed shortly.
>
>
Byebye all, it was nice reading ya... See you in a different OS of your
choice ;-)

Germano
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/9dca4c86/attachment.html>

From buffyg at mac.com  Fri Oct 23 10:03:09 2009
From: buffyg at mac.com (Bayard Bell)
Date: Fri, 23 Oct 2009 18:03:09 +0100
Subject: [zfs-discuss] ZFS for OS X, we hardly knew thee?
In-Reply-To: <38DC1DB8-020C-47FA-A341-9ED5095BA2A0@mac.com>
References: <AC090380-B57D-4229-9B8A-1977B5FB81CA@mac.com>
	<38DC1DB8-020C-47FA-A341-9ED5095BA2A0@mac.com>
Message-ID: <67C98DBD-89A5-47AD-A6D4-6CD8DF42CC04@mac.com>

I think there are two distinct problems here: one is that Apple's  
decided to bail on a filesystem far better than HFS+ and the other is  
that the way they've bailed doesn't show a lot of respect for the  
community that did take up the product. I mean, pulling all the code  
and documentation without giving people a chance to make some  
decisions about how to deal with this shows no consideration for those  
who have, understanding that this is something of a skunkworks  
project, accepted the caveats and made use of this. People should  
expect better than to have an announcement made in the first instance  
by way of redirecting attempts to access code, binaries, and  
documentation. This isn't simply a matter of legality: this is telling  
any observer that Apple brooks no compromises between profit and  
property and community-building.

Apple, please reconsider being gracious about how you wind this down.  
So far, you're not showing a sense of community partnership.

Am 23 Oct 2009 um 17:46 schrieb Byron Servies:

> On Oct 23, 2009, at 9:35 AM, Bayard Bell wrote:
>
>> Just as I'm in the process of building my first zfs pool, using  
>> disk that arrived earlier this week, I was shocked to find the  
>> following text abruptly appearing on zfs.macosforge.org with  
>> today's date:
>>
>> The ZFS project has been discontinued. The mailing list and  
>> repository will also be removed shortly.
>
> Yup.  The HFS long-hairs have won. Most junk will be grafted on to  
> their crufty file system shortly.
>
> Byron

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/7da99ec7/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 195 bytes
Desc: Signierter Teil der Nachricht
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/7da99ec7/attachment-0001.bin>

From mattsnow at gmail.com  Fri Oct 23 10:04:20 2009
From: mattsnow at gmail.com (Matt Snow)
Date: Fri, 23 Oct 2009 10:04:20 -0700
Subject: [zfs-discuss] Apple is looking for a File System Engineer
In-Reply-To: <F1AFB2CB-01C8-48EA-BB90-92F7F3EE4E99@gmail.com>
References: <1684CDA5-B5D9-4123-ACD5-C03BD991A881@h3q.com>
	<F1AFB2CB-01C8-48EA-BB90-92F7F3EE4E99@gmail.com>
Message-ID: <6879ebc80910231004n12b4c12fg37848fecc8a85c1f@mail.gmail.com>

On Fri, Oct 23, 2009 at 9:48 AM, Alexandre Dufour <
alexandre.dufour at gmail.com> wrote:

>
> Le 23 oct. 2009 ? 13:59, John-Paul Bader a ?crit :
>
>> Doesn't sound very ZFS'sy
>>
>> http://jobs.apple.com/index.ajs?method=mExternal.showJob&RID=42559
>>
>> ~ John
>>
>
>
> Le 23 oct. 2009 ? 18:35, Bayard Bell a ?crit :
>
>> Just as I'm in the process of building my first zfs pool, using disk that
>> arrived earlier this week, I was shocked to find the following text abruptly
>> appearing on zfs.macosforge.org with today's date:
>>
>> The ZFS project has been discontinued. The mailing list and repository
>> will also be removed shortly.
>>
>
> Coincidence ?...
>
>
That is a damn shame. looks like i'll be moving my hackintosh ZFS box to
FreeBSD or Solaris this weekend. Glad I held back on buying the mac pro for
my home server.

I honestly believed that Apple pulled ZFS out due to the NetApp BS with Sun
and every other company using ZFS in their products.


booooooooooo apple!

..Matt



> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/16798b32/attachment.html>

From toby at telegraphics.com.au  Fri Oct 23 10:18:06 2009
From: toby at telegraphics.com.au (Toby Thain)
Date: Fri, 23 Oct 2009 13:18:06 -0400
Subject: [zfs-discuss] Apple is looking for a File System Engineer
In-Reply-To: <F1AFB2CB-01C8-48EA-BB90-92F7F3EE4E99@gmail.com>
References: <1684CDA5-B5D9-4123-ACD5-C03BD991A881@h3q.com>
	<F1AFB2CB-01C8-48EA-BB90-92F7F3EE4E99@gmail.com>
Message-ID: <CAAC6182-A0FE-4282-8EC7-058ECC994005@telegraphics.com.au>


On 23-Oct-09, at 12:48 PM, Alexandre Dufour wrote:

>
> Le 23 oct. 2009 ? 13:59, John-Paul Bader a ?crit :
>> Doesn't sound very ZFS'sy
>>
>> http://jobs.apple.com/index.ajs?method=mExternal.showJob&RID=42559
>>
>> ~ John
>
>
> Le 23 oct. 2009 ? 18:35, Bayard Bell a ?crit :
>> Just as I'm in the process of building my first zfs pool, using  
>> disk that arrived earlier this week, I was shocked to find the  
>> following text abruptly appearing on zfs.macosforge.org with  
>> today's date:
>>
>> The ZFS project has been discontinued. The mailing list and  
>> repository will also be removed shortly.
>
>
> Coincidence ?...


Hardly. But I expect a NEW fs, not an iteration of HFS+. Even if  
Apple regrettably (perhaps foolishly) declined to use it, at least  
they recognise that ZFS is something entirely new, and they need to  
offer similar advantages. Which means starting over, as Sun did.

--Toby

> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From nathan.stocks at gmail.com  Fri Oct 23 10:30:35 2009
From: nathan.stocks at gmail.com (Nathan)
Date: Fri, 23 Oct 2009 11:30:35 -0600
Subject: [zfs-discuss] ZFS for OS X, we hardly knew thee?
In-Reply-To: <AC090380-B57D-4229-9B8A-1977B5FB81CA@mac.com>
References: <AC090380-B57D-4229-9B8A-1977B5FB81CA@mac.com>
Message-ID: <96c9d6a80910231030q13956905k5d92979a2c12fce1@mail.gmail.com>

On Fri, Oct 23, 2009 at 10:35 AM, Bayard Bell <buffyg at mac.com> wrote:
> Just as I'm in the process of building my first zfs pool, using disk that
> arrived earlier this week, I was shocked to find the following text abruptly
> appearing on zfs.macosforge.org with today's date:
> The ZFS project has been discontinued. The mailing list and repository will
> also be removed shortly.

That sucks.  That really sucks.

If the actual repository is still accessible somewhere, or if someone
has the latest code, I'd be happy to help getting it up on github or
something like that.  Continuing development may not be very
productive unless some Apple-kernel developers step up, though.

~ Nathan

From buffyg at mac.com  Fri Oct 23 10:32:32 2009
From: buffyg at mac.com (Bayard Bell)
Date: Fri, 23 Oct 2009 18:32:32 +0100
Subject: [zfs-discuss] ZFS for OS X, we hardly knew thee?
In-Reply-To: <EB322EFD-060B-4378-8E99-9A1AC37C04E7@mac.com>
References: <AC090380-B57D-4229-9B8A-1977B5FB81CA@mac.com>
	<38DC1DB8-020C-47FA-A341-9ED5095BA2A0@mac.com>
	<67C98DBD-89A5-47AD-A6D4-6CD8DF42CC04@mac.com>
	<EB322EFD-060B-4378-8E99-9A1AC37C04E7@mac.com>
Message-ID: <BCA121C8-6701-46F8-91D0-CCA98620E174@mac.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Point taken. On the other hand there's ten years of increasingly  
successful examples in community engagement for development in  
between, where you'd hope someone at Apple learned to think  
differently. Since we're talking about ZFS, you can compare what Sun  
did with offering Solaris for Intel ten years ago with what they're  
doing with their development model now, where the latter has done  
something to offset the sense of business risk associated with their  
financial decline. Sure, Sun's been under a lot more pressure to make  
changes than has Apple, but let's not whistle past the graveyard here:  
Apple has some real challenges in maintaining some very complex non- 
mainstream technologies under a fairly conventional proprietary  
development model. It's resisting changes to get away from those kinds  
of self-limiting practices while you're making money that set a  
company up for later problems (e.g. Microsoft).

Am 23 Oct 2009 um 18:10 schrieb Christopher Fernando:

> I'm not entirely sure why this behavior by Apple is surprising. This  
> is how they've handled many of their "skunkworks" projects. I  
> distinctly remember MkLinux having a very similar cycle - being  
> dropped suspiciously and with little word and then all of a sudden  
> Mac OS X Beta appearing with a very similar MACH kernel base except  
> with a better userland.
>
> While it may be painful in the short term, I think the long term  
> results will yield something surprising and amazing.
>
> At least that's my hope.
>
> I appreciate all the work/help everyone has done to make this  
> project happen - I hope to see you on the "flip side" of ZFS.
>
>
> On Oct 23, 2009, at 11:03 AM, Bayard Bell wrote:
>
>> I think there are two distinct problems here: one is that Apple's  
>> decided to bail on a filesystem far better than HFS+ and the other  
>> is that the way they've bailed doesn't show a lot of respect for  
>> the community that did take up the product. I mean, pulling all the  
>> code and documentation without giving people a chance to make some  
>> decisions about how to deal with this shows no consideration for  
>> those who have, understanding that this is something of a  
>> skunkworks project, accepted the caveats and made use of this.  
>> People should expect better than to have an announcement made in  
>> the first instance by way of redirecting attempts to access code,  
>> binaries, and documentation. This isn't simply a matter of  
>> legality: this is telling any observer that Apple brooks no  
>> compromises between profit and property and community-building.
>>
>> Apple, please reconsider being gracious about how you wind this  
>> down. So far, you're not showing a sense of community partnership.
>>
>> Am 23 Oct 2009 um 17:46 schrieb Byron Servies:
>>
>>> On Oct 23, 2009, at 9:35 AM, Bayard Bell wrote:
>>>
>>>> Just as I'm in the process of building my first zfs pool, using  
>>>> disk that arrived earlier this week, I was shocked to find the  
>>>> following text abruptly appearing on zfs.macosforge.org with  
>>>> today's date:
>>>>
>>>> The ZFS project has been discontinued. The mailing list and  
>>>> repository will also be removed shortly.
>>>
>>> Yup.  The HFS long-hairs have won. Most junk will be grafted on to  
>>> their crufty file system shortly.
>>>
>>> Byron
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v2.0.10 (Darwin)

iEYEARECAAYFAkrh6LYACgkQcZQHT1XL9xmEqwCeJ7oO7rAsgS8dpCn+Ru5LyFUq
Rj4An0N2zxU5bzeJWxBifMj2wxQAJRC7
=T1GY
-----END PGP SIGNATURE-----

From nathan.stocks at gmail.com  Fri Oct 23 10:39:23 2009
From: nathan.stocks at gmail.com (Nathan)
Date: Fri, 23 Oct 2009 11:39:23 -0600
Subject: [zfs-discuss] ZFS for OS X, we hardly knew thee?
In-Reply-To: <96c9d6a80910231030q13956905k5d92979a2c12fce1@mail.gmail.com>
References: <AC090380-B57D-4229-9B8A-1977B5FB81CA@mac.com>
	<96c9d6a80910231030q13956905k5d92979a2c12fce1@mail.gmail.com>
Message-ID: <96c9d6a80910231039l2811414am20dac79e680dee57@mail.gmail.com>

On Fri, Oct 23, 2009 at 11:30 AM, Nathan <nathan.stocks at gmail.com> wrote:
> On Fri, Oct 23, 2009 at 10:35 AM, Bayard Bell <buffyg at mac.com> wrote:
>> Just as I'm in the process of building my first zfs pool, using disk that
>> arrived earlier this week, I was shocked to find the following text abruptly
>> appearing on zfs.macosforge.org with today's date:
>> The ZFS project has been discontinued. The mailing list and repository will
>> also be removed shortly.
>
> That sucks. ?That really sucks.
>
> If the actual repository is still accessible somewhere, or if someone
> has the latest code, I'd be happy to help getting it up on github or
> something like that. ?Continuing development may not be very
> productive unless some Apple-kernel developers step up, though.

Okay, I found a cached page that links to all the tarballs (which
links still currently work):

http://74.125.155.132/search?q=cache:UQCKm_BVf44J:zfs.macosforge.org/trac/wiki/downloads+zfs.macosforge.org/trac/wiki/downloads&cd=1&hl=en&ct=clnk&gl=us&client=firefox-a

Now to see if I can find an svn url...

~ Nathan

From nathan.stocks at gmail.com  Fri Oct 23 11:05:28 2009
From: nathan.stocks at gmail.com (Nathan)
Date: Fri, 23 Oct 2009 12:05:28 -0600
Subject: [zfs-discuss] ZFS for OS X, we hardly knew thee?
In-Reply-To: <96c9d6a80910231039l2811414am20dac79e680dee57@mail.gmail.com>
References: <AC090380-B57D-4229-9B8A-1977B5FB81CA@mac.com>
	<96c9d6a80910231030q13956905k5d92979a2c12fce1@mail.gmail.com>
	<96c9d6a80910231039l2811414am20dac79e680dee57@mail.gmail.com>
Message-ID: <96c9d6a80910231105h14b7ea3emfec799e05286bb30@mail.gmail.com>

On Fri, Oct 23, 2009 at 11:39 AM, Nathan <nathan.stocks at gmail.com> wrote:
> On Fri, Oct 23, 2009 at 11:30 AM, Nathan <nathan.stocks at gmail.com> wrote:
>> On Fri, Oct 23, 2009 at 10:35 AM, Bayard Bell <buffyg at mac.com> wrote:
>>> Just as I'm in the process of building my first zfs pool, using disk that
>>> arrived earlier this week, I was shocked to find the following text abruptly
>>> appearing on zfs.macosforge.org with today's date:
>>> The ZFS project has been discontinued. The mailing list and repository will
>>> also be removed shortly.
>>
>> That sucks. ?That really sucks.
>>
>> If the actual repository is still accessible somewhere, or if someone
>> has the latest code, I'd be happy to help getting it up on github or
>> something like that. ?Continuing development may not be very
>> productive unless some Apple-kernel developers step up, though.
>
> Okay, I found a cached page that links to all the tarballs (which
> links still currently work):
>
> http://74.125.155.132/search?q=cache:UQCKm_BVf44J:zfs.macosforge.org/trac/wiki/downloads+zfs.macosforge.org/trac/wiki/downloads&cd=1&hl=en&ct=clnk&gl=us&client=firefox-a
>
> Now to see if I can find an svn url...
>
> ~ Nathan
>

Found it...

http://svn.macosforge.org/repository/zfs

...and imported it into github:

http://github.com/peaceful/zfs-mac

git://github.com/peaceful/zfs-mac.git

...and put the tarballs I saved in github as well (since I don't have
a better place to host them):

http://github.com/peaceful/zfs-mac-tarballs

~ Nathan

From hanche at math.ntnu.no  Fri Oct 23 11:10:31 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Fri, 23 Oct 2009 14:10:31 -0400 (EDT)
Subject: [zfs-discuss] Mailing list
Message-ID: <20091023.141031.172993232.hanche@math.ntnu.no>

They're gonna take our mailing list away too. Many of us aren't
immediately going to stop using zfs on the mac because of this, though
in the long run we probably won't have any choice. In the meantime,
though, we will still need some kind of mailing list or forum in which
to discuss issues.

Quickly now, before this list is yanked: Any suggestions?

- Harald

From dhoffmann at uwalumni.com  Fri Oct 23 11:14:11 2009
From: dhoffmann at uwalumni.com (Dominik Hoffmann)
Date: Fri, 23 Oct 2009 14:14:11 -0400
Subject: [zfs-discuss] Mailing list
In-Reply-To: <20091023.141031.172993232.hanche@math.ntnu.no>
References: <20091023.141031.172993232.hanche@math.ntnu.no>
Message-ID: <DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>

Everybody who wants to get on the new list reply! Then whoever has the  
means to set up a new list can cull the addresses.

I'm in.

Dominik Hoffmann

On Oct 23, 2009, at 2:10 PM, Harald Hanche-Olsen wrote:

> They're gonna take our mailing list away too. Many of us aren't
> immediately going to stop using zfs on the mac because of this, though
> in the long run we probably won't have any choice. In the meantime,
> though, we will still need some kind of mailing list or forum in which
> to discuss issues.
>
> Quickly now, before this list is yanked: Any suggestions?
>
> - Harald
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 2427 bytes
Desc: not available
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/97336ea5/attachment.bin>

From andy at aligature.com  Fri Oct 23 11:24:37 2009
From: andy at aligature.com (Andy Webber)
Date: Fri, 23 Oct 2009 14:24:37 -0400
Subject: [zfs-discuss] Mailing list
In-Reply-To: <DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>
References: <20091023.141031.172993232.hanche@math.ntnu.no> 
	<DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>
Message-ID: <60b50dc10910231124j3ba05051y14aad88332690f45@mail.gmail.com>

I would like to be on the new list.

Thanks.

On Fri, Oct 23, 2009 at 2:14 PM, Dominik Hoffmann <dhoffmann at uwalumni.com>wrote:

> Everybody who wants to get on the new list reply! Then whoever has the
> means to set up a new list can cull the addresses.
>
> I'm in.
>
> Dominik Hoffmann
>
>
> On Oct 23, 2009, at 2:10 PM, Harald Hanche-Olsen wrote:
>
>  They're gonna take our mailing list away too. Many of us aren't
>> immediately going to stop using zfs on the mac because of this, though
>> in the long run we probably won't have any choice. In the meantime,
>> though, we will still need some kind of mailing list or forum in which
>> to discuss issues.
>>
>> Quickly now, before this list is yanked: Any suggestions?
>>
>> - Harald
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/c7e60a32/attachment.html>

From jasonbelec at rogers.com  Fri Oct 23 11:28:55 2009
From: jasonbelec at rogers.com (Jason Belec)
Date: Fri, 23 Oct 2009 11:28:55 -0700 (PDT)
Subject: [zfs-discuss] Mailing list
Message-ID: <913501.69632.qm@web88102.mail.re2.yahoo.com>

I'm in.

Jason

Sent from my iPhone

On 2009-10-23, at 2:14 PM, Dominik Hoffmann <dhoffmann at uwalumni.com> wrote:

Everybody who wants to get on the new list reply! Then whoever has the means to set up a new list can cull the addresses.

I'm in.

Dominik Hoffmann

On Oct 23, 2009, at 2:10 PM, Harald Hanche-Olsen wrote:

They're gonna take our mailing list away too. Many of us aren't
immediately going to stop using zfs on the mac because of this, though
in the long run we probably won't have any choice. In the meantime,
though, we will still need some kind of mailing list or forum in which
to discuss issues.

Quickly now, before this list is yanked: Any suggestions?

- Harald
_______________________________________________
zfs-discuss mailing list
zfs-discuss at lists.macosforge.org
http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


_______________________________________________
zfs-discuss mailing list
zfs-discuss at lists.macosforge.org
http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss



From hanche at math.ntnu.no  Fri Oct 23 11:29:53 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Fri, 23 Oct 2009 14:29:53 -0400 (EDT)
Subject: [zfs-discuss] Mailing list
In-Reply-To: <DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>
References: <20091023.141031.172993232.hanche@math.ntnu.no>
	<DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>
Message-ID: <20091023.142953.195584795.hanche@math.ntnu.no>

+ Dominik Hoffmann <dhoffmann at uwalumni.com>:

> Everybody who wants to get on the new list reply! Then whoever has the
> means to set up a new list can cull the addresses.

If a new list is set up before this one is taken away, a post here
should suffice to let everybody know the new location.

If the timing of events don't favour this, I suggest that whoever sets
up the new list simply collects the address of everyone who posted to
this list over a suitable period, and sends them a one-time mailing
with the location of the new list and instructions on signing up.

Thus, I think it sufficient that only lurkers follow your advice and
reply, since all the active participants will already be included.

- Harald

From baronda.2 at osu.edu  Fri Oct 23 11:30:49 2009
From: baronda.2 at osu.edu (Silas Baronda)
Date: Fri, 23 Oct 2009 14:30:49 -0400
Subject: [zfs-discuss] iSCSI on ZFS (was Re: zfs-10a286 ?)
In-Reply-To: <4F67CD08-08AA-4C3B-97EB-583F546CD4EC@fgm.com>
References: <3c72d6220910171207u3b6bfdebi78263d16ba457dab@mail.gmail.com>
	<3c72d6220910181029r3bc6b1e1t206c5439897f36d4@mail.gmail.com>
	<7AE613A5-62E2-40F7-8E3A-76D14377D3AD@gmail.com>
	<20091018.160615.00168130.hanche@math.ntnu.no>
	<13EAF01B-55D5-4AF9-AB77-949834499C20@yellowspace.net>
	<0764C062-1D67-457E-933B-5A5AA706FC2A@googlemail.com>
	<869DD39E-3739-4908-8A50-11913FCBA5A0@amsi.org.au>
	<3A8DB67C-B2DE-43B0-9F6B-FE09D5294A2D@googlemail.com>
	<4FEB9AC8-4EB0-4ED1-A4E7-FDBC8E9ABF69@googlemail.com>
	<4F67CD08-08AA-4C3B-97EB-583F546CD4EC@fgm.com>
Message-ID: <3977537a0910231130k7878e1f3qcc9b2e581f11c62d@mail.gmail.com>

On Fri, Oct 23, 2009 at 12:52 PM, Adams, Geoff <gadams at fgm.com> wrote:
> On 23 Oct 2009, at 12:10 PM, David Ritchie wrote:
>
>> I used the globalSAN initiator on my Mac Pro to connect to a zvol
>> over GigE. In this way I managed to lower the write performance to
>> <1MB/s - similar to the results Raoul Callaghan gets!!
>
> That's quite surprising.
>
> I actually just last night set up the globalSAN initiator on a
> PowerMac G5 (Mac OS 10.5.x), using the COMSTAR iSCSI target on
> OpenSolaris (build 125) running on a SunFire x4150. Over 100base-T
> (the GigE port on the G5 was lightning damaged), I see over 6 MB/s.
> Granted, this is nothing compared to the roughly 100 GB/s that I can
> see to that same zpool of cheap, large SATA disks locally, but 6 MB/s
> isn't too bad for 100base-T. I'll have to stick a GigE card in the
> client machine to see if it gets any better.
>
> hfuhruhurr:~: dd if=/dev/zero of=/Volumes/Time\ Machine/gig bs=1m
> count=1024
> 1024+0 records in
> 1024+0 records out
> 1073741824 bytes transferred in 160.978045 secs (6670113 bytes/sec)
>
> Not too bad for a first test, I think.

If I am not mistaken I can push 10 MB/s through FTP on a Mini over 100
MB/s network.  I figure that you should get closer to line speed from
iscsi.

>
> - Geoff
> (I apologize for the Exchange email system here at the office that
> insists on rudely HTMLizing my outgoing mail.)
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>

From dmz+lists at tffenterprises.com  Fri Oct 23 11:35:29 2009
From: dmz+lists at tffenterprises.com (Daniel M. Zimmerman)
Date: Fri, 23 Oct 2009 11:35:29 -0700
Subject: [zfs-discuss] Mailing list
In-Reply-To: <DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>
References: <20091023.141031.172993232.hanche@math.ntnu.no>
	<DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>
Message-ID: <8AA232221886E5339659BA4A@d-128-208-244-210.dhcp4.washington.edu>

Count me in.

-Dan

--On 23 October 2009 14:14:11 -0400 Dominik Hoffmann 
<dhoffmann at uwalumni.com> wrote:

> Everybody who wants to get on the new list reply! Then whoever has the
> means to set up a new list can cull the addresses.
>
> I'm in.
>
> Dominik Hoffmann
>
> On Oct 23, 2009, at 2:10 PM, Harald Hanche-Olsen wrote:
>
>> They're gonna take our mailing list away too. Many of us aren't
>> immediately going to stop using zfs on the mac because of this, though
>> in the long run we probably won't have any choice. In the meantime,
>> though, we will still need some kind of mailing list or forum in which
>> to discuss issues.
>>
>> Quickly now, before this list is yanked: Any suggestions?
>>
>> - Harald
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>



------------------------------------------------------------------
Daniel M. Zimmerman                                TFF Enterprises
1900 Commerce St. Box 358426   http://www.tffenterprises.com/~dmz/
Tacoma, WA  98402  USA                      dmz at tffenterprises.com

From alex.blewitt at gmail.com  Fri Oct 23 11:36:28 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Fri, 23 Oct 2009 19:36:28 +0100
Subject: [zfs-discuss] Mailing list
In-Reply-To: <913501.69632.qm@web88102.mail.re2.yahoo.com>
References: <913501.69632.qm@web88102.mail.re2.yahoo.com>
Message-ID: <DBE817C6-5475-4AAF-B8CE-6A747645A923@gmail.com>

I suggest we set up a google group and then add people on via there.  
I'm in though.

Alex

Sent from my (new) iPhone

On 23 Oct 2009, at 19:28, Jason Belec <jasonbelec at rogers.com> wrote:

> I'm in.
>
> Jason
>
> Sent from my iPhone
>
> On 2009-10-23, at 2:14 PM, Dominik Hoffmann <dhoffmann at uwalumni.com>  
> wrote:
>
> Everybody who wants to get on the new list reply! Then whoever has  
> the means to set up a new list can cull the addresses.
>
> I'm in.
>
> Dominik Hoffmann
>
> On Oct 23, 2009, at 2:10 PM, Harald Hanche-Olsen wrote:
>
> They're gonna take our mailing list away too. Many of us aren't
> immediately going to stop using zfs on the mac because of this, though
> in the long run we probably won't have any choice. In the meantime,
> though, we will still need some kind of mailing list or forum in which
> to discuss issues.
>
> Quickly now, before this list is yanked: Any suggestions?
>
> - Harald
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From bakul at bitblocks.com  Fri Oct 23 11:38:35 2009
From: bakul at bitblocks.com (Bakul Shah)
Date: Fri, 23 Oct 2009 11:38:35 -0700
Subject: [zfs-discuss] Mailing list
Message-ID: <20091023183835.A0BFE5B59@mail.bitblocks.com>

Ok, I have created zfs-macos at googelgroups.com -- please go to
    https://groups.google.com/grou/zfs-macos
and add yourself!

I hope a couple of you will step up to take over (or at least
share in) the management of this group. I don't use zfs much
on the mac (mostly on freebsd).  I created this group mainly
because I got tired of the me too messages!

From bakul at bitblocks.com  Fri Oct 23 11:39:51 2009
From: bakul at bitblocks.com (Bakul Shah)
Date: Fri, 23 Oct 2009 11:39:51 -0700
Subject: [zfs-discuss] Mailing list
In-Reply-To: Your message of "Fri, 23 Oct 2009 11:38:35 PDT."
Message-ID: <20091023183951.35A0C5B59@mail.bitblocks.com>

Note the correct address:
      https://groups.google.com/group/zfs-macos

On Fri, 23 Oct 2009 11:38:35 PDT Bakul Shah <bakul at bitblocks.com>  wrote:
> Ok, I have created zfs-macos at googelgroups.com -- please go to
>      https://groups.google.com/grou/zfs-macos
> and add yourself!
> 
> I hope a couple of you will step up to take over (or at least
> share in) the management of this group. I don't use zfs much
> on the mac (mostly on freebsd).  I created this group mainly
> because I got tired of the me too messages!

From alex.blewitt at gmail.com  Fri Oct 23 12:21:33 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Fri, 23 Oct 2009 20:21:33 +0100
Subject: [zfs-discuss] Mailing list
In-Reply-To: <20091023183835.A0BFE5B59@mail.bitblocks.com>
References: <20091023183835.A0BFE5B59@mail.bitblocks.com>
Message-ID: <2ACB916D-3AA6-4C23-BABE-39301FB3C8E5@gmail.com>

I'm happy to moderate and help move this forward.

Sent from my (new) iPhone

On 23 Oct 2009, at 19:38, Bakul Shah <bakul at bitblocks.com> wrote:

> Ok, I have created zfs-macos at googelgroups.com -- please go to
>    https://groups.google.com/grou/zfs-macos
> and add yourself!
>
> I hope a couple of you will step up to take over (or at least
> share in) the management of this group. I don't use zfs much
> on the mac (mostly on freebsd).  I created this group mainly
> because I got tired of the me too messages!
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From nathan.stocks at gmail.com  Fri Oct 23 12:36:09 2009
From: nathan.stocks at gmail.com (Nathan)
Date: Fri, 23 Oct 2009 13:36:09 -0600
Subject: [zfs-discuss] Mailing list
In-Reply-To: <2ACB916D-3AA6-4C23-BABE-39301FB3C8E5@gmail.com>
References: <20091023183835.A0BFE5B59@mail.bitblocks.com>
	<2ACB916D-3AA6-4C23-BABE-39301FB3C8E5@gmail.com>
Message-ID: <96c9d6a80910231236k2465c058pdf6126bc448f38ff@mail.gmail.com>

On Fri, Oct 23, 2009 at 1:21 PM, Alex Blewitt <alex.blewitt at gmail.com> wrote:
> I'm happy to moderate and help move this forward.
>
> Sent from my (new) iPhone
>
> On 23 Oct 2009, at 19:38, Bakul Shah <bakul at bitblocks.com> wrote:
>
>> Ok, I have created zfs-macos at googelgroups.com -- please go to
>> ? https://groups.google.com/grou/zfs-macos
>> and add yourself!
>>
>> I hope a couple of you will step up to take over (or at least
>> share in) the management of this group. I don't use zfs much
>> on the mac (mostly on freebsd). ?I created this group mainly
>> because I got tired of the me too messages!

zfs-discuss has already been removed from from lists.apple.com ...
this list may be down momentarily if it's not down already.

~ Nathan

From lists at chipmunk-app.com  Fri Oct 23 12:36:40 2009
From: lists at chipmunk-app.com (Ruotger Skupin)
Date: Fri, 23 Oct 2009 21:36:40 +0200
Subject: [zfs-discuss] Mailing list
In-Reply-To: <DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>
References: <20091023.141031.172993232.hanche@math.ntnu.no>
	<DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>
Message-ID: <8163ACD0-BF71-4A8B-B05F-1057E9E9CCDE@chipmunk-app.com>

count me in too!

Am 23.10.2009 um 20:14 schrieb Dominik Hoffmann:

> Everybody who wants to get on the new list reply! Then whoever has  
> the means to set up a new list can cull the addresses.
>
> I'm in.
>
> Dominik Hoffmann
>
> On Oct 23, 2009, at 2:10 PM, Harald Hanche-Olsen wrote:
>
>> They're gonna take our mailing list away too. Many of us aren't
>> immediately going to stop using zfs on the mac because of this,  
>> though
>> in the long run we probably won't have any choice. In the meantime,
>> though, we will still need some kind of mailing list or forum in  
>> which
>> to discuss issues.
>>
>> Quickly now, before this list is yanked: Any suggestions?
>>
>> - Harald
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From chris.shuman at gmail.com  Fri Oct 23 12:38:45 2009
From: chris.shuman at gmail.com (Chris Shuman)
Date: Fri, 23 Oct 2009 14:38:45 -0500
Subject: [zfs-discuss] Mailing list
Message-ID: <ac78d4820910231238m749d1808h41bd3c4760bfd74c@mail.gmail.com>

I'm in. I'll need everyone's help when I transition to a diff. OS.
Chris Shuman
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/53ae5a1d/attachment.html>

From james at themacplace.co.uk  Fri Oct 23 12:49:52 2009
From: james at themacplace.co.uk (James Relph)
Date: Fri, 23 Oct 2009 20:49:52 +0100
Subject: [zfs-discuss] Mailing list
In-Reply-To: <20091023.141031.172993232.hanche@math.ntnu.no>
References: <20091023.141031.172993232.hanche@math.ntnu.no>
Message-ID: <CE76D02A-C0F8-40BF-A64C-CE206207CF1C@themacplace.co.uk>

Count me in, willing to provide some server space or other resources  
to help if we can get something going.

James

Website:		www.themacplace.co.uk
Blog:		www.themacplace.co.uk/blog.html


From alex.blewitt at gmail.com  Fri Oct 23 12:54:35 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Fri, 23 Oct 2009 20:54:35 +0100
Subject: [zfs-discuss] ZFS project on Google Code
Message-ID: <AD1B2E16-FF87-4616-A13E-CF6B4B869DD4@gmail.com>

I've created this google code project for storing issues and wiki  
pages related to the project:

http://code.google.com/p/maczfs/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/37216267/attachment.html>

From developer at mvasilakis.com  Fri Oct 23 13:54:51 2009
From: developer at mvasilakis.com (Dev)
Date: Fri, 23 Oct 2009 13:54:51 -0700
Subject: [zfs-discuss] Mailing list
In-Reply-To: <DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>
References: <20091023.141031.172993232.hanche@math.ntnu.no>
	<DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>
Message-ID: <F635D6AA-F32A-457D-8CAB-DA0581A504B4@mvasilakis.com>

I'm in

Sent from my iPhone

On Oct 23, 2009, at 11:14 AM, Dominik Hoffmann  
<dhoffmann at uwalumni.com> wrote:

> Everybody who wants to get on the new list reply! Then whoever has  
> the means to set up a new list can cull the addresses.
>
> I'm in.
>
> Dominik Hoffmann
>
> On Oct 23, 2009, at 2:10 PM, Harald Hanche-Olsen wrote:
>
>> They're gonna take our mailing list away too. Many of us aren't
>> immediately going to stop using zfs on the mac because of this,  
>> though
>> in the long run we probably won't have any choice. In the meantime,
>> though, we will still need some kind of mailing list or forum in  
>> which
>> to discuss issues.
>>
>> Quickly now, before this list is yanked: Any suggestions?
>>
>> - Harald
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From mattsnow at gmail.com  Fri Oct 23 13:57:48 2009
From: mattsnow at gmail.com (Matt Snow)
Date: Fri, 23 Oct 2009 13:57:48 -0700
Subject: [zfs-discuss] Mailing list
In-Reply-To: <DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>
References: <20091023.141031.172993232.hanche@math.ntnu.no>
	<DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>
Message-ID: <6879ebc80910231357n2022b887p8ec7be9e2fc48810@mail.gmail.com>

count me in please.

..Matt

On Fri, Oct 23, 2009 at 11:14 AM, Dominik Hoffmann
<dhoffmann at uwalumni.com>wrote:

> Everybody who wants to get on the new list reply! Then whoever has the
> means to set up a new list can cull the addresses.
>
> I'm in.
>
> Dominik Hoffmann
>
>
> On Oct 23, 2009, at 2:10 PM, Harald Hanche-Olsen wrote:
>
>  They're gonna take our mailing list away too. Many of us aren't
>> immediately going to stop using zfs on the mac because of this, though
>> in the long run we probably won't have any choice. In the meantime,
>> though, we will still need some kind of mailing list or forum in which
>> to discuss issues.
>>
>> Quickly now, before this list is yanked: Any suggestions?
>>
>> - Harald
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/6aad8533/attachment-0001.html>

From alex.blewitt at gmail.com  Fri Oct 23 14:50:32 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Fri, 23 Oct 2009 22:50:32 +0100
Subject: [zfs-discuss] Thanks for everything
Message-ID: <636fd28e0910231450i2e068824pb4bcae3c0f1a83e8@mail.gmail.com>

Don't know if this is going to make the (old) list or not, but thanks
to everyone at Apple who made ZFS what it is today. One assumes that
not all has gone well inside, and that it hasn't gone the way some
might have wanted it to, but thanks all the same for getting it as far
as it did.

Alex

From hanche at math.ntnu.no  Fri Oct 23 15:35:23 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Fri, 23 Oct 2009 18:35:23 -0400 (EDT)
Subject: [zfs-discuss] Thanks for everything
In-Reply-To: <636fd28e0910231450i2e068824pb4bcae3c0f1a83e8@mail.gmail.com>
References: <636fd28e0910231450i2e068824pb4bcae3c0f1a83e8@mail.gmail.com>
Message-ID: <20091023.183523.195837932.hanche@math.ntnu.no>

+ Alex Blewitt <alex.blewitt at gmail.com>:

> Don't know if this is going to make the (old) list or not, but thanks
> to everyone at Apple who made ZFS what it is today. One assumes that
> not all has gone well inside, and that it hasn't gone the way some
> might have wanted it to, but thanks all the same for getting it as far
> as it did.

And especially thanks to No?l, who was often quite helpful on this
list. No?l, I hope you still have a job with Apple. If not, feel free
to come on over to the new mailing list and bitch about it.

And thanks for all the fish. 8-)

- Harald

From macanuck at mac.com  Fri Oct 23 16:56:38 2009
From: macanuck at mac.com (R.D. 'Bob' Shapka)
Date: Fri, 23 Oct 2009 17:56:38 -0600
Subject: [zfs-discuss] Mailing list
In-Reply-To: <DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>
References: <20091023.141031.172993232.hanche@math.ntnu.no>
	<DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>
Message-ID: <3052CD19-8A45-4FF5-9573-5EB784EACC21@mac.com>

I'm in

Bob Shapka

On 2009, Oct 23, , at 12:14 PM, Dominik Hoffmann wrote:

> Everybody who wants to get on the new list reply! Then whoever has  
> the means to set up a new list can cull the addresses.
>
> I'm in.
>
> Dominik Hoffmann
>
> On Oct 23, 2009, at 2:10 PM, Harald Hanche-Olsen wrote:
>
>> They're gonna take our mailing list away too. Many of us aren't
>> immediately going to stop using zfs on the mac because of this,  
>> though
>> in the long run we probably won't have any choice. In the meantime,
>> though, we will still need some kind of mailing list or forum in  
>> which
>> to discuss issues.
>>
>> Quickly now, before this list is yanked: Any suggestions?
>>
>> - Harald
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From david299792 at googlemail.com  Fri Oct 23 17:11:10 2009
From: david299792 at googlemail.com (David Ritchie)
Date: Sat, 24 Oct 2009 01:11:10 +0100
Subject: [zfs-discuss] Mailing list
In-Reply-To: <913501.69632.qm@web88102.mail.re2.yahoo.com>
References: <913501.69632.qm@web88102.mail.re2.yahoo.com>
Message-ID: <CDEE4D63-D676-42C2-B9D2-BA423A122BA0@googlemail.com>

I'm in too.

-- David

On 23 Oct 2009, at 19:28, Jason Belec wrote:

> I'm in.
>
> Jason
>
> Sent from my iPhone
>
> On 2009-10-23, at 2:14 PM, Dominik Hoffmann <dhoffmann at uwalumni.com>  
> wrote:
>
> Everybody who wants to get on the new list reply! Then whoever has  
> the means to set up a new list can cull the addresses.
>
> I'm in.
>
> Dominik Hoffmann
>
> On Oct 23, 2009, at 2:10 PM, Harald Hanche-Olsen wrote:
>
> They're gonna take our mailing list away too. Many of us aren't
> immediately going to stop using zfs on the mac because of this, though
> in the long run we probably won't have any choice. In the meantime,
> though, we will still need some kind of mailing list or forum in which
> to discuss issues.
>
> Quickly now, before this list is yanked: Any suggestions?
>
> - Harald
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From developer at mvasilakis.com  Fri Oct 23 19:56:05 2009
From: developer at mvasilakis.com (mano vasilakis)
Date: Fri, 23 Oct 2009 19:56:05 -0700
Subject: [zfs-discuss] Thanks for everything
In-Reply-To: <2531418D-7584-4933-9EE0-125C1ABACD5C@gmail.com>
References: <636fd28e0910231450i2e068824pb4bcae3c0f1a83e8@mail.gmail.com>
	<2bfa7a9b0910231610s1c65f026qa8a8421c217aae84@mail.gmail.com>
	<2531418D-7584-4933-9EE0-125C1ABACD5C@gmail.com>
Message-ID: <2bfa7a9b0910231956y533cfd0q91952b60307716a0@mail.gmail.com>

Oops. I was in a hurry and didn't notice.
As for the code I meant the 10a2xx version that was in one of the Snow
Leopard betas.
I think they only put 119 up in the repository.



On Fri, Oct 23, 2009 at 4:12 PM, Alex Blewitt <alex.blewitt at gmail.com>wrote:

> That went just to me (I think). The code is available via links on
> http://code.google.com/p/maczfs/
>
> On 24 Oct 2009, at 00:10, mano vasilakis wrote:
>
> Agreed. Thanks for trying. And if it ever becomes possible please post the
> latest code you guys developed. I'm sure it would help devs that pick the
> project up.
>
> I came here late but the group here seems cool. I started to look forward
> to the emails.
>
>
> On Fri, Oct 23, 2009 at 2:50 PM, Alex Blewitt <alex.blewitt at gmail.com>wrote:
>
>> Don't know if this is going to make the (old) list or not, but thanks
>> to everyone at Apple who made ZFS what it is today. One assumes that
>> not all has gone well inside, and that it hasn't gone the way some
>> might have wanted it to, but thanks all the same for getting it as far
>> as it did.
>>
>> Alex
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/ad442440/attachment.html>

From robert.muench at gmail.com  Sat Oct 24 03:49:09 2009
From: robert.muench at gmail.com (Robert Muench)
Date: Sat, 24 Oct 2009 12:49:09 +0200
Subject: [zfs-discuss] How uninstall ZFS?
Message-ID: <9efc39d30910240349g590a2569u83047d7fbf89c384@mail.gmail.com>

Hi, since ZFS will always be an underground project, I want to
uninstall it on my normal system. How do I uninstall it? Just delete
the partition and delete the kernel extension?

-- 
Robert M. M?nch

From alex.blewitt at gmail.com  Sat Oct 24 03:59:25 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Sat, 24 Oct 2009 11:59:25 +0100
Subject: [zfs-discuss] How uninstall ZFS?
In-Reply-To: <9efc39d30910240349g590a2569u83047d7fbf89c384@mail.gmail.com>
References: <9efc39d30910240349g590a2569u83047d7fbf89c384@mail.gmail.com>
Message-ID: <4E2E787F-3D0D-4DC9-9FF7-81C5E139A9E4@gmail.com>

On 24 Oct 2009, at 11:49, Robert Muench wrote:

> Hi, since ZFS will always be an underground project, I want to
> uninstall it on my normal system. How do I uninstall it? Just delete
> the partition and delete the kernel extension?

Good question :-)

I've put a wiki answer on the new code page:

http://code.google.com/p/maczfs/wiki/Uninstalling

Bear in mind that other non-Apple file systems exist and are supported  
(for some value of supported) such as those on http://code.google.com/p/macfuse/ 
  - just because it's not Apple-blessed doesn't mean it's stopped  
working. It does mean, however, that it's up to us and that any OS  
apps (Finder, spotlight) are outside of control, and as such might not  
be able to be fixed.

Alex

From daniel at netwalk.org  Fri Oct 16 20:47:43 2009
From: daniel at netwalk.org (Daniel)
Date: Fri, 16 Oct 2009 20:47:43 -0700
Subject: [zfs-discuss] zfs status
Message-ID: <3c72d6220910162047t56eb88d2v14918e205bd1ee7b@mail.gmail.com>

Hi,

What's the status of this project, and in particular, what's the status of
this project on snow leopard?

-d

-- 
"America was founded by men who understood that the threat of domestic
tyranny is as great as any threat from abroad. If we want to be worthy of
their legacy, we must resist the rush toward ever-increasing state control
of our society. Otherwise, our own government will become a greater threat
to our freedoms than any foreign terrorist."
- Ron Paul, Texas Straight Talk, May 31, 2004
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091016/388ef21d/attachment.html>

From daniel at netwalk.org  Sat Oct 17 12:07:23 2009
From: daniel at netwalk.org (Daniel)
Date: Sat, 17 Oct 2009 12:07:23 -0700
Subject: [zfs-discuss] zfs-10a286 ?
Message-ID: <3c72d6220910171207u3b6bfdebi78263d16ba457dab@mail.gmail.com>

hi, I see talk about zfs-10a286 source in the zfs-10a286 patch for x86_64
kernel thread.

WHere can I download this code? Doesn't show up on the page.

Also, it would be really nice if someone could throw together what's
required to get ZFS working on Snow Leopard.

I'm considering moving from my FreeBSD server to osx, but as I have a lot of
data on ZFS, the lack of ZFS makes it cumbersome.

-d

-- 
"America was founded by men who understood that the threat of domestic
tyranny is as great as any threat from abroad. If we want to be worthy of
their legacy, we must resist the rush toward ever-increasing state control
of our society. Otherwise, our own government will become a greater threat
to our freedoms than any foreign terrorist."
- Ron Paul, Texas Straight Talk, May 31, 2004
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091017/71bd19d7/attachment.html>

From melliott at ncsa.illinois.edu  Fri Oct 23 09:33:02 2009
From: melliott at ncsa.illinois.edu (Matt Elliott)
Date: Fri, 23 Oct 2009 11:33:02 -0500
Subject: [zfs-discuss] RIP
Message-ID: <8846AF4A-0D59-4D0E-8EEA-9FF57E3B884E@ncsa.illinois.edu>

Well, this was a depressing sign on Friday afternoon.

http://zfs.macosforge.org/

- Matt

From c.fernando at mac.com  Fri Oct 23 10:10:00 2009
From: c.fernando at mac.com (Christopher Fernando)
Date: Fri, 23 Oct 2009 11:10:00 -0600
Subject: [zfs-discuss] ZFS for OS X, we hardly knew thee?
In-Reply-To: <67C98DBD-89A5-47AD-A6D4-6CD8DF42CC04@mac.com>
References: <AC090380-B57D-4229-9B8A-1977B5FB81CA@mac.com>
	<38DC1DB8-020C-47FA-A341-9ED5095BA2A0@mac.com>
	<67C98DBD-89A5-47AD-A6D4-6CD8DF42CC04@mac.com>
Message-ID: <EB322EFD-060B-4378-8E99-9A1AC37C04E7@mac.com>

I'm not entirely sure why this behavior by Apple is surprising. This  
is how they've handled many of their "skunkworks" projects. I  
distinctly remember MkLinux having a very similar cycle - being  
dropped suspiciously and with little word and then all of a sudden Mac  
OS X Beta appearing with a very similar MACH kernel base except with a  
better userland.

While it may be painful in the short term, I think the long term  
results will yield something surprising and amazing.

At least that's my hope.

I appreciate all the work/help everyone has done to make this project  
happen - I hope to see you on the "flip side" of ZFS.


On Oct 23, 2009, at 11:03 AM, Bayard Bell wrote:

> I think there are two distinct problems here: one is that Apple's  
> decided to bail on a filesystem far better than HFS+ and the other  
> is that the way they've bailed doesn't show a lot of respect for the  
> community that did take up the product. I mean, pulling all the code  
> and documentation without giving people a chance to make some  
> decisions about how to deal with this shows no consideration for  
> those who have, understanding that this is something of a skunkworks  
> project, accepted the caveats and made use of this. People should  
> expect better than to have an announcement made in the first  
> instance by way of redirecting attempts to access code, binaries,  
> and documentation. This isn't simply a matter of legality: this is  
> telling any observer that Apple brooks no compromises between profit  
> and property and community-building.
>
> Apple, please reconsider being gracious about how you wind this  
> down. So far, you're not showing a sense of community partnership.
>
> Am 23 Oct 2009 um 17:46 schrieb Byron Servies:
>
>> On Oct 23, 2009, at 9:35 AM, Bayard Bell wrote:
>>
>>> Just as I'm in the process of building my first zfs pool, using  
>>> disk that arrived earlier this week, I was shocked to find the  
>>> following text abruptly appearing on zfs.macosforge.org with  
>>> today's date:
>>>
>>> The ZFS project has been discontinued. The mailing list and  
>>> repository will also be removed shortly.
>>
>> Yup.  The HFS long-hairs have won. Most junk will be grafted on to  
>> their crufty file system shortly.
>>
>> Byron
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From wyatt at draggoo.com  Fri Oct 23 20:43:53 2009
From: wyatt at draggoo.com (Wyatt Draggoo)
Date: Fri, 23 Oct 2009 20:43:53 -0700
Subject: [zfs-discuss] Mailing list
In-Reply-To: <DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>
References: <20091023.141031.172993232.hanche@math.ntnu.no>
	<DAA63752-A63C-49A0-A0D6-4D8F88AB9602@uwalumni.com>
Message-ID: <20091024034352.GE22158@royale.draggoo.com>

Been lurking for a while but as someone who's still running ZFS the info is great. I'd like to be on the new list!

On Fri, Oct 23, 2009 at 02:14:11PM -0400, Dominik Hoffmann wrote:
> Everybody who wants to get on the new list reply! Then whoever has
> the means to set up a new list can cull the addresses.
> 
> I'm in.
> 
> Dominik Hoffmann
> 
> On Oct 23, 2009, at 2:10 PM, Harald Hanche-Olsen wrote:
> 
> >They're gonna take our mailing list away too. Many of us aren't
> >immediately going to stop using zfs on the mac because of this, though
> >in the long run we probably won't have any choice. In the meantime,
> >though, we will still need some kind of mailing list or forum in which
> >to discuss issues.
> >
> >Quickly now, before this list is yanked: Any suggestions?
> >
> >- Harald
> >_______________________________________________
> >zfs-discuss mailing list
> >zfs-discuss at lists.macosforge.org
> >http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >
> 



> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 198 bytes
Desc: not available
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091023/35c07809/attachment.bin>

From gwb0908 at 2Realms.com  Fri Oct 23 23:42:42 2009
From: gwb0908 at 2Realms.com (426F6C74 55707269676874)
Date: Sat, 24 Oct 2009 01:42:42 -0500
Subject: [zfs-discuss] Possible alternatives and posting to both lists.
Message-ID: <9ae776de0910232342o545b58e3j16ca5a26260d82df@mail.gmail.com>

Thanks to everybody on the list for all your help.  Please put me in
on the new list, and leave me on the old one as long as it runs.

I follow this list, and several other zfs related lists (Nexenta and
Belenix), and I use several zfs capable OS's in office computers.

I've joined the google list, and I'm posting to both lists,
zfs-discuss at lists.macosforge.org and zfs-macos at googlegroups.com, in
case I miss something from either one in the transition.  Who knows,
maybe they won't yank the macosforge list entirely, or too quickly.  I
think it would be good to at least to document how zfs on mac os x
came to be, and then came not to be.  I don't think its right for
Apple, or anyone else, to change and delete items from their website,
and then pretend it never happened.  When you see a company, group,
government or any kind of organisation using this kind of "memory
hole" technique, you need to call them on it.

>From what I have read on the lists for some time, the speculation
probably is close to the mark, but things may be less complicated or
mysterious than they appear.  Apple's highest profit margins come from
it's gadgets, iPhones and iPods.  Apple also sold more Macs than any
previous quarter - 3.05 million - but the price pressure on computers
is greater, meaning their profit margin on laptops, desktops, etc., is
less than on the gadgets.

Where apple has sold least, and where their profit margin has been
slim or non-existent, has been in the business and enterprise section.
 They seem to have dropped the xserve, and have given their enterprise
business to Promise.  Apple apparently believes that ZFS and os x
server are, largely, business and enterprise products, with little
appeal to their most lucrative market, the retail customer.  So apple
has little incentive to keep and develop enterprise level hardware and
software (xserve, zfs, os x server).

What is unfortunate is that Apple seems able to give this information
in (or it can be easily understood from) their reports to analysts and
investors, but they will not say this to the people who actually buy,
use and want to keep using, their products.  So the best way to
determine what Apple will do is to read the financial information they
give, and also what financial analysts are saying.  And, if you really
want to have a good laugh, ask an Apple rep or employee about
something Apple has already told investors, and watch them deny it, or
claim it never happened, or won't happen.

Do I blame Apple for this?  No.  Do they make great products?  Yes,
they do.  Would I consider them for long term business or enterprise
users, small, large or in between?  Not any more.  Do serious and
successful companies drop an entire line of servers, and then have key
features of an upcoming file system disappear without explanation?
Apparently Apple will be the only one to pull this off, but it is
pretty much unheard of among the Hewlitt Packards, IBM's, Solarises
(Solareses?), and Microsofts.  Would I buy more of their stock?  Sure
I would, just as soon as the price drops back down after the next
correction.  Is it an amazing business model?  You bet, but I'm not
sure they planned to be a mostly retail maker and seller of iPhones
and iPods that sells computers on the side, in their own proprietary
stores.  But, hey, it works for them.

After getting zfs to work on several macs in the office some time ago,
I decided it was sufficiently buggy that I needed a backup.  I mean
that literally.  I needed to move zpools of one version to another,
because the versions on OpenSolaris, FreeBSD and OS X varied over
time.  So I took older, generic machines (any recent intel board with
4 to 8 gigs of memory will do) and set them up as fileservers, running
zfs over cifs (samba).  Of the opensolaris variants, at least 2 keep
fairly up to date with revisions in ZFS:

NexentaCore (the free version of Nexenta)
Belenix

(I don't know how the list handles urls, but you can see the listing
at opensolaris.org, os/downloads/)

The learning curve can be high for these two, and they still have some
kinks.  But they run zfs over samba very well.  Or just use
OpenSolaris.  FreeBSD is also great, and is closer to os x, but the
zpool version lags somewhat compared to opensolaris.

So, yes, by all means, let's try to get zfs better and more stable on
os x, but in the meantime, I would suggest you get another machine
running your zfs pools, and sharing them over samba (or NFS, or
webdav).

It would be great to have time machine working on a zpool.  Some of
the people on this list have probably done it by using a sparse bundle
disk image, and then changing the plist (or whatever apple calls it)
for time machine back ups.  But I would worry about Apple changing or
modifying Time Machine in some way that it won't work with that set up
in future revisions.

So, please, keep up the good work.  I can't help or contribute much to
the 10.6 discussion, because I'm not buying it, or new apple machines,
for the near future.  I'll keep following the list, and if someone is
interested in some of the other zfs capable OS's, I can point them to
sources, and a few "how-to's".

Gordon

From hanche at math.ntnu.no  Sat Oct 24 14:23:09 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Sat, 24 Oct 2009 17:23:09 -0400 (EDT)
Subject: [zfs-discuss] zfs status
In-Reply-To: <3c72d6220910162047t56eb88d2v14918e205bd1ee7b@mail.gmail.com>
References: <3c72d6220910162047t56eb88d2v14918e205bd1ee7b@mail.gmail.com>
Message-ID: <20091024.172309.59324710.hanche@math.ntnu.no>

+ Daniel <daniel at netwalk.org>:

> What's the status of this project, and in particular, what's the
> status of this project on snow leopard?

Where have you been lately? Apple has dropped it. The news is all over
the Intertubes. There's a slashdot article on it too. The good news is
that the old (119) bits still work, even on snow leopard, and the
source code is available for people to hack on. The bad news is that
there may be a shortage of able and willing folks to do the hacking.
More good news is that maybe the fuse version that the linux folks
have been working on could perhaps be ported to work with MacFuse.
More bad news is that this may come at an unacceptable performance
cost. Time will tell. We live in interesting times. News at 11.

- Harald

From james at themacplace.co.uk  Sat Oct 24 15:22:21 2009
From: james at themacplace.co.uk (James Relph)
Date: Sat, 24 Oct 2009 23:22:21 +0100
Subject: [zfs-discuss] Possible alternatives and posting to both lists.
In-Reply-To: <9ae776de0910232342o545b58e3j16ca5a26260d82df@mail.gmail.com>
References: <9ae776de0910232342o545b58e3j16ca5a26260d82df@mail.gmail.com>
Message-ID: <E2A103E7-0914-4DC6-9561-3502B75D4712@themacplace.co.uk>

> They seem to have dropped the xserve, and have given their enterprise
> business to Promise.

I think you're getting your hardware confused there, the Xserve is  
very much alive and kicking (and is actually a pretty well specced  
server for the price).

> Do serious and
> successful companies drop an entire line of servers,

Not a great example seeing as Apple didn't.

> So, yes, by all means, let's try to get zfs better and more stable on
> os x, but in the meantime, I would suggest you get another machine
> running your zfs pools, and sharing them over samba (or NFS, or
> webdav).

Which is fine if you don't need AFP support, or NetBoot, or the  
ability to easily control MCX settings.  There are some situations  
where Mac OS X Server is by far the best solution; it would just be  
good to have ZFS support.  Switching OS just for a filesystem may work  
in some places, but definitely isn't the complete answer here.

> It would be great to have time machine working on a zpool.

Not sure why, if you were using ZFS then Time Machine would be an  
entirely pointless exercise, disk image or not.

James

ACSA 10.5

Website:		www.themacplace.co.uk
Blog:		www.themacplace.co.uk/blog.html
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091024/4f723414/attachment-0001.html>

From nefar at otherware.org  Sat Oct 24 18:45:11 2009
From: nefar at otherware.org (Daniel)
Date: Sat, 24 Oct 2009 18:45:11 -0700
Subject: [zfs-discuss] zfs status
In-Reply-To: <20091024.172309.59324710.hanche@math.ntnu.no>
References: <3c72d6220910162047t56eb88d2v14918e205bd1ee7b@mail.gmail.com>
	<20091024.172309.59324710.hanche@math.ntnu.no>
Message-ID: <3c72d6220910241845w55368541t8413cd6e71574363@mail.gmail.com>

I sent this email 8 days ago. I did follow what's going on, but all that
happened AFTER I sent this original mail.

On Sat, Oct 24, 2009 at 2:23 PM, Harald Hanche-Olsen <hanche at math.ntnu.no>wrote:

> + Daniel <daniel at netwalk.org>:
>
> > What's the status of this project, and in particular, what's the
> > status of this project on snow leopard?
>
> Where have you been lately? Apple has dropped it. The news is all over
> the Intertubes. There's a slashdot article on it too. The good news is
> that the old (119) bits still work, even on snow leopard, and the
> source code is available for people to hack on. The bad news is that
> there may be a shortage of able and willing folks to do the hacking.
> More good news is that maybe the fuse version that the linux folks
> have been working on could perhaps be ported to work with MacFuse.
> More bad news is that this may come at an unacceptable performance
> cost. Time will tell. We live in interesting times. News at 11.
>
> - Harald
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>



-- 
"America was founded by men who understood that the threat of domestic
tyranny is as great as any threat from abroad. If we want to be worthy of
their legacy, we must resist the rush toward ever-increasing state control
of our society. Otherwise, our own government will become a greater threat
to our freedoms than any foreign terrorist."
- Ron Paul, Texas Straight Talk, May 31, 2004
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20091024/c9ddea29/attachment.html>

From hanche at math.ntnu.no  Sat Oct 24 21:44:11 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Sun, 25 Oct 2009 00:44:11 -0400 (EDT)
Subject: [zfs-discuss] zfs status
In-Reply-To: <3c72d6220910241845w55368541t8413cd6e71574363@mail.gmail.com>
References: <3c72d6220910162047t56eb88d2v14918e205bd1ee7b@mail.gmail.com>
	<20091024.172309.59324710.hanche@math.ntnu.no>
	<3c72d6220910241845w55368541t8413cd6e71574363@mail.gmail.com>
Message-ID: <20091025.004411.212119874.hanche@math.ntnu.no>

+ Daniel <nefar at otherware.org>:

> I sent this email 8 days ago.

Ah, indeed. I didn't notice the date; and such delays on mailing lists
are extremely rare in my experience. I wonder what caused it. Maybe it
got stuck in moderation for some oddball reason?

- Harald

From hanche at math.ntnu.no  Sat Oct 24 21:47:12 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Sun, 25 Oct 2009 00:47:12 -0400 (EDT)
Subject: [zfs-discuss] zfs status
In-Reply-To: <20091025.004411.212119874.hanche@math.ntnu.no>
References: <20091024.172309.59324710.hanche@math.ntnu.no>
	<3c72d6220910241845w55368541t8413cd6e71574363@mail.gmail.com>
	<20091025.004411.212119874.hanche@math.ntnu.no>
Message-ID: <20091025.004712.162975957.hanche@math.ntnu.no>

+ Harald Hanche-Olsen <hanche at math.ntnu.no>:

> Ah, indeed. I didn't notice the date; and such delays on mailing lists
> are extremely rare in my experience. I wonder what caused it. Maybe it
> got stuck in moderation for some oddball reason?

Wait, now I see it. You use two different email addresses, and it
seems your first mail was sent from an address different from your
subscription address. That would explain it. Well, sorry if my
response seemed needlessly grumpy.

- Harald

