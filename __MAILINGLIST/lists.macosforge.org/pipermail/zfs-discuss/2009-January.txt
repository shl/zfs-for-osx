From ornstedt at users.sourceforge.net  Thu Jan  1 08:23:54 2009
From: ornstedt at users.sourceforge.net (=?ISO-8859-1?Q?Jan=20=D6rnstedt?=)
Date: Thu, 01 Jan 2009 17:23:54 +0100
Subject: [zfs-discuss] Bug Finder & ZFS interaction.
Message-ID: <1230827034.24476.1292571105@webmail.messagingengine.com>

Hi,

I use the latest zfs 119 beta. I noticed some issues when trying to copy
files to a zpool on a usb drive.

I created the following test scenario.

A folder "TEST" on an HFS volume.

It contains the following 6 files:

Test 1
Test 2
Test 3
Test ?
Test ?
Test ?

If I copy the whole folder all files get copied. However if I try to
only copy the files only the 3 first files will stay in the zfs pool. I
get the impression that all files gets copied however in the end the
last 3 directly gets removed?!

Any ideas? Is this a know issue? Have tried to search but not found
anything.

Happy New Year!
Jan

From dirkschelfhout at mac.com  Thu Jan  1 12:07:27 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Thu, 01 Jan 2009 21:07:27 +0100
Subject: [zfs-discuss] follow up Re:  A Glorious Hack
In-Reply-To: <EC3F4528-DEFB-47A6-83A5-6D1BBAE8E0D6@mac.com>
References: <2b0225fb0812281241q2c1fa480xd7b2b89c311e7a3d@mail.gmail.com>
	<2b0225fb0812281242s40120261j68e061806df8659d@mail.gmail.com>
	<327b821f0812290757u1141628dx1975a3b54b2eac88@mail.gmail.com>
	<EC3F4528-DEFB-47A6-83A5-6D1BBAE8E0D6@mac.com>
Message-ID: <19A16310-210A-4281-AA3C-170B0B801949@mac.com>

I created an mirror on an external firewire drive ( 2 partitions )
exported it under osx.
tried to create hard links to the rdisk ( osx doesn't let me )
soft links work ok.
nfs share the directory containing the 2 links to the zfs partitions.
mount these in solaris running inside virtualbox.
thats how far i got, then it stops working, I think solaris sees the 2  
files
as links to its own /dev directory.

Dirk

On 31 Dec 2008, at 10:24, Dirk Schelfhout wrote:

> Could I do the same with solaris running in virtualbox ?
> unmount my zfs raidz in osx
> create hardlinks to the rdisk slices into a shared folder with solaris
> and then import the zfs in solaris ?
> Can something go wrong ?
> I'll try testing this
> Dirk
> On 29 Dec 2008, at 16:57, Germano Caronni wrote:
>
>> the thing about non-empty mount points blocking snapshots is *very*  
>> useful to know.
>> thanks for sharing!
>>
>>
>> On Sun, Dec 28, 2008 at 21:42, Boyd Waters <bwaters at nrao.edu> wrote:
>> > I created VMWare "raw" disks for each device in the pool, then  
>> added
>> them to an OpenSolaris virtual machine.
>>
>>
>> My RAIDZ is on the four internal SATA bays of my Mac Pro; that's
>> /dev/disk{0,1,2,3}
>>
>> To let a VMWare virtual machine see these, it'll need to have a way  
>> to
>> translate a virtual disk (vmdk) to a real disk. This is called "raw"
>> access.
>>
>> Like this:
>>
>>  /Library/Application\ Support/VMware\ Fusion/vmware-rawdiskCreator \
>>   create \
>>   /dev/disk0 \
>>   1 \
>>   ~/Documents/Virtual\ Machines.localized/Solaris.vmwarevm/rawdisk0 \
>>   ide
>>
>>
>> That command will create a text file that maps the raw disk to the  
>> virtual
>> machine, and a binary file that is just an MBR (master boot record)  
>> that
>> might make Winders happy. I don't think that MBR is needed, but
>> whatever...
>>
>> I can't add more than 4 IDE drives to a virtual machine.
>> vmware-rawdiskCreator claims that it can create SCSI drives as  
>> well, but
>> it lies. Fortunately, the vmdk text file that it spits out is dead  
>> simple
>> to edit; I replaced this line
>>
>>   ddb.adapterType = "ide"
>>
>> with this one
>>
>>   ddb.adapterType = "buslogic"
>>
>>
>> For each of the four hard disks in my RAIDZ.
>>
>> Then I added these vmdk disks to the VM via the VMWare Fusion GUI.  
>> Fine.
>> Fired it up, those four disks show up fine in the OpenSolaris VM,  
>> and away
>> you go!
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090101/a7078c90/attachment.html>

From zorg at sogeeky.net  Thu Jan  1 12:27:22 2009
From: zorg at sogeeky.net (Mr. Zorg)
Date: Thu, 1 Jan 2009 12:27:22 -0800
Subject: [zfs-discuss] follow up Re:  A Glorious Hack
In-Reply-To: <19A16310-210A-4281-AA3C-170B0B801949@mac.com>
References: <2b0225fb0812281241q2c1fa480xd7b2b89c311e7a3d@mail.gmail.com>
	<2b0225fb0812281242s40120261j68e061806df8659d@mail.gmail.com>
	<327b821f0812290757u1141628dx1975a3b54b2eac88@mail.gmail.com>
	<EC3F4528-DEFB-47A6-83A5-6D1BBAE8E0D6@mac.com>
	<19A16310-210A-4281-AA3C-170B0B801949@mac.com>
Message-ID: <0563271D-1703-4ACA-86A7-0D635B6DAF3C@sogeeky.net>

A soft link would, yes. It's nothing more than a text file with a path  
in it. So when solaris reads it, it would logically interpret /dev/ 
whatever to be in its filesystem. For this reason, softlinks never  
work on shared drives.

It probably won't let you hardlink because hardlinks point directly to  
the same space on disk as another file. But /dev devices aren't file  
pointers. They're yet another kind of special pointer to a physical  
device.

On Jan 1, 2009, at 12:07 PM, Dirk Schelfhout <dirkschelfhout at mac.com>  
wrote:

> I created an mirror on an external firewire drive ( 2 partitions )
> exported it under osx.
> tried to create hard links to the rdisk ( osx doesn't let me )
> soft links work ok.
> nfs share the directory containing the 2 links to the zfs partitions.
> mount these in solaris running inside virtualbox.
> thats how far i got, then it stops working, I think solaris sees the  
> 2 files
> as links to its own /dev directory.
>
> Dirk
>
> On 31 Dec 2008, at 10:24, Dirk Schelfhout wrote:
>
>> Could I do the same with solaris running in virtualbox ?
>> unmount my zfs raidz in osx
>> create hardlinks to the rdisk slices into a shared folder with  
>> solaris
>> and then import the zfs in solaris ?
>> Can something go wrong ?
>> I'll try testing this
>> Dirk
>> On 29 Dec 2008, at 16:57, Germano Caronni wrote:
>>
>>> the thing about non-empty mount points blocking snapshots is  
>>> *very* useful to know.
>>> thanks for sharing!
>>>
>>>
>>> On Sun, Dec 28, 2008 at 21:42, Boyd Waters <bwaters at nrao.edu> wrote:
>>> > I created VMWare "raw" disks for each device in the pool, then  
>>> added
>>> them to an OpenSolaris virtual machine.
>>>
>>>
>>> My RAIDZ is on the four internal SATA bays of my Mac Pro; that's
>>> /dev/disk{0,1,2,3}
>>>
>>> To let a VMWare virtual machine see these, it'll need to have a  
>>> way to
>>> translate a virtual disk (vmdk) to a real disk. This is called "raw"
>>> access.
>>>
>>> Like this:
>>>
>>>  /Library/Application\ Support/VMware\ Fusion/vmware- 
>>> rawdiskCreator \
>>>   create \
>>>   /dev/disk0 \
>>>   1 \
>>>   ~/Documents/Virtual\ Machines.localized/Solaris.vmwarevm/ 
>>> rawdisk0 \
>>>   ide
>>>
>>>
>>> That command will create a text file that maps the raw disk to the  
>>> virtual
>>> machine, and a binary file that is just an MBR (master boot  
>>> record) that
>>> might make Winders happy. I don't think that MBR is needed, but
>>> whatever...
>>>
>>> I can't add more than 4 IDE drives to a virtual machine.
>>> vmware-rawdiskCreator claims that it can create SCSI drives as  
>>> well, but
>>> it lies. Fortunately, the vmdk text file that it spits out is dead  
>>> simple
>>> to edit; I replaced this line
>>>
>>>   ddb.adapterType = "ide"
>>>
>>> with this one
>>>
>>>   ddb.adapterType = "buslogic"
>>>
>>>
>>> For each of the four hard disks in my RAIDZ.
>>>
>>> Then I added these vmdk disks to the VM via the VMWare Fusion GUI.  
>>> Fine.
>>> Fired it up, those four disks show up fine in the OpenSolaris VM,  
>>> and away
>>> you go!
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090101/bf12e7cd/attachment.html>

From btm at pobox.com  Thu Jan  1 18:03:30 2009
From: btm at pobox.com (Brett Ault-McCoy)
Date: Thu, 1 Jan 2009 20:03:30 -0600
Subject: [zfs-discuss] Mirror of a Stripe or raidz?
In-Reply-To: <636fd28e0812290211m7fa3a9fam1169201f5cc8ae54@mail.gmail.com>
References: <D3D8D91A-D327-4278-985A-F00289A6D2A0@mac.com>
	<636fd28e0812290211m7fa3a9fam1169201f5cc8ae54@mail.gmail.com>
Message-ID: <7bccd8dc0901011803k5014dff4ud11638312ba67153@mail.gmail.com>

On Mon, Dec 29, 2008 at 4:11 AM, Alex Blewitt <alex.blewitt at gmail.com> wrote:
> On Mon, Dec 1, 2008 at 12:08 AM, Aaron <ax2ron at mac.com> wrote:
>> Since snapshots aren't fully baked in OS X yet
>
> ... and the
> send/recv you can't pipe directly, but those don't impact the utility
> of snapshots.

You sort of can using a fifo.  Do something like:

mkfifo /tmp/blah
zfs send > /tmp/blah
zfs recv < /tmp/blah

Do the send and recv commands in different shells.

It's not perfect, but it does allow you to do a send/recv without
writing to an intermediate file.

++Brett;

From thomas.barnestormer at gmail.com  Thu Jan  1 18:03:34 2009
From: thomas.barnestormer at gmail.com (Thomas Barnes)
Date: Thu, 1 Jan 2009 21:03:34 -0500
Subject: [zfs-discuss] ACE Inheritance problems
Message-ID: <c3efa4270901011803y765041fby3be1c3422aa2b890@mail.gmail.com>

I really appreciate all of your work on this port; I have been both a
MAC and Solaris fan for many years; when I heard there was to be a zfs
port for os x, I could barely contain myself.

I have noticed that ACEs will not inherit on a ZFS filesystem nor
directories created under. E.g

Build 119
OS:  10.5.5 Intel
ZFS Setup
zfs get
BarneStormers/Users        type           filesystem                      -
BarneStormers/Users        creation       Thu Jan  1 18:03 2009           -
BarneStormers/Users        used           41K                             -
BarneStormers/Users        available      457G                            -
BarneStormers/Users        referenced     41K                             -
BarneStormers/Users        compressratio  1.00x                           -
BarneStormers/Users        mounted        yes                             -
BarneStormers/Users        quota          none
   default
BarneStormers/Users        reservation    none
   default
BarneStormers/Users        recordsize     128K
   default
BarneStormers/Users        mountpoint     /Volumes/BarneStormers/Users
   default
BarneStormers/Users        sharenfs       off
   default
BarneStormers/Users        checksum       on
   default
BarneStormers/Users        compression    off
   default
BarneStormers/Users        atime          on
   default
BarneStormers/Users        devices        on
   default
BarneStormers/Users        exec           on
   default
BarneStormers/Users        setuid         on
   default
BarneStormers/Users        readonly       off
   default
BarneStormers/Users        zoned          off
   default
BarneStormers/Users        snapdir        hidden
   default
BarneStormers/Users        aclmode        passthrough                     local
BarneStormers/Users        aclinherit     passthrough                     local
BarneStormers/Users        canmount       on
   default
BarneStormers/Users        shareiscsi     off
   default
BarneStormers/Users        xattr          on
   default
BarneStormers/Users        copies         1
   default
BarneStormers/Users        version        2                               -


on the HPFS file system, it works:

set up a dir in my home

gleeb:tbarnes root# mkdir test
gleeb:tbarnes root# ls -lae test
total 0
drwxr-xr-x   2 root     tbarnes    68 Jan  1 20:44 .
drwxr-xr-x+ 62 tbarnes  tbarnes  2108 Jan  1 20:44 ..
 0: group:everyone deny delete

add a ACE on the test dir
gleeb:tbarnes root# chmod +a \
"tbarnes allow list,search,delete,readattr,writeattr,readextattr,writeextattr,readsecurity,writesecurity,chown,add_file,add_subdirectory,delete_child,directory_inherit,file_inherit"
test

The ACE is applied
gleeb:tbarnes root# ls -lae test
total 0
drwxr-xr-x+  2 root     tbarnes    68 Jan  1 20:44 .
 0: user:tbarnes allow
list,add_file,search,delete,add_subdirectory,delete_child,readattr,writeattr,readextattr,writeextattr,readsecurity,writesecurity,chown,file_inherit,directory_inherit
drwxr-xr-x+ 62 tbarnes  tbarnes  2108 Jan  1 20:44 ..
 0: group:everyone deny delete

Make a test inside test, check inheritance:
gleeb:test root# mkdir test-in-test
gleeb:test root# ls -lae
total 0
drwxr-xr-x+  3 root     tbarnes   102 Jan  1 20:49 .
 0: user:tbarnes allow
list,add_file,search,delete,add_subdirectory,delete_child,readattr,writeattr,readextattr,writeextattr,readsecurity,writesecurity,chown,file_inherit,directory_inherit
drwxr-xr-x+ 62 tbarnes  tbarnes  2108 Jan  1 20:44 ..
 0: group:everyone deny delete
drwxr-xr-x+  2 root     tbarnes    68 Jan  1 20:49 test-in-test
 0: user:tbarnes inherited allow
list,add_file,search,delete,add_subdirectory,delete_child,readattr,writeattr,readextattr,writeextattr,readsecurity,writesecurity,chown,file_inherit,directory_inherit

ZFS:

Go to the zfs Volume, setup a directory
gleeb:test root# cd /Volumes/BarneStormers/Users/
gleeb:Users root# mkdir Thomas
gleeb:Users root# chmod =a "tbarnes allow
list,search,delete,readattr,writeattr,readextattr,writeextattr,readsecurity,writesecurity,chown,add_file,add_subdirectory,delete_child,directory_inherit,file_inherit"
Thomas

gleeb:Users root# ls -lae
total 9
drwxr-xr-x  3 root  wheel  3 Jan  1 20:52 .
drwxr-xr-x@ 5 root  wheel  6 Jan  1 18:03 ..
drwxr-xr-x+ 2 root  wheel  2 Jan  1 20:52 Thomas
 0: user:tbarnes allow
list,add_file,search,delete,add_subdirectory,delete_child,readattr,writeattr,readextattr,writeextattr,readsecurity,writesecurity,chown,file_inherit,directory_inherit

make a dir in Thomas, the inheritance fails; note above, the mode is
set to passthrough for mode and inheritance

gleeb:Thomas root# mkdir test
gleeb:Thomas root# ls -lae
total 9
drwxr-xr-x+ 3 root  wheel  3 Jan  1 20:55 .
 0: user:tbarnes allow
list,add_file,search,delete,add_subdirectory,delete_child,readattr,writeattr,readextattr,writeextattr,readsecurity,writesecurity,chown,file_inherit,directory_inherit
drwxr-xr-x  3 root  wheel  3 Jan  1 20:52 ..
drwxr-xr-x  2 root  wheel  2 Jan  1 20:55 test

So I thought, since root was not in the ACE, it was a writexattr
problem. So I made a directory as the regular user in finder and
notice the same results

gleeb:Thomas tbarnes$ cd untitled\ folder/
gleeb:untitled folder tbarnes$ ls -lae
total 6
drwxr-xr-x  2 tbarnes  wheel  2 Jan  1 21:00 .
drwxr-xr-x+ 4 root     wheel  5 Jan  1 21:00 ..
 0: user:tbarnes allow
list,add_file,search,delete,add_subdirectory,delete_child,readattr,writeattr,readextattr,writeextattr,readsecurity,writesecurity,chown,file_inherit,directory_inherit

Thank you for your time


-Thomas Barnes
thomas.barnestormer at gmail.com

From ksh at ironsoftware.de  Sat Jan  3 10:32:40 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Sat, 3 Jan 2009 13:32:40 -0500
Subject: [zfs-discuss] 2x panic()
Message-ID: <1BAFA205-02B6-4053-B231-30AC64F0785C@ironsoftware.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi,

Happy new year.

Some bug reports:

Fri Jan  2 14:12:10 2009
panic(cpu 1 caller 0x34506CD9): "[ZFS]: assertion failed in /Volumes/ 
pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/zfs_znode.c  
line 1004: zp->z_dbuf_held == 0"@/Volumes/pixie_dust/home/ndellofano/ 
zfs-work/zfs-119/zfs_kext/zfs/zfs_znode.c:1004
Backtrace (CPU 1), Frame : Return Address (4 potential args on stack)
0x35dc3568 : 0x12b4f3 (0x45b13c 0x35dc359c 0x1335e4 0x0)
0x35dc35b8 : 0x34506cd9 (0x345600ec 0x3455fc6c 0x3ec 0x3455fd3c)
0x35dc35f8 : 0x344b115b (0x34447a60 0x1 0x35dc3648 0x142198)
0x35dc3638 : 0x1f3f4b (0x35dc3654 0x212 0x35dc365c 0x4f9f40)
0x35dc3678 : 0x1db4b5 (0x9ff6f80 0x47d95e4 0x0 0x0)
0x35dc36c8 : 0x1db6d1 (0x9ff6f80 0x1 0x35dc36f8 0x19eb5b)
0x35dc3718 : 0x1dd283 (0x0 0x4f93c0 0x45b0e290 0x0)
0x35dc3788 : 0x31f7f6 (0x0 0x34 0x35dc37dc 0xb254e4c)
0x35dc3828 : 0x327dec (0x3ea9804 0x57ecb50 0x35dc3f08 0x35dc3968)
0x35dc3998 : 0x1f2c7d (0x35dc39b8 0x2 0x35dc39e8 0x451c8a)
0x35dc39e8 : 0x1d464c (0x57ecb50 0x35dc3df4 0x35dc3f08 0x47d95e4)
0x35dc3a78 : 0x1d53d1 (0x35dc3ddc 0x100 0x35dc3dfc 0x0)
0x35dc3b38 : 0x1e4060 (0x35dc3ddc 0x0 0x0 0x0)
0x35dc3d88 : 0x1e468f (0xb0080648 0x0 0x0 0x0)
0x35dc3f48 : 0x1e4772 (0xb0080648 0x0 0x0 0x0)
0x35dc3f78 : 0x3df460 (0x47790e4 0x47d94e0 0x47d9524 0x0)
	Backtrace continues...
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0x344a9000->0x34574fff

BSD process name corresponding to current thread: mdworker

Mac OS version:
9G55

Kernel version:
Darwin Kernel Version 9.6.0: Mon Nov 24 17:37:00 PST 2008;  
root:xnu-1228.9.59~1/RELEASE_I386
System model name: MacBook1,1 (Mac-F4208CC8)



Fri Jan  2 16:35:14 2009
panic(cpu 0 caller 0x46234D21): "ZFS: I/O failure (write on <unknown>  
off 0: zio 0x536f018 [L0 ZFS plain file] e400L/e400P  
DVA[0]=<0:140e690800:e400> fletcher2 uncompressed LE contiguous  
birth=136477 fill=1  
cksum 
=26480848a9950102:75a84d27a14d1500:c7913a1e15fb05f2:82b8944c975b836d):  
error " "5"@/Volumes/pixie_dust/home/ndellofano/zfs-work/zfs-119/ 
zfs_kext/zfs/zio.c:918
Backtrace (CPU 0), Frame : Return Address (4 potential args on stack)
0x4781be38 : 0x12b4f3 (0x45b13c 0x4781be6c 0x1335e4 0x0)
0x4781be88 : 0x46234d21 (0x462a46b0 0x462a46a4 0x462a0400 0x462d1670)
0x4781bf08 : 0x462313fb (0x536f018 0x590 0x4781bf28 0x4627818a)
0x4781bf48 : 0x462904a6 (0x536f018 0x4e18080 0x9e70f 0x462b95f4)
0x4781bfc8 : 0x1a017c (0x47d6470 0x0 0x1a30b5 0x829f2e8)
Backtrace terminated-invalid frame pointer 0
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0x4621d000->0x462e8fff

BSD process name corresponding to current thread: kernel_task

Mac OS version:
9G55

Kernel version:
Darwin Kernel Version 9.6.0: Mon Nov 24 17:37:00 PST 2008;  
root:xnu-1228.9.59~1/RELEASE_I386
System model name: MacBook1,1 (Mac-F4208CC8)



Chris.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (Darwin)

iD8DBQFJX69Ip+9ff145KVIRApBRAKCYy7YDWlxn9Lpe+ezQY4saZzKGIwCglpBT
niAj8UaYdEQNXSmsf1vXJ98=
=GcIS
-----END PGP SIGNATURE-----

From lists at chipmunk-app.com  Sun Jan  4 09:35:16 2009
From: lists at chipmunk-app.com (Ruotger Skupin)
Date: Sun, 4 Jan 2009 18:35:16 +0100
Subject: [zfs-discuss] ZFS panics quite easily
Message-ID: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>

Hi,

from what I have read on the list it is a known bug that ZFS panics  
when an external drive gets unpluggged (for whatever reason...).

But apart from that ZFS seems to panic fairly easy when under heavy  
load. For example I tried to install Ubuntu in a Virtual Box VM. Both  
the install CD image file and the harddrive image file were on the  
same ZFS volume (pool containing one drive). After what might have  
been 2/3 of the installation ZFS paniced. I tried this twice and every  
time ZFS paniced.

My app Chipmunk (http://www.chipmunk-app.com) also drives ZFS into a  
panic sometimes on a well filled volume.

regards
	Ruotger


From alex.blewitt at gmail.com  Sun Jan  4 10:16:59 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Sun, 4 Jan 2009 18:16:59 +0000
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>
Message-ID: <636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>

On Sun, Jan 4, 2009 at 5:35 PM, Ruotger Skupin <lists at chipmunk-app.com> wrote:
> from what I have read on the list it is a known bug that ZFS panics when an
> external drive gets unpluggged (for whatever reason...).

I suspect that's the case for a non-mirrored pool where the pool
effectively cannot continue. Having said that, there are times if an
external disk spins down (USB hard drives, I'm looking at you) which
makes the OS think that the drive has become disconnected, and so
panics. I've never experienced that on my FW drive. Not sure what
happens in the case of a raid-z setup.

> But apart from that ZFS seems to panic fairly easy when under heavy load.
> For example I tried to install Ubuntu in a Virtual Box VM. Both the install
> CD image file and the harddrive image file were on the same ZFS volume (pool
> containing one drive). After what might have been 2/3 of the installation
> ZFS paniced. I tried this twice and every time ZFS paniced.
>
> My app Chipmunk (http://www.chipmunk-app.com) also drives ZFS into a panic
> sometimes on a well filled volume.

I've had really bad experiences with USB drives, and only noticed
because ZFS started reporting shedloads of checksum errors. Have you
checked the pool status during your use? Also, is it a USB or FW
drive, and what version of OSX/ZFS have you got installed?

Alex

From dirkschelfhout at mac.com  Sun Jan  4 12:40:06 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Sun, 04 Jan 2009 21:40:06 +0100
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>
Message-ID: <D34CDC9D-7B4D-45B3-A7C1-EF6ED92779AE@mac.com>

I did this more than once.
installed ubuntu,fedora,solaris. the same way you did.
never panics.
my raidz is 3 disks inside a macpro.
Dirk
On 04 Jan 2009, at 18:35, Ruotger Skupin wrote:

> But apart from that ZFS seems to panic fairly easy when under heavy  
> load. For example I tried to install Ubuntu in a Virtual Box VM.  
> Both the install CD image file and the harddrive image file were on  
> the same ZFS volume (pool containing one drive). After what might  
> have been 2/3 of the installation ZFS paniced. I tried this twice  
> and every time ZFS paniced.


From lists at chipmunk-app.com  Mon Jan  5 01:47:44 2009
From: lists at chipmunk-app.com (Ruotger Skupin)
Date: Mon, 5 Jan 2009 10:47:44 +0100
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>
	<636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>
Message-ID: <0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>


Am 04.01.2009 um 19:16 schrieb Alex Blewitt:

> On Sun, Jan 4, 2009 at 5:35 PM, Ruotger Skupin <lists at chipmunk- 
> app.com> wrote:
>> from what I have read on the list it is a known bug that ZFS panics  
>> when an
>> external drive gets unpluggged (for whatever reason...).
>
> I suspect that's the case for a non-mirrored pool where the pool
> effectively cannot continue. Having said that, there are times if an
> external disk spins down (USB hard drives, I'm looking at you) which
> makes the OS think that the drive has become disconnected, and so
> panics. I've never experienced that on my FW drive. Not sure what
> happens in the case of a raid-z setup.
The first time I noticed this was when I was on a congress and someone  
cut my power. So the external USB-Drive was instantly gone (single  
device pool). The macbook kept running but paniced. Accidentally  
unplugging a drive is also a good candidate for a panic.

>
>> But apart from that ZFS seems to panic fairly easy when under heavy  
>> load.
>> For example I tried to install Ubuntu in a Virtual Box VM. Both the  
>> install
>> CD image file and the harddrive image file were on the same ZFS  
>> volume (pool
>> containing one drive). After what might have been 2/3 of the  
>> installation
>> ZFS paniced. I tried this twice and every time ZFS paniced.
>>
>> My app Chipmunk (http://www.chipmunk-app.com) also drives ZFS into  
>> a panic
>> sometimes on a well filled volume.
>
> I've had really bad experiences with USB drives, and only noticed
> because ZFS started reporting shedloads of checksum errors. Have you
> checked the pool status during your use? Also, is it a USB or FW
> drive, and what version of OSX/ZFS have you got installed?
USB-Drive with a SATA drive inside, Mac OS X 10.5.6 / ZFS 119 (lastest  
downloadable).

I'm doing a "zpool status" from time to time but I did not notice any  
errors on this drive ever. Not even after these panics. In that  
respect ZFS is the most robust FS I've ever seen.

Ruotger


From todd.e.moore at gmail.com  Mon Jan  5 06:43:40 2009
From: todd.e.moore at gmail.com (Todd Moore)
Date: Mon, 05 Jan 2009 09:43:40 -0500
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>	<636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>
	<0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>
Message-ID: <49621C9C.1030807@gmail.com>

An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090105/3bc7ae7c/attachment.html>

From lhunath at gmail.com  Mon Jan  5 07:42:40 2009
From: lhunath at gmail.com (Maarten Billemont)
Date: Mon, 5 Jan 2009 16:42:40 +0100
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <49621C9C.1030807@gmail.com>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>
	<636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>
	<0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>
	<49621C9C.1030807@gmail.com>
Message-ID: <9F503C73-8AD3-4DCA-93EE-30319842D465@gmail.com>

Could you refer us where this option is documented and/or how to set it?

On 05 Jan 2009, at 15:43, Todd Moore wrote:

> The architects of ZFS placed a lot of emphasis on data integrity.   
> One of the side effects you are seeing is that ZFS will panic a host  
> if the last device in a zpool becomes unavailable.  In this  
> situation, ZFS cannot guarantee that the information in the cache,  
> ZIL, and media will be consistent.  Some users complained about the  
> behavior and recently putbacks were made to the ZFS code which allow  
> users to determine how they would like ZFS to behave in this  
> situation.  The default behavior is still to panic the system, but  
> there is now an option to raise an error condition and log it for a  
> sys admin to address.
>
>
> Ruotger Skupin wrote:
>>
>>
>> Am 04.01.2009 um 19:16 schrieb Alex Blewitt:
>>
>>> On Sun, Jan 4, 2009 at 5:35 PM, Ruotger Skupin <lists at chipmunk-app.com 
>>> > wrote:
>>>> from what I have read on the list it is a known bug that ZFS  
>>>> panics when an
>>>> external drive gets unpluggged (for whatever reason...).
>>>
>>> I suspect that's the case for a non-mirrored pool where the pool
>>> effectively cannot continue. Having said that, there are times if an
>>> external disk spins down (USB hard drives, I'm looking at you) which
>>> makes the OS think that the drive has become disconnected, and so
>>> panics. I've never experienced that on my FW drive. Not sure what
>>> happens in the case of a raid-z setup.
>> The first time I noticed this was when I was on a congress and  
>> someone cut my power. So the external USB-Drive was instantly gone  
>> (single device pool). The macbook kept running but paniced.  
>> Accidentally unplugging a drive is also a good candidate for a panic.
>>
>>>
>>>> But apart from that ZFS seems to panic fairly easy when under  
>>>> heavy load.
>>>> For example I tried to install Ubuntu in a Virtual Box VM. Both  
>>>> the install
>>>> CD image file and the harddrive image file were on the same ZFS  
>>>> volume (pool
>>>> containing one drive). After what might have been 2/3 of the  
>>>> installation
>>>> ZFS paniced. I tried this twice and every time ZFS paniced.
>>>>
>>>> My app Chipmunk (http://www.chipmunk-app.com) also drives ZFS  
>>>> into a panic
>>>> sometimes on a well filled volume.
>>>
>>> I've had really bad experiences with USB drives, and only noticed
>>> because ZFS started reporting shedloads of checksum errors. Have you
>>> checked the pool status during your use? Also, is it a USB or FW
>>> drive, and what version of OSX/ZFS have you got installed?
>> USB-Drive with a SATA drive inside, Mac OS X 10.5.6 / ZFS 119  
>> (lastest downloadable).
>>
>> I'm doing a "zpool status" from time to time but I did not notice  
>> any errors on this drive ever. Not even after these panics. In that  
>> respect ZFS is the most robust FS I've ever seen.
>>
>> Ruotger
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090105/a49cdc11/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 194 bytes
Desc: This is a digitally signed message part
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090105/a49cdc11/attachment.bin>

From lists at chipmunk-app.com  Mon Jan  5 08:51:47 2009
From: lists at chipmunk-app.com (Ruotger Skupin)
Date: Mon, 5 Jan 2009 17:51:47 +0100
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <49621C9C.1030807@gmail.com>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>	<636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>
	<0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>
	<49621C9C.1030807@gmail.com>
Message-ID: <6644E49D-2290-4B5A-8883-32365975F93F@chipmunk-app.com>


Am 05.01.2009 um 15:43 schrieb Todd Moore:

> The architects of ZFS placed a lot of emphasis on data integrity.   
> One of the side effects you are seeing is that ZFS will panic a host  
> if the last device in a zpool becomes unavailable.  In this  
> situation, ZFS cannot guarantee that the information in the cache,  
> ZIL, and media will be consistent.  Some users complained about the  
> behavior and recently putbacks were made to the ZFS code which allow  
> users to determine how they would like ZFS to behave in this  
> situation.  The default behavior is still to panic the system, but  
> there is now an option to raise an error condition and log it for a  
> sys admin to address.
>

Hm, you could argue, that panicing is not really helping matters,  
since a panic will destroy all data not written to disk even if it is  
supposed to be written to a different pool than the one failing.

Thus it really strikes me as odd that a panic is supposed to help  
"data integrity" in any way. I always thought of a panic as a "last  
resort - hands up in the air - the kernel cannot do anything anymore"  
measure.

So, count me in on the complaining users side... ;-)

Ruotger

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090105/b6a946e3/attachment.html>

From zorg at sogeeky.net  Mon Jan  5 12:02:24 2009
From: zorg at sogeeky.net (Mr. Zorg)
Date: Mon, 5 Jan 2009 12:02:24 -0800
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <6644E49D-2290-4B5A-8883-32365975F93F@chipmunk-app.com>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>
	<636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>
	<0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>
	<49621C9C.1030807@gmail.com>
	<6644E49D-2290-4B5A-8883-32365975F93F@chipmunk-app.com>
Message-ID: <634DFECA-D16A-471A-93A3-D95F07181412@sogeeky.net>

I can see both sides. At least with a panic you know something's wrong  
and you KNOW you lost data...  Better to know for certain than to let  
it go on silently. :)

On Jan 5, 2009, at 8:51 AM, Ruotger Skupin <lists at chipmunk-app.com>  
wrote:

>
> Am 05.01.2009 um 15:43 schrieb Todd Moore:
>
>> The architects of ZFS placed a lot of emphasis on data integrity.   
>> One of the side effects you are seeing is that ZFS will panic a  
>> host if the last device in a zpool becomes unavailable.  In this  
>> situation, ZFS cannot guarantee that the information in the cache,  
>> ZIL, and media will be consistent.  Some users complained about the  
>> behavior and recently putbacks were made to the ZFS code which  
>> allow users to determine how they would like ZFS to behave in this  
>> situation.  The default behavior is still to panic the system, but  
>> there is now an option to raise an error condition and log it for a  
>> sys admin to address.
>>
>
> Hm, you could argue, that panicing is not really helping matters,  
> since a panic will destroy all data not written to disk even if it  
> is supposed to be written to a different pool than the one failing.
>
> Thus it really strikes me as odd that a panic is supposed to help  
> "data integrity" in any way. I always thought of a panic as a "last  
> resort - hands up in the air - the kernel cannot do anything  
> anymore" measure.
>
> So, count me in on the complaining users side... ;-)
>
> Ruotger
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090105/e9a2158b/attachment.html>

From ksh at ironsoftware.de  Mon Jan  5 15:14:14 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Mon, 5 Jan 2009 18:14:14 -0500
Subject: [zfs-discuss] 2x panic()
In-Reply-To: <1BAFA205-02B6-4053-B231-30AC64F0785C@ironsoftware.de>
References: <1BAFA205-02B6-4053-B231-30AC64F0785C@ironsoftware.de>
Message-ID: <D8C5B29A-9842-4EBB-9B44-32C210197081@ironsoftware.de>

Hi,

some more:

Mon Jan  5 17:08:03 2009
panic(cpu 1 caller 0x357F4CD9): "[ZFS]: assertion failed in /Volumes/ 
pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/zfs_znode.c  
line 1004: zp->z_dbuf_held == 0"@/Volumes/pixie_dust/home/ndellofano/ 
zfs-work/zfs-119/zfs_kext/zfs/zfs_znode.c:1004
Backtrace (CPU 1), Frame : Return Address (4 potential args on stack)
0x3d2b34c8 : 0x12b4f3 (0x45b13c 0x3d2b34fc 0x1335e4 0x0)
0x3d2b3518 : 0x357f4cd9 (0x3584e0ec 0x3584dc6c 0x3ec 0x3584dd3c)
0x3d2b3558 : 0x3579f15b (0x378e90e0 0x1 0x3d2b3598 0xc503c20)
0x3d2b3598 : 0x1f3f4b (0x3d2b35b4 0x216 0x3d2b35bc 0x4f9f40)
0x3d2b35d8 : 0x1db4b5 (0x73cb490 0xc781e44 0x0 0x0)
0x3d2b3628 : 0x1db6d1 (0x73cb490 0x1 0x3d2b3658 0x19eb5b)
0x3d2b3678 : 0x1dd283 (0x0 0x4f93c0 0x108 0x1)
0x3d2b36e8 : 0x357f65a0 (0x0 0x34 0x3d2b371c 0x3b9c53ac)
0x3d2b3768 : 0x357f69b0 (0x3b9c53a8 0x0 0x600 0x0)
0x3d2b3818 : 0x357fd48a (0x3d2b38c8 0x0 0x0 0x3d2b38c8)
0x3d2b3898 : 0x357fd654 (0x3d2b38cc 0x36e6b8f0 0x3d2b3958 0x3d2b38c8)
0x3d2b38e8 : 0x3579b3bc (0x36e6b8f0 0x3d2b3958 0x3d2b3df4 0x0)
0x3d2b3998 : 0x1f2c7d (0x3d2b39b8 0x2 0x3d2b39e8 0x451c8a)
0x3d2b39e8 : 0x1d464c (0x6cdf640 0x3d2b3df4 0x3d2b3f08 0xc781e44)
0x3d2b3a78 : 0x1d53d1 (0x3d2b3ddc 0x100 0x3d2b3dfc 0x0)
0x3d2b3b38 : 0x1e4060 (0x3d2b3ddc 0x0 0x0 0x0)
	Backtrace continues...
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0x35797000->0x35862fff

BSD process name corresponding to current thread: du

Mac OS version:
9G55

Kernel version:
Darwin Kernel Version 9.6.0: Mon Nov 24 17:37:00 PST 2008;  
root:xnu-1228.9.59~1/RELEASE_I386
System model name: MacBook1,1 (Mac-F4208CC8)


Cheers,
Chris.

El Jan 3, 2009, a las 1:32 PM, Christian Kendi escribi?:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hi,
>
> Happy new year.
>
> Some bug reports:
>
> Fri Jan  2 14:12:10 2009
> panic(cpu 1 caller 0x34506CD9): "[ZFS]: assertion failed in /Volumes/ 
> pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/zfs_znode.c  
> line 1004: zp->z_dbuf_held == 0"@/Volumes/pixie_dust/home/ndellofano/ 
> zfs-work/zfs-119/zfs_kext/zfs/zfs_znode.c:1004
> Backtrace (CPU 1), Frame : Return Address (4 potential args on stack)
> 0x35dc3568 : 0x12b4f3 (0x45b13c 0x35dc359c 0x1335e4 0x0)
> 0x35dc35b8 : 0x34506cd9 (0x345600ec 0x3455fc6c 0x3ec 0x3455fd3c)
> 0x35dc35f8 : 0x344b115b (0x34447a60 0x1 0x35dc3648 0x142198)
> 0x35dc3638 : 0x1f3f4b (0x35dc3654 0x212 0x35dc365c 0x4f9f40)
> 0x35dc3678 : 0x1db4b5 (0x9ff6f80 0x47d95e4 0x0 0x0)
> 0x35dc36c8 : 0x1db6d1 (0x9ff6f80 0x1 0x35dc36f8 0x19eb5b)
> 0x35dc3718 : 0x1dd283 (0x0 0x4f93c0 0x45b0e290 0x0)
> 0x35dc3788 : 0x31f7f6 (0x0 0x34 0x35dc37dc 0xb254e4c)
> 0x35dc3828 : 0x327dec (0x3ea9804 0x57ecb50 0x35dc3f08 0x35dc3968)
> 0x35dc3998 : 0x1f2c7d (0x35dc39b8 0x2 0x35dc39e8 0x451c8a)
> 0x35dc39e8 : 0x1d464c (0x57ecb50 0x35dc3df4 0x35dc3f08 0x47d95e4)
> 0x35dc3a78 : 0x1d53d1 (0x35dc3ddc 0x100 0x35dc3dfc 0x0)
> 0x35dc3b38 : 0x1e4060 (0x35dc3ddc 0x0 0x0 0x0)
> 0x35dc3d88 : 0x1e468f (0xb0080648 0x0 0x0 0x0)
> 0x35dc3f48 : 0x1e4772 (0xb0080648 0x0 0x0 0x0)
> 0x35dc3f78 : 0x3df460 (0x47790e4 0x47d94e0 0x47d9524 0x0)
> 	Backtrace continues...
>      Kernel loadable modules in backtrace (with dependencies):
>         com.apple.filesystems.zfs(8.0)@0x344a9000->0x34574fff
>
> BSD process name corresponding to current thread: mdworker
>
> Mac OS version:
> 9G55
>
> Kernel version:
> Darwin Kernel Version 9.6.0: Mon Nov 24 17:37:00 PST 2008;  
> root:xnu-1228.9.59~1/RELEASE_I386
> System model name: MacBook1,1 (Mac-F4208CC8)
>
>
>
> Fri Jan  2 16:35:14 2009
> panic(cpu 0 caller 0x46234D21): "ZFS: I/O failure (write on  
> <unknown> off 0: zio 0x536f018 [L0 ZFS plain file] e400L/e400P  
> DVA[0]=<0:140e690800:e400> fletcher2 uncompressed LE contiguous  
> birth=136477 fill=1  
> cksum 
> = 
> 26480848a9950102 
> :75a84d27a14d1500:c7913a1e15fb05f2:82b8944c975b836d): error " "5"@/ 
> Volumes/pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/ 
> zio.c:918
> Backtrace (CPU 0), Frame : Return Address (4 potential args on stack)
> 0x4781be38 : 0x12b4f3 (0x45b13c 0x4781be6c 0x1335e4 0x0)
> 0x4781be88 : 0x46234d21 (0x462a46b0 0x462a46a4 0x462a0400 0x462d1670)
> 0x4781bf08 : 0x462313fb (0x536f018 0x590 0x4781bf28 0x4627818a)
> 0x4781bf48 : 0x462904a6 (0x536f018 0x4e18080 0x9e70f 0x462b95f4)
> 0x4781bfc8 : 0x1a017c (0x47d6470 0x0 0x1a30b5 0x829f2e8)
> Backtrace terminated-invalid frame pointer 0
>      Kernel loadable modules in backtrace (with dependencies):
>         com.apple.filesystems.zfs(8.0)@0x4621d000->0x462e8fff
>
> BSD process name corresponding to current thread: kernel_task
>
> Mac OS version:
> 9G55
>
> Kernel version:
> Darwin Kernel Version 9.6.0: Mon Nov 24 17:37:00 PST 2008;  
> root:xnu-1228.9.59~1/RELEASE_I386
> System model name: MacBook1,1 (Mac-F4208CC8)
>
>
>
> Chris.
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.7 (Darwin)
>
> iD8DBQFJX69Ip+9ff145KVIRApBRAKCYy7YDWlxn9Lpe+ezQY4saZzKGIwCglpBT
> niAj8UaYdEQNXSmsf1vXJ98=
> =GcIS
> -----END PGP SIGNATURE-----
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090105/07b464d0/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: Mensaje firmado digitalmente
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090105/07b464d0/attachment-0001.bin>

From ksh at ironsoftware.de  Mon Jan  5 15:23:04 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Mon, 5 Jan 2009 18:23:04 -0500
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <9F503C73-8AD3-4DCA-93EE-30319842D465@gmail.com>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>
	<636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>
	<0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>
	<49621C9C.1030807@gmail.com>
	<9F503C73-8AD3-4DCA-93EE-30319842D465@gmail.com>
Message-ID: <E19B788F-429E-4E9F-A382-475412BCEA87@ironsoftware.de>

i would like to use this option. I dont feel like having a panic() all  
the time when something is wrong.

What i've seen recently is, that once the filesystem is full you cant  
delete anything. unlink() return "Disk is full", therefore can't
delete files to free some space. panic() follows soon. Id rather like  
to use the system a little bit more to free some space instead of  
panic().

El Jan 5, 2009, a las 10:42 AM, Maarten Billemont escribi?:

> Could you refer us where this option is documented and/or how to set  
> it?
>
> On 05 Jan 2009, at 15:43, Todd Moore wrote:
>
>> The architects of ZFS placed a lot of emphasis on data integrity.   
>> One of the side effects you are seeing is that ZFS will panic a  
>> host if the last device in a zpool becomes unavailable.  In this  
>> situation, ZFS cannot guarantee that the information in the cache,  
>> ZIL, and media will be consistent.  Some users complained about the  
>> behavior and recently putbacks were made to the ZFS code which  
>> allow users to determine how they would like ZFS to behave in this  
>> situation.  The default behavior is still to panic the system, but  
>> there is now an option to raise an error condition and log it for a  
>> sys admin to address.
>>
>>
>> Ruotger Skupin wrote:
>>>
>>>
>>> Am 04.01.2009 um 19:16 schrieb Alex Blewitt:
>>>
>>>> On Sun, Jan 4, 2009 at 5:35 PM, Ruotger Skupin <lists at chipmunk-app.com 
>>>> > wrote:
>>>>> from what I have read on the list it is a known bug that ZFS  
>>>>> panics when an
>>>>> external drive gets unpluggged (for whatever reason...).
>>>>
>>>> I suspect that's the case for a non-mirrored pool where the pool
>>>> effectively cannot continue. Having said that, there are times if  
>>>> an
>>>> external disk spins down (USB hard drives, I'm looking at you)  
>>>> which
>>>> makes the OS think that the drive has become disconnected, and so
>>>> panics. I've never experienced that on my FW drive. Not sure what
>>>> happens in the case of a raid-z setup.
>>> The first time I noticed this was when I was on a congress and  
>>> someone cut my power. So the external USB-Drive was instantly gone  
>>> (single device pool). The macbook kept running but paniced.  
>>> Accidentally unplugging a drive is also a good candidate for a  
>>> panic.
>>>
>>>>
>>>>> But apart from that ZFS seems to panic fairly easy when under  
>>>>> heavy load.
>>>>> For example I tried to install Ubuntu in a Virtual Box VM. Both  
>>>>> the install
>>>>> CD image file and the harddrive image file were on the same ZFS  
>>>>> volume (pool
>>>>> containing one drive). After what might have been 2/3 of the  
>>>>> installation
>>>>> ZFS paniced. I tried this twice and every time ZFS paniced.
>>>>>
>>>>> My app Chipmunk (http://www.chipmunk-app.com) also drives ZFS  
>>>>> into a panic
>>>>> sometimes on a well filled volume.
>>>>
>>>> I've had really bad experiences with USB drives, and only noticed
>>>> because ZFS started reporting shedloads of checksum errors. Have  
>>>> you
>>>> checked the pool status during your use? Also, is it a USB or FW
>>>> drive, and what version of OSX/ZFS have you got installed?
>>> USB-Drive with a SATA drive inside, Mac OS X 10.5.6 / ZFS 119  
>>> (lastest downloadable).
>>>
>>> I'm doing a "zpool status" from time to time but I did not notice  
>>> any errors on this drive ever. Not even after these panics. In  
>>> that respect ZFS is the most robust FS I've ever seen.
>>>
>>> Ruotger
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090105/db0c3e3e/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: Mensaje firmado digitalmente
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090105/db0c3e3e/attachment.bin>

From todd.e.moore at gmail.com  Mon Jan  5 18:02:04 2009
From: todd.e.moore at gmail.com (Todd Moore)
Date: Mon, 05 Jan 2009 21:02:04 -0500
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <9F503C73-8AD3-4DCA-93EE-30319842D465@gmail.com>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>	<636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>	<0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>	<49621C9C.1030807@gmail.com>
	<9F503C73-8AD3-4DCA-93EE-30319842D465@gmail.com>
Message-ID: <4962BB9C.2050907@gmail.com>

An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090105/5bfb9dad/attachment.html>

From jon at halfpast.net  Mon Jan  5 18:46:34 2009
From: jon at halfpast.net (Jon Moog)
Date: Mon, 5 Jan 2009 20:46:34 -0600
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <49621C9C.1030807@gmail.com>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>	<636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>
	<0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>
	<49621C9C.1030807@gmail.com>
Message-ID: <0D064A90-1EA7-475A-A70C-E359C4C75BE9@halfpast.net>

I agree that ZFS places a lot of emphasis on data integrity. But  
please explain how a panic in any situation preserves data? I'm not  
sure about other people but on my systems at any moment I have a dozen  
apps open with documents and editing being done. I rarely reboot  
except in the case of software updates or a machine crash. Rebooting  
is a major inconvenience for any reason. Unless it is a boot drive a  
failed disk should not bring down the system. Notify the user data  
loss may have occurred but don't panic. What about the documents that  
may be open. What about the other 5 volumes on the system that are  
functioning perfectly. The data processing, video encoding, gaming  
etc... that may be occurring on the system completely independent of  
the failed drive. I can't begin to tell you how much information I  
could lose in a situation such as this. Recoverable perhaps, but the  
time and energy could be better spent. A panic takes what might be an  
inconvenience and turns it into a guaranteed catastrophe. I use a unix  
based OS precisely because I desire stability and availability.

-Jon

On Jan 5, 2009, at 8:43 AM, Todd Moore wrote:

> The architects of ZFS placed a lot of emphasis on data integrity.   
> One of the side effects you are seeing is that ZFS will panic a host  
> if the last device in a zpool becomes unavailable.  In this  
> situation, ZFS cannot guarantee that the information in the cache,  
> ZIL, and media will be consistent.  Some users complained about the  
> behavior and recently putbacks were made to the ZFS code which allow  
> users to determine how they would like ZFS to behave in this  
> situation.  The default behavior is still to panic the system, but  
> there is now an option to raise an error condition and log it for a  
> sys admin to address.
>
>
> Ruotger Skupin wrote:
>>
>>
>> Am 04.01.2009 um 19:16 schrieb Alex Blewitt:
>>
>>> On Sun, Jan 4, 2009 at 5:35 PM, Ruotger Skupin <lists at chipmunk-app.com 
>>> > wrote:
>>>> from what I have read on the list it is a known bug that ZFS  
>>>> panics when an
>>>> external drive gets unpluggged (for whatever reason...).
>>>
>>> I suspect that's the case for a non-mirrored pool where the pool
>>> effectively cannot continue. Having said that, there are times if an
>>> external disk spins down (USB hard drives, I'm looking at you) which
>>> makes the OS think that the drive has become disconnected, and so
>>> panics. I've never experienced that on my FW drive. Not sure what
>>> happens in the case of a raid-z setup.
>> The first time I noticed this was when I was on a congress and  
>> someone cut my power. So the external USB-Drive was instantly gone  
>> (single device pool). The macbook kept running but paniced.  
>> Accidentally unplugging a drive is also a good candidate for a panic.
>>
>>>
>>>> But apart from that ZFS seems to panic fairly easy when under  
>>>> heavy load.
>>>> For example I tried to install Ubuntu in a Virtual Box VM. Both  
>>>> the install
>>>> CD image file and the harddrive image file were on the same ZFS  
>>>> volume (pool
>>>> containing one drive). After what might have been 2/3 of the  
>>>> installation
>>>> ZFS paniced. I tried this twice and every time ZFS paniced.
>>>>
>>>> My app Chipmunk (http://www.chipmunk-app.com) also drives ZFS  
>>>> into a panic
>>>> sometimes on a well filled volume.
>>>
>>> I've had really bad experiences with USB drives, and only noticed
>>> because ZFS started reporting shedloads of checksum errors. Have you
>>> checked the pool status during your use? Also, is it a USB or FW
>>> drive, and what version of OSX/ZFS have you got installed?
>> USB-Drive with a SATA drive inside, Mac OS X 10.5.6 / ZFS 119  
>> (lastest downloadable).
>>
>> I'm doing a "zpool status" from time to time but I did not notice  
>> any errors on this drive ever. Not even after these panics. In that  
>> respect ZFS is the most robust FS I've ever seen.
>>
>> Ruotger
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090105/73cbca07/attachment-0001.html>

From lists at chipmunk-app.com  Tue Jan  6 02:22:54 2009
From: lists at chipmunk-app.com (Ruotger Skupin)
Date: Tue, 6 Jan 2009 11:22:54 +0100
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <4962BB9C.2050907@gmail.com>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>	<636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>	<0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>	<49621C9C.1030807@gmail.com>
	<9F503C73-8AD3-4DCA-93EE-30319842D465@gmail.com>
	<4962BB9C.2050907@gmail.com>
Message-ID: <F16630DA-7978-4870-8E9D-F9DF27F9E9E6@chipmunk-app.com>

Hm, seems like both documents explicitly say that "failmode = wait" is  
the default value, which makes way more sense than "panic" if you ask  
me.

Am 06.01.2009 um 03:02 schrieb Todd Moore:

>
> The setting is enabled/disabled on a pool by pool basis as it is a  
> property of the pool.  The setting is called "failmode" with values  
> of "continue", "wait", or "panic".  I haven't found a lot of  
> information on the OpenSolaris site, but this is information from  
> the PSARC case -
> http://jp.opensolaris.org/os/community/arc/caselog/2007/261/commitment-materials/zpool-1m/;jsessionid=B1DA8B4D9E8DAF56AC99AA8D265979B1
>
>
> The feature made it into Solaris 10 10/08 and the information in the  
> "What's New" includes an example output of the property -
> http://docs.sun.com/app/docs/doc/817-0547/ghgdx?l=en&a=view
>
>
> Maarten Billemont wrote:
>>
>> Could you refer us where this option is documented and/or how to  
>> set it?
>>
>> On 05 Jan 2009, at 15:43, Todd Moore wrote:
>>
>>>   The default behavior is still to panic the system, but there is  
>>> now an option to raise an error condition and log it for a sys  
>>> admin to address.
>>>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090106/bd3ff941/attachment.html>

From nathan.stocks at gmail.com  Tue Jan  6 09:09:43 2009
From: nathan.stocks at gmail.com (Nathan)
Date: Tue, 6 Jan 2009 10:09:43 -0700
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <F16630DA-7978-4870-8E9D-F9DF27F9E9E6@chipmunk-app.com>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>
	<636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>
	<0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>
	<49621C9C.1030807@gmail.com>
	<9F503C73-8AD3-4DCA-93EE-30319842D465@gmail.com>
	<4962BB9C.2050907@gmail.com>
	<F16630DA-7978-4870-8E9D-F9DF27F9E9E6@chipmunk-app.com>
Message-ID: <96c9d6a80901060909m2292767bg257e36c318ab7297@mail.gmail.com>

On Tue, Jan 6, 2009 at 3:22 AM, Ruotger Skupin <lists at chipmunk-app.com> wrote:
> Hm, seems like both documents explicitly say that "failmode = wait" is the
> default value, which makes way more sense than "panic" if you ask me.

I also vote for any type of handling other than "panic."  The only
reason a panic would ever be acceptable for me would be a situation
where the OS can't sanely run anymore (boot drive disappears,
catastrophic hardware failure, etc.).

~ Nathan

From vivacarlie at gmail.com  Wed Jan  7 16:59:41 2009
From: vivacarlie at gmail.com (Nehemiah Dacres)
Date: Wed, 7 Jan 2009 18:59:41 -0600
Subject: [zfs-discuss] whats the url to the repo
Message-ID: <65fadfc30901071659xc4bcb34n92ef28955869af07@mail.gmail.com>

so i can pull the most recent code. and what versioning system are you
using?

-- 

"lalalalala! it's not broken because I can use it"

http://linux.slashdot.org/comments.pl?sid=194281&threshold=1&commentsort=0&mode=thread&cid=15927703
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090107/4b28d61b/attachment.html>

From macrbg at mac.com  Wed Jan  7 22:11:36 2009
From: macrbg at mac.com (Robert Gordon)
Date: Thu, 08 Jan 2009 00:11:36 -0600
Subject: [zfs-discuss] whats the url to the repo
In-Reply-To: <65fadfc30901071659xc4bcb34n92ef28955869af07@mail.gmail.com>
References: <65fadfc30901071659xc4bcb34n92ef28955869af07@mail.gmail.com>
Message-ID: <06223C4D-4E2D-42FD-98B3-E54A67204FEA@mac.com>

http://zfs.macosforge.org/trac/wiki is a good place to start..

On Jan 7, 2009, at 6:59 PM, Nehemiah Dacres wrote:

> so i can pull the most recent code. and what versioning system are  
> you using?
>
> -- 
>
> "lalalalala! it's not broken because I can use it"
>
> http://linux.slashdot.org/comments.pl?sid=194281&threshold=1&commentsort=0&mode=thread&cid=15927703
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090108/c4abd0df/attachment.html>

From dhoffmann at uwalumni.com  Thu Jan  8 07:21:26 2009
From: dhoffmann at uwalumni.com (Dominik Hoffmann)
Date: Thu, 8 Jan 2009 10:21:26 -0500
Subject: [zfs-discuss] Why does ZFS volume look like a DMG?
Message-ID: <67115265-C7EB-4143-8D4E-ACDEB344189D@uwalumni.com>

I installed zfs-119 on a PowerMac G4 running 10.5.5 without problems.  
I then formatted three drive mechanisms with these commands

	diskutil partitiondisk /dev/disk0 GPTFormat ZFS %noformat% 100%
	diskutil partitiondisk /dev/disk1 GPTFormat ZFS %noformat% 100%
	diskutil partitiondisk /dev/disk2 GPTFormat ZFS %noformat% 100%

Finally, I executed this ZFS command

	zpool create BackupRAID raidz /dev/disk0s2 /dev/disk1s2 /dev/disk2s2

The resultant ZFS volume shows up on the desktop, but with the icon of  
a removable medium, such as that of a USB stick. Why is that, and why  
is it not the orange icon of an external hard drive?

Dominik Hoffmann

From ksh at ironsoftware.de  Thu Jan  8 08:26:37 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Thu, 8 Jan 2009 11:26:37 -0500
Subject: [zfs-discuss] Why does ZFS volume look like a DMG?
In-Reply-To: <67115265-C7EB-4143-8D4E-ACDEB344189D@uwalumni.com>
References: <67115265-C7EB-4143-8D4E-ACDEB344189D@uwalumni.com>
Message-ID: <89DBE07A-E8AD-4B3F-8402-1D34BE9A490D@ironsoftware.de>

Mounted filesystems will appear on /Volumes, thus on your Desktop.  
Mounting a DMG ist the same procedure.

If you want a diff Volume icon seek for instance the orange external  
drive icon
and copy it to the Volumes root as ".VolumeIcon.icns". remount and the  
icon will be there.


El Jan 8, 2009, a las 10:21 AM, Dominik Hoffmann escribi?:

> I installed zfs-119 on a PowerMac G4 running 10.5.5 without  
> problems. I then formatted three drive mechanisms with these commands
>
> 	diskutil partitiondisk /dev/disk0 GPTFormat ZFS %noformat% 100%
> 	diskutil partitiondisk /dev/disk1 GPTFormat ZFS %noformat% 100%
> 	diskutil partitiondisk /dev/disk2 GPTFormat ZFS %noformat% 100%
>
> Finally, I executed this ZFS command
>
> 	zpool create BackupRAID raidz /dev/disk0s2 /dev/disk1s2 /dev/disk2s2
>
> The resultant ZFS volume shows up on the desktop, but with the icon  
> of a removable medium, such as that of a USB stick. Why is that, and  
> why is it not the orange icon of an external hard drive?
>
> Dominik Hoffmann
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090108/fa67eb62/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: Mensaje firmado digitalmente
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090108/fa67eb62/attachment.bin>

From ksh at ironsoftware.de  Thu Jan  8 08:29:32 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Thu, 8 Jan 2009 11:29:32 -0500
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <4962BB9C.2050907@gmail.com>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>	<636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>	<0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>	<49621C9C.1030807@gmail.com>
	<9F503C73-8AD3-4DCA-93EE-30319842D465@gmail.com>
	<4962BB9C.2050907@gmail.com>
Message-ID: <DB5706B4-97BF-4179-A3EE-E8A8A8CD066B@ironsoftware.de>

any plans to implement this ?
El Jan 5, 2009, a las 9:02 PM, Todd Moore escribi?:

>
> The setting is enabled/disabled on a pool by pool basis as it is a  
> property of the pool.  The setting is called "failmode" with values  
> of "continue", "wait", or "panic".  I haven't found a lot of  
> information on the OpenSolaris site, but this is information from  
> the PSARC case -
> http://jp.opensolaris.org/os/community/arc/caselog/2007/261/commitment-materials/zpool-1m/;jsessionid=B1DA8B4D9E8DAF56AC99AA8D265979B1
>
>
> The feature made it into Solaris 10 10/08 and the information in the  
> "What's New" includes an example output of the property -
> http://docs.sun.com/app/docs/doc/817-0547/ghgdx?l=en&a=view
>
>
> Maarten Billemont wrote:
>>
>> Could you refer us where this option is documented and/or how to  
>> set it?
>>
>> On 05 Jan 2009, at 15:43, Todd Moore wrote:
>>
>>> The architects of ZFS placed a lot of emphasis on data integrity.   
>>> One of the side effects you are seeing is that ZFS will panic a  
>>> host if the last device in a zpool becomes unavailable.  In this  
>>> situation, ZFS cannot guarantee that the information in the cache,  
>>> ZIL, and media will be consistent.  Some users complained about  
>>> the behavior and recently putbacks were made to the ZFS code which  
>>> allow users to determine how they would like ZFS to behave in this  
>>> situation.  The default behavior is still to panic the system, but  
>>> there is now an option to raise an error condition and log it for  
>>> a sys admin to address.
>>>
>>>
>>> Ruotger Skupin wrote:
>>>>
>>>>
>>>> Am 04.01.2009 um 19:16 schrieb Alex Blewitt:
>>>>
>>>>> On Sun, Jan 4, 2009 at 5:35 PM, Ruotger Skupin <lists at chipmunk-app.com 
>>>>> > wrote:
>>>>>> from what I have read on the list it is a known bug that ZFS  
>>>>>> panics when an
>>>>>> external drive gets unpluggged (for whatever reason...).
>>>>>
>>>>> I suspect that's the case for a non-mirrored pool where the pool
>>>>> effectively cannot continue. Having said that, there are times  
>>>>> if an
>>>>> external disk spins down (USB hard drives, I'm looking at you)  
>>>>> which
>>>>> makes the OS think that the drive has become disconnected, and so
>>>>> panics. I've never experienced that on my FW drive. Not sure what
>>>>> happens in the case of a raid-z setup.
>>>> The first time I noticed this was when I was on a congress and  
>>>> someone cut my power. So the external USB-Drive was instantly  
>>>> gone (single device pool). The macbook kept running but paniced.  
>>>> Accidentally unplugging a drive is also a good candidate for a  
>>>> panic.
>>>>
>>>>>
>>>>>> But apart from that ZFS seems to panic fairly easy when under  
>>>>>> heavy load.
>>>>>> For example I tried to install Ubuntu in a Virtual Box VM. Both  
>>>>>> the install
>>>>>> CD image file and the harddrive image file were on the same ZFS  
>>>>>> volume (pool
>>>>>> containing one drive). After what might have been 2/3 of the  
>>>>>> installation
>>>>>> ZFS paniced. I tried this twice and every time ZFS paniced.
>>>>>>
>>>>>> My app Chipmunk (http://www.chipmunk-app.com) also drives ZFS  
>>>>>> into a panic
>>>>>> sometimes on a well filled volume.
>>>>>
>>>>> I've had really bad experiences with USB drives, and only noticed
>>>>> because ZFS started reporting shedloads of checksum errors. Have  
>>>>> you
>>>>> checked the pool status during your use? Also, is it a USB or FW
>>>>> drive, and what version of OSX/ZFS have you got installed?
>>>> USB-Drive with a SATA drive inside, Mac OS X 10.5.6 / ZFS 119  
>>>> (lastest downloadable).
>>>>
>>>> I'm doing a "zpool status" from time to time but I did not notice  
>>>> any errors on this drive ever. Not even after these panics. In  
>>>> that respect ZFS is the most robust FS I've ever seen.
>>>>
>>>> Ruotger
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090108/91739e83/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: Mensaje firmado digitalmente
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090108/91739e83/attachment.bin>

From deric at apple.com  Thu Jan  8 11:04:12 2009
From: deric at apple.com (Deric Horn)
Date: Thu, 8 Jan 2009 11:04:12 -0800
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <DB5706B4-97BF-4179-A3EE-E8A8A8CD066B@ironsoftware.de>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>
	<636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>
	<0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>
	<49621C9C.1030807@gmail.com>
	<9F503C73-8AD3-4DCA-93EE-30319842D465@gmail.com>
	<4962BB9C.2050907@gmail.com>
	<DB5706B4-97BF-4179-A3EE-E8A8A8CD066B@ironsoftware.de>
Message-ID: <039B9F4E-3DB6-4996-AE4C-CE9BDAF18DBF@apple.com>

I wish getting rid of all panics was that easy ;-)

During our ZFS development for Snow Leopard we had to make several  
changes to the kernel.  Internally we tried to maintain a Leopard  
version of ZFS conditionalizing out features dependent on kernel  
changes, but it became clear we couldn't continue down this path.

We're currently in the process of integrating the latest OpenSolaris  
ZFS changes into our code base, and this is one we are obviously  
interested in.  You'll have to wait for a Snow Leopard seed to see it  
in action though.

Deric Horn
Engineering Manager	   File Systems
Apple Inc.			   408.974.3138

On Jan 8, 2009, at 8:29 AM, Christian Kendi wrote:

> any plans to implement this ?
>
> El Jan 5, 2009, a las 9:02 PM, Todd Moore escribi?:
>
>>
>> The setting is enabled/disabled on a pool by pool basis as it is a  
>> property of the pool.  The setting is called "failmode" with values  
>> of "continue", "wait", or "panic".  I haven't found a lot of  
>> information on the OpenSolaris site, but this is information from  
>> the PSARC case -
>> http://jp.opensolaris.org/os/community/arc/caselog/2007/261/commitment-materials/zpool-1m/;jsessionid=B1DA8B4D9E8DAF56AC99AA8D265979B1
>>
>>
>> The feature made it into Solaris 10 10/08 and the information in  
>> the "What's New" includes an example output of the property -
>> http://docs.sun.com/app/docs/doc/817-0547/ghgdx?l=en&a=view
>>
>>
>> Maarten Billemont wrote:
>>>
>>> Could you refer us where this option is documented and/or how to  
>>> set it?
>>>
>>> On 05 Jan 2009, at 15:43, Todd Moore wrote:
>>>
>>>> The architects of ZFS placed a lot of emphasis on data  
>>>> integrity.  One of the side effects you are seeing is that ZFS  
>>>> will panic a host if the last device in a zpool becomes  
>>>> unavailable.  In this situation, ZFS cannot guarantee that the  
>>>> information in the cache, ZIL, and media will be consistent.   
>>>> Some users complained about the behavior and recently putbacks  
>>>> were made to the ZFS code which allow users to determine how they  
>>>> would like ZFS to behave in this situation.  The default behavior  
>>>> is still to panic the system, but there is now an option to raise  
>>>> an error condition and log it for a sys admin to address.
>>>>
>>>>
>>>> Ruotger Skupin wrote:
>>>>>
>>>>>
>>>>> Am 04.01.2009 um 19:16 schrieb Alex Blewitt:
>>>>>
>>>>>> On Sun, Jan 4, 2009 at 5:35 PM, Ruotger Skupin <lists at chipmunk-app.com 
>>>>>> > wrote:
>>>>>>> from what I have read on the list it is a known bug that ZFS  
>>>>>>> panics when an
>>>>>>> external drive gets unpluggged (for whatever reason...).
>>>>>>
>>>>>> I suspect that's the case for a non-mirrored pool where the pool
>>>>>> effectively cannot continue. Having said that, there are times  
>>>>>> if an
>>>>>> external disk spins down (USB hard drives, I'm looking at you)  
>>>>>> which
>>>>>> makes the OS think that the drive has become disconnected, and so
>>>>>> panics. I've never experienced that on my FW drive. Not sure what
>>>>>> happens in the case of a raid-z setup.
>>>>> The first time I noticed this was when I was on a congress and  
>>>>> someone cut my power. So the external USB-Drive was instantly  
>>>>> gone (single device pool). The macbook kept running but paniced.  
>>>>> Accidentally unplugging a drive is also a good candidate for a  
>>>>> panic.
>>>>>
>>>>>>
>>>>>>> But apart from that ZFS seems to panic fairly easy when under  
>>>>>>> heavy load.
>>>>>>> For example I tried to install Ubuntu in a Virtual Box VM.  
>>>>>>> Both the install
>>>>>>> CD image file and the harddrive image file were on the same  
>>>>>>> ZFS volume (pool
>>>>>>> containing one drive). After what might have been 2/3 of the  
>>>>>>> installation
>>>>>>> ZFS paniced. I tried this twice and every time ZFS paniced.
>>>>>>>
>>>>>>> My app Chipmunk (http://www.chipmunk-app.com) also drives ZFS  
>>>>>>> into a panic
>>>>>>> sometimes on a well filled volume.
>>>>>>
>>>>>> I've had really bad experiences with USB drives, and only noticed
>>>>>> because ZFS started reporting shedloads of checksum errors.  
>>>>>> Have you
>>>>>> checked the pool status during your use? Also, is it a USB or FW
>>>>>> drive, and what version of OSX/ZFS have you got installed?
>>>>> USB-Drive with a SATA drive inside, Mac OS X 10.5.6 / ZFS 119  
>>>>> (lastest downloadable).
>>>>>
>>>>> I'm doing a "zpool status" from time to time but I did not  
>>>>> notice any errors on this drive ever. Not even after these  
>>>>> panics. In that respect ZFS is the most robust FS I've ever seen.
>>>>>
>>>>> Ruotger
>>>>>
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090108/0a51a4a6/attachment-0001.html>

From hanche at math.ntnu.no  Thu Jan  8 11:31:45 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Thu, 08 Jan 2009 20:31:45 +0100 (CET)
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <039B9F4E-3DB6-4996-AE4C-CE9BDAF18DBF@apple.com>
References: <4962BB9C.2050907@gmail.com>
	<DB5706B4-97BF-4179-A3EE-E8A8A8CD066B@ironsoftware.de>
	<039B9F4E-3DB6-4996-AE4C-CE9BDAF18DBF@apple.com>
Message-ID: <20090108.203145.153683589.hanche@math.ntnu.no>

+ Deric Horn <deric at apple.com>:

> During our ZFS development for Snow Leopard we had to make several 
> changes to the kernel.  Internally we tried to maintain a Leopard 
> version of ZFS conditionalizing out features dependent on kernel 
> changes, but it became clear we couldn't continue down this path.

Does this mean that the 119 release is going to be the last one for 
Leopard? If so, ouch.

- Harald

From zorg at sogeeky.net  Thu Jan  8 11:46:36 2009
From: zorg at sogeeky.net (Mr. Zorg)
Date: Thu, 8 Jan 2009 11:46:36 -0800
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <20090108.203145.153683589.hanche@math.ntnu.no>
References: <4962BB9C.2050907@gmail.com>
	<DB5706B4-97BF-4179-A3EE-E8A8A8CD066B@ironsoftware.de>
	<039B9F4E-3DB6-4996-AE4C-CE9BDAF18DBF@apple.com>
	<20090108.203145.153683589.hanche@math.ntnu.no>
Message-ID: <8B606D9A-2F1B-4FC6-98B8-0737073C29DA@sogeeky.net>

I second that. Ouch. But hey, we all knew this was beta. There were  
never any promises of full zfs on leopard. Besides, who isn't going to  
run out and get snow leopard anyway?  :)

On Jan 8, 2009, at 11:31 AM, Harald Hanche-Olsen <hanche at math.ntnu.no>  
wrote:

> + Deric Horn <deric at apple.com>:
>
>> During our ZFS development for Snow Leopard we had to make several  
>> changes to the kernel.  Internally we tried to maintain a Leopard  
>> version of ZFS conditionalizing out features dependent on kernel  
>> changes, but it became clear we couldn't continue down this path.
>
> Does this mean that the 119 release is going to be the last one for  
> Leopard? If so, ouch.
>
> - Harald
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From dhoffmann at uwalumni.com  Thu Jan  8 11:54:00 2009
From: dhoffmann at uwalumni.com (Dominik Hoffmann)
Date: Thu, 8 Jan 2009 14:54:00 -0500
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <8B606D9A-2F1B-4FC6-98B8-0737073C29DA@sogeeky.net>
References: <4962BB9C.2050907@gmail.com>
	<DB5706B4-97BF-4179-A3EE-E8A8A8CD066B@ironsoftware.de>
	<039B9F4E-3DB6-4996-AE4C-CE9BDAF18DBF@apple.com>
	<20090108.203145.153683589.hanche@math.ntnu.no>
	<8B606D9A-2F1B-4FC6-98B8-0737073C29DA@sogeeky.net>
Message-ID: <30905C14-D127-4559-8D1C-1AB3DAA5B454@uwalumni.com>

Not me, at least not for the machine I'm running this on, because it's  
a G4. Snow Leopard is not expected to run on PPC.

On Jan 8, 2009, at 2:46 PM, Mr. Zorg wrote:

> I second that. Ouch. But hey, we all knew this was beta. There were  
> never any promises of full zfs on leopard. Besides, who isn't going  
> to run out and get snow leopard anyway?  :)
>
> On Jan 8, 2009, at 11:31 AM, Harald Hanche-Olsen  
> <hanche at math.ntnu.no> wrote:
>
>> + Deric Horn <deric at apple.com>:
>>
>>> During our ZFS development for Snow Leopard we had to make several  
>>> changes to the kernel.  Internally we tried to maintain a Leopard  
>>> version of ZFS conditionalizing out features dependent on kernel  
>>> changes, but it became clear we couldn't continue down this path.
>>
>> Does this mean that the 119 release is going to be the last one for  
>> Leopard? If so, ouch.
>>
>> - Harald
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

From dmz+lists at tffenterprises.com  Thu Jan  8 11:59:02 2009
From: dmz+lists at tffenterprises.com (Daniel M. Zimmerman)
Date: Thu, 08 Jan 2009 11:59:02 -0800
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <8B606D9A-2F1B-4FC6-98B8-0737073C29DA@sogeeky.net>
References: <4962BB9C.2050907@gmail.com>
	<DB5706B4-97BF-4179-A3EE-E8A8A8CD066B@ironsoftware.de>
	<039B9F4E-3DB6-4996-AE4C-CE9BDAF18DBF@apple.com>
	<20090108.203145.153683589.hanche@math.ntnu.no>
	<8B606D9A-2F1B-4FC6-98B8-0737073C29DA@sogeeky.net>
Message-ID: <BE6006298A9D79361942A7EB@D-69-91-151-215.dhcp4.washington.edu>



--On 8 January 2009 11:46:36 -0800 "Mr. Zorg" <zorg at sogeeky.net> wrote:

> I second that. Ouch. But hey, we all knew this was beta. There were never
> any promises of full zfs on leopard. Besides, who isn't going to run out
> and get snow leopard anyway?  :)

Um... everybody who has a PowerPC machine, for one. At least, that's the 
current rumor, which I hope for the sake of all the extant Xserve G5 
hardware out there is not actually true.

-Dan

------------------------------------------------------------------
Daniel M. Zimmerman                                TFF Enterprises
1900 Commerce St. Box 358426   http://www.tffenterprises.com/~dmz/
Tacoma, WA  98402  USA                      dmz at tffenterprises.com

From hanche at math.ntnu.no  Thu Jan  8 12:10:46 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Thu, 08 Jan 2009 21:10:46 +0100 (CET)
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <8B606D9A-2F1B-4FC6-98B8-0737073C29DA@sogeeky.net>
	<20090108.203145.153683589.hanche@math.ntnu.no>
References: <039B9F4E-3DB6-4996-AE4C-CE9BDAF18DBF@apple.com>
	<20090108.203145.153683589.hanche@math.ntnu.no>
	<8B606D9A-2F1B-4FC6-98B8-0737073C29DA@sogeeky.net>
Message-ID: <20090108.211046.70060129.hanche@math.ntnu.no>

+ Harald Hanche-Olsen <hanche at math.ntnu.no>:

> Does this mean that the 119 release is going to be the last one for 
> Leopard? If so, ouch.

+ "Mr. Zorg" <zorg at sogeeky.net>:

> I second that. Ouch. But hey, we all knew this was beta. There were 
> never any promises of full zfs on leopard.

I guess there wasn't, but it surely seemed to be implied.

> Besides, who isn't going to run out and get snow leopard anyway?  :)

As has been pointed out, PPC users, of which there are many and I am 
one. It's a shame, since I have actually paid for it (through a two 
year software support deal that I got through work). But then, as you 
say there were no guarantees.

- Harald

From al at runlevel7.org  Fri Jan  9 09:54:48 2009
From: al at runlevel7.org (Al Gordon)
Date: Fri, 9 Jan 2009 12:54:48 -0500
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <039B9F4E-3DB6-4996-AE4C-CE9BDAF18DBF@apple.com>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>
	<636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>
	<0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>
	<49621C9C.1030807@gmail.com>
	<9F503C73-8AD3-4DCA-93EE-30319842D465@gmail.com>
	<4962BB9C.2050907@gmail.com>
	<DB5706B4-97BF-4179-A3EE-E8A8A8CD066B@ironsoftware.de>
	<039B9F4E-3DB6-4996-AE4C-CE9BDAF18DBF@apple.com>
Message-ID: <63C7F141-F46D-418C-AFC3-1FEF5928F37D@runlevel7.org>

Deric, maybe I'm not quite following your statement, below.  What is  
the purpose of hosting this as an open source project on MacOSXForge  
if you're not going to continue to release source code, and are  
basically requiring that users upgrade to SL to reap the ongoing  
benefits of what's going on in this project?

Are we going to continue to see any new code in this project's public  
svn repo for releases/builds following 119?

Will we get the source to the ZFS code that's actually running in SL?

Thanks,

--

   -- AL --

On Jan 8, 2009, at 2:04 PM, Deric Horn wrote:

> We're currently in the process of integrating the latest OpenSolaris  
> ZFS changes into our code base, and this is one we are obviously  
> interested in.  You'll have to wait for a Snow Leopard seed to see  
> it in action though.
>
> Deric Horn
> Engineering Manager	   File Systems
> Apple Inc.			   408.974.3138

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090109/fcf77a1a/attachment.html>

From deric at apple.com  Fri Jan  9 13:18:16 2009
From: deric at apple.com (Deric Horn)
Date: Fri, 9 Jan 2009 13:18:16 -0800
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <63C7F141-F46D-418C-AFC3-1FEF5928F37D@runlevel7.org>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>
	<636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>
	<0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>
	<49621C9C.1030807@gmail.com>
	<9F503C73-8AD3-4DCA-93EE-30319842D465@gmail.com>
	<4962BB9C.2050907@gmail.com>
	<DB5706B4-97BF-4179-A3EE-E8A8A8CD066B@ironsoftware.de>
	<039B9F4E-3DB6-4996-AE4C-CE9BDAF18DBF@apple.com>
	<63C7F141-F46D-418C-AFC3-1FEF5928F37D@runlevel7.org>
Message-ID: <CF5CA33E-8681-463F-B15F-6C60E8060951@apple.com>

The file system is a fundamental technology within the OS.  I think  
it's pretty safe to say that the ZFS implementation will always have  
dependencies on the latest version of the kernel. We try not to, but  
inevitably things change in the kernel, or we need more functionality  
from the kernel.

We have past the point where we have enough bandwidth to fork our  
work, and are instead committed to delivering a single version of ZFS  
for SL that we can be proud of.

We will continue to post the ZFS source code, but I ask for your  
patience while we work on implementing new features, and fix bugs.

Hope this helps,

Deric Horn
Engineering Manager	   File Systems
Apple Inc.			   408.974.3138

On Jan 9, 2009, at 9:54 AM, Al Gordon wrote:

> Deric, maybe I'm not quite following your statement, below.  What is  
> the purpose of hosting this as an open source project on MacOSXForge  
> if you're not going to continue to release source code, and are  
> basically requiring that users upgrade to SL to reap the ongoing  
> benefits of what's going on in this project?
>
> Are we going to continue to see any new code in this project's  
> public svn repo for releases/builds following 119?
>
> Will we get the source to the ZFS code that's actually running in SL?
>
> Thanks,
>
> --
>
>   -- AL --
>
> On Jan 8, 2009, at 2:04 PM, Deric Horn wrote:
>
>> We're currently in the process of integrating the latest  
>> OpenSolaris ZFS changes into our code base, and this is one we are  
>> obviously interested in.  You'll have to wait for a Snow Leopard  
>> seed to see it in action though.
>>
>> Deric Horn
>> Engineering Manager	   File Systems
>> Apple Inc.			   408.974.3138
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090109/3e3da1be/attachment.html>

From zorg at sogeeky.net  Fri Jan  9 13:33:00 2009
From: zorg at sogeeky.net (Mr. Zorg)
Date: Fri, 9 Jan 2009 13:33:00 -0800
Subject: [zfs-discuss] Fwd:  ZFS panics quite easily
References: <3430E932-4579-4282-8781-0E98408C09FE@sogeeky.net>
Message-ID: <3BD4651D-8AB1-4AB8-BFC9-BA8D7D1E9FEF@sogeeky.net>

Oops, forgot to cc the list...


Begin forwarded message:

> From: "Mr. Zorg" <zorg at sogeeky.net>
> Date: January 9, 2009 1:32:06 PM PST
> To: Deric Horn <deric at apple.com>
> Subject: Re: [zfs-discuss] ZFS panics quite easily
>

> We do appreciate all you've done, particularly your openness and  
> transparency on this group. As long as you post the source and  
> clearly label the point at which you committed to SL dependencies,  
> someone in the FOSS community can work on their own backport. You  
> don't have to to it for us. :)
>
> On Jan 9, 2009, at 1:18 PM, Deric Horn <deric at apple.com> wrote:
>
>> The file system is a fundamental technology within the OS.  I think  
>> it's pretty safe to say that the ZFS implementation will always  
>> have dependencies on the latest version of the kernel. We try not  
>> to, but inevitably things change in the kernel, or we need more  
>> functionality from the kernel.
>>
>> We have past the point where we have enough bandwidth to fork our  
>> work, and are instead committed to delivering a single version of  
>> ZFS for SL that we can be proud of.
>>
>> We will continue to post the ZFS source code, but I ask for your  
>> patience while we work on implementing new features, and fix bugs.
>>
>> Hope this helps,
>>
>> Deric Horn
>> Engineering Manager	   File Systems
>> Apple Inc.			   408.974.3138
>>
>> On Jan 9, 2009, at 9:54 AM, Al Gordon wrote:
>>
>>> Deric, maybe I'm not quite following your statement, below.  What  
>>> is the purpose of hosting this as an open source project on  
>>> MacOSXForge if you're not going to continue to release source  
>>> code, and are basically requiring that users upgrade to SL to reap  
>>> the ongoing benefits of what's going on in this project?
>>>
>>> Are we going to continue to see any new code in this project's  
>>> public svn repo for releases/builds following 119?
>>>
>>> Will we get the source to the ZFS code that's actually running in  
>>> SL?
>>>
>>> Thanks,
>>>
>>> --
>>>
>>>   -- AL --
>>>
>>> On Jan 8, 2009, at 2:04 PM, Deric Horn wrote:
>>>
>>>> We're currently in the process of integrating the latest  
>>>> OpenSolaris ZFS changes into our code base, and this is one we  
>>>> are obviously interested in.  You'll have to wait for a Snow  
>>>> Leopard seed to see it in action though.
>>>>
>>>> Deric Horn
>>>> Engineering Manager	   File Systems
>>>> Apple Inc.			   408.974.3138
>>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090109/87e60ff9/attachment-0001.html>

From lists at chipmunk-app.com  Fri Jan  9 13:38:42 2009
From: lists at chipmunk-app.com (Ruotger Skupin)
Date: Fri, 9 Jan 2009 22:38:42 +0100
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <2011DD85-6B79-400C-8928-0E2FC128DE20@thefloreas.com>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>
	<2011DD85-6B79-400C-8928-0E2FC128DE20@thefloreas.com>
Message-ID: <DBC0A5D5-2E28-437D-A1E2-AED86F282FC8@chipmunk-app.com>

Hi Nathan,

what I actually meant was the following scenario: I had an external  
USB-drive with ZFS connected to the macbook. Then someone else cut my  
power. At the very instance I had a kernel panic. It happened 3 times  
that day. (Don't ask)

I know it's an unusual case. It's sort of Ok too, I can live with it.  
Just wanted to mention it.

My real problem in the original post was that ZFS-119 seems to panic  
under longer-lasting heavy load, which is no fun at all.

After what Deric said about development resources and SL, it doesn't  
make sense to complain about ZFS bugs in Leopard anyway. Well at least  
at the moment. I'm fine with that and I rather have a solid ZFS in SL  
than two half-baked versions.

Ruotger

Am 09.01.2009 um 22:09 schrieb Nathan Florea:

> On Jan 4, 2009, at 9:35 AM, Ruotger Skupin wrote:
>
>> from what I have read on the list it is a known bug that ZFS panics  
>> when an external drive gets unpluggged (for whatever reason...).
>
> I haven't seen this addressed yet in this thread, but this is easy  
> to fix.  After unmounting the ZFS pool, run "zpool export [pool  
> name]".  I still have had a couple of panics, but I think if you  
> simply wait until kernel_task stops chugging, you're fine.
> This was driving me crazy until I actually went back and read the FAQ:
> http://zfs.macosforge.org/trac/wiki/faq
>
> Hope that helps.
>
> Nathan


From nathan.stocks at gmail.com  Fri Jan  9 17:37:03 2009
From: nathan.stocks at gmail.com (Nathan)
Date: Fri, 9 Jan 2009 18:37:03 -0700
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <63C7F141-F46D-418C-AFC3-1FEF5928F37D@runlevel7.org>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>
	<636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>
	<0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>
	<49621C9C.1030807@gmail.com>
	<9F503C73-8AD3-4DCA-93EE-30319842D465@gmail.com>
	<4962BB9C.2050907@gmail.com>
	<DB5706B4-97BF-4179-A3EE-E8A8A8CD066B@ironsoftware.de>
	<039B9F4E-3DB6-4996-AE4C-CE9BDAF18DBF@apple.com>
	<63C7F141-F46D-418C-AFC3-1FEF5928F37D@runlevel7.org>
Message-ID: <96c9d6a80901091737n25c5cc0g45e47c8448b868cb@mail.gmail.com>

On Fri, Jan 9, 2009 at 10:54 AM, Al Gordon <al at runlevel7.org> wrote:
> Deric, maybe I'm not quite following your statement, below.  What is the
> purpose of hosting this as an open source project on MacOSXForge if you're
> not going to continue to release source code, and are basically requiring
> that users upgrade to SL to reap the ongoing benefits of what's going on in
> this project?
> Are we going to continue to see any new code in this project's public svn
> repo for releases/builds following 119?
> Will we get the source to the ZFS code that's actually running in SL?
> Thanks,
> --
>   -- AL --

I would like to point out that it is completely reasonable to release
open-source code that has dependencies on a specific OS, even if the
OS hasn't (yet) been released.  It doesn't make the open-source-ness
any less valuable.  On the contrary, the very fact that it's
open-source makes it potentially possible for someone to patch it to
work on Leopard, assuming someone has the time, skill, and motivation
to actually do so.

~ Nathan S.

From zorg at sogeeky.net  Fri Jan  9 18:01:47 2009
From: zorg at sogeeky.net (Mr. Zorg)
Date: Fri, 9 Jan 2009 18:01:47 -0800
Subject: [zfs-discuss] ZFS panics quite easily
In-Reply-To: <96c9d6a80901091737n25c5cc0g45e47c8448b868cb@mail.gmail.com>
References: <3848E003-8609-4885-B5B8-FD8D273F1005@chipmunk-app.com>
	<636fd28e0901041016y7c2aa6ddt5eecdbccec59eb9f@mail.gmail.com>
	<0EBC7969-BE23-4027-8751-530CF1D94994@chipmunk-app.com>
	<49621C9C.1030807@gmail.com>
	<9F503C73-8AD3-4DCA-93EE-30319842D465@gmail.com>
	<4962BB9C.2050907@gmail.com>
	<DB5706B4-97BF-4179-A3EE-E8A8A8CD066B@ironsoftware.de>
	<039B9F4E-3DB6-4996-AE4C-CE9BDAF18DBF@apple.com>
	<63C7F141-F46D-418C-AFC3-1FEF5928F37D@runlevel7.org>
	<96c9d6a80901091737n25c5cc0g45e47c8448b868cb@mail.gmail.com>
Message-ID: <DB0A9F1B-0B47-4C9B-A3E4-CB9EC9CECA75@sogeeky.net>

Except to the extent that it reveals details about the OS the Apple  
may not be ready to share yet. OS X isn't *all* open source after  
all. :)

On Jan 9, 2009, at 5:37 PM, Nathan <nathan.stocks at gmail.com> wrote:

> On Fri, Jan 9, 2009 at 10:54 AM, Al Gordon <al at runlevel7.org> wrote:
>> Deric, maybe I'm not quite following your statement, below.  What  
>> is the
>> purpose of hosting this as an open source project on MacOSXForge if  
>> you're
>> not going to continue to release source code, and are basically  
>> requiring
>> that users upgrade to SL to reap the ongoing benefits of what's  
>> going on in
>> this project?
>> Are we going to continue to see any new code in this project's  
>> public svn
>> repo for releases/builds following 119?
>> Will we get the source to the ZFS code that's actually running in SL?
>> Thanks,
>> --
>>  -- AL --
>
> I would like to point out that it is completely reasonable to release
> open-source code that has dependencies on a specific OS, even if the
> OS hasn't (yet) been released.  It doesn't make the open-source-ness
> any less valuable.  On the contrary, the very fact that it's
> open-source makes it potentially possible for someone to patch it to
> work on Leopard, assuming someone has the time, skill, and motivation
> to actually do so.
>
> ~ Nathan S.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From dirkschelfhout at mac.com  Sun Jan 11 08:35:56 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Sun, 11 Jan 2009 17:35:56 +0100
Subject: [zfs-discuss] DiskManagement setuid-tool failure on zfs disks
Message-ID: <2F84F764-75A3-4C31-8ACA-23A456F01849@mac.com>

Hi,

Can anyone tell me why diskutil is giving this error
on my zfs disks.
All works fine. ( did a scrub yesterday )
I first noticed this error by using winclone.

Thanks,

Dirk


diskutil list disk0
/dev/disk0
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *232.9 Gi    
disk0
    1:                        EFI                         200.0 Mi    
disk0s1
    2:                  Apple_HFS osx                     80.0 Gi     
disk0s2
    3:                  Apple_HFS empty                   10.0 Gi     
disk0s3
    4:       Microsoft Basic Data windowsXP               116.7 Gi    
disk0s4

Macintosh:~ schelfd$ diskutil list disk1
/dev/disk1
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *465.8 Gi    
disk1

DiskManagement setuid-tool failure

Macintosh:~ schelfd$ diskutil list disk2
/dev/disk2
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *465.8 Gi    
disk2

DiskManagement setuid-tool failure

Macintosh:~ schelfd$ diskutil list disk3
/dev/disk3
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *465.8 Gi    
disk3

DiskManagement setuid-tool failure

Macintosh:~ schelfd$ diskutil list disk4
/dev/disk4
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *233.8 Gi    
disk4
    1:                        EFI                         200.0 Mi    
disk4s1
    2:                  Apple_HFS bootOSX                 233.4 Gi    
disk4s2

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090111/69d63a2a/attachment.html>

From ksh at ironsoftware.de  Tue Jan 13 08:14:39 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Tue, 13 Jan 2009 11:14:39 -0500
Subject: [zfs-discuss] panic()
Message-ID: <6C9C79D3-F23B-4F9B-8112-E52C681EC53D@ironsoftware.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi,

got "new" panic this time.

Mon Jan 12 01:03:59 2009
panic(cpu 1 caller 0x3460A9A0): "[ZFS]: assertion failed in /Volumes/ 
pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/zfs_dir.c  
line 767: error == 0"@/Volumes/pixie_dust/home/ndellofano/zfs-work/ 
zfs-119/zfs_kext/zfs/zfs_dir.c:767
Backtrace (CPU 1), Frame : Return Address (4 potential args on stack)
0x3a3cb678 : 0x12b4f3 (0x45b13c 0x3a3cb6ac 0x1335e4 0x0)
0x3a3cb6c8 : 0x3460a9a0 (0x3465f174 0x3465ef30 0x2ff 0x34620d80)
0x3a3cb748 : 0x345aab8f (0xc8690a0 0x39806998 0xd073400 0x10)
0x3a3cb7f8 : 0x1f4c82 (0x3a3cbb64 0x1 0x0 0x0)
0x3a3cbbb8 : 0x1e6a96 (0x41f7170 0x64053f0 0x3a3cbeec 0x41f7170)
0x3a3cbf78 : 0x3df460 (0x5103a34 0xb93fa40 0xb93fa84 0x0)
0x3a3cbfc8 : 0x1a0887 (0xb8a0f54 0x0 0x1a30b5 0xbeb697c)
No mapping exists for frame pointer
Backtrace terminated-invalid frame pointer 0xbfffad18
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0x345a4000->0x3466ffff

BSD process name corresponding to current thread: iTunes

Mac OS version:
9G55

Kernel version:
Darwin Kernel Version 9.6.0: Mon Nov 24 17:37:00 PST 2008;  
root:xnu-1228.9.59~1/RELEASE_I386
System model name: MacBook1,1 (Mac-F4208CC8)


Chris.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (Darwin)

iD8DBQFJbL3vp+9ff145KVIRAl8FAJ9v3PV6ZtKx5P6QxLDthCGAKfoLLACaA4VT
XNaazYcrco1mbay80ipcD1c=
=/N0G
-----END PGP SIGNATURE-----

From ksh at ironsoftware.de  Tue Jan 13 08:41:41 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Tue, 13 Jan 2009 11:41:41 -0500
Subject: [zfs-discuss] A feature request
In-Reply-To: <180B883C-3ED7-48AE-BE6F-84CA3DF90AF9@sogeeky.net>
References: <fbaadd320812171310j657784a3mbfe3af4a5df029f7@mail.gmail.com>
	<03DD928D-F418-4678-B289-C302389A3B82@apple.com>
	<180B883C-3ED7-48AE-BE6F-84CA3DF90AF9@sogeeky.net>
Message-ID: <0358FD39-466C-40B9-AF3A-82629B26AA54@ironsoftware.de>

you might want to checkout smartmontools.
smartmontools Home Page (last updated $Date: 2008/11/09 15:01:26 $)

Once the relocation event count goes up you will receive emails, sms,  
whatever you configure.
This can pretty precisley forecast an almost dead drive. Last time i  
had 60 minutes in advance
to backup my stuff.

El Dec 23, 2008, a las 3:13 AM, Mr. Zorg escribi?:

> That would be really handy. It doesn't have to be at the filesystem  
> level. Just something like "diskutil blink disk3" would be useful.  
> If I know what disk number the faulted volume is, cool. If not, I  
> can find it by making the *other* drives blink. :)
>
> I swear I saw a *nix program to do that once, but I'll be darned if  
> I can find it now.
>
> Thanks for listening!  Keep up the good work!
>
> On Dec 22, 2008, at 11:57 PM, No?l Dellofano <ndellofano at apple.com>  
> wrote:
>
>> Hmm, I can check with the hardware guys but i'm not sure we have  
>> any control of this sort of thing via the filesystem layer.  Likely  
>> it's out of our control...
>>
>> Noel
>>
>> On Dec 17, 2008, at 1:10 PM, Mr. Zorg ... wrote:
>>
>>> Noel, would it be possible to implement a feature that would cause  
>>> the
>>> drive light to blink on a faulted/offline/whatever disk?  Or is  
>>> there
>>> a tool already that can do this?  Assuming the drive still has some
>>> power, that is.  Using external drives, it's *extremely* difficult  
>>> to
>>> tell which drive is dead/dying since their drive numbers change  
>>> every
>>> time you boot...
>>>
>>> Thanks!
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090113/690a2ce2/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: Mensaje firmado digitalmente
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090113/690a2ce2/attachment.bin>

From ksh at ironsoftware.de  Tue Jan 13 08:42:07 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Tue, 13 Jan 2009 11:42:07 -0500
Subject: [zfs-discuss] TimeMachine (Again)
In-Reply-To: <0424A847-3779-4DEB-AC91-9A30B6114A7C@sbod.at>
References: <41E7C461-E79A-4850-A0D2-441D94DA7CD8@barryfamily.org>
	<0424A847-3779-4DEB-AC91-9A30B6114A7C@sbod.at>
Message-ID: <1DA597A0-A95E-4927-8A32-53B6949F1A67@ironsoftware.de>

I don't remember your last post, but since you are dealing with time  
machine
i'll just point out my solution. Here is what i've done about one year  
ago on Solaris.

1. Setup you ZFS environment
2. Share the Time Machine ZFS via iSCSI
3. Studio Network Solutions - Download iSCSI Initiator for Mac OS X
4. Mount the iSCSI and format the FS as HFS+
5. Setup time machine
6. Snapshot ZFS

Regards
Chris.

El Dec 28, 2008, a las 1:28 PM, Franz Schmalzl escribi?:

> Jup, but ( hence the name ) this only affects network Volumes
>
> thanks for your answer
>
>
> regards
>
>
> franz
>
>
>
> On 28.12.2008, at 19:25, Michael William Barry wrote:
>
>> Have you tried this:
>>
>> defaults write com.apple.systempreferences  
>> TMShowUnsupportedNetworkVolumes 1
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090113/29b9d343/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: Mensaje firmado digitalmente
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090113/29b9d343/attachment.bin>

From vivacarlie at gmail.com  Tue Jan 13 11:57:17 2009
From: vivacarlie at gmail.com (Nehemiah Dacres)
Date: Tue, 13 Jan 2009 13:57:17 -0600
Subject: [zfs-discuss] usb removal
Message-ID: <65fadfc30901131157r5d587ed5i5e8cdf592ea74cf0@mail.gmail.com>

have any of you considered checking for device removal so the kernel doesn't
panic should a usb device be removed from the pool?

-- 

"lalalalala! it's not broken because I can use it"

http://linux.slashdot.org/comments.pl?sid=194281&threshold=1&commentsort=0&mode=thread&cid=15927703
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090113/d323b215/attachment-0001.html>

From zfs at sbod.at  Tue Jan 13 12:08:24 2009
From: zfs at sbod.at (Franz Schmalzl)
Date: Tue, 13 Jan 2009 21:08:24 +0100
Subject: [zfs-discuss] TimeMachine (Again)
In-Reply-To: <1DA597A0-A95E-4927-8A32-53B6949F1A67@ironsoftware.de>
References: <41E7C461-E79A-4850-A0D2-441D94DA7CD8@barryfamily.org>
	<0424A847-3779-4DEB-AC91-9A30B6114A7C@sbod.at>
	<1DA597A0-A95E-4927-8A32-53B6949F1A67@ironsoftware.de>
Message-ID: <969B7C23-5E1C-49FA-A218-D8C2794B1161@sbod.at>

DAS ist ja mal ein Ansatz!

Vielen Dank! Zu meinem Besch?men hatte ich keine Ahung was iSCSI ist...


Werd mir das mal Anschauen..


Franz




On 13.01.2009, at 17:42, Christian Kendi wrote:

> I don't remember your last post, but since you are dealing with time  
> machine
> i'll just point out my solution. Here is what i've done about one  
> year ago on Solaris.
>
> 1. Setup you ZFS environment
> 2. Share the Time Machine ZFS via iSCSI
> 3. Studio Network Solutions - Download iSCSI Initiator for Mac OS X
> 4. Mount the iSCSI and format the FS as HFS+
> 5. Setup time machine
> 6. Snapshot ZFS
>
> Regards
> Chris.
>
> El Dec 28, 2008, a las 1:28 PM, Franz Schmalzl escribi?:
>
>> Jup, but ( hence the name ) this only affects network Volumes
>>
>> thanks for your answer
>>
>>
>> regards
>>
>>
>> franz
>>
>>
>>
>> On 28.12.2008, at 19:25, Michael William Barry wrote:
>>
>>> Have you tried this:
>>>
>>> defaults write com.apple.systempreferences  
>>> TMShowUnsupportedNetworkVolumes 1
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From zfs at sbod.at  Tue Jan 13 12:10:26 2009
From: zfs at sbod.at (Franz Schmalzl)
Date: Tue, 13 Jan 2009 21:10:26 +0100
Subject: [zfs-discuss] TimeMachine (Again)
In-Reply-To: <1DA597A0-A95E-4927-8A32-53B6949F1A67@ironsoftware.de>
References: <41E7C461-E79A-4850-A0D2-441D94DA7CD8@barryfamily.org>
	<0424A847-3779-4DEB-AC91-9A30B6114A7C@sbod.at>
	<1DA597A0-A95E-4927-8A32-53B6949F1A67@ironsoftware.de>
Message-ID: <8B2846B2-1A4D-4AB1-B88A-250CA48E6995@sbod.at>

whoops, sorry guys...

This happens to me quite often ;)

I said:


Now that's a solution!

Thank you! Shame on me i didn't know what iSCSI was...

I'll take a look


Franz



( Free translation :)



Cheers,


Franz 

From zfs at sbod.at  Tue Jan 13 12:19:41 2009
From: zfs at sbod.at (Franz Schmalzl)
Date: Tue, 13 Jan 2009 21:19:41 +0100
Subject: [zfs-discuss] TimeMachine (Again)
In-Reply-To: <8B2846B2-1A4D-4AB1-B88A-250CA48E6995@sbod.at>
References: <41E7C461-E79A-4850-A0D2-441D94DA7CD8@barryfamily.org>
	<0424A847-3779-4DEB-AC91-9A30B6114A7C@sbod.at>
	<1DA597A0-A95E-4927-8A32-53B6949F1A67@ironsoftware.de>
	<8B2846B2-1A4D-4AB1-B88A-250CA48E6995@sbod.at>
Message-ID: <0FA47369-EF6C-4323-8172-2703E2EA3DF0@sbod.at>

Ok, now i've got to ask:

Is there a way to set up an iSCSI target using Mac OS X ?


Thanks,

Franz




On 13.01.2009, at 21:10, Franz Schmalzl wrote:

> whoops, sorry guys...
>
> This happens to me quite often ;)
>
> I said:
>
>
> Now that's a solution!
>
> Thank you! Shame on me i didn't know what iSCSI was...
>
> I'll take a look
>
>
> Franz
>
>
>
> ( Free translation :)
>
>
>
> Cheers,
>
>
> Franz _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From alex.blewitt at gmail.com  Tue Jan 13 15:06:24 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Tue, 13 Jan 2009 23:06:24 +0000
Subject: [zfs-discuss] usb removal
In-Reply-To: <65fadfc30901131157r5d587ed5i5e8cdf592ea74cf0@mail.gmail.com>
References: <65fadfc30901131157r5d587ed5i5e8cdf592ea74cf0@mail.gmail.com>
Message-ID: <636fd28e0901131506i4153f086p5f38c1e0312829bc@mail.gmail.com>

On Tue, Jan 13, 2009 at 7:57 PM, Nehemiah Dacres <vivacarlie at gmail.com> wrote:
> have any of you considered checking for device removal so the kernel doesn't
> panic should a usb device be removed from the pool?

It's not checking for device removal that's the problem; the kernel
knows the device has been removed. It's that it's no longer accessible
as a writable device, and if it's not mirrored, it can't continue to
operate the pool and so reports the error (via panic(), because that's
what it currently does).

Not going to change in Leopard, hopefully by the time Snow Leopard
hits it will be better and configurable to do other things.

Alex

From ksh at ironsoftware.de  Tue Jan 13 18:24:54 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Tue, 13 Jan 2009 21:24:54 -0500
Subject: [zfs-discuss] oh look. a panic()
Message-ID: <55649208-A939-4ED5-AB5F-D2DD83F5848E@ironsoftware.de>

its getting annoying from time to time...

Anyhow. consistancy is astonishing in comparison to HFS+.
On every HFS+ i could resetup my dock and a few applications.

Tue Jan 13 21:10:09 2009
panic(cpu 1 caller 0x345C49A0): "[ZFS]: assertion failed in /Volumes/ 
pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/zfs_dir.c  
line 767: error == 0"@/Volumes/pixie_dust/home/ndellofano/zfs-work/ 
zfs-119/zfs_kext/zfs/zfs_dir.c:767
Backtrace (CPU 1), Frame : Return Address (4 potential args on stack)
0x39927678 : 0x12b4f3 (0x45b13c 0x399276ac 0x1335e4 0x0)
0x399276c8 : 0x345c49a0 (0x34619174 0x34618f30 0x2ff 0x345dad80)
0x39927748 : 0x34564b8f (0x5a44d20 0x3ac458d8 0xe72e200 0x10)
0x399277f8 : 0x1f4c82 (0x39927b64 0x1 0x0 0x0)
0x39927bb8 : 0x1e6a96 (0x857e120 0x815fb50 0x39927eec 0x857e120)
0x39927f78 : 0x3df460 (0x48710e4 0x3f8d9c0 0x3f8da04 0x0)
0x39927fc8 : 0x1a0887 (0x3e3339c 0x0 0x1a30b5 0x3e3339c)
No mapping exists for frame pointer
Backtrace terminated-invalid frame pointer 0xbfffad18
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0x3455e000->0x34629fff

BSD process name corresponding to current thread: iTunes

Mac OS version:
9G55

Kernel version:
Darwin Kernel Version 9.6.0: Mon Nov 24 17:37:00 PST 2008;  
root:xnu-1228.9.59~1/RELEASE_I386
System model name: MacBook1,1 (Mac-F4208CC8)



Sehr geehrter Herr ,



Danke.

Mit freundlichen Gr??en

---
Christian Kendi
Iron Software
Gemeinschaftsstr. 2b
85435 Erding, Germany
mailto: ksh at ironsoftware.de
us mobile: +1-321-507-0653
mobile: +049 (0) 177 / 55 - 31 33 7
phone: +049 (0) 1801 / 6666 26 23 48
*****************************************
Geschaeftsfuehrer: Christian Kendi
Steuernr: 114/235/50572 * Amtsgericht: Erding



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090113/ef0d5782/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: Mensaje firmado digitalmente
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090113/ef0d5782/attachment.bin>

From dirkschelfhout at mac.com  Wed Jan 14 08:34:28 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Wed, 14 Jan 2009 17:34:28 +0100
Subject: [zfs-discuss] [follow-up]Re: DiskManagement setuid-tool failure on
 zfs disks
In-Reply-To: <2F84F764-75A3-4C31-8ACA-23A456F01849@mac.com>
References: <2F84F764-75A3-4C31-8ACA-23A456F01849@mac.com>
Message-ID: <F6827D21-6DD6-445E-81E1-AF6A69D417E6@mac.com>

Strange things are happening.
In single user mode I can get gpt show to list info on my zfs disks.
In multi user mode. with zfs pool unmounted, I get errors.
I am at a loss at what to do next.
All help welcome ,
see below :

Macintosh:etc root# gpt show /dev/disk0
gpt show: unable to open device '/dev/disk0': No such file or directory

Macintosh:etc root# zpool status
   pool: backup
  state: ONLINE
  scrub: none requested
config:

	NAME         STATE     READ WRITE CKSUM
	backup       ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk0s2  ONLINE       0     0     0
	    disk2s2  ONLINE       0     0     0
	    disk3s2  ONLINE       0     0     0

errors: No known data errors

Macintosh:etc root# mount
/dev/disk4s2 on / (hfs, local, journaled)
devfs on /dev (devfs, local)
fdesc on /dev (fdesc, union)
map -hosts on /net (autofs, automounted)
map auto_home on /home (autofs, automounted)
/dev/disk1s2 on /Volumes/osx (hfs, local)
/dev/disk1s3 on /Volumes/empty (hfs, local, journaled)
/dev/disk1s4 on /Volumes/windowsXP (ntfs, local, read-only, noowners)
/dev/disk5 on /Volumes/GB1CULXFRE_EN_DVD (udf, local, nodev, nosuid,  
read-only, noowners)

Macintosh:etc root# dtruss -lf gpt show /dev/disk0
gpt show: unable to open device '/dev/disk0': No such file or directory
	PID/THRD  SYSCALL(args) 		 = return
   412/0xa03d048:  getpid(0x0, 0x0, 0x0)		 = 412 0
   412/0xa03d048:  __sysctl(0xBFFFE5A8, 0x3, 0xBFFFF9C8)		 = 0 0
   412/0xa03d048:  issetugid(0xBFFFE5A8, 0x3, 0xBFFFF9C8)		 = 0 0
   412/0xa03d048:  __sysctl(0xBFFFE464, 0x2, 0xBFFFE42C)		 = 0 0
   412/0xa03d048:  __sysctl(0xBFFFE42C, 0x2, 0xBFFFE4AC)		 = 0 0
   412/0xa03d048:  shared_region_check_np(0xBFFFF9A8, 0x2,  
0xBFFFE4AC)		 = 0 0
   412/0xa03d048:  stat("/usr/lib/dtrace/libdtrace_dyld.dylib\0",  
0xBFFFD618, 0xBFFFE4AC)		 = 0 0
   412/0xa03d048:  open("/usr/lib/dtrace/libdtrace_dyld.dylib\0", 0x0,  
0x0)		 = 3 0
   412/0xa03d048:  pread(0x3, "\312\376\272\276\0", 0x1000, 0x0)		 =  
4096 0
   412/0xa03d048:  pread(0x3, "\316\372\355\376\a\0", 0x1000,  
0x9000)		 = 4096 0
   412/0xa03d048:  mmap(0xA000, 0x1000, 0x5, 0x12, 0x3,  
0xFFFFFFFF00000000)		 = 0xA000 0
   412/0xa03d048:  mmap(0xB000, 0x1000, 0x3, 0x12, 0x3,  
0xFFFFFFFF00000000)		 = 0xB000 0
   412/0xa03d048:  mmap(0xC000, 0x1000, 0x7, 0x12, 0x3,  
0xFFFFFFFF00000000)		 = 0xC000 0
   412/0xa03d048:  mmap(0xD000, 0x1900, 0x1, 0x12, 0x3,  
0xFFFFFFFF00000000)		 = 0xD000 0
   412/0xa03d048:  fcntl(0x3, 0x2C, 0xFFFFFFFFBFFFBC84)		 = 0 0
   412/0xa03d048:  close(0x3)		 = 0 0
   412/0xa03d048:  stat("/usr/lib/libgcc_s.1.dylib\0", 0xBFFFD418,  
0xFFFFFFFFBFFFBC84)		 = 0 0
   412/0xa03d048:  stat("/usr/lib/libSystem.B.dylib\0", 0xBFFFD418,  
0xFFFFFFFFBFFFBC84)		 = 0 0
   412/0xa03d048:  stat("/usr/lib/system/libmathCommon.A.dylib\0",  
0xBFFFD258, 0xFFFFFFFFBFFFBC84)		 = 0 0
   412/0xa03d048:  open("/dev/dtracehelper\0", 0x2, 0xBFFFE434)		 = 3 0
   412/0xa03d048:  ioctl(0x3, 0x80086804, 0xBFFFE3B8)		 = 0 0
   412/0xa03d048:  close(0x3)		 = 0 0
   412/0xa03d048:  __sysctl(0xBFFFE28C, 0x2, 0xBFFFE294)		 = 0 0
   412/0xa03d048:  bsdthread_register(0x91198F30, 0x911D12A4,  
0x1000)		 = 0 0
   412/0xa03d048:  open_nocancel("/dev/urandom\0", 0x0, 0x0)		 = 3 0
   412/0xa03d048:  read_nocancel(0x3, "\001C\021&(\n6\027\340\005\324cm 
\244+\242\310\372\312+\0", 0x20)		 = 32 0
   412/0xa03d048:  close_nocancel(0x3)		 = 0 0
   412/0xa03d048:  mmap(0x0, 0x3000, 0x3, 0x1002, 0x1000000,  
0xFFFFFFFF00000000)		 = 0xF000 0
   412/0xa03d048:  mmap(0x0, 0x200000, 0x3, 0x1002, 0x7000000,  
0xFFFFFFFF00000000)		 = 0x12000 0
   412/0xa03d048:  munmap(0x12000, 0xEE000)		 = 0 0
   412/0xa03d048:  munmap(0x200000, 0x12000)		 = 0 0
   412/0xa03d048:  mmap(0x0, 0x3000, 0x3, 0x1002, 0x1000000,  
0xFFFFFFFF00000000)		 = 0x12000 0
   412/0xa03d048:  getpid(0x0, 0x3000, 0x3)		 = 412 0
   412/0xa03d048:  __sysctl(0xBFFFFA58, 0x2, 0x0)		 = 0 0
   412/0xa03d048:  open("/dev/disk0\0", 0x802, 0x400)		 = -1 Err#16
   412/0xa03d048:  open("/dev//dev/disk0\0", 0x802, 0x5B94)		 = -1 Err#2
   412/0xa03d048:  write_nocancel(0x2, "gpt show: \0", 0xA)		 = 10 0
   412/0xa03d048:  write_nocancel(0x2, "unable to open device '/dev/ 
disk0'\0", 0x22)		 = 34 0
   412/0xa03d048:  write_nocancel(0x2, ": \0", 0x2)		 = 2 0
   412/0xa03d048:  write_nocancel(0x2, "No such file or directory\n 
\0", 0x1A)		 = 26 0

On 11 Jan 2009, at 17:35, Dirk Schelfhout wrote:

> Hi,
>
> Can anyone tell me why diskutil is giving this error
> on my zfs disks.
> All works fine. ( did a scrub yesterday )
> I first noticed this error by using winclone.
>
> Thanks,
>
> Dirk
>
>
> diskutil list disk0
> /dev/disk0
>    #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>    0:      GUID_partition_scheme                        *232.9 Gi    
> disk0
>    1:                        EFI                         200.0 Mi    
> disk0s1
>    2:                  Apple_HFS osx                     80.0 Gi     
> disk0s2
>    3:                  Apple_HFS empty                   10.0 Gi     
> disk0s3
>    4:       Microsoft Basic Data windowsXP               116.7 Gi    
> disk0s4
>
> Macintosh:~ schelfd$ diskutil list disk1
> /dev/disk1
>    #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>    0:      GUID_partition_scheme                        *465.8 Gi    
> disk1
>
> DiskManagement setuid-tool failure
>
> Macintosh:~ schelfd$ diskutil list disk2
> /dev/disk2
>    #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>    0:      GUID_partition_scheme                        *465.8 Gi    
> disk2
>
> DiskManagement setuid-tool failure
>
> Macintosh:~ schelfd$ diskutil list disk3
> /dev/disk3
>    #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>    0:      GUID_partition_scheme                        *465.8 Gi    
> disk3
>
> DiskManagement setuid-tool failure
>
> Macintosh:~ schelfd$ diskutil list disk4
> /dev/disk4
>    #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>    0:      GUID_partition_scheme                        *233.8 Gi    
> disk4
>    1:                        EFI                         200.0 Mi    
> disk4s1
>    2:                  Apple_HFS bootOSX                 233.4 Gi    
> disk4s2
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090114/779e09ce/attachment-0001.html>

From bplist at thinkpink.com  Wed Jan 14 10:41:54 2009
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Wed, 14 Jan 2009 10:41:54 -0800
Subject: [zfs-discuss] [follow-up]Re: DiskManagement setuid-tool failure
	on zfs disks
In-Reply-To: <F6827D21-6DD6-445E-81E1-AF6A69D417E6@mac.com>
References: <2F84F764-75A3-4C31-8ACA-23A456F01849@mac.com>
	<F6827D21-6DD6-445E-81E1-AF6A69D417E6@mac.com>
Message-ID: <980A54B0-418E-456F-8948-4CE5F6E7D536@thinkpink.com>

Looks like a bug in gpt to me.  gpt is opening with O_EXCL | O_RDWR  
(0x802), and the open is returning EBUSY (errno 16), which makes me  
think that the device file system is allowing but one writer on the  
device at a time.  In this case, gpt could clearly open with O_RDONLY  
and succeed, regardless of which other processes have the device open.

bri

On Jan 14, 2009, at 8:34 AM, Dirk Schelfhout wrote:

>   412/0xa03d048:  open("/dev/disk0\0", 0x802, 0x400)		 = -1 Err#16


From dirkschelfhout at mac.com  Wed Jan 14 10:45:10 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Wed, 14 Jan 2009 19:45:10 +0100
Subject: [zfs-discuss] [follow-up]Re: DiskManagement setuid-tool failure
 on zfs disks
In-Reply-To: <980A54B0-418E-456F-8948-4CE5F6E7D536@thinkpink.com>
References: <2F84F764-75A3-4C31-8ACA-23A456F01849@mac.com>
	<F6827D21-6DD6-445E-81E1-AF6A69D417E6@mac.com>
	<980A54B0-418E-456F-8948-4CE5F6E7D536@thinkpink.com>
Message-ID: <1322B81D-A491-49CB-AD9B-1BAA95A5EC2A@mac.com>

But the disk is unmounted.
So no other should be reading it.
On 14 Jan 2009, at 19:41, Brian Pinkerton wrote:

> Looks like a bug in gpt to me.  gpt is opening with O_EXCL | O_RDWR  
> (0x802), and the open is returning EBUSY (errno 16), which makes me  
> think that the device file system is allowing but one writer on the  
> device at a time.  In this case, gpt could clearly open with  
> O_RDONLY and succeed, regardless of which other processes have the  
> device open.
>
> bri
>
> On Jan 14, 2009, at 8:34 AM, Dirk Schelfhout wrote:
>
>>  412/0xa03d048:  open("/dev/disk0\0", 0x802, 0x400)		 = -1 Err#16
>


From bplist at thinkpink.com  Wed Jan 14 10:47:34 2009
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Wed, 14 Jan 2009 10:47:34 -0800
Subject: [zfs-discuss] [follow-up]Re: DiskManagement setuid-tool failure
	on zfs disks
In-Reply-To: <1322B81D-A491-49CB-AD9B-1BAA95A5EC2A@mac.com>
References: <2F84F764-75A3-4C31-8ACA-23A456F01849@mac.com>
	<F6827D21-6DD6-445E-81E1-AF6A69D417E6@mac.com>
	<980A54B0-418E-456F-8948-4CE5F6E7D536@thinkpink.com>
	<1322B81D-A491-49CB-AD9B-1BAA95A5EC2A@mac.com>
Message-ID: <0A73E310-D981-4C2C-87EC-6E8E30CFE92D@thinkpink.com>

I bet ZFS still has it open.  Does it help if you export the pool?

bri

On Jan 14, 2009, at 10:45 AM, Dirk Schelfhout wrote:

> But the disk is unmounted.
> So no other should be reading it.
> On 14 Jan 2009, at 19:41, Brian Pinkerton wrote:
>
>> Looks like a bug in gpt to me.  gpt is opening with O_EXCL | O_RDWR  
>> (0x802), and the open is returning EBUSY (errno 16), which makes me  
>> think that the device file system is allowing but one writer on the  
>> device at a time.  In this case, gpt could clearly open with  
>> O_RDONLY and succeed, regardless of which other processes have the  
>> device open.
>>
>> bri
>>
>> On Jan 14, 2009, at 8:34 AM, Dirk Schelfhout wrote:
>>
>>> 412/0xa03d048:  open("/dev/disk0\0", 0x802, 0x400)		 = -1 Err#16
>>
>


From dirkschelfhout at mac.com  Wed Jan 14 11:08:14 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Wed, 14 Jan 2009 20:08:14 +0100
Subject: [zfs-discuss] [follow-up]Re: DiskManagement setuid-tool failure
 on zfs disks
In-Reply-To: <0A73E310-D981-4C2C-87EC-6E8E30CFE92D@thinkpink.com>
References: <2F84F764-75A3-4C31-8ACA-23A456F01849@mac.com>
	<F6827D21-6DD6-445E-81E1-AF6A69D417E6@mac.com>
	<980A54B0-418E-456F-8948-4CE5F6E7D536@thinkpink.com>
	<1322B81D-A491-49CB-AD9B-1BAA95A5EC2A@mac.com>
	<0A73E310-D981-4C2C-87EC-6E8E30CFE92D@thinkpink.com>
Message-ID: <07E50A73-0018-4651-B5FB-6C7E0D0BDD76@mac.com>

No it doesn't.
On 14 Jan 2009, at 19:47, Brian Pinkerton wrote:

> I bet ZFS still has it open.  Does it help if you export the pool?
>
> bri
>
> On Jan 14, 2009, at 10:45 AM, Dirk Schelfhout wrote:
>
>> But the disk is unmounted.
>> So no other should be reading it.
>> On 14 Jan 2009, at 19:41, Brian Pinkerton wrote:
>>
>>> Looks like a bug in gpt to me.  gpt is opening with O_EXCL |  
>>> O_RDWR (0x802), and the open is returning EBUSY (errno 16), which  
>>> makes me think that the device file system is allowing but one  
>>> writer on the device at a time.  In this case, gpt could clearly  
>>> open with O_RDONLY and succeed, regardless of which other  
>>> processes have the device open.
>>>
>>> bri
>>>
>>> On Jan 14, 2009, at 8:34 AM, Dirk Schelfhout wrote:
>>>
>>>> 412/0xa03d048:  open("/dev/disk0\0", 0x802, 0x400)		 = -1 Err#16
>>>
>>
>


From dirkschelfhout at mac.com  Wed Jan 14 11:26:10 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Wed, 14 Jan 2009 20:26:10 +0100
Subject: [zfs-discuss] [follow-up]Re: DiskManagement setuid-tool failure
 on zfs disks
In-Reply-To: <07E50A73-0018-4651-B5FB-6C7E0D0BDD76@mac.com>
References: <2F84F764-75A3-4C31-8ACA-23A456F01849@mac.com>
	<F6827D21-6DD6-445E-81E1-AF6A69D417E6@mac.com>
	<980A54B0-418E-456F-8948-4CE5F6E7D536@thinkpink.com>
	<1322B81D-A491-49CB-AD9B-1BAA95A5EC2A@mac.com>
	<0A73E310-D981-4C2C-87EC-6E8E30CFE92D@thinkpink.com>
	<07E50A73-0018-4651-B5FB-6C7E0D0BDD76@mac.com>
Message-ID: <0B09BBDF-F45F-4F08-9850-E5E064C1E55B@mac.com>

Oeps I was wrong.
I was looking at the wrong device.
It does help with gpt.
As soon as i run the command :
gpt show /dev/disk1
the finder remounts my zfs raid
diskutil list still fails
Dirk
On 14 Jan 2009, at 20:08, Dirk Schelfhout wrote:

> No it doesn't.
> On 14 Jan 2009, at 19:47, Brian Pinkerton wrote:
>
>> I bet ZFS still has it open.  Does it help if you export the pool?
>>
>> bri
>>
>> On Jan 14, 2009, at 10:45 AM, Dirk Schelfhout wrote:
>>
>>> But the disk is unmounted.
>>> So no other should be reading it.
>>> On 14 Jan 2009, at 19:41, Brian Pinkerton wrote:
>>>
>>>> Looks like a bug in gpt to me.  gpt is opening with O_EXCL |  
>>>> O_RDWR (0x802), and the open is returning EBUSY (errno 16), which  
>>>> makes me think that the device file system is allowing but one  
>>>> writer on the device at a time.  In this case, gpt could clearly  
>>>> open with O_RDONLY and succeed, regardless of which other  
>>>> processes have the device open.
>>>>
>>>> bri
>>>>
>>>> On Jan 14, 2009, at 8:34 AM, Dirk Schelfhout wrote:
>>>>
>>>>> 412/0xa03d048:  open("/dev/disk0\0", 0x802, 0x400)		 = -1 Err#16
>>>>
>>>
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dirkschelfhout at mac.com  Wed Jan 14 11:30:55 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Wed, 14 Jan 2009 20:30:55 +0100
Subject: [zfs-discuss] [follow-up]Re: DiskManagement setuid-tool failure
 on zfs disks
In-Reply-To: <0B09BBDF-F45F-4F08-9850-E5E064C1E55B@mac.com>
References: <2F84F764-75A3-4C31-8ACA-23A456F01849@mac.com>
	<F6827D21-6DD6-445E-81E1-AF6A69D417E6@mac.com>
	<980A54B0-418E-456F-8948-4CE5F6E7D536@thinkpink.com>
	<1322B81D-A491-49CB-AD9B-1BAA95A5EC2A@mac.com>
	<0A73E310-D981-4C2C-87EC-6E8E30CFE92D@thinkpink.com>
	<07E50A73-0018-4651-B5FB-6C7E0D0BDD76@mac.com>
	<0B09BBDF-F45F-4F08-9850-E5E064C1E55B@mac.com>
Message-ID: <ED91D068-B965-40C1-AFA3-CB2BD509B094@mac.com>

This started to fail with the update to 10.5.6
So I suppose other people must have this problem as well.
I reinstalled zfs today, so that is not what is doing it.
I can't even use disk utility to look at the partition table of a disk  
used for zfs !!
Can anyone else check this please ?
Dirk
On 14 Jan 2009, at 20:26, Dirk Schelfhout wrote:

> Oeps I was wrong.
> I was looking at the wrong device.
> It does help with gpt.
> As soon as i run the command :
> gpt show /dev/disk1
> the finder remounts my zfs raid
> diskutil list still fails
> Dirk
> On 14 Jan 2009, at 20:08, Dirk Schelfhout wrote:
>
>> No it doesn't.
>> On 14 Jan 2009, at 19:47, Brian Pinkerton wrote:
>>
>>> I bet ZFS still has it open.  Does it help if you export the pool?
>>>
>>> bri
>>>
>>> On Jan 14, 2009, at 10:45 AM, Dirk Schelfhout wrote:
>>>
>>>> But the disk is unmounted.
>>>> So no other should be reading it.
>>>> On 14 Jan 2009, at 19:41, Brian Pinkerton wrote:
>>>>
>>>>> Looks like a bug in gpt to me.  gpt is opening with O_EXCL |  
>>>>> O_RDWR (0x802), and the open is returning EBUSY (errno 16),  
>>>>> which makes me think that the device file system is allowing but  
>>>>> one writer on the device at a time.  In this case, gpt could  
>>>>> clearly open with O_RDONLY and succeed, regardless of which  
>>>>> other processes have the device open.
>>>>>
>>>>> bri
>>>>>
>>>>> On Jan 14, 2009, at 8:34 AM, Dirk Schelfhout wrote:
>>>>>
>>>>>> 412/0xa03d048:  open("/dev/disk0\0", 0x802, 0x400)		 = -1 Err#16
>>>>>
>>>>
>>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dirkschelfhout at mac.com  Wed Jan 14 12:55:07 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Wed, 14 Jan 2009 21:55:07 +0100
Subject: [zfs-discuss] [follow-up]Re: DiskManagement setuid-tool failure
 on zfs disks
In-Reply-To: <ED91D068-B965-40C1-AFA3-CB2BD509B094@mac.com>
References: <2F84F764-75A3-4C31-8ACA-23A456F01849@mac.com>
	<F6827D21-6DD6-445E-81E1-AF6A69D417E6@mac.com>
	<980A54B0-418E-456F-8948-4CE5F6E7D536@thinkpink.com>
	<1322B81D-A491-49CB-AD9B-1BAA95A5EC2A@mac.com>
	<0A73E310-D981-4C2C-87EC-6E8E30CFE92D@thinkpink.com>
	<07E50A73-0018-4651-B5FB-6C7E0D0BDD76@mac.com>
	<0B09BBDF-F45F-4F08-9850-E5E064C1E55B@mac.com>
	<ED91D068-B965-40C1-AFA3-CB2BD509B094@mac.com>
Message-ID: <E46D86C1-2110-4C88-816E-8EBA9D899C26@mac.com>

I just downloaded the 10.5.6 combo update and installed.
Problem is still the same....
On 14 Jan 2009, at 20:30, Dirk Schelfhout wrote:

> This started to fail with the update to 10.5.6
> So I suppose other people must have this problem as well.
> I reinstalled zfs today, so that is not what is doing it.
> I can't even use disk utility to look at the partition table of a  
> disk used for zfs !!
> Can anyone else check this please ?
> Dirk
> On 14 Jan 2009, at 20:26, Dirk Schelfhout wrote:
>>>>>>

From zslg01 at mac.com  Sat Jan  3 12:11:11 2009
From: zslg01 at mac.com (Sterling Garwood)
Date: Sat, 03 Jan 2009 15:11:11 -0500
Subject: [zfs-discuss] Kernel panic when plugging in USB SSD drive
Message-ID: <D8004776-E03B-4A30-9A3D-5278EE84F087@mac.com>

I installed zfs-119 on top of MacOSX 10.5.6 - used a 'thumb' USB drive  
to test zfs on.
Created pool, copied stuff to it, did scribs and stats etc.
Unmounted via Finder Apple-E then unplugged drive.
when I plugged it back in the kernel panic happened.
is it a known issue?
any debug info I can send to someone?
Is it worth following up on for 10.5 or wait for 10.6 ??

From mathieu.lebon at gmail.com  Mon Jan  5 13:53:53 2009
From: mathieu.lebon at gmail.com (mathieu.email)
Date: Mon, 5 Jan 2009 22:53:53 +0100
Subject: [zfs-discuss] Zpool disaster
Message-ID: <88975A88-D238-4FF1-ABD3-680C69FDAD92@gmail.com>

Hi,

I have a big problem with my ZFS disk. After a kernel panic, I cannot  
import the pool anymore :

------------------------------------------------------------------------------------------------------------------
=> zpool status
no pools available
=> zpool list
no pools available

------------------------------------------------------------------------------------------------------------------
=> zpool import

pool: ZFS
        id: 9004030332584099627
   state: FAULTED
status: The pool metadata is corrupted.
action: The pool cannot be imported due to damaged devices or data.
              The pool may be active on on another system, but can be  
imported using
              the '-f' flag.
     see: http://www.sun.com/msg/ZFS-8000-72
config:

	ZFS            FAULTED  corrupted data
	  disk2s2   ONLINE

   pool: ZFS
        id: 5050959592823553345
   state: FAULTED
status: The pool was last accessed by another system.
action: The pool cannot be imported due to damaged devices or data.
     see: http://www.sun.com/msg/ZFS-8000-EY
config:

	ZFS         UNAVAIL  insufficient replicas
	  disk1     UNAVAIL  cannot open

------------------------------------------------------------------------------------------------------------------
=> zpool import -f 9004030332584099627
cannot import 'ZFS': I/O error
------------------------------------------------------------------------------------------------------------------

I am despaired as I have no backup and all my data are on this drive.

Is there anything I can do ?

Thank you for your help,

Mathieu

From pmau at me.com  Wed Jan 14 16:23:09 2009
From: pmau at me.com (Patrick Mau)
Date: Thu, 15 Jan 2009 01:23:09 +0100
Subject: [zfs-discuss] Question about ZFS status
In-Reply-To: <97880A58-AF3A-485E-A626-8141C4B065A8@me.com>
References: <97880A58-AF3A-485E-A626-8141C4B065A8@me.com>
Message-ID: <33FAB1ED-9F94-435E-8C1F-8B10D9848C8A@me.com>

Hallo everyone

The message below has taken two weeks to be approved. In the meantime  
I've already subscribed and
my comment has been answered already here:

http://lists.macosforge.org/pipermail/zfs-discuss/2009-January/001278.html

Thanks to Deric, I'm hopeful that we will see continous improvements  
that
will benefit early adaptors like myself.

Please disregard my comments, hopefully you don't consider my first  
post as bad netiquette.

I'm using a mirrored 700GB setup with 10.5.6 and zfs-119 and I'm  
pleased by its performance and
stability, minus the issues that are already worked on.

Best regards,
Patrick

On 31.12.2008, at 15:14, Patrick Mau wrote:

> Hallo everyone
>
> First, I'm not yet subscribed to the list, please include my address  
> in the reply, thanks a lot.
>
> I've recently started using ZFS (zfs-119) on my MacPro. It all  
> worked fine, ignoring all the open issues that are still work in  
> progress.
> Looking at the source makes me wonder if there will be additionall  
> releases before the inclusion in MacOS 10.6.
>
> Since Apple published the source, there have been very little  
> changes to the source, so I'd like to ask if it's better to wait for  
> 10.6 before actively converting my existing filesystems. The last  
> change seems to be 5 month ago.
>
> Does anyone have any information wether Apple will continue  
> publishing changes here, or is all the development on ZFS targetting  
> the next OSX release and will only be included in upcoming seeds for  
> Snow Leopard?
>
> Don't get me wrong. I'm just wondering if it's better to wait for  
> official releases, because there's little visible activity with  
> regards to the cdebase.
>
> Thanks, and have a Happy New Year
> Patrick

From alex.blewitt at gmail.com  Wed Jan 14 16:23:50 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Thu, 15 Jan 2009 00:23:50 +0000
Subject: [zfs-discuss] Question about ZFS status
In-Reply-To: <97880A58-AF3A-485E-A626-8141C4B065A8@me.com>
References: <97880A58-AF3A-485E-A626-8141C4B065A8@me.com>
Message-ID: <636fd28e0901141623h4ce5ef8fv6bfedfb1863f3091@mail.gmail.com>

On Wed, Dec 31, 2008 at 2:14 PM, Patrick Mau <pmau at me.com> wrote:
> First, I'm not yet subscribed to the list, please include my address in the
> reply, thanks a lot.
>
> I've recently started using ZFS (zfs-119) on my MacPro. It all worked fine,
> ignoring all the open issues that are still work in progress.
> Looking at the source makes me wonder if there will be additionall releases
> before the inclusion in MacOS 10.6.

I seem to recall a recent conversation being 'no', on the grounds that
the ZFS IOKit integration required some kernel changes, which will
only be in 10.6. So whilst there might be a community effort in
merging, I don't think it's going to be officially supported.

> Since Apple published the source, there have been very little changes to the
> source, so I'd like to ask if it's better to wait for 10.6 before actively
> converting my existing filesystems. The last change seems to be 5 month ago.

The code is old, but works. I think if you're actively playing with
ZFS, then it is probably a good idea to create a few systems to get
used to what it does. Others have reported occasional problems/panics;
especially those with external USB devices.

For me, I converted all my data over a month or two ago (and indeed,
on the laptop that I'm using to write this) and so far so good.

Alex

From alex.blewitt at gmail.com  Wed Jan 14 16:26:14 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Thu, 15 Jan 2009 00:26:14 +0000
Subject: [zfs-discuss] Kernel panic when plugging in USB SSD drive
In-Reply-To: <D8004776-E03B-4A30-9A3D-5278EE84F087@mac.com>
References: <D8004776-E03B-4A30-9A3D-5278EE84F087@mac.com>
Message-ID: <636fd28e0901141626l38842bc1tc53076d52b8048b6@mail.gmail.com>

On Sat, Jan 3, 2009 at 8:11 PM, Sterling Garwood <zslg01 at mac.com> wrote:
> I installed zfs-119 on top of MacOSX 10.5.6 - used a 'thumb' USB drive to
> test zfs on.
> Created pool, copied stuff to it, did scribs and stats etc.
> Unmounted via Finder Apple-E then unplugged drive.
> when I plugged it back in the kernel panic happened.
> is it a known issue?

I'm pretty sure you'd need to do a 'zfs export -f pool' prior to
unplugging the USB drive. The integration with the finder isn't there
in 10.5. After unplugging, the system was probably prone to a panic on
the next usb insert anyway. I assume that after a reboot, you could
import the pool from the USB drive successfully?

Alex

From vivacarlie at gmail.com  Wed Jan 14 16:36:32 2009
From: vivacarlie at gmail.com (Nehemiah Dacres)
Date: Wed, 14 Jan 2009 18:36:32 -0600
Subject: [zfs-discuss] usb removal
In-Reply-To: <636fd28e0901131506i4153f086p5f38c1e0312829bc@mail.gmail.com>
References: <65fadfc30901131157r5d587ed5i5e8cdf592ea74cf0@mail.gmail.com>
	<636fd28e0901131506i4153f086p5f38c1e0312829bc@mail.gmail.com>
Message-ID: <65fadfc30901141636i228a8d95k1f342845bb143cd9@mail.gmail.com>

have any of you considered checking for device removal so the kernel doesn't
panic should a usb device be removed from the pool?
I have words for how much of a bad idea that is, but i can't use them right
now.
-- 

"lalalalala! it's not broken because I can use it"

http://linux.slashdot.org/comments.pl?sid=194281&threshold=1&commentsort=0&mode=thread&cid=15927703
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090114/34a5fd94/attachment.html>

From vivacarlie at gmail.com  Wed Jan 14 17:33:27 2009
From: vivacarlie at gmail.com (Nehemiah Dacres)
Date: Wed, 14 Jan 2009 19:33:27 -0600
Subject: [zfs-discuss] usb removal
In-Reply-To: <636fd28e0901131506i4153f086p5f38c1e0312829bc@mail.gmail.com>
References: <65fadfc30901131157r5d587ed5i5e8cdf592ea74cf0@mail.gmail.com>
	<636fd28e0901131506i4153f086p5f38c1e0312829bc@mail.gmail.com>
Message-ID: <65fadfc30901141733i3b350d74qb3710666653a2ef3@mail.gmail.com>

I'll use them now. Someone Tell me Why a kernel extension is calling a Panic
for an error thats non fatal. shouldn't it do ANYTHING ELSE. like logging
the error and dieing like a good kernel extension should, without killing
the machine.  device failure shouldn't kill the machine especially when the
file system was specially designed to withstand device failure. that was one
of it's star features. Granted that A LOT of sectors would be recorded _BAD_
it should still contenue operation. It's also a little idiosyncratic to
allow removable media to be part of a boot volume.
I want to point out that this is not good setup. perhaps the default
SnowLeopard install will have Users and System on different zpools. Only a
sysadmin would need to know this. a home user shouldn't notice.

On Tue, Jan 13, 2009 at 5:06 PM, Alex Blewitt <alex.blewitt at gmail.com>wrote:

> On Tue, Jan 13, 2009 at 7:57 PM, Nehemiah Dacres <vivacarlie at gmail.com>
> wrote:
> > have any of you considered checking for device removal so the kernel
> doesn't
> > panic should a usb device be removed from the pool?
>
> It's not checking for device removal that's the problem; the kernel
> knows the device has been removed. It's that it's no longer accessible
> as a writable device, and if it's not mirrored, it can't continue to
> operate the pool and so reports the error (via panic(), because that's
> what it currently does).
>
> Not going to change in Leopard, hopefully by the time Snow Leopard
> hits it will be better and configurable to do other things.
>
> Alex
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>



-- 

"lalalalala! it's not broken because I can use it"

http://linux.slashdot.org/comments.pl?sid=194281&threshold=1&commentsort=0&mode=thread&cid=15927703
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090114/2f0bc4ba/attachment.html>

From zorg at sogeeky.net  Wed Jan 14 20:26:36 2009
From: zorg at sogeeky.net (Mr. Zorg)
Date: Wed, 14 Jan 2009 20:26:36 -0800
Subject: [zfs-discuss] usb removal
In-Reply-To: <65fadfc30901141733i3b350d74qb3710666653a2ef3@mail.gmail.com>
References: <65fadfc30901131157r5d587ed5i5e8cdf592ea74cf0@mail.gmail.com>
	<636fd28e0901131506i4153f086p5f38c1e0312829bc@mail.gmail.com>
	<65fadfc30901141733i3b350d74qb3710666653a2ef3@mail.gmail.com>
Message-ID: <50C7FC19-0545-4C6E-B290-51A30DA08E95@sogeeky.net>

It does it because Sun created it and that's what Sun said it SHOULD  
DO when data integrity cannot be guaranteed. So that's what Apple's  
port does too. Sun later changed their mind and made it an option but  
that hasn't been ported yet. They're working on it.

As for ZFS being designed to tolerate device failure, you're a little  
off in your expectations. Bad sectors will be tolerated as you say,  
but a total device failure will only be tolerated if you're running a  
mirrored or raidz pool. If you've just got straight disks and one  
completely dies you're screwed.

The problem is that unplugging an external drive looks like a total  
failure to the current port. A better solution, which Apple has said  
they're looking at, is to recognize that it was removed and not dead  
and possibly pause the filesystem and prompt you to plug it back in.  
But that won't happen in Leopard because it doesn't have the kernel  
hook they'd need, but possibly in Snow Leopard.

I hope this helps clarify things.

P.S. I don't think Sun ever really envisioned ZFS being used on USB or  
Firewire drives. Certainly not in a non mirrored or raidz setup. :)

On Jan 14, 2009, at 5:33 PM, "Nehemiah Dacres" <vivacarlie at gmail.com>  
wrote:

>
>
> I'll use them now. Someone Tell me Why a kernel extension is calling  
> a Panic for an error thats non fatal. shouldn't it do ANYTHING ELSE.  
> like logging the error and dieing like a good kernel extension  
> should, without killing the machine.  device failure shouldn't kill  
> the machine especially when the file system was specially designed  
> to withstand device failure. that was one of it's star features.  
> Granted that A LOT of sectors would be recorded _BAD_ it should  
> still contenue operation. It's also a little idiosyncratic to allow  
> removable media to be part of a boot volume.
> I want to point out that this is not good setup. perhaps the default  
> SnowLeopard install will have Users and System on different zpools.  
> Only a sysadmin would need to know this. a home user shouldn't notice.
>
> On Tue, Jan 13, 2009 at 5:06 PM, Alex Blewitt  
> <alex.blewitt at gmail.com> wrote:
> On Tue, Jan 13, 2009 at 7:57 PM, Nehemiah Dacres  
> <vivacarlie at gmail.com> wrote:
> > have any of you considered checking for device removal so the  
> kernel doesn't
> > panic should a usb device be removed from the pool?
>
> It's not checking for device removal that's the problem; the kernel
> knows the device has been removed. It's that it's no longer accessible
> as a writable device, and if it's not mirrored, it can't continue to
> operate the pool and so reports the error (via panic(), because that's
> what it currently does).
>
> Not going to change in Leopard, hopefully by the time Snow Leopard
> hits it will be better and configurable to do other things.
>
> Alex
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
>
> -- 
>
> "lalalalala! it's not broken because I can use it"
>
> http://linux.slashdot.org/comments.pl?sid=194281&threshold=1&commentsort=0&mode=thread&cid=15927703
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090114/0b11cfe5/attachment-0001.html>

From todd.e.moore at gmail.com  Wed Jan 14 21:18:07 2009
From: todd.e.moore at gmail.com (Todd Moore)
Date: Thu, 15 Jan 2009 00:18:07 -0500
Subject: [zfs-discuss] usb removal
In-Reply-To: <50C7FC19-0545-4C6E-B290-51A30DA08E95@sogeeky.net>
References: <65fadfc30901131157r5d587ed5i5e8cdf592ea74cf0@mail.gmail.com>	<636fd28e0901131506i4153f086p5f38c1e0312829bc@mail.gmail.com>	<65fadfc30901141733i3b350d74qb3710666653a2ef3@mail.gmail.com>
	<50C7FC19-0545-4C6E-B290-51A30DA08E95@sogeeky.net>
Message-ID: <496EC70F.7010200@gmail.com>

An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090115/6102116c/attachment.html>

From james-zfsosx at jrv.org  Thu Jan 15 03:40:43 2009
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Thu, 15 Jan 2009 05:40:43 -0600
Subject: [zfs-discuss] Zpool disaster
In-Reply-To: <88975A88-D238-4FF1-ABD3-680C69FDAD92@gmail.com>
References: <88975A88-D238-4FF1-ABD3-680C69FDAD92@gmail.com>
Message-ID: <496F20BB.4010506@jrv.org>

I just received this note.

This looks like a disk error, not a ZFS error.  Look in
/var/log/system.log after the import and see if the disk reported an
error.  If so mount the hard disk in some other enclosure, with a
different power supply, cables, connector on the system, etc, and see if
the disk can be read that way.

mathieu.email wrote:
> Hi,
>
> I have a big problem with my ZFS disk. After a kernel panic, I cannot
> import the pool anymore :
>
> ------------------------------------------------------------------------------------------------------------------
>
> => zpool status
> no pools available
> => zpool list
> no pools available
>
> ------------------------------------------------------------------------------------------------------------------
>
> => zpool import
>
> pool: ZFS
>        id: 9004030332584099627
>   state: FAULTED
> status: The pool metadata is corrupted.
> action: The pool cannot be imported due to damaged devices or data.
>              The pool may be active on on another system, but can be
> imported using
>              the '-f' flag.
>     see: http://www.sun.com/msg/ZFS-8000-72
> config:
>
>     ZFS            FAULTED  corrupted data
>       disk2s2   ONLINE
>
>   pool: ZFS
>        id: 5050959592823553345
>   state: FAULTED
> status: The pool was last accessed by another system.
> action: The pool cannot be imported due to damaged devices or data.
>     see: http://www.sun.com/msg/ZFS-8000-EY
> config:
>
>     ZFS         UNAVAIL  insufficient replicas
>       disk1     UNAVAIL  cannot open
>
> ------------------------------------------------------------------------------------------------------------------
>
> => zpool import -f 9004030332584099627
> cannot import 'ZFS': I/O error
> ------------------------------------------------------------------------------------------------------------------
>
>
> I am despaired as I have no backup and all my data are on this drive.
>
> Is there anything I can do ?
>
> Thank you for your help,
>
> Mathieu
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dwebb at erbco.com  Wed Jan 14 15:30:55 2009
From: dwebb at erbco.com (Dustin Webb)
Date: Wed, 14 Jan 2009 18:30:55 -0500
Subject: [zfs-discuss] Zpool disaster
In-Reply-To: <88975A88-D238-4FF1-ABD3-680C69FDAD92@gmail.com>
References: <88975A88-D238-4FF1-ABD3-680C69FDAD92@gmail.com>
Message-ID: <0EB4C19C-345D-4C15-98DE-A563DCDB8B70@erbco.com>

You should be able to simply force the import.  Since your system  
froze, it never properly "ejected" the pool.  The command is 'zpool  
import -f' or 'zpool -f import'

Don't Panic!

Dustin Webb
ITguy
Erb Co Inc


On Jan 5, 2009, at 4:53 PM, mathieu.email wrote:

> Hi,
>
> I have a big problem with my ZFS disk. After a kernel panic, I  
> cannot import the pool anymore :
>
> ------------------------------------------------------------------------------------------------------------------
> => zpool status
> no pools available
> => zpool list
> no pools available
>
> ------------------------------------------------------------------------------------------------------------------
> => zpool import
>
> pool: ZFS
>       id: 9004030332584099627
>  state: FAULTED
> status: The pool metadata is corrupted.
> action: The pool cannot be imported due to damaged devices or data.
>             The pool may be active on on another system, but can be  
> imported using
>             the '-f' flag.
>    see: http://www.sun.com/msg/ZFS-8000-72
> config:
>
> 	ZFS            FAULTED  corrupted data
> 	  disk2s2   ONLINE
>
>  pool: ZFS
>       id: 5050959592823553345
>  state: FAULTED
> status: The pool was last accessed by another system.
> action: The pool cannot be imported due to damaged devices or data.
>    see: http://www.sun.com/msg/ZFS-8000-EY
> config:
>
> 	ZFS         UNAVAIL  insufficient replicas
> 	  disk1     UNAVAIL  cannot open
>
> ------------------------------------------------------------------------------------------------------------------
> => zpool import -f 9004030332584099627
> cannot import 'ZFS': I/O error
> ------------------------------------------------------------------------------------------------------------------
>
> I am despaired as I have no backup and all my data are on this drive.
>
> Is there anything I can do ?
>
> Thank you for your help,
>
> Mathieu
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From dirkschelfhout at mac.com  Thu Jan 15 06:21:03 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Thu, 15 Jan 2009 15:21:03 +0100
Subject: [zfs-discuss] [follow-up]Re: DiskManagement setuid-tool failure
 on zfs disks
In-Reply-To: <E46D86C1-2110-4C88-816E-8EBA9D899C26@mac.com>
References: <2F84F764-75A3-4C31-8ACA-23A456F01849@mac.com>
	<F6827D21-6DD6-445E-81E1-AF6A69D417E6@mac.com>
	<980A54B0-418E-456F-8948-4CE5F6E7D536@thinkpink.com>
	<1322B81D-A491-49CB-AD9B-1BAA95A5EC2A@mac.com>
	<0A73E310-D981-4C2C-87EC-6E8E30CFE92D@thinkpink.com>
	<07E50A73-0018-4651-B5FB-6C7E0D0BDD76@mac.com>
	<0B09BBDF-F45F-4F08-9850-E5E064C1E55B@mac.com>
	<ED91D068-B965-40C1-AFA3-CB2BD509B094@mac.com>
	<E46D86C1-2110-4C88-816E-8EBA9D899C26@mac.com>
Message-ID: <F2A01304-425D-4CA0-98A1-E6CEA7D5CE6D@mac.com>

When I enable core dumping and load into gdb :

#0  0x915fdf45 in strlcpy ()
(gdb) where full
#0  0x915fdf45 in strlcpy ()
No symbol table info available.
#1  0x00092555 in GPTReadMedia ()
No symbol table info available.

Can anyone from apple point me in the right direction ?

I can't find the darwin source for diskutil

My next step is to reformat my zfs disks, but I would rather not.
( It takes a long time....... ( resilvering.... ))

Dirk
On 14 Jan 2009, at 21:55, Dirk Schelfhout wrote:

> I just downloaded the 10.5.6 combo update and installed.
> Problem is still the same....
> On 14 Jan 2009, at 20:30, Dirk Schelfhout wrote:
>
>> This started to fail with the update to 10.5.6
>> So I suppose other people must have this problem as well.
>> I reinstalled zfs today, so that is not what is doing it.
>> I can't even use disk utility to look at the partition table of a  
>> disk used for zfs !!
>> Can anyone else check this please ?
>> Dirk
>> On 14 Jan 2009, at 20:26, Dirk Schelfhout wrote:
>>>>>>>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ksh at ironsoftware.de  Thu Jan 15 15:04:14 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Thu, 15 Jan 2009 18:04:14 -0500
Subject: [zfs-discuss] Zpool disaster
In-Reply-To: <0EB4C19C-345D-4C15-98DE-A563DCDB8B70@erbco.com>
References: <88975A88-D238-4FF1-ABD3-680C69FDAD92@gmail.com>
	<0EB4C19C-345D-4C15-98DE-A563DCDB8B70@erbco.com>
Message-ID: <96274A9A-C01E-4A20-98BE-AB27BCA75AD9@ironsoftware.de>

well, he already stated that he used the force "-f" flag.

>> ------------------------------------------------------------------------------------------------------------------
>> => zpool import -f 9004030332584099627
>> cannot import 'ZFS': I/O error


This is a good point. What could we do in such a case ?

Well, thanks to you i'm into poor-mans backup solution,
i.e. attaching-detaching a mirror and letting it resilver each time.


---
Christian Kendi
Iron Software
Gemeinschaftsstr. 2b
85435 Erding, Germany
mailto: ksh at ironsoftware.de
us mobile: +1-321-507-0653
mobile: +049 (0) 177 / 55 - 31 33 7
phone: +049 (0) 1801 / 6666 26 23 48
*****************************************
Geschaeftsfuehrer: Christian Kendi
Steuernr: 114/235/50572 * Amtsgericht: Erding



El Jan 14, 2009, a las 6:30 PM, Dustin Webb escribi?:

> You should be able to simply force the import.  Since your system  
> froze, it never properly "ejected" the pool.  The command is 'zpool  
> import -f' or 'zpool -f import'
>
> Don't Panic!
>
> Dustin Webb
> ITguy
> Erb Co Inc
>
>
> On Jan 5, 2009, at 4:53 PM, mathieu.email wrote:
>
>> Hi,
>>
>> I have a big problem with my ZFS disk. After a kernel panic, I  
>> cannot import the pool anymore :
>>
>> ------------------------------------------------------------------------------------------------------------------
>> => zpool status
>> no pools available
>> => zpool list
>> no pools available
>>
>> ------------------------------------------------------------------------------------------------------------------
>> => zpool import
>>
>> pool: ZFS
>>      id: 9004030332584099627
>> state: FAULTED
>> status: The pool metadata is corrupted.
>> action: The pool cannot be imported due to damaged devices or data.
>>            The pool may be active on on another system, but can be  
>> imported using
>>            the '-f' flag.
>>   see: http://www.sun.com/msg/ZFS-8000-72
>> config:
>>
>> 	ZFS            FAULTED  corrupted data
>> 	  disk2s2   ONLINE
>>
>> pool: ZFS
>>      id: 5050959592823553345
>> state: FAULTED
>> status: The pool was last accessed by another system.
>> action: The pool cannot be imported due to damaged devices or data.
>>   see: http://www.sun.com/msg/ZFS-8000-EY
>> config:
>>
>> 	ZFS         UNAVAIL  insufficient replicas
>> 	  disk1     UNAVAIL  cannot open
>>
>> ------------------------------------------------------------------------------------------------------------------
>> => zpool import -f 9004030332584099627
>> cannot import 'ZFS': I/O error
>> ------------------------------------------------------------------------------------------------------------------
>>
>> I am despaired as I have no backup and all my data are on this drive.
>>
>> Is there anything I can do ?
>>
>> Thank you for your help,
>>
>> Mathieu
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090115/6b66fb1f/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: Mensaje firmado digitalmente
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090115/6b66fb1f/attachment.bin>

From mathieu.email at gmail.com  Thu Jan 15 16:14:22 2009
From: mathieu.email at gmail.com (mathieu.email)
Date: Fri, 16 Jan 2009 01:14:22 +0100
Subject: [zfs-discuss] Zpool disaster
In-Reply-To: <1931595E-16E4-4D4D-934C-75E80E07DFF1@erbco.com>
References: <88975A88-D238-4FF1-ABD3-680C69FDAD92@gmail.com>
	<0EB4C19C-345D-4C15-98DE-A563DCDB8B70@erbco.com>
	<53A8B2DF-0EF6-4257-8663-9787C683EC17@gmail.com>
	<1931595E-16E4-4D4D-934C-75E80E07DFF1@erbco.com>
Message-ID: <84D32EFF-A924-452D-9EEF-B867ACCF87A3@gmail.com>

Well, "zpool status -x" just said :

=> no pools available

I can't get over it... I created this ZFS partition 2 weeks ago cause  
I thought it was very safe,
and now it seems that I have just lost everything, after a simple  
kernel panic...
I can't believe it...

Is it possible to do something like : rebuild metadata ? or scan a  
drive to recover data ?

Mathieu

Le 09-01-15 ? 23:36, Dustin Webb a ?crit :

> Did you try zpool status -x like this link says? http://www.sun.com/msg/ZFS-8000-72?
> I hope that it is not your problem.  Good luck.
>
> Dustin Webb
> ITguy
> Erb Co Inc
>
>
> On Jan 15, 2009, at 4:31 PM, mathieu.email wrote:
>
>> Thanks for your replies,
>>
>> I looked in the /var/log/system.log :
>>
>> - When I plug the drive (external usb drive), I have : "  
>> diskarbitrationd[32]: unable to mount /dev/disk2s2 (status code  
>> 0x00000001). "
>> - With the "zpool import" command, nothing appears.
>>
>> I changed the usb cable, but the problem is still the same (I don't  
>> have another power supply for the moment).
>>
>> I tried "zpool import -f", the error is the same :
>>
>> => zpool import -f
>>
>> pool: ZFS
>>      id: 9004030332584099627
>> state: FAULTED
>> status: The pool metadata is corrupted.
>> action: The pool cannot be imported due to damaged devices or data.
>>            The pool may be active on on another system, but can be  
>> imported using
>>            the '-f' flag.
>>   see: http://www.sun.com/msg/ZFS-8000-72
>> config:
>>
>> 	ZFS            FAULTED  corrupted data
>> 	 disk2s2   ONLINE
>>
>> pool: ZFS
>>      id: 5050959592823553345
>> state: FAULTED
>> status: The pool was last accessed by another system.
>> action: The pool cannot be imported due to damaged devices or data.
>>   see: http://www.sun.com/msg/ZFS-8000-EY
>> config:
>>
>> 	ZFS         UNAVAIL  insufficient replicas
>> 	 disk1     UNAVAIL  cannot open
>>
>>
>> Mathieu
>>
>> Le 09-01-15 ? 00:30, Dustin Webb a ?crit :
>>
>>> You should be able to simply force the import.  Since your system  
>>> froze, it never properly "ejected" the pool.  The command is  
>>> 'zpool import -f' or 'zpool -f import'
>>>
>>> Don't Panic!
>>>
>>> Dustin Webb
>>> ITguy
>>> Erb Co Inc
>>>
>>>
>>> On Jan 5, 2009, at 4:53 PM, mathieu.email wrote:
>>>
>>>> Hi,
>>>>
>>>> I have a big problem with my ZFS disk. After a kernel panic, I  
>>>> cannot import the pool anymore :
>>>>
>>>> ------------------------------------------------------------------------------------------------------------------
>>>> => zpool status
>>>> no pools available
>>>> => zpool list
>>>> no pools available
>>>>
>>>> ------------------------------------------------------------------------------------------------------------------
>>>> => zpool import
>>>>
>>>> pool: ZFS
>>>>      id: 9004030332584099627
>>>> state: FAULTED
>>>> status: The pool metadata is corrupted.
>>>> action: The pool cannot be imported due to damaged devices or data.
>>>>            The pool may be active on on another system, but can  
>>>> be imported using
>>>>            the '-f' flag.
>>>>   see: http://www.sun.com/msg/ZFS-8000-72
>>>> config:
>>>>
>>>> 	ZFS            FAULTED  corrupted data
>>>> 	  disk2s2   ONLINE
>>>>
>>>> pool: ZFS
>>>>      id: 5050959592823553345
>>>> state: FAULTED
>>>> status: The pool was last accessed by another system.
>>>> action: The pool cannot be imported due to damaged devices or data.
>>>>   see: http://www.sun.com/msg/ZFS-8000-EY
>>>> config:
>>>>
>>>> 	ZFS         UNAVAIL  insufficient replicas
>>>> 	  disk1     UNAVAIL  cannot open
>>>>
>>>> ------------------------------------------------------------------------------------------------------------------
>>>> => zpool import -f 9004030332584099627
>>>> cannot import 'ZFS': I/O error
>>>> ------------------------------------------------------------------------------------------------------------------
>>>>
>>>> I am despaired as I have no backup and all my data are on this  
>>>> drive.
>>>>
>>>> Is there anything I can do ?
>>>>
>>>> Thank you for your help,
>>>>
>>>> Mathieu
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090116/56953908/attachment-0001.html>

From ksh at ironsoftware.de  Thu Jan 15 19:50:43 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Thu, 15 Jan 2009 22:50:43 -0500
Subject: [zfs-discuss] i should get paid for this
Message-ID: <7000746B-BD53-4E37-AF74-DF81A3FF9194@ironsoftware.de>

that one happend as i was unmounting a clone. i was trying to recover  
some losses.
clone isnt that good in comparison to .snapshot.

n 15 22:23:00 2009
panic(cpu 1 caller 0x00BB2BD7): "[ZFS]: assertion failed in /Volumes/ 
pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/dnode_sync.c  
line 397: pass < 100"@/Volumes/pixie_dust/home/ndellofano/zfs-work/ 
zfs-119/zfs_kext/zfs/dnode_sync.c:397
Backtrace (CPU 1), Frame : Return Address (4 potential args on stack)
0x36717b28 : 0x12b4f3 (0x45b13c 0x36717b5c 0x1335e4 0x0)
0x36717b78 : 0xbb2bd7 (0xc1f4f4 0xc1ed40 0x18d 0xc1f4e8)
0x36717c98 : 0xbab65d (0x63bd708 0x0 0xc23710 0x0)
0x36717cd8 : 0xb88f9b (0x4f3bae0 0x0 0x0 0xf254ed2)
0x36717d28 : 0x1f1566 (0x63b9258 0x0 0x8d32e84 0x1d3bbc)
0x36717d58 : 0x1dfcb3 (0x63b9258 0x0 0x8d32e84 0x1f5e8e)
0x36717da8 : 0x1dff8c (0x63b9258 0x0 0x1 0x8d32e84)
0x36717dd8 : 0x1e0131 (0x63b9258 0x0 0x8d32e84 0x0)
0x36717f78 : 0x3df460 (0x482199c 0x8d32d80 0x8d32dc4 0x0)
0x36717fc8 : 0x1a0887 (0x6cb21a4 0x0 0x1a30b5 0x3f59128)
No mapping exists for frame pointer
Backtrace terminated-invalid frame pointer 0xbffffe48
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0xb87000->0xc52fff

BSD process name corresponding to current thread: umount

Mac OS version:
9G55

Kernel version:
Darwin Kernel Version 9.6.0: Mon Nov 24 17:37:00 PST 2008;  
root:xnu-1228.9.59~1/RELEASE_I386
System model name: MacBook1,1 (Mac-F4208CC8)




---
Christian Kendi
Iron Software
Gemeinschaftsstr. 2b
85435 Erding, Germany
mailto: ksh at ironsoftware.de
us mobile: +1-321-507-0653
mobile: +049 (0) 177 / 55 - 31 33 7
phone: +049 (0) 1801 / 6666 26 23 48
*****************************************
Geschaeftsfuehrer: Christian Kendi
Steuernr: 114/235/50572 * Amtsgericht: Erding



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090115/d79bba17/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: Mensaje firmado digitalmente
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090115/d79bba17/attachment.bin>

From dwebb at erbco.com  Thu Jan 15 14:36:27 2009
From: dwebb at erbco.com (Dustin Webb)
Date: Thu, 15 Jan 2009 17:36:27 -0500
Subject: [zfs-discuss] Zpool disaster
In-Reply-To: <53A8B2DF-0EF6-4257-8663-9787C683EC17@gmail.com>
References: <88975A88-D238-4FF1-ABD3-680C69FDAD92@gmail.com>
	<0EB4C19C-345D-4C15-98DE-A563DCDB8B70@erbco.com>
	<53A8B2DF-0EF6-4257-8663-9787C683EC17@gmail.com>
Message-ID: <1931595E-16E4-4D4D-934C-75E80E07DFF1@erbco.com>

Did you try zpool status -x like this link says? http://www.sun.com/msg/ZFS-8000-72?
I hope that it is not your problem.  Good luck.

Dustin Webb
ITguy
Erb Co Inc


On Jan 15, 2009, at 4:31 PM, mathieu.email wrote:

> Thanks for your replies,
>
> I looked in the /var/log/system.log :
>
> - When I plug the drive (external usb drive), I have : "  
> diskarbitrationd[32]: unable to mount /dev/disk2s2 (status code  
> 0x00000001). "
> - With the "zpool import" command, nothing appears.
>
> I changed the usb cable, but the problem is still the same (I don't  
> have another power supply for the moment).
>
> I tried "zpool import -f", the error is the same :
>
> => zpool import -f
>
> pool: ZFS
>      id: 9004030332584099627
> state: FAULTED
> status: The pool metadata is corrupted.
> action: The pool cannot be imported due to damaged devices or data.
>            The pool may be active on on another system, but can be  
> imported using
>            the '-f' flag.
>   see: http://www.sun.com/msg/ZFS-8000-72
> config:
>
> 	ZFS            FAULTED  corrupted data
> 	 disk2s2   ONLINE
>
> pool: ZFS
>      id: 5050959592823553345
> state: FAULTED
> status: The pool was last accessed by another system.
> action: The pool cannot be imported due to damaged devices or data.
>   see: http://www.sun.com/msg/ZFS-8000-EY
> config:
>
> 	ZFS         UNAVAIL  insufficient replicas
> 	 disk1     UNAVAIL  cannot open
>
>
> Mathieu
>
> Le 09-01-15 ? 00:30, Dustin Webb a ?crit :
>
>> You should be able to simply force the import.  Since your system  
>> froze, it never properly "ejected" the pool.  The command is 'zpool  
>> import -f' or 'zpool -f import'
>>
>> Don't Panic!
>>
>> Dustin Webb
>> ITguy
>> Erb Co Inc
>>
>>
>> On Jan 5, 2009, at 4:53 PM, mathieu.email wrote:
>>
>>> Hi,
>>>
>>> I have a big problem with my ZFS disk. After a kernel panic, I  
>>> cannot import the pool anymore :
>>>
>>> ------------------------------------------------------------------------------------------------------------------
>>> => zpool status
>>> no pools available
>>> => zpool list
>>> no pools available
>>>
>>> ------------------------------------------------------------------------------------------------------------------
>>> => zpool import
>>>
>>> pool: ZFS
>>>      id: 9004030332584099627
>>> state: FAULTED
>>> status: The pool metadata is corrupted.
>>> action: The pool cannot be imported due to damaged devices or data.
>>>            The pool may be active on on another system, but can be  
>>> imported using
>>>            the '-f' flag.
>>>   see: http://www.sun.com/msg/ZFS-8000-72
>>> config:
>>>
>>> 	ZFS            FAULTED  corrupted data
>>> 	  disk2s2   ONLINE
>>>
>>> pool: ZFS
>>>      id: 5050959592823553345
>>> state: FAULTED
>>> status: The pool was last accessed by another system.
>>> action: The pool cannot be imported due to damaged devices or data.
>>>   see: http://www.sun.com/msg/ZFS-8000-EY
>>> config:
>>>
>>> 	ZFS         UNAVAIL  insufficient replicas
>>> 	  disk1     UNAVAIL  cannot open
>>>
>>> ------------------------------------------------------------------------------------------------------------------
>>> => zpool import -f 9004030332584099627
>>> cannot import 'ZFS': I/O error
>>> ------------------------------------------------------------------------------------------------------------------
>>>
>>> I am despaired as I have no backup and all my data are on this  
>>> drive.
>>>
>>> Is there anything I can do ?
>>>
>>> Thank you for your help,
>>>
>>> Mathieu
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090115/566420fe/attachment-0001.html>

From james-zfsosx at jrv.org  Fri Jan 16 04:30:30 2009
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Fri, 16 Jan 2009 06:30:30 -0600
Subject: [zfs-discuss] Zpool disaster
In-Reply-To: <1931595E-16E4-4D4D-934C-75E80E07DFF1@erbco.com>
References: <88975A88-D238-4FF1-ABD3-680C69FDAD92@gmail.com>
	<0EB4C19C-345D-4C15-98DE-A563DCDB8B70@erbco.com>
	<53A8B2DF-0EF6-4257-8663-9787C683EC17@gmail.com>
	<1931595E-16E4-4D4D-934C-75E80E07DFF1@erbco.com>
Message-ID: <49707DE6.8070707@jrv.org>

Dustin Webb wrote:
> Did you try zpool status -x like this link
> says? http://www.sun.com/msg/ZFS-8000-72?
> I hope that it is not your problem.  Good luck.
>
> Dustin Webb
> ITguy
> Erb Co Inc

Even in this case it's not necessarily lost - there have been reports of
people getting around this by clobbering the most recent uberblock. 
Unfortunately I know of no tools to do this at all: you have to clobber
the right sectors on disk manually.

If this is the problem it's also disturbing since it would imply that
cache flushes through the USB layer may not be working right.  This is
the weak point of ZFS in my opinion: the strong dependence on
write-ordering around an uberblock update.

From james-zfsosx at jrv.org  Fri Jan 16 04:58:47 2009
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Fri, 16 Jan 2009 06:58:47 -0600
Subject: [zfs-discuss] Zpool disaster
In-Reply-To: <53A8B2DF-0EF6-4257-8663-9787C683EC17@gmail.com>
References: <88975A88-D238-4FF1-ABD3-680C69FDAD92@gmail.com>
	<0EB4C19C-345D-4C15-98DE-A563DCDB8B70@erbco.com>
	<53A8B2DF-0EF6-4257-8663-9787C683EC17@gmail.com>
Message-ID: <49708487.2020400@jrv.org>

mathieu.email wrote:
> Thanks for your replies,
>
> I looked in the /var/log/system.log :
>
> - When I plug the drive (external usb drive), I have : "
> diskarbitrationd[32]: unable to mount /dev/disk2s2 (status code
> 0x00000001). "
> - With the "zpool import" command, nothing appears.

The diskarbitration error is "normal".  Since there is no error logged
when ZFS reports an I/O error the ZFS message is probably bogus and the
disk is fine.

Because this happened after a panic I'm guessing it means USB didn't do
cache flushes correctly and the uberblock on the disk point to blocks
that never got written.  There's a good chance that the uberblock from
the previous transaction group will work, but getting ZFS to try to use
that previous uberblock requires either modifying the ZFS code or
clobbering the uberblock that ZFS is trying to use.  You can look at the
Sun/Solaris lists and see if anyone has written a tool to do this.

The data is very likely still on your disk - don't erase it!  The
problem is how to get to the data.

From ksh at ironsoftware.de  Fri Jan 16 13:22:14 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Fri, 16 Jan 2009 16:22:14 -0500
Subject: [zfs-discuss] Zpool disaster
In-Reply-To: <49708487.2020400@jrv.org>
References: <88975A88-D238-4FF1-ABD3-680C69FDAD92@gmail.com>
	<0EB4C19C-345D-4C15-98DE-A563DCDB8B70@erbco.com>
	<53A8B2DF-0EF6-4257-8663-9787C683EC17@gmail.com>
	<49708487.2020400@jrv.org>
Message-ID: <5B7C083B-12A5-4D85-9A26-3FEE1E7AB468@ironsoftware.de>

well, invite me to the club.

As i stated before i was "scared" this could happen to me as well.
Resilvering was ongoing but didn't finish over the night (100gb on  
USB2.0), still stuck at 0.59% !!
However, the system stuck while the USB drive dissappeard. Reboot and  
the same result for me.
I/O error in import. Solaris wasnt able to import as well. The  
statement was that some crucial metadata
was damaged.

The unfinished resilvering disk coulnd help to restore. I keep you  
posted on this. I will continue to test
poor-mans backup solution and keep a "real" backup.

I wonder what i could have done else instead of creating a new pool.  
Even sun says if the metadata is
corrupted i should restore from a backup.

Good luck anyways.

---
Christian Kendi
Iron Software
Gemeinschaftsstr. 2b
85435 Erding, Germany
mailto: ksh at ironsoftware.de
us mobile: +1-321-507-0653
mobile: +049 (0) 177 / 55 - 31 33 7
phone: +049 (0) 1801 / 6666 26 23 48
*****************************************
Geschaeftsfuehrer: Christian Kendi
Steuernr: 114/235/50572 * Amtsgericht: Erding



El Jan 16, 2009, a las 7:58 AM, James R. Van Artsdalen escribi?:

> mathieu.email wrote:
>> Thanks for your replies,
>>
>> I looked in the /var/log/system.log :
>>
>> - When I plug the drive (external usb drive), I have : "
>> diskarbitrationd[32]: unable to mount /dev/disk2s2 (status code
>> 0x00000001). "
>> - With the "zpool import" command, nothing appears.
>
> The diskarbitration error is "normal".  Since there is no error logged
> when ZFS reports an I/O error the ZFS message is probably bogus and  
> the
> disk is fine.
>
> Because this happened after a panic I'm guessing it means USB didn't  
> do
> cache flushes correctly and the uberblock on the disk point to blocks
> that never got written.  There's a good chance that the uberblock from
> the previous transaction group will work, but getting ZFS to try to  
> use
> that previous uberblock requires either modifying the ZFS code or
> clobbering the uberblock that ZFS is trying to use.  You can look at  
> the
> Sun/Solaris lists and see if anyone has written a tool to do this.
>
> The data is very likely still on your disk - don't erase it!  The
> problem is how to get to the data.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090116/a6a9784a/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: Mensaje firmado digitalmente
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090116/a6a9784a/attachment.bin>

From alex.blewitt at gmail.com  Fri Jan 16 14:32:52 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Fri, 16 Jan 2009 22:32:52 +0000
Subject: [zfs-discuss] Zpool disaster
In-Reply-To: <49707DE6.8070707@jrv.org>
References: <88975A88-D238-4FF1-ABD3-680C69FDAD92@gmail.com>
	<0EB4C19C-345D-4C15-98DE-A563DCDB8B70@erbco.com>
	<53A8B2DF-0EF6-4257-8663-9787C683EC17@gmail.com>
	<1931595E-16E4-4D4D-934C-75E80E07DFF1@erbco.com>
	<49707DE6.8070707@jrv.org>
Message-ID: <F20E166A-1F86-4919-A2C7-3E92542AD49E@gmail.com>

Please, don't use USB disks if you value your data. Get a decent  
FireWire enclosure instead, which outperforms USB as well.

I wrote a blog entry on the fragility of USB drives on Leopard a while  
ago, but sadly most people think that 480>400 implies USB is faster -  
and with Apple's apparent abandonment of FireWire that it is somehow a  
legacy technology.

None of this helps what happens when the disks do fail, but bear in  
mind that it's not necessarily ZFS's fault - in fact, it has detected  
a failure earlier on than perhaps other file systems would and perhaps  
still has the data on it.

I'd advise going out and buying a set of disks and doing a dd copy to  
make a backup of the data before trying to mount it again. It should  
be possible to see if there is any recoverable data by looking at the  
disk's uberblocks, but you would need to write some code rather than a  
utility which will do it automatically. You might also find that  
booting up a copy of opensolaris with a more recent version of ZFS to  
see if it will work.

Alex

Sent from my (new) iPhone

On 16 Jan 2009, at 12:30, "James R. Van Artsdalen" <james- 
zfsosx at jrv.org> wrote:

> Dustin Webb wrote:
>> Did you try zpool status -x like this link
>> says? http://www.sun.com/msg/ZFS-8000-72?
>> I hope that it is not your problem.  Good luck.
>>
>> Dustin Webb
>> ITguy
>> Erb Co Inc
>
> Even in this case it's not necessarily lost - there have been  
> reports of
> people getting around this by clobbering the most recent uberblock.
> Unfortunately I know of no tools to do this at all: you have to  
> clobber
> the right sectors on disk manually.
>
> If this is the problem it's also disturbing since it would imply that
> cache flushes through the USB layer may not be working right.  This is
> the weak point of ZFS in my opinion: the strong dependence on
> write-ordering around an uberblock update.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From ksh at ironsoftware.de  Fri Jan 16 17:34:43 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Fri, 16 Jan 2009 20:34:43 -0500
Subject: [zfs-discuss] plastic surgery
Message-ID: <5E9696CB-D104-44E6-8648-3C854F81FB41@ironsoftware.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi together,

a short update on poor-mans backup solution. A resilver takes about  
5h. Even when
i just changed a single byte.

Back to the surgery:
When i created my home directory in home/ksh i also created home/ksh/ 
Movies and home/ksh/Documents and so on.

Once i import the pool the subdirectories will be automatically  
mounted, which is fine.

However, in Finder they are just named "home" and marked as a  
removeable device.
Is there  a way i can make the mounts appear solid, with their actual  
name and an icon ?

Greets,
Chris.

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (Darwin)

iD8DBQFJcTW0p+9ff145KVIRAh+CAJ9S0lX6+QXW2rP0ORXV1hmawfLucACfd55f
FQW3D8BGVzjvu8J1pZMrbDk=
=R8Jt
-----END PGP SIGNATURE-----

From dirkschelfhout at mac.com  Fri Jan 16 18:53:38 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Sat, 17 Jan 2009 03:53:38 +0100
Subject: [zfs-discuss] deleted directories and files
Message-ID: <1671B563-344C-455F-B1D7-E25DA23308C4@mac.com>

Stupidly ran an rsync with --delete-after to the root of my zfs
with the wrong syntax. So the rsync started to delete all folders in
my root directory on zfs that did not belong to the copy.
I stopped it but still lost a lot of files.
I recently removed all my snapshots to free up space.
Is there any way I can get those files back ?

Dirk

From ksh at ironsoftware.de  Sat Jan 17 08:52:01 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Sat, 17 Jan 2009 11:52:01 -0500
Subject: [zfs-discuss] panic() zfs send
Message-ID: <9154D65D-BFD1-41ED-A31D-5B44A409EDBC@ironsoftware.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

this one was with zfs send > fifo ; zfs recv < fifo.

Sat Jan 17 11:37:23 2009
panic(cpu 0 caller 0x0019F4EF): "trying to interlock destroyed mutex  
0xd3baac4"@/SourceCache/xnu/xnu-1228.9.59/osfmk/i386/locks_i386.c:1760
Backtrace (CPU 0), Frame : Return Address (4 potential args on stack)
0x34283e08 : 0x12b4f3 (0x45b13c 0x34283e3c 0x1335e4 0x0)
0x34283e58 : 0x19f4ef (0x462eb0 0xd3baac4 0xd3baac4 0x53a27250)
0x34283e78 : 0x19c568 (0xd3baac4 0x206 0x34283ea8 0x4edc010b)
0x34283e88 : 0x4edc010b (0xd3baac4 0x8da842d6 0x34283ea8 0x4edc018a)
0x34283ea8 : 0x4ed83ce0 (0xd3baac4 0x34283ec8 0x19c584 0x53a2728c)
0x34283ee8 : 0x4ed85938 (0x53a2728c 0x5310000 0x34283f28 0xb4c6ee8)
0x34283f08 : 0x4ed7f898 (0x8cba930 0xb4c6ee8 0xede0 0x61544)
0x34283f28 : 0x4ed7fe0b (0x4ee29948 0x4ee29948 0x5ceba7 0x4edef8a5)
0x34283fc8 : 0x1a017c (0x0 0x0 0x1a30b5 0x4a705d0)
Backtrace terminated-invalid frame pointer 0
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0x4ed65000->0x4ee30fff

BSD process name corresponding to current thread: kernel_task

Mac OS version:
9G55

Kernel version:
Darwin Kernel Version 9.6.0: Mon Nov 24 17:37:00 PST 2008;  
root:xnu-1228.9.59~1/RELEASE_I386
System model name: MacBook1,1 (Mac-F4208CC8)
  
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (Darwin)

iD8DBQFJcgyyp+9ff145KVIRAppZAJ0f2mFoIv/GX2qn1p8+I2YSi2gcSgCffohN
Phc59Uk1AVO9CKYziWGpVF0=
=iq3j
-----END PGP SIGNATURE-----

From darwinskernel at gmail.com  Sat Jan 17 08:52:36 2009
From: darwinskernel at gmail.com (Charles Darwin)
Date: Sat, 17 Jan 2009 11:52:36 -0500
Subject: [zfs-discuss] Applications/
Message-ID: <7FEB6D36-6A67-4147-8AD6-0C451ABAF1B7@gmail.com>

I know this was discussed here but I can't find it on the list so here  
I go:

My HOME is already in zfs now how do I move `Applications/' directory  
to zfs?

Thanks

From alex.blewitt at gmail.com  Sat Jan 17 09:59:23 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Sat, 17 Jan 2009 17:59:23 +0000
Subject: [zfs-discuss] Applications/
In-Reply-To: <7FEB6D36-6A67-4147-8AD6-0C451ABAF1B7@gmail.com>
References: <7FEB6D36-6A67-4147-8AD6-0C451ABAF1B7@gmail.com>
Message-ID: <F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>

Easiest thing would be:

Create a filesystem (say, Applications) on your pool
Copy your stuff over to Pool/Applications
Rename the old /Applications dir
Set mountpoint property of pool/Applications to be /Applications

Eg

zfs create pool/Applications
rsync -cav /Applications/* /pool/Applications
mv /Applications /Applications.bak
zfs set mountpoint pool/Applications /Applications

Should be good to go. If it isn't there, try a zfs unmount/mount combo

Sent from my (new) iPhone

On 17 Jan 2009, at 16:52, Charles Darwin <darwinskernel at gmail.com>  
wrote:

> I know this was discussed here but I can't find it on the list so  
> here I go:
>
> My HOME is already in zfs now how do I move `Applications/'  
> directory to zfs?
>
> Thanks
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From darwinskernel at gmail.com  Sat Jan 17 10:27:43 2009
From: darwinskernel at gmail.com (Charles Darwin)
Date: Sat, 17 Jan 2009 13:27:43 -0500
Subject: [zfs-discuss] Applications/
In-Reply-To: <F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>
References: <7FEB6D36-6A67-4147-8AD6-0C451ABAF1B7@gmail.com>
	<F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>
Message-ID: <C2D5FE7C-3FF9-44A0-8B07-BC03777BACDA@gmail.com>


On 17-Jan-09, at 12:59 PM, Alex Blewitt wrote:

> Set mountpoint property of pool/Applications to be /Applications

How do I do that?

Charles

From darwinskernel at gmail.com  Sat Jan 17 10:32:16 2009
From: darwinskernel at gmail.com (Charles Darwin)
Date: Sat, 17 Jan 2009 13:32:16 -0500
Subject: [zfs-discuss] Applications/
In-Reply-To: <F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>
References: <7FEB6D36-6A67-4147-8AD6-0C451ABAF1B7@gmail.com>
	<F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>
Message-ID: <12F87672-C457-45D6-8949-B8E64E619E7E@gmail.com>


On 17-Jan-09, at 12:59 PM, Alex Blewitt wrote:

> Copy your stuff over to Pool/Applications

What about the ownership-permissions? Would this work?

sudo rsync -avz /Applications/ /Volumes/pool/Applications/


From darwinskernel at gmail.com  Sat Jan 17 10:38:54 2009
From: darwinskernel at gmail.com (Charles Darwin)
Date: Sat, 17 Jan 2009 13:38:54 -0500
Subject: [zfs-discuss] Applications/
In-Reply-To: <F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>
References: <7FEB6D36-6A67-4147-8AD6-0C451ABAF1B7@gmail.com>
	<F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>
Message-ID: <207A8F77-5DA0-4B33-802D-592CB2727B43@gmail.com>


On 17-Jan-09, at 12:59 PM, Alex Blewitt wrote:

> Copy your stuff over to Pool/Applications

What about the ownership-permissions? Would this work?

sudo rsync -avz /Applications/ /Volumes/pool/Applications/

O.K.:

sudo rsync -avzr /Applications/ /Volumes/pool/Applications/

From johnemac at tekserve.com  Sat Jan 17 10:51:14 2009
From: johnemac at tekserve.com (John McAdams)
Date: Sat, 17 Jan 2009 13:51:14 -0500
Subject: [zfs-discuss] Applications/
In-Reply-To: <207A8F77-5DA0-4B33-802D-592CB2727B43@gmail.com>
References: <7FEB6D36-6A67-4147-8AD6-0C451ABAF1B7@gmail.com>
	<F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>
	<207A8F77-5DA0-4B33-802D-592CB2727B43@gmail.com>
Message-ID: <40AF76D4-8403-4DBD-8896-7E63501AF136@tekserve.com>

Bombich has a good article on rsync. He uses some patches to make it  
work better with OS X issues. I've been using this to backup to a zfs  
pool without any problems.

<http://www.bombich.com/mactips/rsync.html>

-- jmca

On Jan 17, 2009, at 1:38 PM, Charles Darwin wrote:

>
> On 17-Jan-09, at 12:59 PM, Alex Blewitt wrote:
>
>> Copy your stuff over to Pool/Applications
>
> What about the ownership-permissions? Would this work?
>
> sudo rsync -avz /Applications/ /Volumes/pool/Applications/
>
> O.K.:
>
> sudo rsync -avzr /Applications/ /Volumes/pool/Applications/
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From darwinskernel at gmail.com  Sat Jan 17 10:54:17 2009
From: darwinskernel at gmail.com (Charles Darwin)
Date: Sat, 17 Jan 2009 13:54:17 -0500
Subject: [zfs-discuss] Applications/
In-Reply-To: <F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>
References: <7FEB6D36-6A67-4147-8AD6-0C451ABAF1B7@gmail.com>
	<F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>
Message-ID: <97712203-016B-41C1-83F6-7483BC2E4421@gmail.com>


On 17-Jan-09, at 12:59 PM, Alex Blewitt wrote:

> Eg
>
> zfs create pool/Applications
> rsync -cav /Applications/* /pool/Applications
> mv /Applications /Applications.bak
> zfs set mountpoint pool/Applications /Applications

Oops!! didn't see this part, I thought Eg was your signature:) Now  
it's syncing the two dirs. I'm logged in as root `sudo -i` to keep the  
ownership-permissions the same.

From darwinskernel at gmail.com  Sat Jan 17 10:59:19 2009
From: darwinskernel at gmail.com (Charles Darwin)
Date: Sat, 17 Jan 2009 13:59:19 -0500
Subject: [zfs-discuss] Applications/
In-Reply-To: <F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>
References: <7FEB6D36-6A67-4147-8AD6-0C451ABAF1B7@gmail.com>
	<F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>
Message-ID: <FE3F3BC4-C0F8-4450-A22F-1810D38BB2D9@gmail.com>


On 17-Jan-09, at 12:59 PM, Alex Blewitt wrote:

> zfs set mountpoint pool/Applications /Applications

-sh-3.2# zfs set mountpoint pool/Applications /Applications
missing value in property=value argument
-sh-3.2# zfs set mountpoint /Volumes/neo/Applications /Applications
missing value in property=value argument
-sh-3.2# zfs set mountpoint=/Volumes/pool/Applications /Applications
cannot open '/Applications': invalid dataset name
-sh-3.2# zfs set mountpoint=pool/Applications /Applications
cannot open '/Applications': invalid dataset name


From darwinskernel at gmail.com  Sat Jan 17 11:01:28 2009
From: darwinskernel at gmail.com (Charles Darwin)
Date: Sat, 17 Jan 2009 14:01:28 -0500
Subject: [zfs-discuss] Applications/
In-Reply-To: <F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>
References: <7FEB6D36-6A67-4147-8AD6-0C451ABAF1B7@gmail.com>
	<F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>
Message-ID: <BEAE6126-8B10-432D-B3D0-18000D5523C6@gmail.com>


On 17-Jan-09, at 12:59 PM, Alex Blewitt wrote:

> zfs set mountpoint pool/Applications /Applications

I think you meant:

zfs set mountpoint=/Applications pool/Applications

From ksh at ironsoftware.de  Sat Jan 17 11:02:05 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Sat, 17 Jan 2009 14:02:05 -0500
Subject: [zfs-discuss] Applications/
In-Reply-To: <FE3F3BC4-C0F8-4450-A22F-1810D38BB2D9@gmail.com>
References: <7FEB6D36-6A67-4147-8AD6-0C451ABAF1B7@gmail.com>
	<F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>
	<FE3F3BC4-C0F8-4450-A22F-1810D38BB2D9@gmail.com>
Message-ID: <6F9E0AAF-32A4-4415-B12E-C62B9E1B58B0@ironsoftware.de>

common. read some docs first.

unmount pool/Applications first.
diskutil unmount ...
zfs set mountpoint=/Applications pool/Applications

if you pool is named "pool".

El Jan 17, 2009, a las 1:59 PM, Charles Darwin escribi?:

>
> On 17-Jan-09, at 12:59 PM, Alex Blewitt wrote:
>
>> zfs set mountpoint pool/Applications /Applications
>
> -sh-3.2# zfs set mountpoint pool/Applications /Applications
> missing value in property=value argument
> -sh-3.2# zfs set mountpoint /Volumes/neo/Applications /Applications
> missing value in property=value argument
> -sh-3.2# zfs set mountpoint=/Volumes/pool/Applications /Applications
> cannot open '/Applications': invalid dataset name
> -sh-3.2# zfs set mountpoint=pool/Applications /Applications
> cannot open '/Applications': invalid dataset name
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090117/38d0bebf/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: Mensaje firmado digitalmente
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090117/38d0bebf/attachment.bin>

From darwinskernel at gmail.com  Sat Jan 17 11:15:17 2009
From: darwinskernel at gmail.com (Charles Darwin)
Date: Sat, 17 Jan 2009 14:15:17 -0500
Subject: [zfs-discuss] Applications/
In-Reply-To: <6F9E0AAF-32A4-4415-B12E-C62B9E1B58B0@ironsoftware.de>
References: <7FEB6D36-6A67-4147-8AD6-0C451ABAF1B7@gmail.com>
	<F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>
	<FE3F3BC4-C0F8-4450-A22F-1810D38BB2D9@gmail.com>
	<6F9E0AAF-32A4-4415-B12E-C62B9E1B58B0@ironsoftware.de>
Message-ID: <0DD5E515-226A-4008-A7B2-55E8054C7A6D@gmail.com>


On 17-Jan-09, at 2:02 PM, Christian Kendi wrote:

> common. read some docs first.
>
>
> unmount pool/Applications first.
> diskutil unmount ...
> zfs set mountpoint=/Applications pool/Applications

common. read carefully first. Alex had it backwards; compare these two:

zfs set mountpoint pool/Applications /Applications
zfs set mountpoint=/Applications pool/Applications

From zorg at sogeeky.net  Sat Jan 17 11:23:37 2009
From: zorg at sogeeky.net (Mr. Zorg)
Date: Sat, 17 Jan 2009 11:23:37 -0800
Subject: [zfs-discuss] Applications/
In-Reply-To: <0DD5E515-226A-4008-A7B2-55E8054C7A6D@gmail.com>
References: <7FEB6D36-6A67-4147-8AD6-0C451ABAF1B7@gmail.com>
	<F667D1FE-BB30-4053-9527-6508A4B7A2C3@gmail.com>
	<FE3F3BC4-C0F8-4450-A22F-1810D38BB2D9@gmail.com>
	<6F9E0AAF-32A4-4415-B12E-C62B9E1B58B0@ironsoftware.de>
	<0DD5E515-226A-4008-A7B2-55E8054C7A6D@gmail.com>
Message-ID: <093FE724-B187-4BE1-B95E-52749D865DA0@sogeeky.net>

Um, no. Check the zfs man pages. They clearly state the syntax is "zfs  
set property=value filesystem".  Go to the command line and type "man  
zfs" to see for yourself. :)

On Jan 17, 2009, at 11:15 AM, Charles Darwin <darwinskernel at gmail.com>  
wrote:

>
> On 17-Jan-09, at 2:02 PM, Christian Kendi wrote:
>
>> common. read some docs first.
>>
>>
>> unmount pool/Applications first.
>> diskutil unmount ...
>> zfs set mountpoint=/Applications pool/Applications
>
> common. read carefully first. Alex had it backwards; compare these  
> two:
>
> zfs set mountpoint pool/Applications /Applications
> zfs set mountpoint=/Applications pool/Applications
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From jliechty200 at gmail.com  Sat Jan 17 21:35:07 2009
From: jliechty200 at gmail.com (Josh Liechty)
Date: Sun, 18 Jan 2009 00:35:07 -0500
Subject: [zfs-discuss] iTunes can't see within ZFS filesystems
Message-ID: <3AEAE648-A1B4-42AC-8487-B7782375C163@gmail.com>

It seems that I've encountered an issue with how iTunes works with  
files on a ZFS filesystem within a pool.  As we know, when a  
filesystem is created, it shows up in the Leopard finder as an alias.   
I thought this just related to how the filesystem is displayed, so it  
shouldn't have been an issue.  Anyway, say we do the following on a  
pool called "Data":

zfs create Data/Music
mkdir /Volumes/Data/Music/iTunes
Choose iTunes > Preferences > Advanced > iTunes music folder location,  
attempt to set to "Data:Music:iTunes".

In this case, iTunes wants to put its library on "Data:iTunes",  
resulting in vast quantities of fail.  Will I have to abstain from the  
use of filesystems for organization (one of the best things about ZFS  
apart from data integrity) until Snow Leopard is released, or is there  
a way to correct iTunes' flawed view of filesystem structure now?
--
Josh Liechty

From ksh at ironsoftware.de  Sun Jan 18 10:53:28 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Sun, 18 Jan 2009 13:53:28 -0500
Subject: [zfs-discuss] something new :) panic()
Message-ID: <2CE701BE-370F-40A6-B87C-DBBEF293A069@ironsoftware.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

some bug fixes would be really appreciated. If you can provide the  
diffs i would
compile the kext on my own.

Sun Jan 18 13:42:15 2009
panic(cpu 0 caller 0x00B9ED21): "ZFS: I/O failure (write on <unknown>  
off 0: zio 0x5983de0 [L0 ZFS plain file] 20000L/20000P  
DVA[0]=<0:1513b00000:20000> fletcher2 uncompressed LE contiguous  
birth=42051 fill=1 cksum=9dd117c699ec055f: 
4f45bcd872254a4f:f3663b2cb9e51a6a:4312a699444f6ad2): error " "5"@/ 
Volumes/pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/zio.c: 
918
Backtrace (CPU 0), Frame : Return Address (4 potential args on stack)
0x36c47e38 : 0x12b4f3 (0x45b13c 0x36c47e6c 0x1335e4 0x0)
0x36c47e88 : 0xb9ed21 (0xc0e6b0 0xc0e6a4 0xc0a400 0xc3b670)
0x36c47f08 : 0xb9b3fb (0x5983de0 0x1332 0x36c47f28 0xbe218a)
0x36c47f48 : 0xbfa4a6 (0x5983de0 0x50fd080 0x20c4ab 0xc235f4)
0x36c47fc8 : 0x1a017c (0x47d5470 0x0 0x1a30b5 0x812ea78)
Backtrace terminated-invalid frame pointer 0
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0xb87000->0xc52fff

BSD process name corresponding to current thread: kernel_task

Mac OS version:
9G55

Kernel version:
Darwin Kernel Version 9.6.0: Mon Nov 24 17:37:00 PST 2008;  
root:xnu-1228.9.59~1/RELEASE_I386
System model name: MacBook1,1 (Mac-F4208CC8)

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (Darwin)

iD8DBQFJc3qpp+9ff145KVIRApj4AJ0ckTYtUvVpcy/IMTHvXHh0Kd8T1ACaAtFG
rUjCIPEevo5Qa3/lSlT5JKU=
=xU/F
-----END PGP SIGNATURE-----

From i_see at macnews.de  Sun Jan 18 15:03:34 2009
From: i_see at macnews.de (Ralf Bertling)
Date: Mon, 19 Jan 2009 00:03:34 +0100
Subject: [zfs-discuss] zfs /GUID-partition table Recovery tools
In-Reply-To: <mailman.3325.1232256913.184.zfs-discuss@lists.macosforge.org>
References: <mailman.3325.1232256913.184.zfs-discuss@lists.macosforge.org>
Message-ID: <42FF4881-403D-4400-841A-7CB90407A7C7@macnews.de>

Hi list,
I use an Mac-mini and a couple of USB-enclosures for my data-storage  
needs.
due to cabling problems, (and due to the fact I don't want a couple of  
humming HDs in my living-room) I can currently only use USB for my  
data transfer.

I don't mind (very much) that the transfer speed is not too high, i  
just need a cheap and reliable solution. (Cheap should also apply to  
the cost of electricity.)

Until recently this has worked reasonably well, but somehow the  
combination of a particular enclosure with my relatively new WD 1GB  
GreenLine disk has produced a number of problems.

ZFS is generally quite robust in a mirrored configuration, but leaves  
one in the complete cold when it cannot make sense out of the main  
ueberblocks (or disk labels). As I have understood, it should be  
relatively easy to write a tool that gets date from the latest  
snapshot if the main ueberblock gets damaged afterwards, but to my  
knowledge such a tool has not been written yet (That could be zfs list/ 
send applied to a non-importable pool.)

Another odd problem I have is a damaged partition map on my WD drive.
> bertlinge:~ Susalf$ ls -l /dev/disk2*
> brw-r-----  1 Susalf  operator   14,   6 15 Jan 22:00 /dev/disk2
> brw-r-----  1 Susalf  operator   14,   7 15 Jan 22:00 /dev/disk2s1
> brw-r-----  1 Susalf  operator   14,   8 15 Jan 22:00 /dev/disk2s2
> brw-r-----  1 Susalf  operator   14,  10 15 Jan 22:00 /dev/disk2s3
> brw-r-----  1 Susalf  operator   14,  11 15 Jan 22:00 /dev/disk2s4
> brw-r-----  1 Susalf  operator   14,   9 15 Jan 22:00 /dev/disk2s5
> bertlinge:~ Susalf$ zpool status
>   pool: Katz
>  state: ONLINE
> status: One or more devices has experienced an unrecoverable error.   
> An
> 	attempt was made to correct the error.  Applications are unaffected.
> action: Determine if the device needs to be replaced, and clear the  
> errors
> 	using 'zpool clear' or replace the device with 'zpool replace'.
>    see: http://www.sun.com/msg/ZFS-8000-9P
>  scrub: scrub completed with 0 errors on Fri Jan 16 04:08:19 2009
> config:
>
> 	NAME         STATE     READ WRITE CKSUM
> 	Katz         ONLINE       0     0     0
> 	  mirror     ONLINE       0     0     0
> 	    disk2s5  ONLINE       0     0     1
> 	    disk1s2  ONLINE       0     0     0
>
> errors: No known data errors
> bertlinge:~ Susalf$ diskutil list /dev/disk2
> /dev/disk2
>    #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>    0:      GUID_partition_scheme                        *931.5 Gi    
> disk2
>    1:                        EFI                         200.0 Mi    
> disk2s1
>    2:                        ZFS                         120.0 Gi    
> disk2s2
> bertlinge:~ Susalf$
Somehow diskutil does not recognize most of my partitions while ZFS is  
merrily accessing at least one of my "invisible" partitions.
The point is, I would really like to get some date from an HFS+  
partition on that drive and from another damaged ZFS pool that is  
currently non-importable.
Now it looks a bit like all the redundant blocks in ZFS are of little  
help when you can still have a single point of failure in your  
partition map.

 From what I had read, I suspected the Mac port would use diskutil to  
search for ZFS partitions, but this seems to be not entirely correct.
1. Is there a recommended way to try mounting and/or repairing the  
partitions.
2. What data would need to save do some external non-volatile storage  
to be able to repair the partition-map on a GUID-disk
3. Whats the easiest way to re-import get the data from a broken ZFS- 
Volume.
4. Are there any firmware patches or settings that force testing the  
data written to a USB-device has actually been written to the disk? (I  
mean, no matter how superior FireWire is, its just binary data, so its  
either there or not. It should be possible to get it where it belongs,  
even if its slower than it could/should be).
5. Since some enclosures work very well, can anyone point me to the  
right ones or are there good test utilities, so I can be reasonably  
sure before I move my data onto them?
6. (Probably this list is the wrong place to ask this). I've got some  
old server hardware and want to build a file-/backup server on it,  
preferably deploying ZFS.
Obviously, I want to access it from a Mac, other potential users will  
likely use Windows or Linux. Can anyone point me to good how-tos or  
recommendations  on disk-controllers and solaris vs. Linux vs. FreeBsd  
vs. Darwin as a Server OS. (This really has to be cheap, so I cannot  
make investments on Hard- or Software.

Thanks,	
	ralf

From ksh at ironsoftware.de  Sun Jan 18 15:06:48 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Sun, 18 Jan 2009 18:06:48 -0500
Subject: [zfs-discuss] bug report, MS Word & ZFS.
Message-ID: <5311BF45-3602-4C66-96E4-C94864DA5D23@ironsoftware.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi,

this time im not sure if this is related to ZFS or Microsoft Word. I  
assume ZFS because on HFS+ it was fine.

Im writing a doc in Documents/path1/path2/... and so on. Documents is  
an own ZFS fs which is regularly snapshotted.
So if I open a Document within this path1/path2/..., do a change, save  
the Document and compare the filedesc. they do not match any more.
before:
Microsoft 549  ksh   20u     REG      46,6   7545994    16600 /Users/ 
ksh/Documents/.../.../.../Document/fancy_title.docx
after:
Microsoft 549  ksh   46u     REG      46,6   7545329    16394 /Users/ 
ksh/Documents/fancy_title.docx

It helps to save the Document by hand back to the origin. Further  
changes will then be saved to the correct path.
Can this be related to ZFS and some internals apon reopening a new  
filedesc?
It looks to me MS is closing the old fdesc and opening a new one.

Greets
Chris.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (Darwin)

iD8DBQFJc7YJp+9ff145KVIRAqLfAJ9zR/2oxHnBNPn4g/bHIySMjHbbCwCggufD
5sEAo2QphOWhsp58g6rx4GQ=
=ynZt
-----END PGP SIGNATURE-----

From mailinglists at MailNewsRSS.com  Sun Jan 18 19:53:33 2009
From: mailinglists at MailNewsRSS.com (Jason Todd Slack-Moehrle)
Date: Sun, 18 Jan 2009 19:53:33 -0800
Subject: [zfs-discuss] Changing from ZFS to HFS+
Message-ID: <C599393D.259B4%mailinglists@MailNewsRSS.com>

Hi All,

Since switching to ZFS I get a lot of ?beach balls?. I think for
productivity sake I should switch back to HFS+.  My home directory was on
this ZFS partition.

I backed up my data to another drive and tried using Disk Utility to select
my ZFS partition, un-mount it and format just that partition HFS+. This
failed. I manually unmounted my ZFS Partition using zfs unmount and it
unmounted but Disk Utility still gives an error.

Can I do this or am I looking at re-partitioning the whole drive and
starting from scratch with Leopard, BootCamp partition and an HFS+ partition
for my data?

Or, is a new release coming out that might relieve me of some of these
issues?

Thanks,
-Jason

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090118/62900654/attachment.html>

From lopez.on.the.lists at yellowspace.net  Mon Jan 19 02:43:14 2009
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Mon, 19 Jan 2009 11:43:14 +0100
Subject: [zfs-discuss] Question about ZFS status
In-Reply-To: <636fd28e0901141623h4ce5ef8fv6bfedfb1863f3091@mail.gmail.com>
References: <97880A58-AF3A-485E-A626-8141C4B065A8@me.com>
	<636fd28e0901141623h4ce5ef8fv6bfedfb1863f3091@mail.gmail.com>
Message-ID: <D99D8129-D130-4FF1-B84C-098CD83C6196@yellowspace.net>


On 15.01.2009, at 01:23, Alex Blewitt wrote:

> On Wed, Dec 31, 2008 at 2:14 PM, Patrick Mau <pmau at me.com> wrote:
>> First, I'm not yet subscribed to the list, please include my  
>> address in the
>> reply, thanks a lot.
>>
>> I've recently started using ZFS (zfs-119) on my MacPro. It all  
>> worked fine,
>> ignoring all the open issues that are still work in progress.
>> Looking at the source makes me wonder if there will be additionall  
>> releases
>> before the inclusion in MacOS 10.6.
>
> I seem to recall a recent conversation being 'no', on the grounds that
> the ZFS IOKit integration required some kernel changes, which will
> only be in 10.6. So whilst there might be a community effort in
> merging, I don't think it's going to be officially supported.

Maybe I'm a little bit naiive - but I do actually await a
leopard (10.5.x) merge spearheaded by the Apple team, if ZFS on
the mac is to be adopted widely (be it only in the server arena
at first) once SL is released. I'd like to know that I don't have
to update every single client just to be able to access zfs media
in the future. I can understand that there will be no support for
Tiger - but even taking into account a few drawbacks, I'd like to
see zfs debugged working with Leopard. Even if that means waiting
for 10.5.7, 10.5.8 or something to have the most important
changes done in the kernel. Well, OK, maybe I'm just dreaming, but
I started using these feeds in the hope that there will be
ongoing development (which was the case until ... now).
Not merging any of the good work done now is not really the
behavior of a "first class citizen" in the open source community...

Regards,


Lorenzo

From ksh at ironsoftware.de  Mon Jan 19 18:46:03 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Mon, 19 Jan 2009 21:46:03 -0500
Subject: [zfs-discuss] strange behaviour
Message-ID: <28398AB5-3BAC-4B0A-97D4-E5EE8A9BFEF9@ironsoftware.de>

Hi,

i had this a couple of times before. Not often but sometimes it  
happens that directories or files become inaccessible.
When listing directory contents:

$ ls -alt
ls: whatever: No such file or directory

No big error, nothing. syscall reports this:
lstat64("whatever\0", 0x100BE0, 0x1000)		 = -1 Err#2

This is not permanent. On the next export/import it will be fine.
The last time a "scrub" did the job. But that takes too long (Always  
hours)
Any other solution?

Chris.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090119/8328cc3d/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: Mensaje firmado digitalmente
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090119/8328cc3d/attachment.bin>

From slgarwood at charter.net  Wed Jan 21 13:13:20 2009
From: slgarwood at charter.net (Sterling Garwood)
Date: Wed, 21 Jan 2009 16:13:20 -0500
Subject: [zfs-discuss] speed? nah....
Message-ID: <588AAD3F-52B3-4842-A9C4-73FC0D6B9315@charter.net>

I have been testing zfs on MacOSXLeopard (10.5.6) on a MDD (dual 1.25  
Ghz ppc processors with 2 Gb memory).
I have a pair of firewire 500GB drives that have been mirrored under  
HFS+.
I broke the mirror, used diskutil and zfs to create a zfs volume on  
one of the two.
I am now in the process of copying the HFS+ drive to the zfs drive.
My plan is to then reformat the HFS drive and add it back as a mirror  
drive.

Only issue (outside of Finder weirdness with zfs) is a slow copy ---
Finder drag and drop to copy says about 280,000 items for 417.16GB  
will take 7 hours!!
Thats slow.
Its Firewire 400 but still ... thats only 60 GB an hour or a GB a  
minute or less than 16 megabytes (128 megabits) per second on a 400  
megabit channel...
the drives are on separate firewire controllers so data movement in  
the machine and file system overhead is the prime suspect.
Any thoughts out there to speed it up?


From richmc at gmail.com  Wed Jan 21 13:31:10 2009
From: richmc at gmail.com (Rich McClellan)
Date: Wed, 21 Jan 2009 13:31:10 -0800
Subject: [zfs-discuss] speed? nah....
In-Reply-To: <588AAD3F-52B3-4842-A9C4-73FC0D6B9315@charter.net>
References: <588AAD3F-52B3-4842-A9C4-73FC0D6B9315@charter.net>
Message-ID: <203e36e30901211331x4cb8de9ap8d062db240e5dd09@mail.gmail.com>

On Wed, Jan 21, 2009 at 1:13 PM, Sterling Garwood <slgarwood at charter.net> wrote:
[snip]
> Any thoughts out there to speed it up?

Get more and faster drives.

BTW, you can watch IOPS, etc., with `zpool iostat <delay in seconds>`.

From toby at telegraphics.com.au  Wed Jan 21 13:41:49 2009
From: toby at telegraphics.com.au (Toby Thain)
Date: Wed, 21 Jan 2009 16:41:49 -0500
Subject: [zfs-discuss] speed? nah....
In-Reply-To: <588AAD3F-52B3-4842-A9C4-73FC0D6B9315@charter.net>
References: <588AAD3F-52B3-4842-A9C4-73FC0D6B9315@charter.net>
Message-ID: <2C9102A2-1C76-40E6-BB96-76CF42E8C2AB@telegraphics.com.au>


On 21-Jan-09, at 4:13 PM, Sterling Garwood wrote:

> I have been testing zfs on MacOSXLeopard (10.5.6) on a MDD (dual  
> 1.25 Ghz ppc processors with 2 Gb memory).
> I have a pair of firewire 500GB drives that have been mirrored  
> under HFS+.
> I broke the mirror, used diskutil and zfs to create a zfs volume on  
> one of the two.
> I am now in the process of copying the HFS+ drive to the zfs drive.
> My plan is to then reformat the HFS drive and add it back as a  
> mirror drive.
>
> Only issue (outside of Finder weirdness with zfs) is a slow copy ---
> Finder drag and drop to copy says about 280,000 items for 417.16GB  
> will take 7 hours!!

IMHO you need better measurements. If you want raw speed, time a  
single large file (e.g. 'time cp...' as a rough test), that will give  
you the limiting bulk speed.

> Thats slow.


Obviously the smaller the pieces (as here), the slower your copy with  
take!

--Toby


> Its Firewire 400 but still ... thats only 60 GB an hour or a GB a  
> minute or less than 16 megabytes (128 megabits) per second on a 400  
> megabit channel...
> the drives are on separate firewire controllers so data movement in  
> the machine and file system overhead is the prime suspect.
> Any thoughts out there to speed it up?
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From jon at halfpast.net  Wed Jan 21 14:53:28 2009
From: jon at halfpast.net (Jon Moog)
Date: Wed, 21 Jan 2009 16:53:28 -0600
Subject: [zfs-discuss] speed? nah....
In-Reply-To: <588AAD3F-52B3-4842-A9C4-73FC0D6B9315@charter.net>
References: <588AAD3F-52B3-4842-A9C4-73FC0D6B9315@charter.net>
Message-ID: <A878327F-6861-4D63-8FCF-D3A2FC938B2A@halfpast.net>

This is not empirical but more anecdotal, however I have seen very  
poor performance with the Finder and ZFS volumes. It has been stated  
that the current Leopard releases are not optimized for performance  
but more an effort of bringing functionality. That being said I have  
seen good performance using command line tools that greatly eclipse  
the GUI performance. As for high end numbers, even given the lack of  
tuning and beta quality of the Leopard ZFS implementation the right  
hardware can get you over 1GB/Sec throughput. Even with a couple of  
internal 750 gig drives in a MacPro I consistently get 150MB/Sec which  
isn't much different than a HFS+ volume.

For the curious, the configuration was an xServe 2.8 x 8-way Xeon with  
8 x 4gb fiber channel 10 TB hardware raid units. Running as a stripe  
over 1GB/Sec was achieved and in a raidz it was closer to .5GB/Sec.

-Jon

On Jan 21, 2009, at 3:13 PM, Sterling Garwood wrote:

> I have been testing zfs on MacOSXLeopard (10.5.6) on a MDD (dual  
> 1.25 Ghz ppc processors with 2 Gb memory).
> I have a pair of firewire 500GB drives that have been mirrored under  
> HFS+.
> I broke the mirror, used diskutil and zfs to create a zfs volume on  
> one of the two.
> I am now in the process of copying the HFS+ drive to the zfs drive.
> My plan is to then reformat the HFS drive and add it back as a  
> mirror drive.
>
> Only issue (outside of Finder weirdness with zfs) is a slow copy ---
> Finder drag and drop to copy says about 280,000 items for 417.16GB  
> will take 7 hours!!
> Thats slow.
> Its Firewire 400 but still ... thats only 60 GB an hour or a GB a  
> minute or less than 16 megabytes (128 megabits) per second on a 400  
> megabit channel...
> the drives are on separate firewire controllers so data movement in  
> the machine and file system overhead is the prime suspect.
> Any thoughts out there to speed it up?
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From slgarwood at charter.net  Wed Jan 21 16:51:03 2009
From: slgarwood at charter.net (Sterling Garwood)
Date: Wed, 21 Jan 2009 19:51:03 -0500
Subject: [zfs-discuss] speed? nope...
Message-ID: <A20EBF79-CB55-4547-9882-FDE795D0D321@charter.net>

here is a zpool iostat on a 5 sec delay for a little bit ....
as you can see I have 2 pools - the one I am building the mirror on is  
'zfsstg'

                capacity     operations    bandwidth
pool         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
mypool       367K   189G      0      0    187  1.13K
zfsstg       265G   199G      1    154  95.4K  17.7M
----------  -----  -----  -----  -----  -----  -----
mypool       367K   189G      0      0      0      0
zfsstg       265G   199G     26    108  1.78M  6.79M
----------  -----  -----  -----  -----  -----  -----
mypool       367K   189G      0      0      0      0
zfsstg       265G   199G     35    120  2.86M  9.66M
----------  -----  -----  -----  -----  -----  -----
mypool       367K   189G      0      0      0      0
zfsstg       265G   199G     39    158  4.26M  12.9M
----------  -----  -----  -----  -----  -----  -----
mypool       367K   189G      0      0      0      0
zfsstg       265G   199G     53    121  4.79M  12.5M
----------  -----  -----  -----  -----  -----  -----
mypool       367K   189G      0      0      0      0
zfsstg       265G   199G     56     21  4.56M   669K
----------  -----  -----  -----  -----  -----  -----
mypool       367K   189G      0      0      0      0
zfsstg       265G   199G     28    142  1.88M  12.3M
----------  -----  -----  -----  -----  -----  -----
mypool       367K   189G      0      0      0      0
zfsstg       265G   199G     16     41  1.03M  3.37M
----------  -----  -----  -----  -----  -----  -----
mypool       367K   189G      0      0      0      0
zfsstg       265G   199G     33    119  2.22M  12.4M
----------  -----  -----  -----  -----  -----  -----
mypool       367K   189G      0      0      0      0
zfsstg       265G   199G     23    138  2.25M  12.8M
----------  -----  -----  -----  -----  -----  -----
mypool       367K   189G      0      0      0      0
zfsstg       265G   199G     54     73  3.98M  7.59M
----------  -----  -----  -----  -----  -----  -----


I tried to get a normal Unix iostat on the Firewire drive but for some  
reason Unix iostat can not see the HFS+ Firewire drive...hmmm time to  
file a bug report.

From Jonathan.Edwards at Sun.COM  Wed Jan 21 19:57:15 2009
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Wed, 21 Jan 2009 22:57:15 -0500
Subject: [zfs-discuss] speed? nope...
In-Reply-To: <A20EBF79-CB55-4547-9882-FDE795D0D321@charter.net>
References: <A20EBF79-CB55-4547-9882-FDE795D0D321@charter.net>
Message-ID: <0312B948-8ECE-4E8D-A383-73FDB3CD2B62@Sun.COM>

This is actually pretty normal for streaming writes on zfs .. what  
you're essentially seeing is a cyclical behaviour as the transaction  
groups flush writes down to the disk, and essentially pauses as the  
memory subsystem deals with the ARC filling up and evicting pages.   
Where you will see good performance (particularly on large memory  
systems) is a large file system buffer cache to deal with reads or  
file updates that can be easily cached.  Of course if the ARC has  
consumed so much that you're swapping in the VM then your performance  
is going to be particularly bad since the darwin VM will create  
pagefiles on your HFS+ root volume to read and write to which kind of  
defeats the purpose.

I don't think that they've enabled some of these tunables yet, but  
restricting the ARC to a fixed size can be beneficial here since you  
can often end up in a situation with more memory contention on other  
processes.  As for increasing speed for replicating from another  
filesystem - there's a couple of issues that typically come into play  
here:
1) crawling lots of metadata can be slow .. frequent readdir() open()  
calls while crawling large metadata trees (lots of files) can impose a  
lot of I/O slowdown on the system
2) the IOP speeds and latencies of the disks in question .. generally  
a 7200 RPM disk will only be able to do something on the order of  
80-90 IOPS peak (typically 60-70) - so depending on what your transfer  
size is - you're most likely going to be limited here on the disk.
3) other mac Finder latencies .. use the DTrace port to take a look at  
what's going on with your transfer with something like "sudo dtruss -p  
<pid_of_finder>" .. doing a quick test from my local drive transfering  
to my 5400RPM 250GB USB zfs volume i see a lot of 2MB pread/pwrite  
calls and a heck of a lot of getuid(), geteuid() calls .. (methinks  
there could be something goofy with permissions here)
4) getting frustrated and shouting at your disks to go faster can add  
latency and slow things down :) .. see the fishworks analytics viral  
video over at http://www.youtube.com/watch?v=tDacjrSCeq4

On Jan 21, 2009, at 7:51 PM, Sterling Garwood wrote:

> here is a zpool iostat on a 5 sec delay for a little bit ....
> as you can see I have 2 pools - the one I am building the mirror on  
> is 'zfsstg'
>
>               capacity     operations    bandwidth
> pool         used  avail   read  write   read  write
> ----------  -----  -----  -----  -----  -----  -----
> mypool       367K   189G      0      0    187  1.13K
> zfsstg       265G   199G      1    154  95.4K  17.7M
> ----------  -----  -----  -----  -----  -----  -----
> mypool       367K   189G      0      0      0      0
> zfsstg       265G   199G     26    108  1.78M  6.79M
> ----------  -----  -----  -----  -----  -----  -----
> mypool       367K   189G      0      0      0      0
> zfsstg       265G   199G     35    120  2.86M  9.66M
> ----------  -----  -----  -----  -----  -----  -----
> mypool       367K   189G      0      0      0      0
> zfsstg       265G   199G     39    158  4.26M  12.9M
> ----------  -----  -----  -----  -----  -----  -----
> mypool       367K   189G      0      0      0      0
> zfsstg       265G   199G     53    121  4.79M  12.5M
> ----------  -----  -----  -----  -----  -----  -----
> mypool       367K   189G      0      0      0      0
> zfsstg       265G   199G     56     21  4.56M   669K
> ----------  -----  -----  -----  -----  -----  -----
> mypool       367K   189G      0      0      0      0
> zfsstg       265G   199G     28    142  1.88M  12.3M
> ----------  -----  -----  -----  -----  -----  -----
> mypool       367K   189G      0      0      0      0
> zfsstg       265G   199G     16     41  1.03M  3.37M
> ----------  -----  -----  -----  -----  -----  -----
> mypool       367K   189G      0      0      0      0
> zfsstg       265G   199G     33    119  2.22M  12.4M
> ----------  -----  -----  -----  -----  -----  -----
> mypool       367K   189G      0      0      0      0
> zfsstg       265G   199G     23    138  2.25M  12.8M
> ----------  -----  -----  -----  -----  -----  -----
> mypool       367K   189G      0      0      0      0
> zfsstg       265G   199G     54     73  3.98M  7.59M
> ----------  -----  -----  -----  -----  -----  -----

btw - transaction groups will flush every 5 seconds by default .. i  
believe we changed this in later revs of ZFS, but you may not be  
getting the stat on after the full flush of the filesystem - you might  
want to increase your interval to 30 or so - you tend to get better  
averages over time .. also if your pool consisted of multiple disks  
you can see the stats for each vdev in the pool with a "zpool iostat - 
v [interval]" ..

> I tried to get a normal Unix iostat on the Firewire drive but for  
> some reason Unix iostat can not see the HFS+ Firewire drive...hmmm  
> time to file a bug report.

you can specify the drive explicitly with something like "iostat  
[disk#] [interval]" where disk# is the "diskutil list" disk# of your  
firewire drive .. if you have a lot of disks, you may be getting  
limited to just the stats on the first few .. so, for example, if  
disk5 is the HFS+ firewire drive and disk2 is the ZFS drive I'd do:

# iostat disk5 disk2 30

at smaller intervals i'd expect that you'd see a lot of reads from the  
HFS+ drive, and more infrequent pushes to the ZFS pool, whereas at  
larger intervals you should see a closer balance between the 2

hth
---
jonathan


From sd at pixelmilk.de  Thu Jan 22 06:02:05 2009
From: sd at pixelmilk.de (=?ISO-8859-1?Q?Sebastian_D=F6ll?=)
Date: Thu, 22 Jan 2009 15:02:05 +0100
Subject: [zfs-discuss] ZFS + Server Admin Tools
Message-ID: <75F27D88-FAE6-45B1-97FE-87F91EAB9585@pixelmilk.de>

Hi,

I'm new to this dicussion list,
but I've read in old theads that some people
hat problem to get sharing trough the Server Admin Tools running.

It's right that the zfs drives don't show up in the tool,
but you can simply create a sharing mountpoint with the command line
tool "sharing".

For example you can use

sharing -a /Volumes/tank/Users -A Users -g

to create a sharing point for the home folders.
This sharing point will show up in the Server Admin Tools
and can then be added to the OD.

It may help some people to get things running with ZFS.

Bye
Sebastian

"ZFS is ready"

From slgarwood at charter.net  Thu Jan 22 14:35:32 2009
From: slgarwood at charter.net (Sterling Garwood)
Date: Thu, 22 Jan 2009 17:35:32 -0500
Subject: [zfs-discuss] I lost 500GB of backup data in zfs!!!
Message-ID: <BDFFD7D6-56AF-46B5-AC23-6A560875C16C@charter.net>

I finished copying my ~500GB of data (see previous posts for IO  
thruput question notes). Full copy completed from Firewire drive to  
USB external drive. Got ready to build a mirror set using 2 Firewire  
drives.
But first I decided to run Software Update (new Server Admin Tool  
updates and a Quicktime bump)....ARRGGHHH!!!! Bad error on my part.  
The update went on fine but....
After system came back up my 5 ZFS drives (2 internal and 3 external)  
appeared as MSDOS FAT partitions. Nothing would bring the ZFS pools/ 
Partition types etc back ... poof ... 500GB of data in 2 separate  
locations gone.
It is definitely not prime-time ready!!!
This is on Leopard 10.5.6 with the developer preview ZFS.
I give up on ZFS until its next drop ... too much wasted time with  
this one.
Now to restore to an HFS+ mirror set using the 2 Firewire drives using  
TimeMachine.


From zorg at sogeeky.net  Thu Jan 22 15:13:18 2009
From: zorg at sogeeky.net (Mr. Zorg)
Date: Thu, 22 Jan 2009 15:13:18 -0800
Subject: [zfs-discuss] I lost 500GB of backup data in zfs!!!
In-Reply-To: <BDFFD7D6-56AF-46B5-AC23-6A560875C16C@charter.net>
References: <BDFFD7D6-56AF-46B5-AC23-6A560875C16C@charter.net>
Message-ID: <8921E633-1B44-4551-9C87-071C6901C83E@sogeeky.net>

I'm sure others will chime in, but I've taken updates all the time  
with no problems at all. Did you follow the directions on the wiki  
carefully and make sure to format and label the disk correctly?  Maybe  
if you provide some output dumps from diskutil list, zpool list, zpool  
status, etc, someone can help. There's not enough to go on.

On Jan 22, 2009, at 2:35 PM, Sterling Garwood <slgarwood at charter.net>  
wrote:

> I finished copying my ~500GB of data (see previous posts for IO  
> thruput question notes). Full copy completed from Firewire drive to  
> USB external drive. Got ready to build a mirror set using 2 Firewire  
> drives.
> But first I decided to run Software Update (new Server Admin Tool  
> updates and a Quicktime bump)....ARRGGHHH!!!! Bad error on my part.  
> The update went on fine but....
> After system came back up my 5 ZFS drives (2 internal and 3  
> external) appeared as MSDOS FAT partitions. Nothing would bring the  
> ZFS pools/Partition types etc back ... poof ... 500GB of data in 2  
> separate locations gone.
> It is definitely not prime-time ready!!!
> This is on Leopard 10.5.6 with the developer preview ZFS.
> I give up on ZFS until its next drop ... too much wasted time with  
> this one.
> Now to restore to an HFS+ mirror set using the 2 Firewire drives  
> using TimeMachine.
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From slgarwood at charter.net  Thu Jan 22 15:27:52 2009
From: slgarwood at charter.net (Sterling Garwood)
Date: Thu, 22 Jan 2009 18:27:52 -0500
Subject: [zfs-discuss] I lost 500GB of backup data in zfs!!!
Message-ID: <5A5F576C-CA41-46A6-B6B2-40DF8625E650@charter.net>

Below is the system log from the time period ... note the 3 error  
messages from ZFS as Software Update reboots......


Jan 22 16:05:13 dualg4 sshd[2404]: DEAD_PROCESS: 2410 ttys001
Jan 22 16:40:39 dualg4 Software Update[2473]: arguments=(null)
Jan 22 16:40:43 dualg4 login[1708]: DEAD_PROCESS: 1708 ttys000
Jan 22 16:41:50 dualg4 authexec[2479]: executing /System/Library/ 
CoreServices/Software Update.app/Contents/Resources/suapphelper
Jan 22 16:42:28 dualg4 login[2484]: USER_PROCESS: 2484 ttys000
Jan 22 16:42:46 dualg4 login[2484]: DEAD_PROCESS: 2484 ttys000
Jan 22 16:42:58 dualg4 quicklookd[2500]: Unable to use font: no glyphs  
present.
Jan 22 16:42:58 dualg4 com.apple.quicklook[2500]: Thu Jan 22 16:42:58  
dualg4.local quicklookd[2500] <Error>: Unable to use font: no glyphs  
present.
Jan 22 16:44:14 dualg4 authexec[2505]: executing /usr/bin/ditto
Jan 22 16:45:04 dualg4 login[2507]: USER_PROCESS: 2507 ttys000
Jan 22 16:45:05 dualg4 login[2507]: DEAD_PROCESS: 2507 ttys000
Jan 22 16:45:54 dualg4 authexec[2518]: executing /usr/bin/ditto
Jan 22 16:46:00 dualg4 loginwindow[34]: DEAD_PROCESS: 0 console
Jan 22 16:46:02 dualg4 com.apple.launchd[142]  
([0x0-0x2f02f].com.apple.AppleSpell[490]): Exited: Terminated
Jan 22 16:46:02 dualg4 com.apple.launchd[142]  
(com.wacom.pentablet[147]): Exited with exit code: 255
Jan 22 16:46:02 dualg4 com.apple.launchd[142] (com.apple.pboard[157]):  
Exited: Terminated
Jan 22 16:46:02 dualg4 com.apple.launchd[142]  
(com.apple.UserEventAgent-Aqua[154]): Exited: Terminated
Jan 22 16:46:02 dualg4 com.apple.launchd[142]  
(com.apple.ScreenSharing.server[151]): Exited: Terminated
Jan 22 16:46:02 dualg4 com.apple.launchd[142]  
([0x0-0x15015].com.apple.iChatAgent[205]): Exited: Terminated
Jan 22 16:46:02 dualg4 /System/Library/CoreServices/loginwindow.app/ 
Contents/MacOS/loginwindow[2523]: Login Window Application Started --  
Threaded auth
Jan 22 16:46:02 dualg4 com.apple.WindowServer[57]: Name ATY,Apache  
matches IOService:/MacRISC2PE/pci at f0000000/AppleMacRiscAGP/ 
ATY,ApacheParent at 10/ATY,Apache_A at 0, new version 010080dd, current  
version 01018039
Jan 22 16:46:02 dualg4 com.apple.WindowServer[57]: Name ATY,Apache  
matches IOService:/MacRISC2PE/pci at f0000000/AppleMacRiscAGP/ 
ATY,ApacheParent at 10/ATY,Apache_B at 1, new version 010080dd, current  
version 01018039
Jan 22 16:46:04 dualg4 loginwindow[2523]: USER_PROCESS: 2523 console
Jan 22 16:46:04 dualg4 Software Update[2533]: arguments={\n     
RootInstallMode = YES;\n    SkipConfirm = YES;\n}
Jan 22 16:46:04 dualg4 Software Update[2533]: cookie=""
Jan 22 16:46:04 dualg4 Software Update[2533]: Looking for products to  
install
Jan 22 16:46:06 dualg4 Software Update[2533]: Performing preflight
Jan 22 16:46:06 dualg4 Software Update[2533]: Preparing session
Jan 22 16:46:07 dualg4 Software Update[2533]: Starting session
Jan 22 16:46:07 dualg4 /System/Library/CoreServices/Software  
Update.app/Contents/MacOS/Software Update[2533]: Folder Manager is  
being asked to create a folder (flnt) while running as uid 0
Jan 22 16:47:13 dualg4 kernel[0]: Limiting icmp unreach response from  
251 to 250 packets per second
Jan 22 16:47:16 dualg4 sshd[2571]: USER_PROCESS: 2579 ttys000
Jan 22 16:47:18 dualg4 kernel[0]: Limiting icmp unreach response from  
311 to 250 packets per second
Jan 22 16:47:19 dualg4 sudo[2606]:     root : TTY=unknown ; PWD=/ ;  
USER=root ; COMMAND=/Library/Updates/QuickTime/Packages/ 
QuickTime76_Leopard.pkg/Contents/Resources/ConfigureMimeTypes -rescan - 
verbose
Jan 22 16:47:26 dualg4 Software Update[2533]: kextcache returned 0
Jan 22 16:47:26 dualg4 Software Update[2533]: Running /sbin/reboot
Jan 22 16:47:26 dualg4 kextd[10]: error reconsidering volume /Volumes/ 
mypool
Jan 22 16:47:26 dualg4 kextd[10]: error reconsidering volume /Volumes/ 
zfsstg
Jan 22 16:47:26 dualg4 kextd[10]: error reconsidering volume /Volumes/ 
remove
Jan 22 16:47:26 dualg4 reboot[2628]: rebooted by _securityagent
Jan 22 16:47:26 dualg4 reboot[2628]: SHUTDOWN_TIME: 1232660846 347832
Jan 22 16:47:27 dualg4 mDNSResponder mDNSResponder-176.3 (Sep 30 2008  
16:59:41)[16]: stopping
Jan 22 16:47:27 dualg4 Software Update[2533]: Terminating
Jan 22 16:47:27 dualg4 PenTabletDriver[2527]:  
CGSShutdownServerConnections: Detaching application from window server
Jan 22 16:47:27 dualg4 AppleVNCServer[2528]:  
CGSShutdownServerConnections: Detaching application from window server


From Jonathan.Edwards at Sun.COM  Thu Jan 22 15:38:17 2009
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Thu, 22 Jan 2009 18:38:17 -0500
Subject: [zfs-discuss] I lost 500GB of backup data in zfs!!!
In-Reply-To: <5A5F576C-CA41-46A6-B6B2-40DF8625E650@charter.net>
References: <5A5F576C-CA41-46A6-B6B2-40DF8625E650@charter.net>
Message-ID: <DAE6C9DE-6303-4292-9247-C87163F762FD@sun.com>


On Jan 22, 2009, at 6:27 PM, Sterling Garwood wrote:

> Jan 22 16:47:26 dualg4 Software Update[2533]: kextcache returned 0
> Jan 22 16:47:26 dualg4 Software Update[2533]: Running /sbin/reboot
> Jan 22 16:47:26 dualg4 kextd[10]: error reconsidering volume / 
> Volumes/mypool
> Jan 22 16:47:26 dualg4 kextd[10]: error reconsidering volume / 
> Volumes/zfsstg
> Jan 22 16:47:26 dualg4 kextd[10]: error reconsidering volume / 
> Volumes/remove
> Jan 22 16:47:26 dualg4 reboot[2628]: rebooted by _securityagent

erm .. if you're talking about these - somebody else chime in, but i  
believe this is simply that it couldn't unmount these volumes before  
it rebooted .. not to worry though - i suspect the ZFS volumes already  
had a FAT header on them - how are you determining these as FAT? - are  
you saying that you can't:

# zpool import -f mypool
# zpool import -f zfsstg
# zpool import -f remove

and it's not clear which zfs kernel version you're using here ..

Jonathan

From slgarwood at charter.net  Thu Jan 22 16:02:20 2009
From: slgarwood at charter.net (Sterling Garwood)
Date: Thu, 22 Jan 2009 19:02:20 -0500
Subject: [zfs-discuss] I lost 500GB of backup data in zfs!!!
In-Reply-To: <DAE6C9DE-6303-4292-9247-C87163F762FD@sun.com>
References: <5A5F576C-CA41-46A6-B6B2-40DF8625E650@charter.net>
	<DAE6C9DE-6303-4292-9247-C87163F762FD@sun.com>
Message-ID: <EDB0EAEF-0E40-4668-99F8-36E905309888@charter.net>

I am running the currently available Mac zfs drop from the mac  
developers site - says its 119.

The sequence of events across the whole day is
(I already had a pool with 2 internal drives in it called 'mypool')
1. create pool on 1st Firewire 500 GB drive called zfsstg.. worked ok
2. Copied all 500 GB data from a HFS+ drive to the above pool - worked  
ok
3. Reformatted the HFS+ Firewire drive and add it as 2nd Firewire 500  
GB drive to zfsstg pool (what I really wanted to do was create a  
mirror but I forgot the magic 'mirror' word :-)
4. create another pool called 'remove' on another external (USB) 500GB  
drive.
5. copied zfsstg to remove  ... copy worked ok. DId several 'ls -al'  
to check ... same number of files etc....
6. then noticed a software update was pending so since I was gonna  
reboot anyway let it install.
I was planning on powering down both Firewire drives, rebooting,  
powering them up 1 at a time and reformatting, then creating a zfs  
mirror. I then planned on copying pool 'remove' to the new mirror.
7. upon reboot got error messages that all zfs pool drives were  
'unknown format' including the 2 internal drives in the pool I hadn't  
touched all day.... <sigh>


On Jan 22, 2009, at 6:38 PM, Jonathan Edwards wrote:

>
> On Jan 22, 2009, at 6:27 PM, Sterling Garwood wrote:
>
>> Jan 22 16:47:26 dualg4 Software Update[2533]: kextcache returned 0
>> Jan 22 16:47:26 dualg4 Software Update[2533]: Running /sbin/reboot
>> Jan 22 16:47:26 dualg4 kextd[10]: error reconsidering volume / 
>> Volumes/mypool
>> Jan 22 16:47:26 dualg4 kextd[10]: error reconsidering volume / 
>> Volumes/zfsstg
>> Jan 22 16:47:26 dualg4 kextd[10]: error reconsidering volume / 
>> Volumes/remove
>> Jan 22 16:47:26 dualg4 reboot[2628]: rebooted by _securityagent
>
> erm .. if you're talking about these - somebody else chime in, but i  
> believe this is simply that it couldn't unmount these volumes before  
> it rebooted .. not to worry though - i suspect the ZFS volumes  
> already had a FAT header on them - how are you determining these as  
> FAT? - are you saying that you can't:
>
> # zpool import -f mypool
> # zpool import -f zfsstg
> # zpool import -f remove
>
> and it's not clear which zfs kernel version you're using here ..
>
> Jonathan


From zorg at sogeeky.net  Thu Jan 22 16:09:14 2009
From: zorg at sogeeky.net (Mr. Zorg)
Date: Thu, 22 Jan 2009 16:09:14 -0800
Subject: [zfs-discuss] I lost 500GB of backup data in zfs!!!
In-Reply-To: <EDB0EAEF-0E40-4668-99F8-36E905309888@charter.net>
References: <5A5F576C-CA41-46A6-B6B2-40DF8625E650@charter.net>
	<DAE6C9DE-6303-4292-9247-C87163F762FD@sun.com>
	<EDB0EAEF-0E40-4668-99F8-36E905309888@charter.net>
Message-ID: <EEBA88E9-5B1C-4458-B215-3C599FCCDA4D@sogeeky.net>

At steps 1 and 4, did you format the disks EXACTLY as described in the  
wiki?  If not, that's likely the cause of your woes.

On Jan 22, 2009, at 4:02 PM, Sterling Garwood <slgarwood at charter.net>  
wrote:

> I am running the currently available Mac zfs drop from the mac  
> developers site - says its 119.
>
> The sequence of events across the whole day is
> (I already had a pool with 2 internal drives in it called 'mypool')
> 1. create pool on 1st Firewire 500 GB drive called zfsstg.. worked ok
> 2. Copied all 500 GB data from a HFS+ drive to the above pool -  
> worked ok
> 3. Reformatted the HFS+ Firewire drive and add it as 2nd Firewire  
> 500 GB drive to zfsstg pool (what I really wanted to do was create a  
> mirror but I forgot the magic 'mirror' word :-)
> 4. create another pool called 'remove' on another external (USB)  
> 500GB drive.
> 5. copied zfsstg to remove  ... copy worked ok. DId several 'ls -al'  
> to check ... same number of files etc....
> 6. then noticed a software update was pending so since I was gonna  
> reboot anyway let it install.
> I was planning on powering down both Firewire drives, rebooting,  
> powering them up 1 at a time and reformatting, then creating a zfs  
> mirror. I then planned on copying pool 'remove' to the new mirror.
> 7. upon reboot got error messages that all zfs pool drives were  
> 'unknown format' including the 2 internal drives in the pool I  
> hadn't touched all day.... <sigh>
>
>
> On Jan 22, 2009, at 6:38 PM, Jonathan Edwards wrote:
>
>>
>> On Jan 22, 2009, at 6:27 PM, Sterling Garwood wrote:
>>
>>> Jan 22 16:47:26 dualg4 Software Update[2533]: kextcache returned 0
>>> Jan 22 16:47:26 dualg4 Software Update[2533]: Running /sbin/reboot
>>> Jan 22 16:47:26 dualg4 kextd[10]: error reconsidering volume / 
>>> Volumes/mypool
>>> Jan 22 16:47:26 dualg4 kextd[10]: error reconsidering volume / 
>>> Volumes/zfsstg
>>> Jan 22 16:47:26 dualg4 kextd[10]: error reconsidering volume / 
>>> Volumes/remove
>>> Jan 22 16:47:26 dualg4 reboot[2628]: rebooted by _securityagent
>>
>> erm .. if you're talking about these - somebody else chime in, but  
>> i believe this is simply that it couldn't unmount these volumes  
>> before it rebooted .. not to worry though - i suspect the ZFS  
>> volumes already had a FAT header on them - how are you determining  
>> these as FAT? - are you saying that you can't:
>>
>> # zpool import -f mypool
>> # zpool import -f zfsstg
>> # zpool import -f remove
>>
>> and it's not clear which zfs kernel version you're using here ..
>>
>> Jonathan
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From Jonathan.Edwards at Sun.COM  Thu Jan 22 16:50:27 2009
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Thu, 22 Jan 2009 19:50:27 -0500
Subject: [zfs-discuss] I lost 500GB of backup data in zfs!!!
In-Reply-To: <EEBA88E9-5B1C-4458-B215-3C599FCCDA4D@sogeeky.net>
References: <5A5F576C-CA41-46A6-B6B2-40DF8625E650@charter.net>
	<DAE6C9DE-6303-4292-9247-C87163F762FD@sun.com>
	<EDB0EAEF-0E40-4668-99F8-36E905309888@charter.net>
	<EEBA88E9-5B1C-4458-B215-3C599FCCDA4D@sogeeky.net>
Message-ID: <0556B37F-140C-4C69-9BED-4A94CF507E6C@Sun.COM>

you might be okay .. it really depends on where the zfs label is on  
the drive:

1) make sure the zfs kext is loaded
# kextstat | grep zfs
2) check to see if you can see your pools
# zpool list
3) check to see if you can import your pools
# sudo import [-f] <pool>

btw - if you have an existing unmirrored single disk pool (eg: mypool  
say on disk3) you can make it a mirrored pool (to say disk4) with  
something like:
# zpool attach mypool disk3 disk4

hth
Jonathan

On Jan 22, 2009, at 7:09 PM, Mr. Zorg wrote:

> At steps 1 and 4, did you format the disks EXACTLY as described in  
> the wiki?  If not, that's likely the cause of your woes.
>
> On Jan 22, 2009, at 4:02 PM, Sterling Garwood  
> <slgarwood at charter.net> wrote:
>
>> I am running the currently available Mac zfs drop from the mac  
>> developers site - says its 119.
>>
>> The sequence of events across the whole day is
>> (I already had a pool with 2 internal drives in it called 'mypool')
>> 1. create pool on 1st Firewire 500 GB drive called zfsstg.. worked ok
>> 2. Copied all 500 GB data from a HFS+ drive to the above pool -  
>> worked ok
>> 3. Reformatted the HFS+ Firewire drive and add it as 2nd Firewire  
>> 500 GB drive to zfsstg pool (what I really wanted to do was create  
>> a mirror but I forgot the magic 'mirror' word :-)
>> 4. create another pool called 'remove' on another external (USB)  
>> 500GB drive.
>> 5. copied zfsstg to remove  ... copy worked ok. DId several 'ls - 
>> al' to check ... same number of files etc....
>> 6. then noticed a software update was pending so since I was gonna  
>> reboot anyway let it install.
>> I was planning on powering down both Firewire drives, rebooting,  
>> powering them up 1 at a time and reformatting, then creating a zfs  
>> mirror. I then planned on copying pool 'remove' to the new mirror.
>> 7. upon reboot got error messages that all zfs pool drives were  
>> 'unknown format' including the 2 internal drives in the pool I  
>> hadn't touched all day.... <sigh>
>


From slgarwood at charter.net  Thu Jan 22 17:20:52 2009
From: slgarwood at charter.net (Sterling Garwood)
Date: Thu, 22 Jan 2009 20:20:52 -0500
Subject: [zfs-discuss] I lost 500GB of backup data in zfs!!!
In-Reply-To: <049B3E60-2C18-4585-A80C-8662086EB7EA@charter.net>
References: <5A5F576C-CA41-46A6-B6B2-40DF8625E650@charter.net>
	<DAE6C9DE-6303-4292-9247-C87163F762FD@sun.com>
	<EDB0EAEF-0E40-4668-99F8-36E905309888@charter.net>
	<EEBA88E9-5B1C-4458-B215-3C599FCCDA4D@sogeeky.net>
	<0556B37F-140C-4C69-9BED-4A94CF507E6C@Sun.COM>
	<049B3E60-2C18-4585-A80C-8662086EB7EA@charter.net>
Message-ID: <490E2E5D-4E09-4507-AB6B-1462FCDF1E9C@charter.net>


On Jan 22, 2009, at 8:07 PM, Sterling Garwood wrote:

> 1. kext is loaded (and its NOT the Apple read-only one)
> 2. nope ... no pools listed
> 3. nope ...
>
> I tried the attach trick (I thought that's what it did too) but it  
> did not show up as a mirror ... just a concatenated set of drives  
> (the total pool storage was the sum of both drives added together).
> I think something stepped on the first part of the disk (partition  
> table, etc) ... on all zfs drives. Maybe a bug, maybe a stray cosmic  
> ray (Apple Powermac memory is not ECCd AFAIK).
> I'll try and see if I can somehow hex dump a bit of one drive and  
> see whats what...
>
> On Jan 22, 2009, at 7:50 PM, Jonathan Edwards wrote:
>
>> you might be okay .. it really depends on where the zfs label is on  
>> the drive:
>>
>> 1) make sure the zfs kext is loaded
>> # kextstat | grep zfs
>> 2) check to see if you can see your pools
>> # zpool list
>> 3) check to see if you can import your pools
>> # sudo import [-f] <pool>
>>
>> btw - if you have an existing unmirrored single disk pool (eg:  
>> mypool say on disk3) you can make it a mirrored pool (to say disk4)  
>> with something like:
>> # zpool attach mypool disk3 disk4
>>
>> hth
>> Jonathan
>>
>> On Jan 22, 2009, at 7:09 PM, Mr. Zorg wrote:
>>
>>> At steps 1 and 4, did you format the disks EXACTLY as described in  
>>> the wiki?  If not, that's likely the cause of your woes.
>>>
>>> On Jan 22, 2009, at 4:02 PM, Sterling Garwood  
>>> <slgarwood at charter.net> wrote:
>>>
>>>> I am running the currently available Mac zfs drop from the mac  
>>>> developers site - says its 119.
>>>>
>>>> The sequence of events across the whole day is
>>>> (I already had a pool with 2 internal drives in it called 'mypool')
>>>> 1. create pool on 1st Firewire 500 GB drive called zfsstg..  
>>>> worked ok
>>>> 2. Copied all 500 GB data from a HFS+ drive to the above pool -  
>>>> worked ok
>>>> 3. Reformatted the HFS+ Firewire drive and add it as 2nd Firewire  
>>>> 500 GB drive to zfsstg pool (what I really wanted to do was  
>>>> create a mirror but I forgot the magic 'mirror' word :-)
>>>> 4. create another pool called 'remove' on another external (USB)  
>>>> 500GB drive.
>>>> 5. copied zfsstg to remove  ... copy worked ok. DId several 'ls - 
>>>> al' to check ... same number of files etc....
>>>> 6. then noticed a software update was pending so since I was  
>>>> gonna reboot anyway let it install.
>>>> I was planning on powering down both Firewire drives, rebooting,  
>>>> powering them up 1 at a time and reformatting, then creating a  
>>>> zfs mirror. I then planned on copying pool 'remove' to the new  
>>>> mirror.
>>>> 7. upon reboot got error messages that all zfs pool drives were  
>>>> 'unknown format' including the 2 internal drives in the pool I  
>>>> hadn't touched all day.... <sigh>
>>>
>>
>


From james-zfsosx at jrv.org  Thu Jan 22 17:28:29 2009
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Thu, 22 Jan 2009 19:28:29 -0600
Subject: [zfs-discuss] I lost 500GB of backup data in zfs!!!
In-Reply-To: <490E2E5D-4E09-4507-AB6B-1462FCDF1E9C@charter.net>
References: <5A5F576C-CA41-46A6-B6B2-40DF8625E650@charter.net>	<DAE6C9DE-6303-4292-9247-C87163F762FD@sun.com>	<EDB0EAEF-0E40-4668-99F8-36E905309888@charter.net>	<EEBA88E9-5B1C-4458-B215-3C599FCCDA4D@sogeeky.net>	<0556B37F-140C-4C69-9BED-4A94CF507E6C@Sun.COM>	<049B3E60-2C18-4585-A80C-8662086EB7EA@charter.net>
	<490E2E5D-4E09-4507-AB6B-1462FCDF1E9C@charter.net>
Message-ID: <49791D3D.9040704@jrv.org>

Did you use this command:

# diskutil partitiondisk /dev/disk99 GPTFormat ZFS %noformat% 100%

as per the wiki at
http://zfs.macosforge.org/trac/wiki/get_the_party_started before
creating the pool?  This step is necessary.

From slgarwood at charter.net  Thu Jan 22 17:59:51 2009
From: slgarwood at charter.net (Sterling Garwood)
Date: Thu, 22 Jan 2009 20:59:51 -0500
Subject: [zfs-discuss] I lost 500GB of backup data in zfs!!!
In-Reply-To: <49791D3D.9040704@jrv.org>
References: <5A5F576C-CA41-46A6-B6B2-40DF8625E650@charter.net>	<DAE6C9DE-6303-4292-9247-C87163F762FD@sun.com>	<EDB0EAEF-0E40-4668-99F8-36E905309888@charter.net>	<EEBA88E9-5B1C-4458-B215-3C599FCCDA4D@sogeeky.net>	<0556B37F-140C-4C69-9BED-4A94CF507E6C@Sun.COM>	<049B3E60-2C18-4585-A80C-8662086EB7EA@charter.net>
	<490E2E5D-4E09-4507-AB6B-1462FCDF1E9C@charter.net>
	<49791D3D.9040704@jrv.org>
Message-ID: <F03B63B6-73AB-4E69-BACC-906C3CF0A6DA@charter.net>

Yes ,,,
the wierd thing that makes me think its a bug is that not only did my  
new pools get clobbered but also the older pool I'd build several days  
ago which went thru several reboots.

On Jan 22, 2009, at 8:28 PM, James R. Van Artsdalen wrote:

> Did you use this command:
>
> # diskutil partitiondisk /dev/disk99 GPTFormat ZFS %noformat% 100%
>
> as per the wiki at
> http://zfs.macosforge.org/trac/wiki/get_the_party_started before
> creating the pool?  This step is necessary.


From Jonathan.Edwards at Sun.COM  Thu Jan 22 19:25:31 2009
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Thu, 22 Jan 2009 22:25:31 -0500
Subject: [zfs-discuss] I lost 500GB of backup data in zfs!!!
In-Reply-To: <490E2E5D-4E09-4507-AB6B-1462FCDF1E9C@charter.net>
References: <5A5F576C-CA41-46A6-B6B2-40DF8625E650@charter.net>
	<DAE6C9DE-6303-4292-9247-C87163F762FD@sun.com>
	<EDB0EAEF-0E40-4668-99F8-36E905309888@charter.net>
	<EEBA88E9-5B1C-4458-B215-3C599FCCDA4D@sogeeky.net>
	<0556B37F-140C-4C69-9BED-4A94CF507E6C@Sun.COM>
	<049B3E60-2C18-4585-A80C-8662086EB7EA@charter.net>
	<490E2E5D-4E09-4507-AB6B-1462FCDF1E9C@charter.net>
Message-ID: <6FA682F3-3988-453B-A35F-DBEEBBF1CF83@sun.com>


On Jan 22, 2009, at 8:20 PM, Sterling Garwood wrote:

> On Jan 22, 2009, at 8:07 PM, Sterling Garwood wrote:
>
>> 1. kext is loaded (and its NOT the Apple read-only one)
>> 2. nope ... no pools listed
>> 3. nope ...
>>
>> I tried the attach trick (I thought that's what it did too) but it  
>> did not show up as a mirror ... just a concatenated set of drives  
>> (the total pool storage was the sum of both drives added together).

hrm .. you sure it wasn't an add by mistake?  for example:
$ zpool create mypool /var/tmp/file1
$ zpool status
<snip>
config:

	NAME              STATE     READ WRITE CKSUM
	mypool            ONLINE       0     0     0
	  /var/tmp/file1  ONLINE       0     0     0

$ zpool add mypool /var/tmp/file2
$ zpool status
<snip>
config:

	NAME              STATE     READ WRITE CKSUM
	mypool            ONLINE       0     0     0
	  /var/tmp/file1  ONLINE       0     0     0
	  /var/tmp/file2  ONLINE       0     0     0

errors: No known data errors

--vs--

$ zpool create mypool /var/tmp/file1
$ zpool status
<snip>
config:

	NAME              STATE     READ WRITE CKSUM
	mypool            ONLINE       0     0     0
	  /var/tmp/file1  ONLINE       0     0     0

$ zpool attach mypool /var/tmp/file1 /var/tmp/file2
$ zpool status
  <snip>
config:

	NAME                STATE     READ WRITE CKSUM
	mypool              ONLINE       0     0     0
	  mirror            ONLINE       0     0     0
	    /var/tmp/file1  ONLINE       0     0     0
	    /var/tmp/file2  ONLINE       0     0     0

---
you can check the man page for zpool attach to see what you should get  
there


>> I think something stepped on the first part of the disk (partition  
>> table, etc) ... on all zfs drives. Maybe a bug, maybe a stray  
>> cosmic ray (Apple Powermac memory is not ECCd AFAIK).
>> I'll try and see if I can somehow hex dump a bit of one drive and  
>> see whats what...

take a look at pp 6-8 on the ODF document here:
http://opensolaris.org/os/community/zfs/docs/ondiskformat0822.pdf

you should have 4 redundant ZFS labels .. if you haven't already  
destroyed the data on the disks just look for the 256KB blocks  
starting with:
0x2f5b007b10c
(or 0c b1 07 b0 f5 02 - depending on endian-ness)

two labels are the 512K at the front of the disk after the GPT and two  
labels are at the tail end before the backup GPT label .. if you  
didn't do a GPT - I'm guessing they might be somewhere around the  
pdisk or fdisk layout cylinders

each of these are copies of one another .. essentially 128KB of header  
info (blank space, header, nv pairs) and then 128KB of uberblock  
entries each beginning with:
0x00bab10c  (read oo-ba-block)
(or 0c b1 ba 00 - depending on endian-ness)

if you can .. you should be able to dd them to the right place after a  
proper disk label .. provided you're careful and don't have data/label  
overlap you should be good to go ..

hth
Jonathan

From Jonathan.Edwards at Sun.COM  Thu Jan 22 19:43:56 2009
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Thu, 22 Jan 2009 22:43:56 -0500
Subject: [zfs-discuss] I lost 500GB of backup data in zfs!!!
In-Reply-To: <6FA682F3-3988-453B-A35F-DBEEBBF1CF83@sun.com>
References: <5A5F576C-CA41-46A6-B6B2-40DF8625E650@charter.net>
	<DAE6C9DE-6303-4292-9247-C87163F762FD@sun.com>
	<EDB0EAEF-0E40-4668-99F8-36E905309888@charter.net>
	<EEBA88E9-5B1C-4458-B215-3C599FCCDA4D@sogeeky.net>
	<0556B37F-140C-4C69-9BED-4A94CF507E6C@Sun.COM>
	<049B3E60-2C18-4585-A80C-8662086EB7EA@charter.net>
	<490E2E5D-4E09-4507-AB6B-1462FCDF1E9C@charter.net>
	<6FA682F3-3988-453B-A35F-DBEEBBF1CF83@sun.com>
Message-ID: <099E996A-9753-4A13-B569-CBEA66C3E4A1@sun.com>

> if you can .. you should be able to dd them to the right place after  
> a proper disk label .. provided you're careful and don't have data/ 
> label overlap you should be good to go ..

erm .. correction .. actually if it is pdisk or fdisk layout initially  
- **don't format to GPT** .. the 16KB at the front and end of disk  
will probably throw everything out of whack (wrong spacing between the  
label and the data) - you should really just try to restore it to how  
it was before to get to your data (if you can), and then work to zfs  
snapshot/clone into a new filesystem/pool/disk properly formatted for  
GPT

Jonathan

ps - sorry to reply to my own post .. i just realized what i wrote  
here .. hopefully it's not a "cut the red wire <snip> .. but first  
disconnect the green wire" sort of moment .. :P

From ksh at ironsoftware.de  Sat Jan 24 14:32:30 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Sat, 24 Jan 2009 17:32:30 -0500
Subject: [zfs-discuss] endless scrub
Message-ID: <BC816926-44FF-436A-B1F7-4AC3DC008C80@ironsoftware.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi,

what could cause an endless scrub ?

I started to scrub 14h ago and the drive is still scrubbing + the  
progress seems to
be reset from time to time. i never saw the scrub exceed 1%.

NAME   USED  AVAIL  REFER  MOUNTPOINT
home  66.5G  37.9G   360K  none

ksh at kshs-Computer:~ $ zpool status
   pool: home
  state: ONLINE
  scrub: scrub in progress, 1.92% done, 5h15m to go
config:

	NAME        STATE     READ WRITE CKSUM
	home        ONLINE       0     0     0
	  disk2     ONLINE       0     0     0

errors: No known data errors
ksh at kshs-Computer:~ $ zpool status
   pool: home
  state: ONLINE
  scrub: scrub in progress, 0.01% done, 103h9m to go
config:

	NAME        STATE     READ WRITE CKSUM
	home        ONLINE       0     0     0
	  disk2     ONLINE       0     0     0

errors: No known data errors


Chris.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (Darwin)

iD8DBQFJe5b/p+9ff145KVIRAi3oAJ4mEW+ZMHef9t6qOrnEtLzDD1U8UQCfXXj/
Xzoj96e/8rd33K0DDZjQ3v0=
=9Dze
-----END PGP SIGNATURE-----

From zorg at sogeeky.net  Sat Jan 24 17:21:17 2009
From: zorg at sogeeky.net (Mr. Zorg)
Date: Sat, 24 Jan 2009 17:21:17 -0800
Subject: [zfs-discuss] endless scrub
In-Reply-To: <BC816926-44FF-436A-B1F7-4AC3DC008C80@ironsoftware.de>
References: <BC816926-44FF-436A-B1F7-4AC3DC008C80@ironsoftware.de>
Message-ID: <998E39FF-A2D7-401D-92E3-446201EB2E2F@sogeeky.net>

Others who have seen such behavior had snapshots scheduled in their  
crontab. Do you?  If so, try turning them off.

On Jan 24, 2009, at 2:32 PM, Christian Kendi <ksh at ironsoftware.de>  
wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hi,
>
> what could cause an endless scrub ?
>
> I started to scrub 14h ago and the drive is still scrubbing + the  
> progress seems to
> be reset from time to time. i never saw the scrub exceed 1%.
>
> NAME   USED  AVAIL  REFER  MOUNTPOINT
> home  66.5G  37.9G   360K  none
>
> ksh at kshs-Computer:~ $ zpool status
>  pool: home
> state: ONLINE
> scrub: scrub in progress, 1.92% done, 5h15m to go
> config:
>
>    NAME        STATE     READ WRITE CKSUM
>    home        ONLINE       0     0     0
>      disk2     ONLINE       0     0     0
>
> errors: No known data errors
> ksh at kshs-Computer:~ $ zpool status
>  pool: home
> state: ONLINE
> scrub: scrub in progress, 0.01% done, 103h9m to go
> config:
>
>    NAME        STATE     READ WRITE CKSUM
>    home        ONLINE       0     0     0
>      disk2     ONLINE       0     0     0
>
> errors: No known data errors
>
>
> Chris.
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.7 (Darwin)
>
> iD8DBQFJe5b/p+9ff145KVIRAi3oAJ4mEW+ZMHef9t6qOrnEtLzDD1U8UQCfXXj/
> Xzoj96e/8rd33K0DDZjQ3v0=
> =9Dze
> -----END PGP SIGNATURE-----
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From ksh at ironsoftware.de  Sat Jan 24 20:10:16 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Sat, 24 Jan 2009 23:10:16 -0500
Subject: [zfs-discuss] endless scrub
In-Reply-To: <998E39FF-A2D7-401D-92E3-446201EB2E2F@sogeeky.net>
References: <BC816926-44FF-436A-B1F7-4AC3DC008C80@ironsoftware.de>
	<998E39FF-A2D7-401D-92E3-446201EB2E2F@sogeeky.net>
Message-ID: <EEB4A546-78E7-4EB2-AFBA-C455DC62A7E9@ironsoftware.de>

Yes, i have. Thanks.
El Jan 24, 2009, a las 8:21 PM, Mr. Zorg escribi?:

> Others who have seen such behavior had snapshots scheduled in their  
> crontab. Do you?  If so, try turning them off.
>
> On Jan 24, 2009, at 2:32 PM, Christian Kendi <ksh at ironsoftware.de>  
> wrote:
>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> Hi,
>>
>> what could cause an endless scrub ?
>>
>> I started to scrub 14h ago and the drive is still scrubbing + the  
>> progress seems to
>> be reset from time to time. i never saw the scrub exceed 1%.
>>
>> NAME   USED  AVAIL  REFER  MOUNTPOINT
>> home  66.5G  37.9G   360K  none
>>
>> ksh at kshs-Computer:~ $ zpool status
>> pool: home
>> state: ONLINE
>> scrub: scrub in progress, 1.92% done, 5h15m to go
>> config:
>>
>>   NAME        STATE     READ WRITE CKSUM
>>   home        ONLINE       0     0     0
>>     disk2     ONLINE       0     0     0
>>
>> errors: No known data errors
>> ksh at kshs-Computer:~ $ zpool status
>> pool: home
>> state: ONLINE
>> scrub: scrub in progress, 0.01% done, 103h9m to go
>> config:
>>
>>   NAME        STATE     READ WRITE CKSUM
>>   home        ONLINE       0     0     0
>>     disk2     ONLINE       0     0     0
>>
>> errors: No known data errors
>>
>>
>> Chris.
>> -----BEGIN PGP SIGNATURE-----
>> Version: GnuPG v1.4.7 (Darwin)
>>
>> iD8DBQFJe5b/p+9ff145KVIRAi3oAJ4mEW+ZMHef9t6qOrnEtLzDD1U8UQCfXXj/
>> Xzoj96e/8rd33K0DDZjQ3v0=
>> =9Dze
>> -----END PGP SIGNATURE-----
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090124/a2a5b078/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: Mensaje firmado digitalmente
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090124/a2a5b078/attachment.bin>

From alex.blewitt at gmail.com  Sun Jan 25 02:30:40 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Sun, 25 Jan 2009 10:30:40 +0000
Subject: [zfs-discuss] endless scrub
In-Reply-To: <EEB4A546-78E7-4EB2-AFBA-C455DC62A7E9@ironsoftware.de>
References: <BC816926-44FF-436A-B1F7-4AC3DC008C80@ironsoftware.de>
	<998E39FF-A2D7-401D-92E3-446201EB2E2F@sogeeky.net>
	<EEB4A546-78E7-4EB2-AFBA-C455DC62A7E9@ironsoftware.de>
Message-ID: <39CA9DC1-42AB-4EA7-BBBD-60FAABE0ED0D@gmail.com>

In my crontab, I explicitly look for a scrub completed message before  
doing an hourly snapshot. That way, the scrub can complete and  
snapshots automatically resume. I posted it on by blog  
(alblue.blogspot.com) a while back if you're interested.

Alex

Sent from my (new) iPhone

On 25 Jan 2009, at 04:10, Christian Kendi <ksh at ironsoftware.de> wrote:

> Yes, i have. Thanks.
>
> El Jan 24, 2009, a las 8:21 PM, Mr. Zorg escribi?:
>
>> Others who have seen such behavior had snapshots scheduled in their  
>> crontab. Do you?  If so, try turning them off.
>>
>> On Jan 24, 2009, at 2:32 PM, Christian Kendi <ksh at ironsoftware.de>  
>> wrote:
>>
>>> -----BEGIN PGP SIGNED MESSAGE-----
>>> Hash: SHA1
>>>
>>> Hi,
>>>
>>> what could cause an endless scrub ?
>>>
>>> I started to scrub 14h ago and the drive is still scrubbing + the  
>>> progress seems to
>>> be reset from time to time. i never saw the scrub exceed 1%.
>>>
>>> NAME   USED  AVAIL  REFER  MOUNTPOINT
>>> home  66.5G  37.9G   360K  none
>>>
>>> ksh at kshs-Computer:~ $ zpool status
>>> pool: home
>>> state: ONLINE
>>> scrub: scrub in progress, 1.92% done, 5h15m to go
>>> config:
>>>
>>>   NAME        STATE     READ WRITE CKSUM
>>>   home        ONLINE       0     0     0
>>>     disk2     ONLINE       0     0     0
>>>
>>> errors: No known data errors
>>> ksh at kshs-Computer:~ $ zpool status
>>> pool: home
>>> state: ONLINE
>>> scrub: scrub in progress, 0.01% done, 103h9m to go
>>> config:
>>>
>>>   NAME        STATE     READ WRITE CKSUM
>>>   home        ONLINE       0     0     0
>>>     disk2     ONLINE       0     0     0
>>>
>>> errors: No known data errors
>>>
>>>
>>> Chris.
>>> -----BEGIN PGP SIGNATURE-----
>>> Version: GnuPG v1.4.7 (Darwin)
>>>
>>> iD8DBQFJe5b/p+9ff145KVIRAi3oAJ4mEW+ZMHef9t6qOrnEtLzDD1U8UQCfXXj/
>>> Xzoj96e/8rd33K0DDZjQ3v0=
>>> =9Dze
>>> -----END PGP SIGNATURE-----
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090125/ff9ad31b/attachment.html>

From ksh at ironsoftware.de  Sun Jan 25 09:49:19 2009
From: ksh at ironsoftware.de (Christian Kendi)
Date: Sun, 25 Jan 2009 12:49:19 -0500
Subject: [zfs-discuss] endless scrub
In-Reply-To: <39CA9DC1-42AB-4EA7-BBBD-60FAABE0ED0D@gmail.com>
References: <BC816926-44FF-436A-B1F7-4AC3DC008C80@ironsoftware.de>
	<998E39FF-A2D7-401D-92E3-446201EB2E2F@sogeeky.net>
	<EEB4A546-78E7-4EB2-AFBA-C455DC62A7E9@ironsoftware.de>
	<39CA9DC1-42AB-4EA7-BBBD-60FAABE0ED0D@gmail.com>
Message-ID: <0CD8FD26-4B45-4477-8C54-66D503EE0EA7@ironsoftware.de>

actually good to know. Is this a limitation to the Mac port or is  
solaris having the same problem?

zpool status home | grep scrub | egrep -q "in progress"
if [ $? -eq 0 ]; then
         echo "Can't snapshot due to running scrub"
         exit 1;
fi

will do the trick.

Chris.


El Jan 25, 2009, a las 5:30 AM, Alex Blewitt escribi?:

> In my crontab, I explicitly look for a scrub completed message  
> before doing an hourly snapshot. That way, the scrub can complete  
> and snapshots automatically resume. I posted it on by blog  
> (alblue.blogspot.com) a while back if you're interested.
>
> Alex
>
> Sent from my (new) iPhone
>
> On 25 Jan 2009, at 04:10, Christian Kendi <ksh at ironsoftware.de> wrote:
>
>> Yes, i have. Thanks.
>>
>> El Jan 24, 2009, a las 8:21 PM, Mr. Zorg escribi?:
>>
>>> Others who have seen such behavior had snapshots scheduled in  
>>> their crontab. Do you?  If so, try turning them off.
>>>
>>> On Jan 24, 2009, at 2:32 PM, Christian Kendi <ksh at ironsoftware.de>  
>>> wrote:
>>>
>>>> -----BEGIN PGP SIGNED MESSAGE-----
>>>> Hash: SHA1
>>>>
>>>> Hi,
>>>>
>>>> what could cause an endless scrub ?
>>>>
>>>> I started to scrub 14h ago and the drive is still scrubbing + the  
>>>> progress seems to
>>>> be reset from time to time. i never saw the scrub exceed 1%.
>>>>
>>>> NAME   USED  AVAIL  REFER  MOUNTPOINT
>>>> home  66.5G  37.9G   360K  none
>>>>
>>>> ksh at kshs-Computer:~ $ zpool status
>>>> pool: home
>>>> state: ONLINE
>>>> scrub: scrub in progress, 1.92% done, 5h15m to go
>>>> config:
>>>>
>>>>   NAME        STATE     READ WRITE CKSUM
>>>>   home        ONLINE       0     0     0
>>>>     disk2     ONLINE       0     0     0
>>>>
>>>> errors: No known data errors
>>>> ksh at kshs-Computer:~ $ zpool status
>>>> pool: home
>>>> state: ONLINE
>>>> scrub: scrub in progress, 0.01% done, 103h9m to go
>>>> config:
>>>>
>>>>   NAME        STATE     READ WRITE CKSUM
>>>>   home        ONLINE       0     0     0
>>>>     disk2     ONLINE       0     0     0
>>>>
>>>> errors: No known data errors
>>>>
>>>>
>>>> Chris.
>>>> -----BEGIN PGP SIGNATURE-----
>>>> Version: GnuPG v1.4.7 (Darwin)
>>>>
>>>> iD8DBQFJe5b/p+9ff145KVIRAi3oAJ4mEW+ZMHef9t6qOrnEtLzDD1U8UQCfXXj/
>>>> Xzoj96e/8rd33K0DDZjQ3v0=
>>>> =9Dze
>>>> -----END PGP SIGNATURE-----
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090125/1e37dc12/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: Mensaje firmado digitalmente
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090125/1e37dc12/attachment.bin>

From alex.blewitt at gmail.com  Sun Jan 25 13:17:16 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Sun, 25 Jan 2009 21:17:16 +0000
Subject: [zfs-discuss] endless scrub
In-Reply-To: <0CD8FD26-4B45-4477-8C54-66D503EE0EA7@ironsoftware.de>
References: <BC816926-44FF-436A-B1F7-4AC3DC008C80@ironsoftware.de>
	<998E39FF-A2D7-401D-92E3-446201EB2E2F@sogeeky.net>
	<EEB4A546-78E7-4EB2-AFBA-C455DC62A7E9@ironsoftware.de>
	<39CA9DC1-42AB-4EA7-BBBD-60FAABE0ED0D@gmail.com>
	<0CD8FD26-4B45-4477-8C54-66D503EE0EA7@ironsoftware.de>
Message-ID: <636fd28e0901251317u5d3f53d6h11b006de8a296f44@mail.gmail.com>

On Sun, Jan 25, 2009 at 5:49 PM, Christian Kendi <ksh at ironsoftware.de> wrote:
> actually good to know. Is this a limitation to the Mac port or is solaris
> having the same problem?
> zpool status home | grep scrub | egrep -q "in progress"
> if [ $? -eq 0 ]; then
>         echo "Can't snapshot due to running scrub"
>         exit 1;
> fi
> will do the trick.
> Chris.

It was a bug in Solaris, but fixed in a version (80?) behind where we
are with the (public) ZFS port (72). So it's still a known bug with no
hope of fixing in the Leopard time schedule, since the public
development has gone private :-(

PS Mike (http://www.blogger.com/profile/17454659201337993537)
commented on my blog
(http://alblue.blogspot.com/2008/11/crontab-generated-zfs-snapshots.html)
that you need to also look for 'none requested', since that'll be the
state of a new pool and otherwise you won't see the snapshots ever
starting, or at least until the next time a nightly scrub happens:

/usr/bin/egrep -q "scrub completed|none requested"

Alex

From vivacarlie at gmail.com  Sun Jan 25 18:27:06 2009
From: vivacarlie at gmail.com (Nehemiah Dacres)
Date: Sun, 25 Jan 2009 20:27:06 -0600
Subject: [zfs-discuss] TimeMachine (Again)
In-Reply-To: <0FA47369-EF6C-4323-8172-2703E2EA3DF0@sbod.at>
References: <41E7C461-E79A-4850-A0D2-441D94DA7CD8@barryfamily.org>
	<0424A847-3779-4DEB-AC91-9A30B6114A7C@sbod.at>
	<1DA597A0-A95E-4927-8A32-53B6949F1A67@ironsoftware.de>
	<8B2846B2-1A4D-4AB1-B88A-250CA48E6995@sbod.at>
	<0FA47369-EF6C-4323-8172-2703E2EA3DF0@sbod.at>
Message-ID: <65fadfc30901251827h1b3bd7f0gd614dc24a1d57c42@mail.gmail.com>

http://www.studionetworksolutions.com/products/product_detail.php?t=more&pi=11

On Tue, Jan 13, 2009 at 2:19 PM, Franz Schmalzl <zfs at sbod.at> wrote:

> Ok, now i've got to ask:
>
> Is there a way to set up an iSCSI target using Mac OS X ?
>
>
> Thanks,
>
> Franz
>
>
>
>
>
> On 13.01.2009, at 21:10, Franz Schmalzl wrote:
>
>  whoops, sorry guys...
>>
>> This happens to me quite often ;)
>>
>> I said:
>>
>>
>> Now that's a solution!
>>
>> Thank you! Shame on me i didn't know what iSCSI was...
>>
>> I'll take a look
>>
>>
>> Franz
>>
>>
>>
>> ( Free translation :)
>>
>>
>>
>> Cheers,
>>
>>
>> Franz _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>



-- 

"lalalalala! it's not broken because I can use it"

http://linux.slashdot.org/comments.pl?sid=194281&threshold=1&commentsort=0&mode=thread&cid=15927703
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090125/872ae59c/attachment.html>

From vivacarlie at gmail.com  Mon Jan 26 15:05:04 2009
From: vivacarlie at gmail.com (Nehemiah Dacres)
Date: Mon, 26 Jan 2009 17:05:04 -0600
Subject: [zfs-discuss] deleted directories and files
In-Reply-To: <1671B563-344C-455F-B1D7-E25DA23308C4@mac.com>
References: <1671B563-344C-455F-B1D7-E25DA23308C4@mac.com>
Message-ID: <65fadfc30901261505m549363b9yd2e876085de2018c@mail.gmail.com>

On Fri, Jan 16, 2009 at 8:53 PM, Dirk Schelfhout <dirkschelfhout at mac.com>wrote:

> Stupidly ran an rsync with --delete-after to the root of my zfs
> with the wrong syntax. So the rsync started to delete all folders in
> my root directory on zfs that did not belong to the copy.
> I stopped it but still lost a lot of files.
> I recently removed all my snapshots to free up space.
> Is there any way I can get those files back ?
>
> Dirk
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
you removed ALL your snapshots? if you had at least one snapshot from any
time that those files existed there might be a chance. If you removed all of
them, you may be missing the point of snapshots.

"I keep my feathers numbered for just such an ocasion"
 -- Foghorn Leghorn

-- 

"lalalalala! it's not broken because I can use it"

http://linux.slashdot.org/comments.pl?sid=194281&threshold=1&commentsort=0&mode=thread&cid=15927703
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090126/51710427/attachment.html>

From richmc at gmail.com  Wed Jan 28 19:55:09 2009
From: richmc at gmail.com (Richard McClellan)
Date: Wed, 28 Jan 2009 19:55:09 -0800
Subject: [zfs-discuss] iMovie 09 and ZFS problem
Message-ID: <9B653A4F-07C2-4794-868E-2A31150DC23A@gmail.com>

I've no idea who's responsible and understand there's little chance  
the public version of ZFS will see this as a priority, but I thought  
I'd mention that iMovie 09 cannot see the contents of my ZFS pools.  
(iMovie 08 and Premiere CS4 can see them, BTW.)  ZFS pools/partitions  
show up in iMovie 09's Event Library listing but there is an  
exclamation point on the hard drive (yellow triangle background) and  
the contents are empty.

		Rich

From mattsnow at gmail.com  Wed Jan 28 21:04:27 2009
From: mattsnow at gmail.com (Matt Snow)
Date: Wed, 28 Jan 2009 21:04:27 -0800
Subject: [zfs-discuss] iMovie 09 and ZFS problem
In-Reply-To: <9B653A4F-07C2-4794-868E-2A31150DC23A@gmail.com>
References: <9B653A4F-07C2-4794-868E-2A31150DC23A@gmail.com>
Message-ID: <6879ebc80901282104x2802a94dmb54214804c39934d@mail.gmail.com>

How did you get imovie 08 to work? I have 5x SATA drives directly attached
in a RAIDZ and I don't see my zpool when I try to capture data from my Canon
HV30 camera. I have to save the data to my root disk, then move it to the
zpool. I was hoping 09 ould fix this. :(

..Matt


On Wed, Jan 28, 2009 at 7:55 PM, Richard McClellan <richmc at gmail.com> wrote:

> I've no idea who's responsible and understand there's little chance the
> public version of ZFS will see this as a priority, but I thought I'd mention
> that iMovie 09 cannot see the contents of my ZFS pools. (iMovie 08 and
> Premiere CS4 can see them, BTW.)  ZFS pools/partitions show up in iMovie
> 09's Event Library listing but there is an exclamation point on the hard
> drive (yellow triangle background) and the contents are empty.
>
>                Rich
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090128/149d4616/attachment.html>

From dirkschelfhout at mac.com  Wed Jan 28 21:05:25 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Thu, 29 Jan 2009 06:05:25 +0100
Subject: [zfs-discuss] [follow-up]Re: DiskManagement setuid-tool failure
 on zfs disks
In-Reply-To: <F2A01304-425D-4CA0-98A1-E6CEA7D5CE6D@mac.com>
References: <2F84F764-75A3-4C31-8ACA-23A456F01849@mac.com>
	<F6827D21-6DD6-445E-81E1-AF6A69D417E6@mac.com>
	<980A54B0-418E-456F-8948-4CE5F6E7D536@thinkpink.com>
	<1322B81D-A491-49CB-AD9B-1BAA95A5EC2A@mac.com>
	<0A73E310-D981-4C2C-87EC-6E8E30CFE92D@thinkpink.com>
	<07E50A73-0018-4651-B5FB-6C7E0D0BDD76@mac.com>
	<0B09BBDF-F45F-4F08-9850-E5E064C1E55B@mac.com>
	<ED91D068-B965-40C1-AFA3-CB2BD509B094@mac.com>
	<E46D86C1-2110-4C88-816E-8EBA9D899C26@mac.com>
	<F2A01304-425D-4CA0-98A1-E6CEA7D5CE6D@mac.com>
Message-ID: <9AE39165-05B7-4C10-BED8-14F321CA3BE2@mac.com>

Hi,
I reformatted all my zfs drives again. ( create partition table )
now the problem is gone.
Dirk
On 15 Jan 2009, at 15:21, Dirk Schelfhout wrote:

> When I enable core dumping and load into gdb :
>
> #0  0x915fdf45 in strlcpy ()
> (gdb) where full
> #0  0x915fdf45 in strlcpy ()
> No symbol table info available.
> #1  0x00092555 in GPTReadMedia ()
> No symbol table info available.
>
> Can anyone from apple point me in the right direction ?
>
> I can't find the darwin source for diskutil
>
> My next step is to reformat my zfs disks, but I would rather not.
> ( It takes a long time....... ( resilvering.... ))
>
> Dirk
> On 14 Jan 2009, at 21:55, Dirk Schelfhout wrote:
>
>> I just downloaded the 10.5.6 combo update and installed.
>> Problem is still the same....
>> On 14 Jan 2009, at 20:30, Dirk Schelfhout wrote:
>>
>>> This started to fail with the update to 10.5.6
>>> So I suppose other people must have this problem as well.
>>> I reinstalled zfs today, so that is not what is doing it.
>>> I can't even use disk utility to look at the partition table of a  
>>> disk used for zfs !!
>>> Can anyone else check this please ?
>>> Dirk
>>> On 14 Jan 2009, at 20:26, Dirk Schelfhout wrote:
>>>>>>>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dirkschelfhout at mac.com  Wed Jan 28 21:20:44 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Thu, 29 Jan 2009 06:20:44 +0100
Subject: [zfs-discuss] adding a disk to raidz
Message-ID: <E416616D-50CE-452F-8F7E-E0B2722D23A5@mac.com>

Hi,

This could be usefull for somebody........
( I have 5 disks in the macpro. hfsplus to boot from sitting in the  
extra cd tray.
3 for raidz. 1 for backup off the boot disk plus windowsxp ( for games  
only...... , ge us some games steve !!!!! ) )

I needed some more disk space for windows on my macpro,
so I bought a WD10EADS with the plan to use about 500 gigs of it to  
add to my raidz.

This is about what I did :
1) remove the old disk ( windows xp and some other stuff ), replace  
with the new disk
2) create a zfs mirror on the new disk.
3) send a snapshot from raidz to the mirror
4) create a ufs file system on a usb stick, put a 500 gig sparse file  
on it.
5) destroy the raidz pool
6) create a new raidz with the 3 disks plus the sparse file on the ufs  
file system
( system crashed on this , upon reboot the ufs file was unavailable  
( no problem see below ) )
( my plan was to put the ufs file offline , but didn't need to do that )
7) send the snapshot from the mirror to the raidz
8 ) destroy the mirror pool
9 ) replace the ufs file with 1 of the zfs partitions from the new disk
10 ) let it resilver
11) split the other zfs partition on the new disk in a few
12 ) now running winclone to put xp on that disk .....

Only problem is that all my files are sitting in : /Volumes/backup/ 
backup
instead of /Volumes/backup
Maybe I should have done something different, I will just move the  
files later.

I attach what I did on the terminal to this mail ( except the non zfs  
stuff at the end )

-------------- next part --------------
A non-text attachment was scrubbed...
Name: recreate zfs raidz.rtf
Type: text/rtf
Size: 10153 bytes
Desc: not available
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090129/d8997efa/attachment.bin>
-------------- next part --------------


From richmc at gmail.com  Wed Jan 28 21:39:52 2009
From: richmc at gmail.com (Richard McClellan)
Date: Wed, 28 Jan 2009 21:39:52 -0800
Subject: [zfs-discuss] iMovie 09 and ZFS problem
In-Reply-To: <6879ebc80901282104x2802a94dmb54214804c39934d@mail.gmail.com>
References: <9B653A4F-07C2-4794-868E-2A31150DC23A@gmail.com>
	<6879ebc80901282104x2802a94dmb54214804c39934d@mail.gmail.com>
Message-ID: <46BF315B-8FBA-466F-A510-2B6060A526FD@gmail.com>

I didn't do anything special to make iMovie 08 work.  It just worked.
		
		Rich

On Jan 28, 2009, at 21:04 , Matt Snow wrote:

> How did you get imovie 08 to work? I have 5x SATA drives directly  
> attached in a RAIDZ and I don't see my zpool when I try to capture  
> data from my Canon HV30 camera. I have to save the data to my root  
> disk, then move it to the zpool. I was hoping 09 ould fix this. :(
>
> ..Matt
>
>
> On Wed, Jan 28, 2009 at 7:55 PM, Richard McClellan  
> <richmc at gmail.com> wrote:
> I've no idea who's responsible and understand there's little chance  
> the public version of ZFS will see this as a priority, but I thought  
> I'd mention that iMovie 09 cannot see the contents of my ZFS pools.  
> (iMovie 08 and Premiere CS4 can see them, BTW.)  ZFS pools/ 
> partitions show up in iMovie 09's Event Library listing but there is  
> an exclamation point on the hard drive (yellow triangle background)  
> and the contents are empty.
>
>                Rich
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090128/b9ad2f54/attachment-0001.html>

From slgarwood at charter.net  Thu Jan 29 05:31:01 2009
From: slgarwood at charter.net (Sterling Garwood)
Date: Thu, 29 Jan 2009 08:31:01 -0500
Subject: [zfs-discuss] do you upgrade your pools?
Message-ID: <D651C462-BDBB-427C-8A8E-FAF590C59DFC@charter.net>

I have made it a habit to upgrade an new pools before I use them. The  
developer code is IIRC zfs release 8 but new pools are created at  
release 6.
My theory is newer is better. Anyone else do this as a practice? What  
are the tradeoffs of doing it versus not?

From hanche at math.ntnu.no  Thu Jan 29 05:45:53 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Thu, 29 Jan 2009 14:45:53 +0100 (CET)
Subject: [zfs-discuss] do you upgrade your pools?
In-Reply-To: <D651C462-BDBB-427C-8A8E-FAF590C59DFC@charter.net>
References: <D651C462-BDBB-427C-8A8E-FAF590C59DFC@charter.net>
Message-ID: <20090129.144553.1049078534823934866.hanche@math.ntnu.no>

+ Sterling Garwood <slgarwood at charter.net>:

> I have made it a habit to upgrade an new pools before I use them. The
> developer code is IIRC zfs release 8 but new pools are created at
> release 6.
> My theory is newer is better. Anyone else do this as a practice? What
> are the tradeoffs of doing it versus not?

I don't, because I move the pools between macos and freebsd, and the
latter is still at zfs release 6. Other than that, I don't know of any
reason not to upgrade.

- Harald

From mathieu.lebon at gmail.com  Thu Jan 15 13:31:01 2009
From: mathieu.lebon at gmail.com (mathieu.email)
Date: Thu, 15 Jan 2009 22:31:01 +0100
Subject: [zfs-discuss] Zpool disaster
In-Reply-To: <0EB4C19C-345D-4C15-98DE-A563DCDB8B70@erbco.com>
References: <88975A88-D238-4FF1-ABD3-680C69FDAD92@gmail.com>
	<0EB4C19C-345D-4C15-98DE-A563DCDB8B70@erbco.com>
Message-ID: <53A8B2DF-0EF6-4257-8663-9787C683EC17@gmail.com>

Thanks for your replies,

I looked in the /var/log/system.log :

- When I plug the drive (external usb drive), I have : "  
diskarbitrationd[32]: unable to mount /dev/disk2s2 (status code  
0x00000001). "
- With the "zpool import" command, nothing appears.

I changed the usb cable, but the problem is still the same (I don't  
have another power supply for the moment).

I tried "zpool import -f", the error is the same :

=> zpool import -f

pool: ZFS
      id: 9004030332584099627
state: FAULTED
status: The pool metadata is corrupted.
action: The pool cannot be imported due to damaged devices or data.
            The pool may be active on on another system, but can be  
imported using
            the '-f' flag.
   see: http://www.sun.com/msg/ZFS-8000-72
config:

	ZFS            FAULTED  corrupted data
	 disk2s2   ONLINE

pool: ZFS
      id: 5050959592823553345
state: FAULTED
status: The pool was last accessed by another system.
action: The pool cannot be imported due to damaged devices or data.
   see: http://www.sun.com/msg/ZFS-8000-EY
config:

	ZFS         UNAVAIL  insufficient replicas
	 disk1     UNAVAIL  cannot open


Mathieu

Le 09-01-15 ? 00:30, Dustin Webb a ?crit :

> You should be able to simply force the import.  Since your system  
> froze, it never properly "ejected" the pool.  The command is 'zpool  
> import -f' or 'zpool -f import'
>
> Don't Panic!
>
> Dustin Webb
> ITguy
> Erb Co Inc
>
>
> On Jan 5, 2009, at 4:53 PM, mathieu.email wrote:
>
>> Hi,
>>
>> I have a big problem with my ZFS disk. After a kernel panic, I  
>> cannot import the pool anymore :
>>
>> ------------------------------------------------------------------------------------------------------------------
>> => zpool status
>> no pools available
>> => zpool list
>> no pools available
>>
>> ------------------------------------------------------------------------------------------------------------------
>> => zpool import
>>
>> pool: ZFS
>>      id: 9004030332584099627
>> state: FAULTED
>> status: The pool metadata is corrupted.
>> action: The pool cannot be imported due to damaged devices or data.
>>            The pool may be active on on another system, but can be  
>> imported using
>>            the '-f' flag.
>>   see: http://www.sun.com/msg/ZFS-8000-72
>> config:
>>
>> 	ZFS            FAULTED  corrupted data
>> 	  disk2s2   ONLINE
>>
>> pool: ZFS
>>      id: 5050959592823553345
>> state: FAULTED
>> status: The pool was last accessed by another system.
>> action: The pool cannot be imported due to damaged devices or data.
>>   see: http://www.sun.com/msg/ZFS-8000-EY
>> config:
>>
>> 	ZFS         UNAVAIL  insufficient replicas
>> 	  disk1     UNAVAIL  cannot open
>>
>> ------------------------------------------------------------------------------------------------------------------
>> => zpool import -f 9004030332584099627
>> cannot import 'ZFS': I/O error
>> ------------------------------------------------------------------------------------------------------------------
>>
>> I am despaired as I have no backup and all my data are on this drive.
>>
>> Is there anything I can do ?
>>
>> Thank you for your help,
>>
>> Mathieu
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090115/2190c505/attachment.html>

From colin_s_johnson at comcast.net  Thu Jan 29 21:59:08 2009
From: colin_s_johnson at comcast.net (Colin Johnson)
Date: Thu, 29 Jan 2009 21:59:08 -0800
Subject: [zfs-discuss] Removing the binary package?
Message-ID: <CC7F936F-DE83-4F0D-9C9F-8CBC1BA754D5@comcast.net>

I installed the binary package and after only one day running my  
raidz2 array faulted when there was nothing wrong with the hardware.
I don't need to try this until it's way more stable.
How do I remove ZFS support?

Thanks

Colin

From dirkschelfhout at mac.com  Thu Jan 29 22:02:49 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Fri, 30 Jan 2009 07:02:49 +0100
Subject: [zfs-discuss] zpool offline problem
Message-ID: <520FB1ED-D7A5-4CD8-9377-A369E911C36A@mac.com>

After replacing a disk and booting up, the disk device numbers have  
changed.
So know I get this annoying output :

zpool status
   pool: backup
  state: DEGRADED
status: One or more devices has been taken offline by the administrator.
	Sufficient replicas exist for the pool to continue functioning in a
	degraded state.
action: Online the device using 'zpool online' or replace the device  
with
	'zpool replace'.
  scrub: resilver completed with 0 errors on Fri Jan 30 07:00:57 2009
config:

	NAME         STATE     READ WRITE CKSUM
	backup       DEGRADED     0     0     0
	  raidz1     DEGRADED     0     0     0
	    disk1s2  ONLINE       0     0     0
	    disk3s2  ONLINE       0     0     0
	    disk2s2  ONLINE       0     0     0
	    disk1s2  OFFLINE      0     0     0

errors: No known data errors


The only thing different is that I am now not booted through refit.

Should I try a replace on disk1s2 ?

Any other way to do this ?

Dirk

From hanche at math.ntnu.no  Thu Jan 29 23:33:05 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Fri, 30 Jan 2009 08:33:05 +0100 (CET)
Subject: [zfs-discuss] Removing the binary package?
In-Reply-To: <CC7F936F-DE83-4F0D-9C9F-8CBC1BA754D5@comcast.net>
References: <CC7F936F-DE83-4F0D-9C9F-8CBC1BA754D5@comcast.net>
Message-ID: <20090130.083305.34775001.hanche@math.ntnu.no>

+ Colin Johnson <colin_s_johnson at comcast.net>:

> How do I remove ZFS support?

Why do you need to? It does little or no harm if you don't use it.

But if you want to remove every trace of zfs support, throw away these
files and directories:

/System/Library/Extensions/zfs.kext/
/System/Library/Extensions/zfs.readonly.kext/
/System/Library/Filesystems/zfs.fs/
/usr/lib/libzfs.dylib
/usr/sbin/zfs
/usr/sbin/zpool

(Note that zfs.readonly.kext did not come with the binary package
though; it came with the original Leopard system.)

- Harald

From hanche at math.ntnu.no  Thu Jan 29 23:36:37 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Fri, 30 Jan 2009 08:36:37 +0100 (CET)
Subject: [zfs-discuss] zpool offline problem
In-Reply-To: <520FB1ED-D7A5-4CD8-9377-A369E911C36A@mac.com>
References: <520FB1ED-D7A5-4CD8-9377-A369E911C36A@mac.com>
Message-ID: <20090130.083637.51181845.hanche@math.ntnu.no>

+ Dirk Schelfhout <dirkschelfhout at mac.com>:

> After replacing a disk and booting up, the disk device numbers have changed.

I think exporting, then importing the filesystem should get things
back in sync.

- Harald

From dirkschelfhout at mac.com  Fri Jan 30 06:04:29 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Fri, 30 Jan 2009 15:04:29 +0100
Subject: [zfs-discuss] zpool offline problem
In-Reply-To: <520FB1ED-D7A5-4CD8-9377-A369E911C36A@mac.com>
References: <520FB1ED-D7A5-4CD8-9377-A369E911C36A@mac.com>
Message-ID: <E27ED5CE-E92F-401B-9192-3F82D0E3EB54@mac.com>

Can I hot plug sata disks inside the macpro ?
This way I can force the device nrs to be the way I want them. ( I  
think )
I would boot without the 3 sata disks that make up the raidz.
Plug them in after I verified the device nrs of the other disks.

What would happen if I ran :
zpool replace backup disk1s2 disk0s2
( if disk0s2 was the new zfs partiton )
Would it try to replace the offline disk ?
Dirk
On 30 Jan 2009, at 07:02, Dirk Schelfhout wrote:

> After replacing a disk and booting up, the disk device numbers have  
> changed.
> So know I get this annoying output :
>
> zpool status
>  pool: backup
> state: DEGRADED
> status: One or more devices has been taken offline by the  
> administrator.
> 	Sufficient replicas exist for the pool to continue functioning in a
> 	degraded state.
> action: Online the device using 'zpool online' or replace the device  
> with
> 	'zpool replace'.
> scrub: resilver completed with 0 errors on Fri Jan 30 07:00:57 2009
> config:
>
> 	NAME         STATE     READ WRITE CKSUM
> 	backup       DEGRADED     0     0     0
> 	  raidz1     DEGRADED     0     0     0
> 	    disk1s2  ONLINE       0     0     0
> 	    disk3s2  ONLINE       0     0     0
> 	    disk2s2  ONLINE       0     0     0
> 	    disk1s2  OFFLINE      0     0     0
>
> errors: No known data errors
>
>
> The only thing different is that I am now not booted through refit.
>
> Should I try a replace on disk1s2 ?
>
> Any other way to do this ?
>
> Dirk
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dirkschelfhout at mac.com  Fri Jan 30 10:10:12 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Fri, 30 Jan 2009 19:10:12 +0100
Subject: [zfs-discuss] (fixed) Re:  zpool offline problem
In-Reply-To: <E27ED5CE-E92F-401B-9192-3F82D0E3EB54@mac.com>
References: <520FB1ED-D7A5-4CD8-9377-A369E911C36A@mac.com>
	<E27ED5CE-E92F-401B-9192-3F82D0E3EB54@mac.com>
Message-ID: <60B2A2C2-648D-4C18-91E0-2FCBDF14BE7E@mac.com>

I swapped the disks around in the bays until
the online disks were using disk0 disk2 and disk3.
Maybe some luck was involved as well.
resilvering now.
On 30 Jan 2009, at 15:04, Dirk Schelfhout wrote:

> Can I hot plug sata disks inside the macpro ?
> This way I can force the device nrs to be the way I want them. ( I  
> think )
> I would boot without the 3 sata disks that make up the raidz.
> Plug them in after I verified the device nrs of the other disks.
>
> What would happen if I ran :
> zpool replace backup disk1s2 disk0s2
> ( if disk0s2 was the new zfs partiton )
> Would it try to replace the offline disk ?
> Dirk
> On 30 Jan 2009, at 07:02, Dirk Schelfhout wrote:
>
>> After replacing a disk and booting up, the disk device numbers have  
>> changed.
>> So know I get this annoying output :
>>
>> zpool status
>> pool: backup
>> state: DEGRADED
>> status: One or more devices has been taken offline by the  
>> administrator.
>> 	Sufficient replicas exist for the pool to continue functioning in a
>> 	degraded state.
>> action: Online the device using 'zpool online' or replace the  
>> device with
>> 	'zpool replace'.
>> scrub: resilver completed with 0 errors on Fri Jan 30 07:00:57 2009
>> config:
>>
>> 	NAME         STATE     READ WRITE CKSUM
>> 	backup       DEGRADED     0     0     0
>> 	  raidz1     DEGRADED     0     0     0
>> 	    disk1s2  ONLINE       0     0     0
>> 	    disk3s2  ONLINE       0     0     0
>> 	    disk2s2  ONLINE       0     0     0
>> 	    disk1s2  OFFLINE      0     0     0
>>
>> errors: No known data errors
>>
>>
>> The only thing different is that I am now not booted through refit.
>>
>> Should I try a replace on disk1s2 ?
>>
>> Any other way to do this ?
>>
>> Dirk
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


