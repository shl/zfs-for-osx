From hanche at math.ntnu.no  Mon Dec  1 01:10:17 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Mon, 01 Dec 2008 10:10:17 +0100 (CET)
Subject: [zfs-discuss] zfs and kernel memory?
Message-ID: <20081201.101017.447857017412032666.hanche@math.ntnu.no>

I'm wondering if the 119 zfs release uses (much) more kernel memory
than the original leopard one?

The reason I am asking is this: I installed 119 on my wife's old 12
inch powerbook (with 640 MB of RAM), since we have some removable
disks with zfs on them. Since then, applications (notably, Firefox and
Mail) have started dying from malloc errors.

It doesn't make much sense since virtual memory is limited by free
disk space, and there is over ten gigabytes of free space on the
disk. Also, Activity Monitor shows plenty (~ 200 MB) of free memory.

I know from experience on freebsd that zfs is quite hungry for kernel
memory, and you need to do a bit of tuning to make it work reliably.
Is something similar true on macosx? I see nothing about it the FAQ.
Is using zfs on a 640 MB machine perhaps asking too much?

- Harald

From alex.blewitt at gmail.com  Mon Dec  1 01:42:24 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Mon, 1 Dec 2008 09:42:24 +0000
Subject: [zfs-discuss] Automatic scrub?
In-Reply-To: <636fd28e0811300535h712f4d43v4a4f484275130e2@mail.gmail.com>
References: <636fd28e0811281106g687d68d9y3ddafb7cbd8b9cd0@mail.gmail.com>
	<6879ebc80811291120q8add2a0o9b5f7f76e31fce52@mail.gmail.com>
	<A107989A-1CDE-42EB-8B0A-86265AB3FB00@gmail.com>
	<636fd28e0811291243s492b5e4j59df6d4928a7a05e@mail.gmail.com>
	<493200BA.70408@loveturtle.net>
	<636fd28e0811300535h712f4d43v4a4f484275130e2@mail.gmail.com>
Message-ID: <636fd28e0812010142o59f31b1fj623c211358eac8a2@mail.gmail.com>

On Sun, Nov 30, 2008 at 1:35 PM, Alex Blewitt <alex.blewitt at gmail.com> wrote:
> On Sun, Nov 30, 2008 at 2:55 AM, Dillon Kass <lists at loveturtle.net> wrote:
>> On 11/29/08 3:43 PM, Alex Blewitt wrote:
>>>
>>> I wonder if it doesn't have time to finish the
>>> nightly scrub between 00:00 and 01:00, and thus at 01:00 re-starts the
>>> scrubbing, ad infinitum?
>>
>> Sounds like exactly what is happening to me. I believe that was fixed in OS
>> a little while ago...We'll probably get that when ever we get the latest
>> bits.
>
> Yeah,I turned off the hourly snapshot and the nightly scrub took an
> hour and a half, completed with no problems. Oh well, I'll just tweak
> my crontab to give it a 2h window overnight.

For those that are interested, it worked:

2008-12-01.00:00:00 zpool scrub Data
2008-12-01.00:00:17 zfs snapshot -r Data at AutoH-2008-12-01T00:00
2008-12-01.00:00:17 zfs snapshot -r Data at AutoD-2008-12-01
2008-12-01.00:00:18 zfs snapshot -r Data at AutoM-2008-12
2008-12-01.00:00:20 zfs destroy -r Data at AutoH-2008-11-29T06:00
2008-12-01.00:00:21 zfs destroy -r Data at AutoD-2008-11-23
2008-12-01.02:00:01 zfs snapshot -r Data at AutoH-2008-12-01T02:00
2008-12-01.02:00:05 zfs destroy -r Data at AutoH-2008-11-29T07:00

My system is in a scrub-free status from about 1:30am onwards:

mini:~ alex$ zpool status
  pool: Data
 state: ONLINE
 scrub: scrub completed with 0 errors on Mon Dec  1 01:30:52 2008

I achieved it by adding a precondition to the hourly run:

@hourly	/usr/sbin/zpool status Data | /usr/bin/grep -q "scrub
completed" && /usr/sbin/zfs snapshot -r Data at AutoH-`date
+"\%FT\%H:\%M"`

HTH,

Alex

From lopez.on.the.lists at yellowspace.net  Mon Dec  1 06:02:10 2008
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Mon, 1 Dec 2008 15:02:10 +0100
Subject: [zfs-discuss] Places home icon showing as pool name,
	not user name
In-Reply-To: <A2C169F4-5994-4B3F-825C-375F5D852A1F@apple.com>
References: <636fd28e0811171305n1e934009n7d8b1e98cb44d4a4@mail.gmail.com>
	<A2C169F4-5994-4B3F-825C-375F5D852A1F@apple.com>
Message-ID: <F843E47D-2D9D-4250-93E2-2311F86C03B0@yellowspace.net>


On 17.11.2008, at 23:36, No?l Dellofano wrote:

> This bug has been fixed recently.  We were reporting the spa name  
> instead of the filesystem name.
> Sorry!
> It will be fixed in the next rev.

Hey this sounds really great! Had almost given
up on this until snow leopard...

Makes me even hungrier for the next bits.

You don't want FreeBSD 7-STABLE to get it
out faster, do you! ;)

Regards, happy coding and hopefully
happy releasing some of that goodness soon :)

Lorenzo


From hanche at math.ntnu.no  Mon Dec  1 14:28:37 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Mon, 01 Dec 2008 23:28:37 +0100 (CET)
Subject: [zfs-discuss] zfs and kernel memory?
In-Reply-To: <20081201.101017.447857017412032666.hanche@math.ntnu.no>
References: <20081201.101017.447857017412032666.hanche@math.ntnu.no>
Message-ID: <20081201.232837.34391258.hanche@math.ntnu.no>

+ Harald Hanche-Olsen <hanche at math.ntnu.no>:

> I'm wondering if the 119 zfs release uses (much) more kernel memory
> than the original leopard one?
> 
> The reason I am asking is this: I installed 119 on my wife's old 12
> inch powerbook (with 640 MB of RAM), since we have some removable
> disks with zfs on them. Since then, applications (notably, Firefox and
> Mail) have started dying from malloc errors.

I downgraded to the vanilla Leopard read only zfs, and the problems
seem to have gone away. To make sure, I should probably reupgrade and
see if the problems come back, but that will have to be some other
time. My tentative conclusion is that 640 MB of RAM is not enough for
the machine to survive ZFS.

- Harald

From alex.blewitt at gmail.com  Mon Dec  1 17:18:21 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Tue, 2 Dec 2008 01:18:21 +0000
Subject: [zfs-discuss] zfs and kernel memory?
In-Reply-To: <20081201.232837.34391258.hanche@math.ntnu.no>
References: <20081201.101017.447857017412032666.hanche@math.ntnu.no>
	<20081201.232837.34391258.hanche@math.ntnu.no>
Message-ID: <636fd28e0812011718t7b603df0mb6920e0b4a67837c@mail.gmail.com>

On Mon, Dec 1, 2008 at 10:28 PM, Harald Hanche-Olsen
<hanche at math.ntnu.no> wrote:
> + Harald Hanche-Olsen <hanche at math.ntnu.no>:
>
>> I'm wondering if the 119 zfs release uses (much) more kernel memory
>> than the original leopard one?

I had a look for tuning parameters in the ZFS 119 release, and
couldn't find any. There's a bunch of parameters
(http://www.solarisinternals.com/wiki/index.php/ZFS_Evil_Tuning_Guide)
that are available for Solaris, but the general gist for ZFS is that
it should manage itself in terms of size.

One thing that's probably obvious is the size of the memory is almost
certainly relative to the amount of files loaded and processed. So, if
you just downgrade and start again, it'll probably use less memory
than for a ZFS pool that's been used for some time. I found when
copying a large amount of files to my ZFS pool in the first place that
it was taking 1.5G of memory and there was very little free memory
available (and that most of it seemed to be wired rather than
pageable). There's supposed to be some logic that says if the ARC
cache is experiencing memory constraints to let some memory go (at
least in some version of Solaris ZFS - don't know how well that
translates to OSX's port) but I seem to recall that if you have less
than 1G of memory, the ARC doesn't get used.

Anyway, I'd have a gut feeling that 640M is probably too small for a
ZFS system. You can throw $50 at the problem to increase it to 1280m
(assuming this is the right model) which would probably help.

http://www.crucial.com/store/listparts.aspx?model=PowerBook%20G4%201.33GHz%20%2812-inch%20Display%29

Alex

From fruitboi at gmail.com  Mon Dec  1 18:39:45 2008
From: fruitboi at gmail.com (Aaron Berland)
Date: Mon, 1 Dec 2008 20:39:45 -0600
Subject: [zfs-discuss] Possible Mirror a stripe (or raidz) pool? Or
	Replicate to separate pool?
Message-ID: <f8207d190812011839h3a1a21d6v1e3b2bb0b6077b6@mail.gmail.com>

Hi All,

I've totally fallen in love with ZFS.  I started on OpenSolaris and have
migrated to using it on OS X.  As we all have seen things aren't quite there
yet with OS X / Finder integration and there are definitely bugs with
interaction with DiskUtility.

Anyway, I am trying to figure out the best way to backup or replicate (yes,
I know the difference) my storage pool. I have 4x 320GB Seagate 7200.10
drives.  I also have 3x  Seagate 250 GB USB drives.  These have given me
nothing but headache and won't stay connected, so they have been deemed
unreliable for ZFS (sigh - I was hoping to use these as a backup for the
main pool.)

Is it possible to mirror a raidz or striped pool to a single device
effectively?
Is it better to replicate the original pool to a separate pool?  If so, what
is the best way?  Snapshots and send/recv are still a little taboo for me
:-/

I would like to pickup a 1TB drive and increase my storage space and
redundancy by:
a. converting the raidz pool to a striped pool and mirror it onto the 1TB
drive (3x320 stripe + 1TB mirror of the stripe)
b. Keep the raidz pool as is and do replication of that pool onto the 1TB
drive as a separate 1 drive zfs "pool"
c. Go crazy and use NexentaSTOR on a VM to export the storage with iSCSI and
have a real HFS+ volume on the Mac (I actually did this, but performance is
abysmal.  Average of 5MB/s writes and peaks at about 20MB/s.  Reads are no
faster.)

Since snapshots aren't fully baked in OS X yet, I think the easiest thing
would be option a.  (Feedback welcome! I haven't played with snapshots yet)
but I can't figure how to do a mirror of a stripe.

I'm also considering buying another small SATA drive for OS X so I would
have 4x 320GB for a raidz pool.

Basically my question is: what is a good, simple backup or replication plan
for this scenario?

Thoughts?
Aaron
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081201/2225c91a/attachment.html>

From bplist at thinkpink.com  Mon Dec  1 21:32:40 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Mon, 1 Dec 2008 21:32:40 -0800
Subject: [zfs-discuss] zfs and kernel memory?
In-Reply-To: <20081201.232837.34391258.hanche@math.ntnu.no>
References: <20081201.101017.447857017412032666.hanche@math.ntnu.no>
	<20081201.232837.34391258.hanche@math.ntnu.no>
Message-ID: <65730058-B8E2-47CE-98BE-97B482B91AAA@thinkpink.com>

640MB is pretty small for ZFS (maybe fine for ZFS alone, but it has to  
share with everything else!)  The general recommended minimum over on  
zfs-discuss seems to be 4GB, but of course it's subject to whatever  
real-world workload you throw at it.  See

http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide#Memory_and_Dynamic_Reconfiguration_Recommendations

for some more info (not all applicable to OS X, but you'll get the  
idea).

bri




From richmc at gmail.com  Mon Dec  1 22:38:55 2008
From: richmc at gmail.com (Rich McClellan)
Date: Mon, 1 Dec 2008 22:38:55 -0800
Subject: [zfs-discuss] Possible Mirror a stripe (or raidz) pool? Or
	Replicate to separate pool?
In-Reply-To: <f8207d190812011839h3a1a21d6v1e3b2bb0b6077b6@mail.gmail.com>
References: <f8207d190812011839h3a1a21d6v1e3b2bb0b6077b6@mail.gmail.com>
Message-ID: <203e36e30812012238j55e73a13ie42685bcb22973ac@mail.gmail.com>

On Mon, Dec 1, 2008 at 6:39 PM, Aaron Berland <fruitboi at gmail.com> wrote:
> Is it possible to mirror a raidz or striped pool to a single device
> effectively?

No.  It's antithetical to the ZFS Way.  However, it can be forced with
the -f option.

> Is it better to replicate the original pool to a separate pool?  If so, what
> is the best way?  Snapshots and send/recv are still a little taboo for me
> :-/

If you're not ready for snapshots, get ready.  If it's impossible for
you to embrace them, use rsync.

>
> I would like to pickup a 1TB drive and increase my storage space and
> redundancy by:
> a. converting the raidz pool to a striped pool and mirror it onto the 1TB
> drive (3x320 stripe + 1TB mirror of the stripe)

Do you mean create a mirror vdev out of the 3x320 + 1TB drives?  See
comment above about -f. Again, it goes against the grain, but you can
force it.  Split the 1TB drive in to 3 partitions equal in size to
your 320 GB drives and create a mirror vdev out of the six "disks".

> b. Keep the raidz pool as is and do replication of that pool onto the 1TB
> drive as a separate 1 drive zfs "pool"

Sure, this works. Use rsync or better yet snapshot.

> c. Go crazy and use NexentaSTOR on a VM to export the storage with iSCSI and
> have a real HFS+ volume on the Mac (I actually did this, but performance is
> abysmal.  Average of 5MB/s writes and peaks at about 20MB/s.  Reads are no
> faster.)

I do something similar to this using Linux as the iSCSI target/server
with the GlobalSAN iSCSI Mac OS X initiator.  Group together the
various LUNs and create a single target out of it.  Let ZFS on the Mac
create the vdevs.  It works for me.  I see my Gb network saturate when
transferring large files.  No stability issues.  I ended up using real
hardware because the VM approach for Solaris 10 and Linux was slow, as
you observed.  (I also tried Mac OS X's software RAID & HFS+ and was
impressed by the performance.)

>
> Since snapshots aren't fully baked in OS X yet, I think the easiest thing
> would be option a.  (Feedback welcome! I haven't played with snapshots yet)
> but I can't figure how to do a mirror of a stripe.

I think you mean a simple volume, not a stripped volume.  Most vdevs
are (dynamically) stripped (raidz is stripped, more than one mirror
vdev is stripped, ...).

I think the cake will be fully baked Real Soon Now.  It seems like the
ZFS team has to provide a significant update for Snow Leopard some
time soon.  Don't invest too much in to your alternative solution
because ZFS will fully support snapshots before you know it (Feb?).

>
> I'm also considering buying another small SATA drive for OS X so I would
> have 4x 320GB for a raidz pool.

Best idea yet.  Consider creating a mirror out of them.  You'll get
better IOPS and can easily add disk to the pool (unlike raidz).

>
> Basically my question is: what is a good, simple backup or replication plan
> for this scenario?
>
> Thoughts?
> Aaron
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>

From hanche at math.ntnu.no  Mon Dec  1 23:51:45 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Tue, 02 Dec 2008 08:51:45 +0100 (CET)
Subject: [zfs-discuss] zfs and kernel memory?
In-Reply-To: <636fd28e0812011718t7b603df0mb6920e0b4a67837c@mail.gmail.com>
References: <20081201.101017.447857017412032666.hanche@math.ntnu.no>
	<20081201.232837.34391258.hanche@math.ntnu.no>
	<636fd28e0812011718t7b603df0mb6920e0b4a67837c@mail.gmail.com>
Message-ID: <20081202.085145.203226893.hanche@math.ntnu.no>

+ "Alex Blewitt" <alex.blewitt at gmail.com>:

> Anyway, I'd have a gut feeling that 640M is probably too small for a
> ZFS system. You can throw $50 at the problem to increase it to 1280m
> (assuming this is the right model) which would probably help.

Unfortunately, that is not an option. It's the older 12 in powerbook
which cannot grow beyond 640 MB. We'll just have to suck it up until we
are ready to get a new laptop. That one is over five years old and has
been heavily used, so we have got our money's worth by now. (But for
our uses, I still consider it the best laptop Apple ever made. It's a
shame to have to let it go.)

- Harald

From toby at telegraphics.com.au  Tue Dec  2 08:13:58 2008
From: toby at telegraphics.com.au (Toby Thain)
Date: Tue, 2 Dec 2008 11:13:58 -0500
Subject: [zfs-discuss] zfs and kernel memory?
In-Reply-To: <20081202.085145.203226893.hanche@math.ntnu.no>
References: <20081201.101017.447857017412032666.hanche@math.ntnu.no>
	<20081201.232837.34391258.hanche@math.ntnu.no>
	<636fd28e0812011718t7b603df0mb6920e0b4a67837c@mail.gmail.com>
	<20081202.085145.203226893.hanche@math.ntnu.no>
Message-ID: <11DC5E90-6A57-496A-8476-3CE9EA651ABE@telegraphics.com.au>


On 2-Dec-08, at 2:51 AM, Harald Hanche-Olsen wrote:

> + "Alex Blewitt" <alex.blewitt at gmail.com>:
>
>> Anyway, I'd have a gut feeling that 640M is probably too small for a
>> ZFS system. You can throw $50 at the problem to increase it to 1280m
>> (assuming this is the right model) which would probably help.
>
> Unfortunately, that is not an option. It's the older 12 in powerbook
> which cannot grow beyond 640 MB. We'll just have to suck it up  
> until we
> are ready to get a new laptop. That one is over five years old and has
> been heavily used, so we have got our money's worth by now. (But for
> our uses, I still consider it the best laptop Apple ever made. It's a
> shame to have to let it go.)

Well HFS+ is very nice too. Nobody insists you use ZFS. :)

--T

>
> - Harald
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From hanche at math.ntnu.no  Tue Dec  2 08:23:16 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Tue, 02 Dec 2008 17:23:16 +0100 (CET)
Subject: [zfs-discuss] zfs and kernel memory?
In-Reply-To: <11DC5E90-6A57-496A-8476-3CE9EA651ABE@telegraphics.com.au>
References: <636fd28e0812011718t7b603df0mb6920e0b4a67837c@mail.gmail.com>
	<20081202.085145.203226893.hanche@math.ntnu.no>
	<11DC5E90-6A57-496A-8476-3CE9EA651ABE@telegraphics.com.au>
Message-ID: <20081202.172316.1143232451457846743.hanche@math.ntnu.no>

+ Toby Thain <toby at telegraphics.com.au>:

> Well HFS+ is very nice too. Nobody insists you use ZFS. :)

Nobody but me insists, though clearly I must relent. One big reason
for me is the portability of ZFS pools between macosx and freebsd,
which no other file system achieves (no proper file system, that is
... FAT and friends need not apply).

- Harald

From fruitboi at gmail.com  Tue Dec  2 10:47:10 2008
From: fruitboi at gmail.com (Aaron Berland)
Date: Tue, 2 Dec 2008 12:47:10 -0600
Subject: [zfs-discuss] Possible Mirror a stripe (or raidz) pool? Or
	Replicate to separate pool?
In-Reply-To: <203e36e30812012238j55e73a13ie42685bcb22973ac@mail.gmail.com>
References: <f8207d190812011839h3a1a21d6v1e3b2bb0b6077b6@mail.gmail.com>
	<203e36e30812012238j55e73a13ie42685bcb22973ac@mail.gmail.com>
Message-ID: <f8207d190812021047j337d61e5ka039585e5575ce42@mail.gmail.com>

Rich, Thank you so much for your reply!

I have been reading more into snapshots and will start to experiment (with
redundant data of course).

I'm going to set my 1TB drive it as a separate pool and figure out
snapshots.  I sure can't wait for new bits that are much closer to release
quality and functionality!  Thanks for everyone contributing to the
project!

Just for fun I'm going to install NexentaSTOR on my box and see what iSCSI
performance looks like on bare metal to my MBP.  Never hurts to have the
experience, right?

Thank you again!  I really appreciate your response, Rich.

Regards,
Aaron

On Tue, Dec 2, 2008 at 12:38 AM, Rich McClellan <richmc at gmail.com> wrote:

> On Mon, Dec 1, 2008 at 6:39 PM, Aaron Berland <fruitboi at gmail.com> wrote:
> > Is it possible to mirror a raidz or striped pool to a single device
> > effectively?
>
> No.  It's antithetical to the ZFS Way.  However, it can be forced with
> the -f option.
>
> > Is it better to replicate the original pool to a separate pool?  If so,
> what
> > is the best way?  Snapshots and send/recv are still a little taboo for me
> > :-/
>
> If you're not ready for snapshots, get ready.  If it's impossible for
> you to embrace them, use rsync.
>
> >
> > I would like to pickup a 1TB drive and increase my storage space and
> > redundancy by:
> > a. converting the raidz pool to a striped pool and mirror it onto the 1TB
> > drive (3x320 stripe + 1TB mirror of the stripe)
>
> Do you mean create a mirror vdev out of the 3x320 + 1TB drives?  See
> comment above about -f. Again, it goes against the grain, but you can
> force it.  Split the 1TB drive in to 3 partitions equal in size to
> your 320 GB drives and create a mirror vdev out of the six "disks".
>
> > b. Keep the raidz pool as is and do replication of that pool onto the 1TB
> > drive as a separate 1 drive zfs "pool"
>
> Sure, this works. Use rsync or better yet snapshot.
>
> > c. Go crazy and use NexentaSTOR on a VM to export the storage with iSCSI
> and
> > have a real HFS+ volume on the Mac (I actually did this, but performance
> is
> > abysmal.  Average of 5MB/s writes and peaks at about 20MB/s.  Reads are
> no
> > faster.)
>
> I do something similar to this using Linux as the iSCSI target/server
> with the GlobalSAN iSCSI Mac OS X initiator.  Group together the
> various LUNs and create a single target out of it.  Let ZFS on the Mac
> create the vdevs.  It works for me.  I see my Gb network saturate when
> transferring large files.  No stability issues.  I ended up using real
> hardware because the VM approach for Solaris 10 and Linux was slow, as
> you observed.  (I also tried Mac OS X's software RAID & HFS+ and was
> impressed by the performance.)
>
> >
> > Since snapshots aren't fully baked in OS X yet, I think the easiest thing
> > would be option a.  (Feedback welcome! I haven't played with snapshots
> yet)
> > but I can't figure how to do a mirror of a stripe.
>
> I think you mean a simple volume, not a stripped volume.  Most vdevs
> are (dynamically) stripped (raidz is stripped, more than one mirror
> vdev is stripped, ...).
>
> I think the cake will be fully baked Real Soon Now.  It seems like the
> ZFS team has to provide a significant update for Snow Leopard some
> time soon.  Don't invest too much in to your alternative solution
> because ZFS will fully support snapshots before you know it (Feb?).
>
> >
> > I'm also considering buying another small SATA drive for OS X so I would
> > have 4x 320GB for a raidz pool.
>
> Best idea yet.  Consider creating a mirror out of them.  You'll get
> better IOPS and can easily add disk to the pool (unlike raidz).
>
> >
> > Basically my question is: what is a good, simple backup or replication
> plan
> > for this scenario?
> >
> > Thoughts?
> > Aaron
> >
> >
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss at lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081202/8aa20121/attachment.html>

From mike666 at mac.com  Tue Dec  2 12:23:34 2008
From: mike666 at mac.com (Mike Prather)
Date: Tue, 02 Dec 2008 12:23:34 -0800
Subject: [zfs-discuss] zfs and kernel memory?
In-Reply-To: <mailman.9.1228230012.74715.zfs-discuss@lists.macosforge.org>
References: <mailman.9.1228230012.74715.zfs-discuss@lists.macosforge.org>
Message-ID: <753BA674-7C99-4BD2-A414-F03E9CC53B53@mac.com>

FYI, even the original 867MHz PBG4 12" can take up to 1.12GB of memory  
even though the official max is 640MB.  Just get a DDR266 1GB SODIMM.   
I'd recommend Trans International - they supply quality DRAM and have  
a lifetime warranty - plus they know Macs.  http://www.transintl.com/store/category.cfm?Category=941&RequestTimeOut=500 
   $42.  This probably won't solve your zfs woes but it WILL make your  
PB noticeably more responsive in general.

Mike

On Dec 2, 2008, at 7:00 AM, zfs-discuss-request at lists.macosforge.org  
wrote:

> ------------------------------
>
> Message: 6
> Date: Tue, 02 Dec 2008 08:51:45 +0100 (CET)
> From: Harald Hanche-Olsen <hanche at math.ntnu.no>
> Subject: Re: [zfs-discuss] zfs and kernel memory?
> To: alex.blewitt at gmail.com
> Cc: zfs-discuss at lists.macosforge.org
> Message-ID: <20081202.085145.203226893.hanche at math.ntnu.no>
> Content-Type: Text/Plain; charset=us-ascii
>
> + "Alex Blewitt" <alex.blewitt at gmail.com>:
>
>> Anyway, I'd have a gut feeling that 640M is probably too small for a
>> ZFS system. You can throw $50 at the problem to increase it to 1280m
>> (assuming this is the right model) which would probably help.
>
> Unfortunately, that is not an option. It's the older 12 in powerbook
> which cannot grow beyond 640 MB. We'll just have to suck it up until  
> we
> are ready to get a new laptop. That one is over five years old and has
> been heavily used, so we have got our money's worth by now. (But for
> our uses, I still consider it the best laptop Apple ever made. It's a
> shame to have to let it go.)
>
> - Harald
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081202/d4c0c6c0/attachment.html>

From alex.blewitt at gmail.com  Wed Dec  3 00:53:04 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Wed, 3 Dec 2008 08:53:04 +0000
Subject: [zfs-discuss] zfs and kernel memory?
In-Reply-To: <753BA674-7C99-4BD2-A414-F03E9CC53B53@mac.com>
References: <mailman.9.1228230012.74715.zfs-discuss@lists.macosforge.org>
	<753BA674-7C99-4BD2-A414-F03E9CC53B53@mac.com>
Message-ID: <636fd28e0812030053n4d52429avba0b87fd32db35f5@mail.gmail.com>

On Tue, Dec 2, 2008 at 8:23 PM, Mike Prather <mike666 at mac.com> wrote:
> FYI, even the original 867MHz PBG4 12" can take up to 1.12GB of memory even
> though the official max is 640MB.

Yeah, my Mac Mini has a top advertised limit of 2G but it didn't stop
me putting 4G in there. It can't address about 1/2 off the top, but
3.5 is still significantly better than 2, and I got it for the same
price as Apple's 2G would have cost me.

Alex

From bwaters at nrao.edu  Wed Dec  3 11:18:18 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Wed, 3 Dec 2008 12:18:18 -0700
Subject: [zfs-discuss] nested filesystems, any problems?
Message-ID: <295CD6C2-334E-48BF-9D57-6241F1710813@nrao.edu>

Is anyone really testing out nested filesystems?

ZFS filesystems can contain other filesystems. Like a directory

"That's crazy talk. Why would you want to do that?"

I want to perform snapshots of a sub-directory independently of  
snapshots on the parent.


For example, my top-level pool is a mounted as a ZFS filesystem called  
"pool". I have a home directory under that called "bwaters" so that  
filesystem is pool/bwaters ... I can have other filesystems under my  
home directory...


Everything is working OK over here with that setup. Except snapshots.

cannot create snapshot 'pool/bwaters at 2008-12-03T12:12:55': dataset is  
busy


I logged out, logged in as an admin user, performed a scrub of the  
entire pool (this took about six hours), and there were no data errors  
reported but I still cannot create snapshots.

I used to create snapshots all the time.


Back when I had only one pool attached... hmm... now I have an  
external SATA enclosure with another raidz1 mounted.  That one is OK,  
too. Why should it matter?

# zpool status
   pool: pool
  state: ONLINE
  scrub: scrub completed with 0 errors on Tue Dec  2 23:05:14 2008
config:

	NAME         STATE     READ WRITE CKSUM
	pool         ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk0s2  ONLINE       0     0     0
	    disk3s2  ONLINE       0     0     0
	    disk2s2  ONLINE       0     0     0
	    disk1s2  ONLINE       0     0     0

errors: No known data errors

   pool: tank
  state: ONLINE
  scrub: none requested
config:

	NAME         STATE     READ WRITE CKSUM
	tank         ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk6s2  ONLINE       0     0     0
	    disk9s2  ONLINE       0     0     0
	    disk7s2  ONLINE       0     0     0
	    disk8s2  ONLINE       0     0     0

errors: No known data errors



IS ANYONE RUNNING INTO THIS "CANNOT CREATE SNAPSHOTS" PROBLEM?
ANY SUGGESTIONS TO FIX THIS?

(the pool is 1.9 Terabytes at the moment. exporting/importing is do- 
able, but I'll need to procure *even more* disk to do so... I'd  
purchase 4x1.5T and that would set me back at least $350... any other  
idea?)



From andy at aligature.com  Wed Dec  3 11:30:40 2008
From: andy at aligature.com (Andrew Webber)
Date: Wed, 3 Dec 2008 14:30:40 -0500
Subject: [zfs-discuss] nested filesystems, any problems?
In-Reply-To: <295CD6C2-334E-48BF-9D57-6241F1710813@nrao.edu>
References: <295CD6C2-334E-48BF-9D57-6241F1710813@nrao.edu>
Message-ID: <60b50dc10812031130w3a8ec652y947d1f5f7aac4794@mail.gmail.com>

For what it's worth, I create snapshots on my sub-filesystem all the time

zfs snapshot pool/docs at 2008-12-03


On Wed, Dec 3, 2008 at 2:18 PM, Boyd Waters <bwaters at nrao.edu> wrote:

> Is anyone really testing out nested filesystems?
>
> ZFS filesystems can contain other filesystems. Like a directory
>
> "That's crazy talk. Why would you want to do that?"
>
> I want to perform snapshots of a sub-directory independently of snapshots
> on the parent.
>
>
> For example, my top-level pool is a mounted as a ZFS filesystem called
> "pool". I have a home directory under that called "bwaters" so that
> filesystem is pool/bwaters ... I can have other filesystems under my home
> directory...
>
>
> Everything is working OK over here with that setup. Except snapshots.
>
> cannot create snapshot 'pool/bwaters at 2008-12-03T12:12:55': dataset is busy
>
>
> I logged out, logged in as an admin user, performed a scrub of the entire
> pool (this took about six hours), and there were no data errors reported but
> I still cannot create snapshots.
>
> I used to create snapshots all the time.
>
>
> Back when I had only one pool attached... hmm... now I have an external
> SATA enclosure with another raidz1 mounted.  That one is OK, too. Why should
> it matter?
>
> # zpool status
>  pool: pool
>  state: ONLINE
>  scrub: scrub completed with 0 errors on Tue Dec  2 23:05:14 2008
> config:
>
>        NAME         STATE     READ WRITE CKSUM
>        pool         ONLINE       0     0     0
>          raidz1     ONLINE       0     0     0
>            disk0s2  ONLINE       0     0     0
>            disk3s2  ONLINE       0     0     0
>            disk2s2  ONLINE       0     0     0
>            disk1s2  ONLINE       0     0     0
>
> errors: No known data errors
>
>  pool: tank
>  state: ONLINE
>  scrub: none requested
> config:
>
>        NAME         STATE     READ WRITE CKSUM
>        tank         ONLINE       0     0     0
>          raidz1     ONLINE       0     0     0
>            disk6s2  ONLINE       0     0     0
>            disk9s2  ONLINE       0     0     0
>            disk7s2  ONLINE       0     0     0
>            disk8s2  ONLINE       0     0     0
>
> errors: No known data errors
>
>
>
> IS ANYONE RUNNING INTO THIS "CANNOT CREATE SNAPSHOTS" PROBLEM?
> ANY SUGGESTIONS TO FIX THIS?
>
> (the pool is 1.9 Terabytes at the moment. exporting/importing is do-able,
> but I'll need to procure *even more* disk to do so... I'd purchase 4x1.5T
> and that would set me back at least $350... any other idea?)
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081203/ff741eb2/attachment.html>

From lists at loveturtle.net  Wed Dec  3 12:01:17 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Wed, 03 Dec 2008 15:01:17 -0500
Subject: [zfs-discuss] nested filesystems, any problems?
In-Reply-To: <295CD6C2-334E-48BF-9D57-6241F1710813@nrao.edu>
References: <295CD6C2-334E-48BF-9D57-6241F1710813@nrao.edu>
Message-ID: <4936E58D.1090804@loveturtle.net>

Yeah, really. I have some way deep zfs filesystems, it's totally normal, 
everyone should do it!

I've had a similar problem a couple times on FreeBSD. One time it was 
because the parent fs (actually the root fs for the pool itself) had the 
zfs mountpoint property set to legacy and wasn't mounted anywhere at 
all. This shouldn't have made a difference but it did, I googled and if 
I remember correctly it was a known zfs problem that was fixed a whlie 
ago. Creating some useless directory and mounting the parent fs there 
solved the problem.

One other time it happend for no reason I can explain and simply 
unmounting and remounting the fs fixed it. That would be the easiest one 
to try.
I think you'll need to use "diskutil unmount /mountpoint" to do it, zfs 
unmount and umount will probably just get confused.

Obviously if you're going to unmount your home directory you'll need to 
not be logged in at the time. I like to log out and then log in as 
">console" which will drop you to the cli and then you're free to trash 
most everything and still be okay :-)

You could try that and see if it makes a difference....If not I have no 
clue :-)

On 12/3/08 2:18 PM, Boyd Waters wrote:
> Is anyone really testing out nested filesystems?
>
> ZFS filesystems can contain other filesystems. Like a directory
>
> "That's crazy talk. Why would you want to do that?"
>
> I want to perform snapshots of a sub-directory independently of 
> snapshots on the parent.
>
>
> For example, my top-level pool is a mounted as a ZFS filesystem called 
> "pool". I have a home directory under that called "bwaters" so that 
> filesystem is pool/bwaters ... I can have other filesystems under my 
> home directory...
>
>
> Everything is working OK over here with that setup. Except snapshots.
>
> cannot create snapshot 'pool/bwaters at 2008-12-03T12:12:55': dataset is 
> busy
>
>
> I logged out, logged in as an admin user, performed a scrub of the 
> entire pool (this took about six hours), and there were no data errors 
> reported but I still cannot create snapshots.
>
> I used to create snapshots all the time.
>
>
> Back when I had only one pool attached... hmm... now I have an 
> external SATA enclosure with another raidz1 mounted.  That one is OK, 
> too. Why should it matter?
>
> # zpool status
>   pool: pool
>  state: ONLINE
>  scrub: scrub completed with 0 errors on Tue Dec  2 23:05:14 2008
> config:
>
>     NAME         STATE     READ WRITE CKSUM
>     pool         ONLINE       0     0     0
>       raidz1     ONLINE       0     0     0
>         disk0s2  ONLINE       0     0     0
>         disk3s2  ONLINE       0     0     0
>         disk2s2  ONLINE       0     0     0
>         disk1s2  ONLINE       0     0     0
>
> errors: No known data errors
>
>   pool: tank
>  state: ONLINE
>  scrub: none requested
> config:
>
>     NAME         STATE     READ WRITE CKSUM
>     tank         ONLINE       0     0     0
>       raidz1     ONLINE       0     0     0
>         disk6s2  ONLINE       0     0     0
>         disk9s2  ONLINE       0     0     0
>         disk7s2  ONLINE       0     0     0
>         disk8s2  ONLINE       0     0     0
>
> errors: No known data errors
>
>
>
> IS ANYONE RUNNING INTO THIS "CANNOT CREATE SNAPSHOTS" PROBLEM?
> ANY SUGGESTIONS TO FIX THIS?
>
> (the pool is 1.9 Terabytes at the moment. exporting/importing is 
> do-able, but I'll need to procure *even more* disk to do so... I'd 
> purchase 4x1.5T and that would set me back at least $350... any other 
> idea?)
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From alex.blewitt at gmail.com  Wed Dec  3 12:26:52 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Wed, 3 Dec 2008 20:26:52 +0000
Subject: [zfs-discuss] nested filesystems, any problems?
In-Reply-To: <60b50dc10812031130w3a8ec652y947d1f5f7aac4794@mail.gmail.com>
References: <295CD6C2-334E-48BF-9D57-6241F1710813@nrao.edu>
	<60b50dc10812031130w3a8ec652y947d1f5f7aac4794@mail.gmail.com>
Message-ID: <4CD3DC7C-D922-4E4F-B648-788A30384907@gmail.com>

I too have many sub file systems - max depth of 4 meetings, 15 fs and  
over 1000 snapshots (all created with -r; see earlier posts/blog for  
crontab). Only problem I've had is using static automounts since the  
automounter doesn't like nested mounts - but if you're not sharing  
over NFS then this doesn't matter.

Alex

Sent from my (new) iPhone

On 3 Dec 2008, at 19:30, "Andrew Webber" <andy at aligature.com> wrote:

> For what it's worth, I create snapshots on my sub-filesystem all the  
> time
>
> zfs snapshot pool/docs at 2008-12-03
>
>
> On Wed, Dec 3, 2008 at 2:18 PM, Boyd Waters <bwaters at nrao.edu> wrote:
> Is anyone really testing out nested filesystems?
>
> ZFS filesystems can contain other filesystems. Like a directory
>
> "That's crazy talk. Why would you want to do that?"
>
> I want to perform snapshots of a sub-directory independently of  
> snapshots on the parent.
>
>
> For example, my top-level pool is a mounted as a ZFS filesystem  
> called "pool". I have a home directory under that called "bwaters"  
> so that filesystem is pool/bwaters ... I can have other filesystems  
> under my home directory...
>
>
> Everything is working OK over here with that setup. Except snapshots.
>
> cannot create snapshot 'pool/bwaters at 2008-12-03T12:12:55': dataset  
> is busy
>
>
> I logged out, logged in as an admin user, performed a scrub of the  
> entire pool (this took about six hours), and there were no data  
> errors reported but I still cannot create snapshots.
>
> I used to create snapshots all the time.
>
>
> Back when I had only one pool attached... hmm... now I have an  
> external SATA enclosure with another raidz1 mounted.  That one is  
> OK, too. Why should it matter?
>
> # zpool status
>  pool: pool
>  state: ONLINE
>  scrub: scrub completed with 0 errors on Tue Dec  2 23:05:14 2008
> config:
>
>        NAME         STATE     READ WRITE CKSUM
>        pool         ONLINE       0     0     0
>          raidz1     ONLINE       0     0     0
>            disk0s2  ONLINE       0     0     0
>            disk3s2  ONLINE       0     0     0
>            disk2s2  ONLINE       0     0     0
>            disk1s2  ONLINE       0     0     0
>
> errors: No known data errors
>
>  pool: tank
>  state: ONLINE
>  scrub: none requested
> config:
>
>        NAME         STATE     READ WRITE CKSUM
>        tank         ONLINE       0     0     0
>          raidz1     ONLINE       0     0     0
>            disk6s2  ONLINE       0     0     0
>            disk9s2  ONLINE       0     0     0
>            disk7s2  ONLINE       0     0     0
>            disk8s2  ONLINE       0     0     0
>
> errors: No known data errors
>
>
>
> IS ANYONE RUNNING INTO THIS "CANNOT CREATE SNAPSHOTS" PROBLEM?
> ANY SUGGESTIONS TO FIX THIS?
>
> (the pool is 1.9 Terabytes at the moment. exporting/importing is do- 
> able, but I'll need to procure *even more* disk to do so... I'd  
> purchase 4x1.5T and that would set me back at least $350... any  
> other idea?)
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081203/144ff272/attachment-0001.html>

From bwaters at nrao.edu  Wed Dec  3 13:25:10 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Wed, 3 Dec 2008 14:25:10 -0700
Subject: [zfs-discuss] nested filesystems, any problems?
In-Reply-To: <60b50dc10812031130w3a8ec652y947d1f5f7aac4794@mail.gmail.com>
References: <295CD6C2-334E-48BF-9D57-6241F1710813@nrao.edu>
	<60b50dc10812031130w3a8ec652y947d1f5f7aac4794@mail.gmail.com>
Message-ID: <2b0225fb0812031325g54c9f0c8ic74504da804af289@mail.gmail.com>

@alex, @andrew
Thanks for the feedback!

I wonder what to do...

I will try un-mounting the external raid and rebooting, see if that
changes anything.


On Wed, Dec 3, 2008 at 12:30 PM, Andrew Webber <andy at aligature.com> wrote:
>
> For what it's worth, I create snapshots on my sub-filesystem all the time
>
> zfs snapshot pool/docs at 2008-12-03

From alex.blewitt at gmail.com  Wed Dec  3 13:49:02 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Wed, 3 Dec 2008 21:49:02 +0000
Subject: [zfs-discuss] nested filesystems, any problems?
In-Reply-To: <2b0225fb0812031325g54c9f0c8ic74504da804af289@mail.gmail.com>
References: <295CD6C2-334E-48BF-9D57-6241F1710813@nrao.edu>
	<60b50dc10812031130w3a8ec652y947d1f5f7aac4794@mail.gmail.com>
	<2b0225fb0812031325g54c9f0c8ic74504da804af289@mail.gmail.com>
Message-ID: <636fd28e0812031349m6c19d145v5ef970dd2988b86b@mail.gmail.com>

On Wed, Dec 3, 2008 at 9:25 PM, Boyd Waters <bwaters at nrao.edu> wrote:
> @alex, @andrew
> Thanks for the feedback!
>
> I wonder what to do...
>
> I will try un-mounting the external raid and rebooting, see if that
> changes anything.

A few posts (http://mail.opensolaris.org/pipermail/zfs-discuss/2006-June/032304.html)
suggest the dataset is busy is something you can fix by unmounting and
then remounting (using zfs umount/zfs mount, not mount) which clears
the busy bit.

What version of ZFS are you running? I put an installer together for
the latest 119 bits.

Alex

From hanche at math.ntnu.no  Wed Dec  3 13:56:35 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Wed, 03 Dec 2008 22:56:35 +0100 (CET)
Subject: [zfs-discuss] nested filesystems, any problems?
In-Reply-To: <4936E58D.1090804@loveturtle.net>
References: <295CD6C2-334E-48BF-9D57-6241F1710813@nrao.edu>
	<4936E58D.1090804@loveturtle.net>
Message-ID: <20081203.225635.57229284.hanche@math.ntnu.no>

+ Dillon Kass <lists at loveturtle.net>:

> Obviously if you're going to unmount your home directory you'll 
> need to not be logged in at the time. I like to log out and then 
> log in as ">console" which will drop you to the cli and then you're 
> free to trash most everything and still be okay :-)

That sounded intriguing, so I googled it and found a tip on how to 
set this up. However, it did not work as advertised - it just created 
an ordinary user with a funny name. But the screenshots of the tip 
looked pre-Leopard, so maybe it is just outdated.

Anyhow, having a an admin user whose home directory is on the boot 
disk is probably a very good idea. And for really hard core system 
hacking, there is always single user mode.

- Harald

From lists at loveturtle.net  Wed Dec  3 14:10:12 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Wed, 03 Dec 2008 17:10:12 -0500
Subject: [zfs-discuss] nested filesystems, any problems?
In-Reply-To: <20081203.225635.57229284.hanche@math.ntnu.no>
References: <295CD6C2-334E-48BF-9D57-6241F1710813@nrao.edu>	<4936E58D.1090804@loveturtle.net>
	<20081203.225635.57229284.hanche@math.ntnu.no>
Message-ID: <493703C4.50702@loveturtle.net>

There's nothing to set up, don't try to create a user with that name or 
anything.

Log out, at the login window click on "Other" (assuming you have the 
graphical one where you click on valid users icons) or if you're normal 
and just have a Name: Password: field just simply put
 >console
in the username field, no password, click log in

you can give it a shot right now!

Sometimes it acts a little weird and you don't see the Login: i think 
maybe it prints before the ui turns off or something..but whatever, 
you're at a login prompt. login as root and start blowing things up!

On 12/3/08 4:56 PM, Harald Hanche-Olsen wrote:
> + Dillon Kass <lists at loveturtle.net>:
>
>> Obviously if you're going to unmount your home directory you'll need 
>> to not be logged in at the time. I like to log out and then log in as 
>> ">console" which will drop you to the cli and then you're free to 
>> trash most everything and still be okay :-)
>
> That sounded intriguing, so I googled it and found a tip on how to set 
> this up. However, it did not work as advertised - it just created an 
> ordinary user with a funny name. But the screenshots of the tip looked 
> pre-Leopard, so maybe it is just outdated.
>
> Anyhow, having a an admin user whose home directory is on the boot 
> disk is probably a very good idea. And for really hard core system 
> hacking, there is always single user mode.
>
> - Harald
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From mailmaczfs at theyeti.com  Thu Dec  4 08:17:31 2008
From: mailmaczfs at theyeti.com (Michael M)
Date: Thu, 4 Dec 2008 08:17:31 -0800
Subject: [zfs-discuss] StoreGPU, hashing on CUDA
Message-ID: <a0624080dc55db0bdcd8e@[192.168.0.102]>

StoreGPU: Exploiting Graphics Processing Units to Accelerate 
Distributed Storage Systems

Today GPUs are a largely under exploited resource on existing 
desktops and a possible cost-effective enhancement to 
high-performance systems. To date, most applications that exploit 
GPUs are specialized scientific applications. Little attention has 
been paid to harnessing these highly-parallel devices to support more 
generic functionality at the operating system or middleware level. 
This study starts from the hypothesis that generic middleware level 
techniques that improve distributed system reliability or performance 
(such as content addressing, erasure coding, or data similarity 
detection) can be significantly accelerated using GPU support.

We take a first step towards validating this hypothesis, focusing on 
distributed storage systems. As a proof of concept, we design 
StoreGPU, a library that accelerates a number of hashing based 
primitives popular in distributed storage system implementations. Our 
evaluation shows that StoreGPU enables up to ten fold performance 
gains on synthetic benchmarks and up to nine-fold gains for a 
high-level application, online similarity detection between large 
data files.

http://www.ece.ubc.ca/~samera/projects/StoreGPU/

I cam across this and thought that I would toss it out where the 
people who write this can see it. I assume that the grand central 
thing to allow people to use the gpu more effectively will be coming 
along. I also assume that most of the time the gpu will sit largely 
unutilized, especially on large file servers. This proof of concept 
shows how you can use a relatively cheap and uncontested resource to 
accelerate some of the hashing that needs to be done. I don't know if 
their source implements the hash that zfs uses, but that does not 
mean that it can't be done. I was thinking that this could cheapen 
performing a scrub.

-- 
---------------------------
Never attribute to malice what can more easily be explained by incompetence.

From i_see at macnews.de  Thu Dec  4 12:56:00 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Thu, 4 Dec 2008 21:56:00 +0100
Subject: [zfs-discuss] Implicit growth of mirrored vdevs?
In-Reply-To: <mailman.6.1228402832.68610.zfs-discuss@lists.macosforge.org>
References: <mailman.6.1228402832.68610.zfs-discuss@lists.macosforge.org>
Message-ID: <8DFF474A-3554-4C8D-ACD8-B2AD6E17E762@macnews.de>

Hi list,
I didn't post for a while but ran into a strange problem (w/ zfs-117  
Mac OS 10.5.5) I upgraded to 119, but that does not fix the state of  
my pool for now.

Before I tell the whole story my theory is:
On importing a pool (or activating a vdev) ZFS re-evaluates the size  
of the VDEV based on the size all available devices. If the sizes of  
your mirrored devices happen to differ and the smaller device is  
unavailable at import time, ZFS automagically grows the pool, leaving  
the smaller device in the cold.


My pool consists of two mirrored vdevs with one side sitting on a 1TB  
disk, the others are 2 small disks. All disks are external USB- 
enclosures. Since I didn't go very far into partitioning, the  
partitions on the big disk only roughly match the size of the external  
disks, or to be exact to the size available to ZFS for diskXs2.

When I tried to import my pool for the last time one enclosure/disk  
was slow to become available, so the pool went into a degraded state  
like this:

   pool: Doggie
  state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas  
exist for
	the pool to continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
    see: http://www.sun.com/msg/ZFS-8000-2Q
  scrub: resilver completed with 0 errors on Wed Dec  3 23:21:57 2008
config:

	NAME         STATE     READ WRITE CKSUM
	Doggie       DEGRADED     0     0     0
	  mirror     ONLINE       0     0     0
	    disk2s2  ONLINE       0     0     0
	    disk3s4  ONLINE       0     0     0
	  mirror     DEGRADED     0     0     0
	    disk7s2  UNAVAIL      0     0     0  cannot open
	    disk3s2  ONLINE       0     0     0

errors: No known data errors

unfortunately, I don't know a way to ONLINE a device that has a  
different name then ZFS thinks (it was probably disk5s2), so I  
exportet the pool and tried to import it again. -
I got "Invalid vdev configuration" or "no pool available for import"  
no matte what I initially tried.
Then I tried to trick ZFS into accepting the disk by mounting some  
disk images and made sure my external disk was actually disk7s2
 > zpool online Doggie disk7s2
*Kernel panic*

After rebooting, I tried to replace the disk with itsself (sitting on  
disk4s2)
$ zpool replace Doggie disk7s2 disk4s2
cannot replace disk7s2 with disk4s2: device is too small

Trying the "trick" didn't help:
$ zpool replace Doggie disk7s2
cannot replace disk7s2 with disk7s2: device is too small

Then I tried this only to find out my pool has actually grown:
$ zpool iostat -v Doggie
                 capacity     operations    bandwidth
pool          used  avail   read  write   read  write
-----------  -----  -----  -----  -----  -----  -----
Doggie        202G   102G      0      0    116  1,04K
   mirror      185G   507M      0      0     45    319
     disk2s2      -      -      0      0    119    337
     disk3s4      -      -      0      0    108    337
   mirror     17,2G   102G      0      0     71    748
     disk7s2      -      -      0      0      0      0
     disk3s2      -      -      0      0    105    766
-----------  -----  -----  -----  -----  -----  -----

$ diskutil list disk7s2
/dev/disk7
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *111.8 Gi    
disk7
    1:                        EFI                         200.0 Mi    
disk7s1
    2:                        ZFS                         111.5 Gi    
disk7s2

The second VDEV is now 120G though my external "120G" disk (which I  
used to create the pool) only has 111.5G available for ZFS.
So, due to the fact that ZFS pools don't like to shrink, cannot re- 
attach my disk to the pool.

Obviously, I can move the data around into a new pool and start over,  
but I cannot tell why this would not happen again.
If ZFS implicitly grows a vdev when the smallest device is unavailable  
I would consider that a bug.

@Noel: if it is of any help, I can collect some extra output for you.  
However, since I got strange output from diskutil, I did a
# diskutil partitiondisk /dev/disk7 GPTFormat ZFS %noformat% 100%
The output from diskutil got lost but it was something like

$ diskutil list disk7
/dev/disk7
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *111.8 Gi    
disk7
    1:                        EFI                         200.0 Mi    
disk7s1
    2:                        ZFS                         299.8 Gi    
disk7s2
Note the size of the partition was significantly bigger then the size  
of the disk. (It was roughly the size of the pool, but I got that  
twice, and I do not know how that number gets there, (which might be a  
different issue).

Greetings from Aachen,
	ralf



From lopez.on.the.lists at yellowspace.net  Thu Dec  4 13:38:30 2008
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Thu, 4 Dec 2008 22:38:30 +0100
Subject: [zfs-discuss] Binary installer for ZFS-119
In-Reply-To: <636fd28e0811021459k356238e1m3566d5ba3ccd4a66@mail.gmail.com>
References: <636fd28e0811021459k356238e1m3566d5ba3ccd4a66@mail.gmail.com>
Message-ID: <43485157-9F86-4B60-8905-B6660AC5D0E5@yellowspace.net>

hi, just wanted to get the binaries installed on
another mac, and thought why not use your handy installer.
well just a note: for people naive enough to click on
the link of your blog - instead of issuing a
curl -O 'http://www.bandlem.com/ZFS-119.pkg' - you
might as well configure your webserver to send the
correct mimetype (or just application/binary-stream)
for .pkg files (or it will hose most browsers)... ;)

Regards,

Lorenzo

On 02.11.2008, at 23:59, Alex Blewitt wrote:

> It's taken me a long time, but I've put together a binary installer
> (.pkg) for the 119 ZFS binaries available (from
> http://zfs.macosforge.org/trac/wiki/downloads) to make it easier for
> people to play around with ZFS.  It saves you having to get the right
> sequence of copy commands and ensuring the permissions are sorted out
> etc. So if you've got friends who are interested in playing with ZFS
> but don't want to follow the simple instructions, I've put an
> installer together via
> http://alblue.blogspot.com/2008/11/zfs-119-on-mac-os-x.html
>
> No doubt No?l et al will tell me ZFS-120 is just around the corner ...
>
> Alex
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From michael.heller at gmail.com  Fri Dec  5 18:12:30 2008
From: michael.heller at gmail.com (Mike Heller)
Date: Fri, 5 Dec 2008 21:12:30 -0500
Subject: [zfs-discuss] ZFS Kernel Panic
Message-ID: <43480f020812051812k54a32a5bi13c3bd3d5be18de5@mail.gmail.com>

Noel,

I got this kernel panic yesterday.  The machine was basically idle, but I
have implemented Alex's crontab to snapshot hourly.  I don't have scrubs
scheduled.

Regards,
Mike

Fri Dec  5 14:05:19 2008
panic(cpu 0 caller 0x00B6DBD7): "[ZFS]: assertion failed in
/Volumes/pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/dnode_sync.c
line 397: pass <
100"@/Volumes/pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/dnode_sync.c:397
Backtrace (CPU 0), Frame : Return Address (4 potential args on stack)
0x47b1fa38 : 0x12b0fa (0x459234 0x47b1fa6c 0x133243 0x0)
0x47b1fa88 : 0xb6dbd7 (0xbda4f4 0xbd9d40 0x18d 0xbda4e8)
0x47b1fba8 : 0xb6665d (0x167734f8 0x0 0x47b1fc18 0x5a97600)
0x47b1fbe8 : 0xb66866 (0x47b1fc28 0x0 0x5a97600 0x11cfca40)
0x47b1fc48 : 0xb741ef (0x1210c600 0x12150800 0x47b1fc68 0xb9d18a)
0x47b1fc68 : 0xb604a7 (0x7b616050 0x1210c600 0x0 0x11c61890)
0x47b1fc88 : 0xb6112e (0x7b6160a0 0x11c61890 0x47b1fcb8 0xb9e4ab)
0x47b1fcd8 : 0xb66455 (0x7b616050 0x11c61890 0x11c61890 0x0)
0x47b1fcf8 : 0xba3c22 (0x11c61890 0x16ae0400 0x800 0x47b1fd1c)
0x47b1fd38 : 0xba5a23 (0x16ae0000 0x0 0x0 0x7b4f5000)
0x47b1fd78 : 0x202e0c (0x1f000000 0xcf1c5a11 0x16ae0000 0x3)
0x47b1fdb8 : 0x1f619c (0x47b1fde8 0x246 0x47b1fe18 0x1da35d)
0x47b1fe18 : 0x1ec476 (0xacddea0 0xcf1c5a11 0x16ae0000 0x3)
0x47b1fe78 : 0x3661c2 (0x583f0f0 0xcf1c5a11 0x16ae0000 0x47b1ff50)
0x47b1fe98 : 0x38cb52 (0x583f0f0 0xcf1c5a11 0x16ae0000 0x47b1ff50)
0x47b1ff78 : 0x3ddd6e (0xc63d6f0 0xbd15340 0xbd15384 0xffffffff)
Backtrace continues...
      Kernel loadable modules in backtrace (with dependencies):
         com.apple.filesystems.zfs(8.0)@0xb42000->0xc0dfff

BSD process name corresponding to current thread: zfs

Mac OS version:
9F33

Kernel version:
Darwin Kernel Version 9.5.0: Wed Sep  3 11:29:43 PDT 2008;
root:xnu-1228.7.58~1/RELEASE_I386
System model name: MacBookPro2,2 (Mac-F42187C8)


Model: MacBookPro2,2, BootROM MBP22.00A5.B07, 2 processors, Intel Core 2
Duo, 2.33 GHz, 3 GB
Graphics: kHW_ATIr520Item, ATY,RadeonX1600, spdisplays_pcie_device, 256 MB
Memory Module: BANK 0/DIMM0, 2 GB, DDR2 SDRAM, 667 MHz
Memory Module: BANK 1/DIMM1, 1 GB, DDR2 SDRAM, 667 MHz
AirPort: spairport_wireless_card_type_airport_extreme, 1.4.8.0
Bluetooth: Version 2.1.0f17, 2 service, 0 devices, 1 incoming serial ports
Network Service: Parallels Shared Networking Adapter, Ethernet, en2
Network Service: Parallels Host-Only Networking Adapter, Ethernet, en3
Serial ATA Device: WDC WD2500BEVS-22UST0, 232.89 GB
Parallel ATA Device: HL-DT-ST DVDRW GWA4080MA
USB Device: Built-in iSight, Micron, high_speed, 500 mA
USB Device: Bluetooth USB Host Controller, Apple, Inc., full_speed, 500 mA
USB Device: IR Receiver, Apple Computer, Inc., full_speed, 500 mA
USB Device: Apple Internal Keyboard / Trackpad, Apple Computer, full_speed,
500 mA
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081205/6ff98a9c/attachment.html>

From fruitboi at gmail.com  Fri Dec  5 18:29:59 2008
From: fruitboi at gmail.com (Aaron Berland)
Date: Fri, 5 Dec 2008 20:29:59 -0600
Subject: [zfs-discuss] Case of the missing file system
Message-ID: <f8207d190812051829v4ef76e61oc09985eb446709b0@mail.gmail.com>

Hi all,

I finally dug down into snapshots.  I made a snapshot of my entire zpool
sending it to a file on my new, empty pool.  Here is my bash history.  You
can see I forgot about piped sends not working.

407  zfs snapshot macz/ab at dec04-2008-migration
  408  zfs send macz/ab/@dec04-2008-migration | zfs recv puddle/ab
  409  zfs send macz/ab at dec04-2008-migration | zfs recv puddle/ab
  410  zfs list
  411  zfs send macz/ab at dec04-2008-migration | zfs recv puddle/ab
  412  zfs send macz/ab at dec04-2008-migration >
puddle/back/migration_snapfile
  413  zfs send macz/ab at dec04-2008-migration >
/Volumes/puddle/back/migration_snap
...
424  zfs recv puddle/ab <migration_snap

MacStor:puddle ab$ zfs list
NAME                             USED  AVAIL  REFER  MOUNTPOINT
lil                              208G  20.7G   396K  /Volumes/lil
lil/junk                         208G  20.7G   208G  /Volumes/lil/junk
macz                             519G  30.0G  74.6K  /Volumes/macz
macz/ab                          517G  30.0G   517G  /Volumes/macz/ab
macz/ab at sunday                   108K      -   517G  -
macz/ab at dec04-2008-migration        0      -   517G  -
macz/backup                      411K  30.0G   411K  /Volumes/macz/backup
macz/corey                      31.3K  30.0G  31.3K  /Volumes/macz/corey
.....
macz/deb                        1.52G  30.0G  1.52G  /Volumes/macz/deb
puddle                          1.02T   324G  89.2K  /Volumes/puddle
puddle/ab                        523G   324G   523G  /Volumes/puddle/ab
puddle/ab at dec04-2008-migration      0      -   523G  -
puddle/back                      525G   324G   525G  /Volumes/puddle/back

We can clearly see that the puddle pool has a zfs filesystem & snap of the
corresponding receive for puddle/ab.

MacStor:puddle ab$ ls
back

But puddle/ab is not in the directory and I cannot access the files!

One thing to note is that I started the recv and went to bed and my laptop
went to sleep.  The ssh connection from my MBP to my zfs box was reset so I
did not get any errors that may have been thrown.  Can't find anything in
Console, but I think it wrapped.

I exported the pool and tried to import it.. Poof! It's gone?!

MacStor:Volumes ab$ zpool export puddle
MacStor:Volumes ab$ zpool import puddle
cannot import 'puddle': no such pool available
MacStor:Volumes ab$ zpool import
no pools available to import
MacStor:Volumes ab$ zfs list
NAME                           USED  AVAIL  REFER  MOUNTPOINT
lil                            208G  20.7G   393K  /Volumes/lil
lil/junk                       208G  20.7G   208G  /Volumes/lil/junk
macz                           519G  30.0G  77.9K  /Volumes/macz
macz/ab                        517G  30.0G   517G  /Volumes/macz/ab
macz/ab at sunday                 108K      -   517G  -
macz/ab at dec04-2008-migration  78.6K      -   517G  -
macz/backup                    414K  30.0G   414K  /Volumes/macz/backup
macz/corey                    33.3K  30.0G  33.3K  /Volumes/macz/corey
macz/deb                      1.52G  30.0G  1.52G  /Volumes/macz/deb

I logged my user out of the gui and then tried the import via ssh and it did
work.  Now I can see and browse puddle/ab   Whew!

Is this a bug with the Mac ZFS port or something I did wrong?

Aaron
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081205/54e799c6/attachment.html>

From mailinglists at MailNewsRSS.com  Mon Dec  8 15:40:26 2008
From: mailinglists at MailNewsRSS.com (Jason Todd Slack-Moehrle)
Date: Mon, 8 Dec 2008 15:40:26 -0800
Subject: [zfs-discuss] ColdFusion Developer 8 and ZFS issue
Message-ID: <AEAF57F4-FEC1-470A-B62C-D082D7F3499B@MailNewsRSS.com>

Hi All,

I have a ZFS partition where my ~ folder is and all of my data. I  
changed OS X to point to the ZFS partition for my ~ folder.

I am trying to install ColdFusion Developer 8 and with it starts the  
install I get a permissions error for /Library/Startup items. if the  
error is true it says my current user does not have permissions to  
install a startupitem in /Library Startup.

Does anyone know if this is related to my ~ being on a ZFS partition?  
I have never had this error before.

I can try installing as a users whos ~ is not on the ZFS partition and  
see if it goes away.

-Jason


From alex.blewitt at gmail.com  Mon Dec  8 15:47:39 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Mon, 8 Dec 2008 23:47:39 +0000
Subject: [zfs-discuss] ColdFusion Developer 8 and ZFS issue
In-Reply-To: <AEAF57F4-FEC1-470A-B62C-D082D7F3499B@MailNewsRSS.com>
References: <AEAF57F4-FEC1-470A-B62C-D082D7F3499B@MailNewsRSS.com>
Message-ID: <636fd28e0812081547q530624f6ld2715980d860deb9@mail.gmail.com>

It might be a ZFS issue that is masquerading as a different problem
... but it might also be that your user isn't a member of the 'admin'
group (whether the 'user can administer this computer' is checked in
the user accounts preference) or the preferences on the
/Library/StartupItems are broken (or non existent).

Try creating an empty directory under /Library/StartupItems, and if
that doesn't work, then try as an admin user. Note that there's no
space between 'startup' and 'items', although I'd be surprised if the
ColdFusion gets it wrong.

Alex

On Mon, Dec 8, 2008 at 11:40 PM, Jason Todd Slack-Moehrle
<mailinglists at mailnewsrss.com> wrote:
> Hi All,
>
> I have a ZFS partition where my ~ folder is and all of my data. I changed OS
> X to point to the ZFS partition for my ~ folder.
>
> I am trying to install ColdFusion Developer 8 and with it starts the install
> I get a permissions error for /Library/Startup items. if the error is true
> it says my current user does not have permissions to install a startupitem
> in /Library Startup.
>
> Does anyone know if this is related to my ~ being on a ZFS partition? I have
> never had this error before.
>
> I can try installing as a users whos ~ is not on the ZFS partition and see
> if it goes away.
>
> -Jason
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

From mailinglists at MailNewsRSS.com  Tue Dec  9 10:39:36 2008
From: mailinglists at MailNewsRSS.com (Jason Todd Slack-Moehrle)
Date: Tue, 9 Dec 2008 10:39:36 -0800
Subject: [zfs-discuss] ZFS Deleting Trash
Message-ID: <E6BC389D-D1D6-4164-A865-D1B92E0D439B@MailNewsRSS.com>

Hi All,

I have a ZFS partition that is where my ! directory is stored.

When I empty the trash from the Finder:

a. it takes forever!

b. Any data that I put in the trash that was from my ZFS partition is  
not emptied.

Is there a ZFS command to empty the trash from a command-line? I would  
not initially think so since that is really the OS's job.

I can just do an 'rm -rf ~/.Trash'.

Are there any other Trash locations?

-Jason


From mailinglists at MailNewsRSS.com  Tue Dec  9 11:05:52 2008
From: mailinglists at MailNewsRSS.com (Jason Todd Slack-Moehrle)
Date: Tue, 9 Dec 2008 11:05:52 -0800
Subject: [zfs-discuss] ZFS Deleting Trash
In-Reply-To: <38764EBD-2959-47F7-9CE3-E0BF6AF55E47@mac.com>
References: <E6BC389D-D1D6-4164-A865-D1B92E0D439B@MailNewsRSS.com>
	<38764EBD-2959-47F7-9CE3-E0BF6AF55E47@mac.com>
Message-ID: <1D7BEC56-A48C-4E18-AEF0-9F6EF11C6697@MailNewsRSS.com>

Hi Dirk,

>> Are there any other Trash locations?
>
> If you delete as root I believe the trash will be on / of the zfs  
> volume.
> ( can't check for the moment )

Yup there is a .Trashes directory there.

Thanks,

-Jason


From alex.blewitt at gmail.com  Tue Dec  9 14:11:21 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Tue, 9 Dec 2008 22:11:21 +0000
Subject: [zfs-discuss] ZFS Deleting Trash
In-Reply-To: <E6BC389D-D1D6-4164-A865-D1B92E0D439B@MailNewsRSS.com>
References: <E6BC389D-D1D6-4164-A865-D1B92E0D439B@MailNewsRSS.com>
Message-ID: <EE6B6AA7-AE59-4F59-B770-F94EA51A626C@gmail.com>

There's a known issue that means trashes on ZFS partitions don't get  
emptied with the "empty trash" command. However, you can "securely  
empty trash" or as you noted fire up an rm -rf on your .trash dir.

Alex

Sent from my (new) iPhone

On 9 Dec 2008, at 18:39, Jason Todd Slack-Moehrle <mailinglists at MailNewsRSS.com 
 > wrote:

> Hi All,
>
> I have a ZFS partition that is where my ! directory is stored.
>
> When I empty the trash from the Finder:
>
> a. it takes forever!
>
> b. Any data that I put in the trash that was from my ZFS partition  
> is not emptied.
>
> Is there a ZFS command to empty the trash from a command-line? I  
> would not initially think so since that is really the OS's job.
>
> I can just do an 'rm -rf ~/.Trash'.
>
> Are there any other Trash locations?
>
> -Jason
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From dirkschelfhout at mac.com  Tue Dec  9 18:10:26 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Wed, 10 Dec 2008 03:10:26 +0100
Subject: [zfs-discuss] zfs on solaris in virtualbox
Message-ID: <5945F6E3-C8F2-427E-94FF-EAB9B84C81DC@mac.com>

Hi,

I finally managed to get solaris running inside virtualbox. Doing this  
on a macpro
that has a 3 disk raidz. I Haven't given solaris access yet to the esata
controller as I don't want to mess things up. Currently the vdi image  
is on the raidz , but I could change this.

Has anyone tried something like this ? Wouldn't this be the most  
stable zfs for osx if it can be done ? ( giving up 1 cpu )
And nfs sharing the zfs into osx ?

Any ideas or any suggestions on which steps to follow ?

It was nice meeting up with good old solaris again, even met an old  
friend called snoop . :-)

Dirk

From beier.andreas at gmail.com  Wed Dec 10 03:41:50 2008
From: beier.andreas at gmail.com (Beier Andreas)
Date: Wed, 10 Dec 2008 12:41:50 +0100
Subject: [zfs-discuss] Using ZFS- Partition as Home- Directory
Message-ID: <2C5148C5-E33D-4085-8B84-85EEF3430B6A@gmail.com>

Dear List,

after two weaks of using zfs as my HomeDirectory i have the following  
problems and no solution:

1. Adding Disks to the Sidebar:
I did an zfs create "HomeSweetHome/Users/username/Documents" (and for  
Library/Downloads/...)
I can't add this items to the Sidebar. But i can add them to the  
"Topbar" next to the Search Field

2. Searching in Mail.app
I cannot search in the body of an email, "During indexing you cannot  
search". This point is really annoying,
because i should be able to search through my emails.

Thanks for your help or comments/tips about using zfs for  
HomeDirectories

Kind Regards

Andreas

From andy at aligature.com  Wed Dec 10 05:58:47 2008
From: andy at aligature.com (Andrew Webber)
Date: Wed, 10 Dec 2008 08:58:47 -0500
Subject: [zfs-discuss] Using ZFS- Partition as Home- Directory
In-Reply-To: <2C5148C5-E33D-4085-8B84-85EEF3430B6A@gmail.com>
References: <2C5148C5-E33D-4085-8B84-85EEF3430B6A@gmail.com>
Message-ID: <60b50dc10812100558laab903as632ba69a6e118a97@mail.gmail.com>

You can't search with Mail.app, because I believe that it uses spotlight to
power it's search feature.  Spotlight on ZFS is broken for the time being.
Hopefully they'll have it fixed for Snow Leopard.



On Wed, Dec 10, 2008 at 6:41 AM, Beier Andreas <beier.andreas at gmail.com>wrote:

> Dear List,
>
> after two weaks of using zfs as my HomeDirectory i have the following
> problems and no solution:
>
> 1. Adding Disks to the Sidebar:
> I did an zfs create "HomeSweetHome/Users/username/Documents" (and for
> Library/Downloads/...)
> I can't add this items to the Sidebar. But i can add them to the "Topbar"
> next to the Search Field
>
> 2. Searching in Mail.app
> I cannot search in the body of an email, "During indexing you cannot
> search". This point is really annoying,
> because i should be able to search through my emails.
>
> Thanks for your help or comments/tips about using zfs for HomeDirectories
>
> Kind Regards
>
> Andreas
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081210/18d77081/attachment.html>

From zfs at encode.mine.nu  Wed Dec 10 15:59:01 2008
From: zfs at encode.mine.nu (Nathan)
Date: Thu, 11 Dec 2008 10:59:01 +1100
Subject: [zfs-discuss] rsync backups to ZFS volume
Message-ID: <7f2931472cea820a55908c7656df0872@localhost>



 Hi,  

 I recently got a new MBP, and thought I should do something about
backing it up regularly.  I had an external drive formatted as zfs,
and I figured I'd use rsync to copy files over, then do a ZFS
snapshot.  

 I'm wondering what people's thoughts are on the effectiveness of
rsync to backup a HFS+ volume to a ZFS volume.  I'm using rsync 3.0.4,
installed via darwinports, with the following flags (which I got from
http://www.jeffawaddell.com/2008/05/rsync-solaris-problems.html). 
/Volumes/backup is my ZFS volume, I'm using the ZFS-119 binaries.  

 sudo rsync -avxXHAN --fileflags --protect-args --force-change /
/Volumes/backup  

 I plan on excluding the following files and folders, although I
haven't included that in the above command:  

 /tmp/*
 /Network/*
 /cores/*
 */.Trash
 /afs/*
 /automount/*
 /private/tmp/*
 /private/var/run/*
 /private/var/spool/postfix/*
 /private/var/vm/*
 /Previous Systems.localized
 .Spotlight-*/
 Can anyone think of any issues doing this?
 I
notice applications seem to have the correct icons in
/Volumes/backup/Applications, and I can run some of them from there
with no issues.  Photoshop CS3 quits straight away, not sure if thats
due to the application itself or something I'm overlooking.  

 Thanks!  

 Regards,
 Nathan. 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081211/d1408728/attachment.html>

From alex.blewitt at gmail.com  Wed Dec 10 16:33:01 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Thu, 11 Dec 2008 00:33:01 +0000
Subject: [zfs-discuss] rsync backups to ZFS volume
In-Reply-To: <7f2931472cea820a55908c7656df0872@localhost>
References: <7f2931472cea820a55908c7656df0872@localhost>
Message-ID: <636fd28e0812101633r74358817kacffd57d52fefc69@mail.gmail.com>

I've not come across --force-change before. What does it do? It
doesn't show up in my manpage (2.6) or the one on the 3.0 series:

http://samba.anu.edu.au/ftp/rsync/rsync.html

Alex

On Wed, Dec 10, 2008 at 11:59 PM, Nathan <zfs at encode.mine.nu> wrote:
> Hi,
>
> I recently got a new MBP, and thought I should do something about backing it
> up regularly.  I had an external drive formatted as zfs, and I figured I'd
> use rsync to copy files over, then do a ZFS snapshot.
>
> I'm wondering what people's thoughts are on the effectiveness of rsync to
> backup a HFS+ volume to a ZFS volume.  I'm using rsync 3.0.4, installed via
> darwinports, with the following flags (which I got from
> http://www.jeffawaddell.com/2008/05/rsync-solaris-problems.html).
> /Volumes/backup is my ZFS volume, I'm using the ZFS-119 binaries.
>
> sudo rsync -avxXHAN --fileflags --protect-args --force-change /
> /Volumes/backup
>
> I plan on excluding the following files and folders, although I haven't
> included that in the above command:
>
> /tmp/*
> /Network/*
> /cores/*
> */.Trash
> /afs/*
> /automount/*
> /private/tmp/*
> /private/var/run/*
> /private/var/spool/postfix/*
> /private/var/vm/*
> /Previous Systems.localized
> .Spotlight-*/
>
> Can anyone think of any issues doing this?
>
> I notice applications seem to have the correct icons in
> /Volumes/backup/Applications, and I can run some of them from there with no
> issues.  Photoshop CS3 quits straight away, not sure if thats due to the
> application itself or something I'm overlooking.
>
> Thanks!
>
> Regards,
> Nathan.
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>

From zorg at sogeeky.net  Wed Dec 10 16:36:45 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Wed, 10 Dec 2008 16:36:45 -0800
Subject: [zfs-discuss] rsync backups to ZFS volume
In-Reply-To: <7f2931472cea820a55908c7656df0872@localhost>
References: <7f2931472cea820a55908c7656df0872@localhost>
Message-ID: <402698AC-1275-47B6-BE01-FFDFFE5D8CB4@sogeeky.net>

While I can appreciate the desire to use zfs, why make it hard on  
yourself?  Just format the drive hfs and let TimeMachine run your  
backups. It's so much simpler.

On Dec 10, 2008, at 3:59 PM, Nathan <zfs at encode.mine.nu> wrote:

> Hi,
>
> I recently got a new MBP, and thought I should do something about  
> backing it up regularly.  I had an external drive formatted as zfs,  
> and I figured I'd use rsync to copy files over, then do a ZFS  
> snapshot.
>
> I'm wondering what people's thoughts are on the effectiveness of  
> rsync to backup a HFS+ volume to a ZFS volume.  I'm using rsync  
> 3.0.4, installed via darwinports, with the following flags (which I  
> got from http://www.jeffawaddell.com/2008/05/rsync-solaris-problems.html 
> ).  /Volumes/backup is my ZFS volume, I'm using the ZFS-119 binaries.
>
> sudo rsync -avxXHAN --fileflags --protect-args --force-change / / 
> Volumes/backup
>
> I plan on excluding the following files and folders, although I  
> haven't included that in the above command:
>
> /tmp/*
> /Network/*
> /cores/*
> */.Trash
> /afs/*
> /automount/*
> /private/tmp/*
> /private/var/run/*
> /private/var/spool/postfix/*
> /private/var/vm/*
> /Previous Systems.localized
> .Spotlight-*/
>
> Can anyone think of any issues doing this?
>
> I notice applications seem to have the correct icons in /Volumes/ 
> backup/Applications, and I can run some of them from there with no  
> issues.  Photoshop CS3 quits straight away, not sure if thats due to  
> the application itself or something I'm overlooking.
>
> Thanks!
>
> Regards,
> Nathan.
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From i_see at macnews.de  Sat Dec 13 15:47:32 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Sun, 14 Dec 2008 00:47:32 +0100
Subject: [zfs-discuss] Coding around: Steps to userland tools
In-Reply-To: <mailman.4.1229007607.6882.zfs-discuss@lists.macosforge.org>
References: <mailman.4.1229007607.6882.zfs-discuss@lists.macosforge.org>
Message-ID: <9CE4BA75-F0C0-4057-9604-084EF2651D5F@macnews.de>

Hi list,

for no apparent reason ZFS refuses to recognize the label on one of my  
partitions and thus to import the pool. As I would like to access the  
data in the pool it would be nice to be able and debug what is wrong  
during the label detection.
I only need a few files from the backup of that pool so I'd probably  
like to code something like zcp of zfscp where the user can specify a  
pool and filenames and copy them from the pool. Like in the first  
Leopard ZFS "release", I would prefer to disable any modification of  
data on disks.

Having a small toolchain in a userland process would allow debugging,  
could be portable (Tiger Macs or a Windows client would be nice, just  
to get some data out of that pool).

Has anyone dug around in the code enough to sketch the steps necessary  
to create such a tool or does anyone know if something like this has  
already be done.
(maybe I'll need to cross-post to the ZFS-FUSE-Guys or the solaris- 
list, but for now, Mac specific hints would be very welcome)

Cheers,
	ralf 

From alex.blewitt at gmail.com  Sun Dec 14 02:53:55 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Sun, 14 Dec 2008 10:53:55 +0000
Subject: [zfs-discuss] Coding around: Steps to userland tools
In-Reply-To: <9CE4BA75-F0C0-4057-9604-084EF2651D5F@macnews.de>
References: <mailman.4.1229007607.6882.zfs-discuss@lists.macosforge.org>
	<9CE4BA75-F0C0-4057-9604-084EF2651D5F@macnews.de>
Message-ID: <D219EF75-C6C6-408B-8F50-03A0B4E9FF54@gmail.com>

I believe that an RO implementation of zfs is available in grub to  
permit booting from a zfs drive - maybe that's a good starting point?

Alex

Sent from my (new) iPhone

On 13 Dec 2008, at 23:47, Ralf Bertling <i_see at macnews.de> wrote:

> Hi list,
>
> for no apparent reason ZFS refuses to recognize the label on one of  
> my partitions and thus to import the pool. As I would like to access  
> the data in the pool it would be nice to be able and debug what is  
> wrong during the label detection.
> I only need a few files from the backup of that pool so I'd probably  
> like to code something like zcp of zfscp where the user can specify  
> a pool and filenames and copy them from the pool. Like in the first  
> Leopard ZFS "release", I would prefer to disable any modification of  
> data on disks.
>
> Having a small toolchain in a userland process would allow  
> debugging, could be portable (Tiger Macs or a Windows client would  
> be nice, just to get some data out of that pool).
>
> Has anyone dug around in the code enough to sketch the steps  
> necessary to create such a tool or does anyone know if something  
> like this has already be done.
> (maybe I'll need to cross-post to the ZFS-FUSE-Guys or the solaris- 
> list, but for now, Mac specific hints would be very welcome)
>
> Cheers,
>    ralf_______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From bhofmann at mac.com  Sun Dec 14 05:43:58 2008
From: bhofmann at mac.com (Bernd Hofmann)
Date: Sun, 14 Dec 2008 14:43:58 +0100
Subject: [zfs-discuss] OpenSolaris as Server for MacOS X 10.5 - [a bit
	off-topic, sorry]
Message-ID: <BA842F97-5678-4D69-90D5-61AC0F5CB0E2@mac.com>

Hi List!

Im on my new MacBook Pro now and while i have a bunch of hds which i  
could connect via FW800 to it i would like to have the flexibility to  
go with my shiny book wherever i want in my house and have all the hds  
in a ZFS-related Server, so, maybe OSolaris in it's newest Version.

I think this could be the way to go for the time being as long as zfs  
on mac os isn't finally mature.

So the question is: is there anyone on the list who considered this or  
have already an OS-server running in his home? Any recommendations or  
links for that?

What i would like to do is to serve my media-library (musik, films,  
learning media, etc.) from that server with 5 or max. 8 HDs as raidz.  
Also i would like to use it for Apache, RoR and Webapplication related  
work, it would be great if there was a way to remote-administer the  
server from my macbook in an timbuktu-like style way...

Any takers on this?


Thanks, and sorry for the off-topicness of this mail,



Bernd



From alex.blewitt at gmail.com  Sun Dec 14 16:00:38 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Mon, 15 Dec 2008 00:00:38 +0000
Subject: [zfs-discuss] OpenSolaris as Server for MacOS X 10.5 - [a bit
	off-topic, sorry]
In-Reply-To: <BA842F97-5678-4D69-90D5-61AC0F5CB0E2@mac.com>
References: <BA842F97-5678-4D69-90D5-61AC0F5CB0E2@mac.com>
Message-ID: <D52C0FDB-4933-489B-8AD9-463FC8C63BA6@gmail.com>

I run a mac mini with 4g of ram (only 3 usable) and it acts as a hub,  
fileserver, mail server etc. with FireWire drives. The Mac can handle  
it - before I was running the lot on a  450Mhz Cube with osx server.  
The configuration of the system is really the only bit that you have  
to get used to - OpenSolaris is different from Mac which is different  
from Linux.

I'd say the NFSv4 support is only available in Solaris, but NFSv3 is  
OK in Mac; but the zfs share is missing in Mac and won't be there for  
a while - that's about the only downside to the Mac solution.

Unless you have money to burn, I wouldn't bother getting OSX Server.  
Yes, it makes some things easy but it also makes the average things  
difficult and pretty undocumented. Most unix packages can be  
recompiled for OSX anyway.

Alex

Sent from my (new) iPhone

On 14 Dec 2008, at 13:43, Bernd Hofmann <bhofmann at mac.com> wrote:

> Hi List!
>
> Im on my new MacBook Pro now and while i have a bunch of hds which i  
> could connect via FW800 to it i would like to have the flexibility  
> to go with my shiny book wherever i want in my house and have all  
> the hds in a ZFS-related Server, so, maybe OSolaris in it's newest  
> Version.
>
> I think this could be the way to go for the time being as long as  
> zfs on mac os isn't finally mature.
>
> So the question is: is there anyone on the list who considered this  
> or have already an OS-server running in his home? Any  
> recommendations or links for that?
>
> What i would like to do is to serve my media-library (musik, films,  
> learning media, etc.) from that server with 5 or max. 8 HDs as  
> raidz. Also i would like to use it for Apache, RoR and  
> Webapplication related work, it would be great if there was a way to  
> remote-administer the server from my macbook in an timbuktu-like  
> style way...
>
> Any takers on this?
>
>
> Thanks, and sorry for the off-topicness of this mail,
>
>
>
> Bernd
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From macrbg at mac.com  Sun Dec 14 16:18:29 2008
From: macrbg at mac.com (Robert Gordon)
Date: Sun, 14 Dec 2008 18:18:29 -0600
Subject: [zfs-discuss] OpenSolaris as Server for MacOS X 10.5 - [a bit
 off-topic, sorry]
In-Reply-To: <BA842F97-5678-4D69-90D5-61AC0F5CB0E2@mac.com>
References: <BA842F97-5678-4D69-90D5-61AC0F5CB0E2@mac.com>
Message-ID: <37205DCA-7C59-4CB3-957B-BCAB9DF74C0C@mac.com>


I'm still using this http://blogs.sun.com/macrbg/entry/storage and i  
also use this http://www.apple.com/downloads/macosx/system_disk_utilities/globalsaniscsiinitiator.html 
  .. Things seem to operate well, For my MacAir I use Time Machine,  
but for my iMac i still use the backup program (the one that is  
available via .mac/me.com .. I use NFSv3 to mount media files to the  
iMac, and have no issues streaming to an Apple TV or MBP / MacAir /  
Mac Mini -- Yes, i have an addiction to Apple products.

Robert..

On Dec 14, 2008, at 7:43 AM, Bernd Hofmann wrote:

> Hi List!
>
> Im on my new MacBook Pro now and while i have a bunch of hds which i  
> could connect via FW800 to it i would like to have the flexibility  
> to go with my shiny book wherever i want in my house and have all  
> the hds in a ZFS-related Server, so, maybe OSolaris in it's newest  
> Version.
>
> I think this could be the way to go for the time being as long as  
> zfs on mac os isn't finally mature.
>
> So the question is: is there anyone on the list who considered this  
> or have already an OS-server running in his home? Any  
> recommendations or links for that?
>
> What i would like to do is to serve my media-library (musik, films,  
> learning media, etc.) from that server with 5 or max. 8 HDs as  
> raidz. Also i would like to use it for Apache, RoR and  
> Webapplication related work, it would be great if there was a way to  
> remote-administer the server from my macbook in an timbuktu-like  
> style way...
>
> Any takers on this?
>
>
> Thanks, and sorry for the off-topicness of this mail,
>
>
>
> Bernd
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From lopez.on.the.lists at yellowspace.net  Mon Dec 15 03:29:04 2008
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Mon, 15 Dec 2008 12:29:04 +0100
Subject: [zfs-discuss] OpenSolaris as Server for MacOS X 10.5 - [a bit
	off-topic, sorry]
In-Reply-To: <BA842F97-5678-4D69-90D5-61AC0F5CB0E2@mac.com>
References: <BA842F97-5678-4D69-90D5-61AC0F5CB0E2@mac.com>
Message-ID: <3033784D-77E2-4F53-8F34-0581FB32931F@yellowspace.net>

I'd give a look at nexenta - http://www.nexenta.org/
haven't used it yet but if I had the need for a working
ZFS pool server I'd go for that.
You should be able to get netatalk (2.x) compiled
on it (there's no package for it yet) so that
shouldn't give any problems for the beginning.

The downside is extended attributes and acls support:
the former aren't placed in their appropriate space
(but rather homed in several .AppleDouble directories),
the latter aren't supported at all afaik. So that means
You'd have to retransfer the files over the Finder when
ZFS is available bugfree for Mac OS X.
(same problem with NFS afaik - with the only difference
that you'd have lots of ._file files instead of
the netatalk .AppleDouble dirs)

Regards,

Lorenzo

On 14.12.2008, at 14:43, Bernd Hofmann wrote:

> Hi List!
>
> Im on my new MacBook Pro now and while i have a bunch of hds which i  
> could connect via FW800 to it i would like to have the flexibility  
> to go with my shiny book wherever i want in my house and have all  
> the hds in a ZFS-related Server, so, maybe OSolaris in it's newest  
> Version.
>
> I think this could be the way to go for the time being as long as  
> zfs on mac os isn't finally mature.
>
> So the question is: is there anyone on the list who considered this  
> or have already an OS-server running in his home? Any  
> recommendations or links for that?
>
> What i would like to do is to serve my media-library (musik, films,  
> learning media, etc.) from that server with 5 or max. 8 HDs as  
> raidz. Also i would like to use it for Apache, RoR and  
> Webapplication related work, it would be great if there was a way to  
> remote-administer the server from my macbook in an timbuktu-like  
> style way...
>
> Any takers on this?
>
>
> Thanks, and sorry for the off-topicness of this mail,
>
>
>
> Bernd
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From zorg at sogeeky.net  Tue Dec 16 23:36:49 2008
From: zorg at sogeeky.net (Mr. Zorg ...)
Date: Tue, 16 Dec 2008 23:36:49 -0800
Subject: [zfs-discuss] Questions about hot spares
Message-ID: <fbaadd320812162336o6feb57dfve6c4445de25f947f@mail.gmail.com>

Uh oh, it's my turn to ask for help/advice.  First, I want to say I
have not lost any data, so no worries there.  But, my zfs pool is on a
set of external firewire drives and I've had ongoing problems with bad
capacitors taking out the power supplies on them.  I fix them when
that happens and resilver and all is well since I'm running as raidz1.
 Recently I added a hot spare on the idea that it would automatically
spring in to action a replace a failed drive.  But it didn't.  Here's
what things looked like:

$ zpool status
  pool: Storage
 state: DEGRADED
status: One or more devices could not be used because the label is missing or
	invalid.  Sufficient replicas exist for the pool to continue
	functioning in a degraded state.
action: Replace the device using 'zpool replace'.
   see: http://www.sun.com/msg/ZFS-8000-4J
 scrub: none requested
config:

	NAME                    STATE     READ WRITE CKSUM
	Storage                 DEGRADED     0     0     0
	  raidz1                DEGRADED     0     0     0
	    disk7s2             ONLINE       0     0     0
	    disk6s2             ONLINE       0     0     0
	    699198789361538667  FAULTED      0     0     0  was /dev/disk6s2
	    disk4s2             ONLINE       0     0     0
	    disk3s2             ONLINE       0     0     0
	    disk2s2             ONLINE       0     0     0
	spares
	  disk1s2               AVAIL

errors: No known data errors

So my questions are as follows:

1) Why didn't the hot spare automatically fill in for the faulted
disk?  Isn't that what they're for?

2) I know you can't grow a raidz, but can you add a disk to convert a
raidz1 to raidz2?

3) While the disk was in a quazi-weird state (that's a technical
term!) I couldn't run zpool status without locking up.  I guessed at
what was happening and tracked down the back drive.  Only when it was
powered off could I run zpool status successfully.  Others have seen
similar issues, as I recall.  Has this been fixed yet?  Any ETA on new
bits?

Thank you!

From ndellofano at apple.com  Wed Dec 17 08:18:38 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed, 17 Dec 2008 08:18:38 -0800
Subject: [zfs-discuss] Questions about hot spares
In-Reply-To: <fbaadd320812162336o6feb57dfve6c4445de25f947f@mail.gmail.com>
References: <fbaadd320812162336o6feb57dfve6c4445de25f947f@mail.gmail.com>
Message-ID: <7D2F31B4-D5DE-4C6A-A542-1BEE48502269@apple.com>

> 1) Why didn't the hot spare automatically fill in for the faulted
> disk?  Isn't that what they're for?

Eek, my fault for not advertising this issue better since I didn't  
think anyone was using it.  Hot spares don't currently work because  
there is no disk ioctl notification in OSX to tell me that a disk has  
died/disappeard/gone AWOL.  There is notification for disks via IOkit  
and the iokit layer.  But to play in that sandbox you need to be an  
iokit kext.  We've recently just finished converting zfs into an IOkit  
kext, however because if changes on both our side and kernel, these  
bits will not work on Leopard.  It has to be the new release, hence if  
you're on the Seed program you'll get it soon, otherwise you will have  
to wait till Snow Leopard gets released.

> 2) I know you can't grow a raidz, but can you add a disk to convert a
> raidz1 to raidz2?

Nope, this isn't possible either.  This migration also would require  
rewriting lots of block pointers which gets messy.  Sun is working on  
getting block pointer rewrite to work, but it's not ready yet last I'd  
checked.

> 3) While the disk was in a quazi-weird state (that's a technical
> term!) I couldn't run zpool status without locking up.  I guessed at
> what was happening and tracked down the back drive.  Only when it was
> powered off could I run zpool status successfully.  Others have seen
> similar issues, as I recall.  Has this been fixed yet?  Any ETA on new
> bits?

This issue should be fixed in the new IOkit bits, now that we get  
actual disk status. (p.s. I like your new technical term :)  )

Noel

On Dec 16, 2008, at 11:36 PM, Mr. Zorg ... wrote:

>
> 2) I know you can't grow a raidz, but can you add a disk to convert a
> raidz1 to raidz2?
>
> 3) While the disk was in a quazi-weird state (that's a technical
> term!) I couldn't run zpool status without locking up.  I guessed at
> what was happening and tracked down the back drive.  Only when it was
> powered off could I run zpool status successfully.  Others have seen
> similar issues, as I recall.  Has this been fixed yet?  Any ETA on new
> bits?


From dillon at terranova.net  Wed Dec 17 08:33:16 2008
From: dillon at terranova.net (Dillon)
Date: Wed, 17 Dec 2008 11:33:16 -0500
Subject: [zfs-discuss] Questions about hot spares
In-Reply-To: <7D2F31B4-D5DE-4C6A-A542-1BEE48502269@apple.com>
References: <fbaadd320812162336o6feb57dfve6c4445de25f947f@mail.gmail.com>
	<7D2F31B4-D5DE-4C6A-A542-1BEE48502269@apple.com>
Message-ID: <494929CC.2060503@terranova.net>

On 12/17/08 11:18 AM, No?l Dellofano wrote:
>  however because if changes on both our side and kernel, these bits 
> will not work on Leopard.  It has to be the new release, hence if 
> you're on the Seed program you'll get it soon, otherwise you will have 
> to wait till Snow Leopard gets released.
So does this mean that there isn't going to be another zfs release that 
will work with 10.5? or does it just mean that the next release will not 
fix this problem because 10.6 is required?

From johnemac at tekserve.com  Wed Dec 17 08:56:35 2008
From: johnemac at tekserve.com (John McAdams)
Date: Wed, 17 Dec 2008 11:56:35 -0500
Subject: [zfs-discuss] zfs 10.6
Message-ID: <67AF937E-2A8E-4B74-83C4-8943048B3287@tekserve.com>

Regarding the discussion about hot spares and the IOKit in 10.6...

I saw an article somewhere that had some of the ZFS commands included  
with 10.6 (I have 10.6 build 10A222, and I'm working setting up a zfs  
share). My 119 build ZFS mirror is crashing during a copy so I thought  
I would rebuild them. It looks like I can still use all the old  
commands zpool & zfs but I thought I saw some new ones, I just can't  
recall where. Is there any documentation for ZFS in 10.6?

My apologies if this is the wrong place to discuss 10.6.

Best regards,
-- jmca

John McAdams
Professional Services Engineer
johnemac at tekserve.com
c: 917-496-0106
? ACSA 10.5


From zorg at sogeeky.net  Wed Dec 17 09:38:55 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Wed, 17 Dec 2008 09:38:55 -0800
Subject: [zfs-discuss] Questions about hot spares
In-Reply-To: <7D2F31B4-D5DE-4C6A-A542-1BEE48502269@apple.com>
References: <fbaadd320812162336o6feb57dfve6c4445de25f947f@mail.gmail.com>
	<7D2F31B4-D5DE-4C6A-A542-1BEE48502269@apple.com>
Message-ID: <7927312F-B5F1-4868-9EA5-4392E3630032@sogeeky.net>

My data is still intact so no harm no foul. That's the hazards of  
playing with a beta. :)

Keep up the good work, I look forward to 10.6!

On Dec 17, 2008, at 8:18 AM, No?l Dellofano <ndellofano at apple.com>  
wrote:

>> 1) Why didn't the hot spare automatically fill in for the faulted
>> disk?  Isn't that what they're for?
>
> Eek, my fault for not advertising this issue better since I didn't  
> think anyone was using it.  Hot spares don't currently work because  
> there is no disk ioctl notification in OSX to tell me that a disk  
> has died/disappeard/gone AWOL.  There is notification for disks via  
> IOkit and the iokit layer.  But to play in that sandbox you need to  
> be an iokit kext.  We've recently just finished converting zfs into  
> an IOkit kext, however because if changes on both our side and  
> kernel, these bits will not work on Leopard.  It has to be the new  
> release, hence if you're on the Seed program you'll get it soon,  
> otherwise you will have to wait till Snow Leopard gets released.
>
>> 2) I know you can't grow a raidz, but can you add a disk to convert a
>> raidz1 to raidz2?
>
> Nope, this isn't possible either.  This migration also would require  
> rewriting lots of block pointers which gets messy.  Sun is working  
> on getting block pointer rewrite to work, but it's not ready yet  
> last I'd checked.
>
>> 3) While the disk was in a quazi-weird state (that's a technical
>> term!) I couldn't run zpool status without locking up.  I guessed at
>> what was happening and tracked down the back drive.  Only when it was
>> powered off could I run zpool status successfully.  Others have seen
>> similar issues, as I recall.  Has this been fixed yet?  Any ETA on  
>> new
>> bits?
>
> This issue should be fixed in the new IOkit bits, now that we get  
> actual disk status. (p.s. I like your new technical term :)  )
>
> Noel
>
> On Dec 16, 2008, at 11:36 PM, Mr. Zorg ... wrote:
>
>>
>> 2) I know you can't grow a raidz, but can you add a disk to convert a
>> raidz1 to raidz2?
>>
>> 3) While the disk was in a quazi-weird state (that's a technical
>> term!) I couldn't run zpool status without locking up.  I guessed at
>> what was happening and tracked down the back drive.  Only when it was
>> powered off could I run zpool status successfully.  Others have seen
>> similar issues, as I recall.  Has this been fixed yet?  Any ETA on  
>> new
>> bits?
>

From zorg at sogeeky.net  Wed Dec 17 11:36:00 2008
From: zorg at sogeeky.net (Mr. Zorg ...)
Date: Wed, 17 Dec 2008 11:36:00 -0800
Subject: [zfs-discuss] Questions about hot spares
In-Reply-To: <7927312F-B5F1-4868-9EA5-4392E3630032@sogeeky.net>
References: <fbaadd320812162336o6feb57dfve6c4445de25f947f@mail.gmail.com>
	<7D2F31B4-D5DE-4C6A-A542-1BEE48502269@apple.com>
	<7927312F-B5F1-4868-9EA5-4392E3630032@sogeeky.net>
Message-ID: <fbaadd320812171136r63da4c85ne459b641c0169733@mail.gmail.com>

Ack, I spoke too soon!  Now a second power supply has failed, dang it!
 I know the drives are still good, it's just the power supplies are
flaky.  So, now I've powered everything off until I can order some
more.  Once those are in, I should be able to power the drives back
up, they should no longer register as faulted and zfs should resilver
and fix itself.  I hope.  :)

On Wed, Dec 17, 2008 at 9:38 AM, Mr. Zorg <zorg at sogeeky.net> wrote:
> My data is still intact so no harm no foul. That's the hazards of playing
> with a beta. :)
>
> Keep up the good work, I look forward to 10.6!

From zorg at sogeeky.net  Wed Dec 17 13:10:31 2008
From: zorg at sogeeky.net (Mr. Zorg ...)
Date: Wed, 17 Dec 2008 13:10:31 -0800
Subject: [zfs-discuss] A feature request
Message-ID: <fbaadd320812171310j657784a3mbfe3af4a5df029f7@mail.gmail.com>

Noel, would it be possible to implement a feature that would cause the
drive light to blink on a faulted/offline/whatever disk?  Or is there
a tool already that can do this?  Assuming the drive still has some
power, that is.  Using external drives, it's *extremely* difficult to
tell which drive is dead/dying since their drive numbers change every
time you boot...

Thanks!

From zfs at sbod.at  Thu Dec 18 10:32:52 2008
From: zfs at sbod.at (Franz Schmalzl)
Date: Thu, 18 Dec 2008 19:32:52 +0100
Subject: [zfs-discuss] Wishlist
Message-ID: <64378FED-4268-443B-92E0-48FE3AEDA2E8@sbod.at>

[ To north-pole ]

Dear Mother Noelmas!

My Wishes for this Christmas:

Please bring me :

-Encrypted ZFS ( Very important )
-Bootable ZFS  ( Also very important )
-Resizable Zpools
-A nice ZFS GUI ( klickibunti! )
-Time Machine patch to handle snapshots and do my backups for me.
- Peace on Earth


Milk and Cookies will be waiting for you in Salzburg, since my chimney  
is not big enough you are welcome to use the front door.
I hope my wishes will be fulfilled, since i was a very nice little boy  
this year ( sort of ).

Kind regards,

Franz Schmalzl


;-)







From zorg at sogeeky.net  Thu Dec 18 14:17:23 2008
From: zorg at sogeeky.net (Mr. Zorg ...)
Date: Thu, 18 Dec 2008 14:17:23 -0800
Subject: [zfs-discuss] zpool status -v freezes system
Message-ID: <fbaadd320812181417m7d5166fcje58fbe27e3766063@mail.gmail.com>

I've seen this discussed here before, but I haven't seen any
conclusive answers.  During my degraded state yesterday, I did get a
little bit of corruption (probably new data trying to be written?).
zpool status tells me to run -v to see which files were affected, but
doing so locks up my system.  I've got 6x firewire drives in a raidz
(as shown in my zpool status yesterday).  Were there any conclusions
reached about this?  How else can I get the data of what files were
corrupted?

Thank you!

From mcamou at tecnoguru.com  Thu Dec 18 15:27:33 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Fri, 19 Dec 2008 00:27:33 +0100
Subject: [zfs-discuss] zpool status -v freezes system
In-Reply-To: <fbaadd320812181417m7d5166fcje58fbe27e3766063@mail.gmail.com>
References: <fbaadd320812181417m7d5166fcje58fbe27e3766063@mail.gmail.com>
Message-ID: <5678F403-4B6C-4AEE-915B-628314735489@tecnoguru.com>

Hi,

The only way I found of doing that was booting my Mac Mini with an  
OpenSolaris LiveDVD (http://www.opensolaris.org) and doing the checks  
from there. If the drives were USB you could use VMWare or Parallels,  
but AFAIK they don't support sharing of FireWire drives.

HTH,
-Mario.

On 18 Dec 2008, at 23:17, Mr. Zorg ... wrote:

> I've seen this discussed here before, but I haven't seen any
> conclusive answers.  During my degraded state yesterday, I did get a
> little bit of corruption (probably new data trying to be written?).
> zpool status tells me to run -v to see which files were affected, but
> doing so locks up my system.  I've got 6x firewire drives in a raidz
> (as shown in my zpool status yesterday).  Were there any conclusions
> reached about this?  How else can I get the data of what files were
> corrupted?
>
> Thank you!
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dwebb at erbco.com  Thu Dec 18 15:58:25 2008
From: dwebb at erbco.com (Dustin Webb)
Date: Thu, 18 Dec 2008 18:58:25 -0500
Subject: [zfs-discuss] zpool status -v freezes system
In-Reply-To: <5678F403-4B6C-4AEE-915B-628314735489@tecnoguru.com>
References: <fbaadd320812181417m7d5166fcje58fbe27e3766063@mail.gmail.com>
	<5678F403-4B6C-4AEE-915B-628314735489@tecnoguru.com>
Message-ID: <8694B076-D859-4336-A280-7EA048DA0281@erbco.com>

If using another OS is the only way then I would checkout Nexenta  
(free, no ZFS GUI) or NexentaStor (free for under 1 TB, great NAS  
style GUI).  I used them for my ZFS needs for a while and I would  
think they support firewire.

Dustin


On Dec 18, 2008, at 6:27 PM, Mario Camou wrote:

> Hi,
>
> The only way I found of doing that was booting my Mac Mini with an  
> OpenSolaris LiveDVD (http://www.opensolaris.org) and doing the  
> checks from there. If the drives were USB you could use VMWare or  
> Parallels, but AFAIK they don't support sharing of FireWire drives.
>
> HTH,
> -Mario.
>
> On 18 Dec 2008, at 23:17, Mr. Zorg ... wrote:
>
>> I've seen this discussed here before, but I haven't seen any
>> conclusive answers.  During my degraded state yesterday, I did get a
>> little bit of corruption (probably new data trying to be written?).
>> zpool status tells me to run -v to see which files were affected, but
>> doing so locks up my system.  I've got 6x firewire drives in a raidz
>> (as shown in my zpool status yesterday).  Were there any conclusions
>> reached about this?  How else can I get the data of what files were
>> corrupted?
>>
>> Thank you!
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From bwaters at nrao.edu  Thu Dec 18 21:22:44 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Thu, 18 Dec 2008 22:22:44 -0700
Subject: [zfs-discuss] zpool status -v freezes system
In-Reply-To: <8694B076-D859-4336-A280-7EA048DA0281@erbco.com>
References: <fbaadd320812181417m7d5166fcje58fbe27e3766063@mail.gmail.com>
	<5678F403-4B6C-4AEE-915B-628314735489@tecnoguru.com>
	<8694B076-D859-4336-A280-7EA048DA0281@erbco.com>
Message-ID: <164ABB22-F185-47C6-942A-9A49E5210187@nrao.edu>

If you're going to use Nexenta, be aware that the userspace tools are  
GNU (not BSD). In particular, the 'ls' command doesn't recognize the '- 
@' option to display extended attributes.

OpenSolaris has gotten pretty nice these days. (it's still not close  
to Mac OSX for desktop use.)


On Dec 18, 2008, at 4:58 PM, Dustin Webb wrote:

> If using another OS is the only way then I would checkout Nexenta



From zorg at sogeeky.net  Thu Dec 18 22:30:50 2008
From: zorg at sogeeky.net (Mr. Zorg ...)
Date: Thu, 18 Dec 2008 22:30:50 -0800
Subject: [zfs-discuss] zpool status -v freezes system
In-Reply-To: <164ABB22-F185-47C6-942A-9A49E5210187@nrao.edu>
References: <fbaadd320812181417m7d5166fcje58fbe27e3766063@mail.gmail.com>
	<5678F403-4B6C-4AEE-915B-628314735489@tecnoguru.com>
	<8694B076-D859-4336-A280-7EA048DA0281@erbco.com>
	<164ABB22-F185-47C6-942A-9A49E5210187@nrao.edu>
Message-ID: <fbaadd320812182230y4f248ae5nc22a2d5c8fc5e7e0@mail.gmail.com>

OK, so this is interesting.  After completing the resilver, I then ran
a scrub.  Here's the zpool status just before the scrub finished, and
just afterwards.  Note that after the scrub it is no longer reporting
data errors.  Does that mean there is no corruption after all?  I
hadn't gotten around to trying an opensolaris distro yet to run zpool
status -v, but now it looks like I don't need to?

$ zpool status
  pool: Storage
 state: ONLINE
status: One or more devices has experienced an error resulting in data
	corruption.  Applications may be affected.
action: Restore the file in question if possible.  Otherwise restore the
	entire pool from backup.
   see: http://www.sun.com/msg/ZFS-8000-8A
 scrub: scrub in progress, 96.93% done, 0h13m to go
config:

	NAME         STATE     READ WRITE CKSUM
	Storage      ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk8s2  ONLINE       0     0     0
	    disk7s2  ONLINE       0     0     0
	    disk2s2  ONLINE       0     0     0
	    disk5s2  ONLINE       0     0     0
	    disk4s2  ONLINE       0     0     0
	    disk1s2  ONLINE       0     0     0
	spares
	  disk6s2    AVAIL

errors: 163 data errors, use '-v' for a list

$ zpool status
  pool: Storage
 state: ONLINE
 scrub: scrub completed with 0 errors on Thu Dec 18 21:43:58 2008
config:

	NAME         STATE     READ WRITE CKSUM
	Storage      ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk8s2  ONLINE       0     0     0
	    disk7s2  ONLINE       0     0     0
	    disk2s2  ONLINE       0     0     0
	    disk5s2  ONLINE       0     0     0
	    disk4s2  ONLINE       0     0     0
	    disk1s2  ONLINE       0     0     0
	spares
	  disk6s2    AVAIL

errors: No known data errors


On Thu, Dec 18, 2008 at 9:22 PM, Boyd Waters <bwaters at nrao.edu> wrote:
> If you're going to use Nexenta, be aware that the userspace tools are GNU
> (not BSD). In particular, the 'ls' command doesn't recognize the '-@' option
> to display extended attributes.
>
> OpenSolaris has gotten pretty nice these days. (it's still not close to Mac
> OSX for desktop use.)
>
>
> On Dec 18, 2008, at 4:58 PM, Dustin Webb wrote:
>
>> If using another OS is the only way then I would checkout Nexenta
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

From mcamou at tecnoguru.com  Fri Dec 19 02:41:56 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Fri, 19 Dec 2008 11:41:56 +0100
Subject: [zfs-discuss] zpool status -v freezes system
In-Reply-To: <fbaadd320812182230y4f248ae5nc22a2d5c8fc5e7e0@mail.gmail.com>
References: <fbaadd320812181417m7d5166fcje58fbe27e3766063@mail.gmail.com>
	<5678F403-4B6C-4AEE-915B-628314735489@tecnoguru.com>
	<8694B076-D859-4336-A280-7EA048DA0281@erbco.com>
	<164ABB22-F185-47C6-942A-9A49E5210187@nrao.edu>
	<fbaadd320812182230y4f248ae5nc22a2d5c8fc5e7e0@mail.gmail.com>
Message-ID: <762437f0812190241k112b6d5uc906bc3162a3237d@mail.gmail.com>

Now that you metion it, I seem to recall that this also happened to me. I
booted into OpenSolaris, got the list of "damaged" files and then did a
zpool scrub (in OpenSolaris). Afterwards it told me there were no more data
errors. I did a cursory check of some of the "damaged" files and they all
seemed to be OK. Bizarre.

-Mario.

On Fri, Dec 19, 2008 at 7:30 AM, Mr. Zorg ... <zorg at sogeeky.net> wrote:

> OK, so this is interesting.  After completing the resilver, I then ran
> a scrub.  Here's the zpool status just before the scrub finished, and
> just afterwards.  Note that after the scrub it is no longer reporting
> data errors.  Does that mean there is no corruption after all?  I
> hadn't gotten around to trying an opensolaris distro yet to run zpool
> status -v, but now it looks like I don't need to?
>
> $ zpool status
>  pool: Storage
>  state: ONLINE
> status: One or more devices has experienced an error resulting in data
>        corruption.  Applications may be affected.
> action: Restore the file in question if possible.  Otherwise restore the
>        entire pool from backup.
>   see: http://www.sun.com/msg/ZFS-8000-8A
>  scrub: scrub in progress, 96.93% done, 0h13m to go
> config:
>
>        NAME         STATE     READ WRITE CKSUM
>        Storage      ONLINE       0     0     0
>          raidz1     ONLINE       0     0     0
>            disk8s2  ONLINE       0     0     0
>            disk7s2  ONLINE       0     0     0
>            disk2s2  ONLINE       0     0     0
>            disk5s2  ONLINE       0     0     0
>            disk4s2  ONLINE       0     0     0
>            disk1s2  ONLINE       0     0     0
>        spares
>          disk6s2    AVAIL
>
> errors: 163 data errors, use '-v' for a list
>
> $ zpool status
>  pool: Storage
>  state: ONLINE
>  scrub: scrub completed with 0 errors on Thu Dec 18 21:43:58 2008
> config:
>
>        NAME         STATE     READ WRITE CKSUM
>        Storage      ONLINE       0     0     0
>          raidz1     ONLINE       0     0     0
>            disk8s2  ONLINE       0     0     0
>            disk7s2  ONLINE       0     0     0
>            disk2s2  ONLINE       0     0     0
>            disk5s2  ONLINE       0     0     0
>            disk4s2  ONLINE       0     0     0
>            disk1s2  ONLINE       0     0     0
>        spares
>          disk6s2    AVAIL
>
> errors: No known data errors
>
>
> On Thu, Dec 18, 2008 at 9:22 PM, Boyd Waters <bwaters at nrao.edu> wrote:
> > If you're going to use Nexenta, be aware that the userspace tools are GNU
> > (not BSD). In particular, the 'ls' command doesn't recognize the '-@'
> option
> > to display extended attributes.
> >
> > OpenSolaris has gotten pretty nice these days. (it's still not close to
> Mac
> > OSX for desktop use.)
> >
> >
> > On Dec 18, 2008, at 4:58 PM, Dustin Webb wrote:
> >
> >> If using another OS is the only way then I would checkout Nexenta
> >
> >
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss at lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>



-- 
I want to change the world but they won't give me the source code.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081219/3415bc18/attachment-0001.html>

From ksh at ironsoftware.de  Fri Dec 19 14:28:04 2008
From: ksh at ironsoftware.de (Christian Kendi)
Date: Fri, 19 Dec 2008 17:28:04 -0500
Subject: [zfs-discuss] panic()
Message-ID: <D057934A-25DA-444F-A9C1-8C12628AD436@ironsoftware.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi,

im just off to report a kernel panic. The website wont send me an  
activation mail, so i just do it here.

Thu Dec 18 15:28:26 2008
panic(cpu 0 caller 0x00C08D21): "ZFS: I/O failure (write on <unknown>  
off 0: zio 0x4a2eab0 [L0 ZFS plain file] 1800L/1800P  
DVA[0]=<0:144b278800:1800> fletcher2 uncompressed LE contiguous  
birth=26685 fill=1  
cksum=758849a10b2fd5b6:4bfc8c90062499f1:c0fc06747bfaa98b: 
1cf76936dedc736d): error " "5"@/Volumes/pixie_dust/home/ndellofano/zfs- 
work/zfs-119/zfs_kext/zfs/zio.c:918
Backtrace (CPU 0), Frame : Return Address (4 potential args on stack)
0x38893e38 : 0x12b4f3 (0x45b13c 0x38893e6c 0x1335e4 0x0)
0x38893e88 : 0xc08d21 (0xc786b0 0xc786a4 0xc74400 0xca5670)
0x38893f08 : 0xc053fb (0x4a2eab0 0x16f5 0x38893f28 0xc4c18a)
0x38893f48 : 0xc644a6 (0x4a2eab0 0x47f8800 0x271356 0xc8d5f4)
0x38893fc8 : 0x1a017c (0x481c500 0x0 0x1a30b5 0xd2036b0)
Backtrace terminated-invalid frame pointer 0
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0xbf1000->0xcbcfff

BSD process name corresponding to current thread: kernel_task

Mac OS version:
9G55

Kernel version:
Darwin Kernel Version 9.6.0: Mon Nov 24 17:37:00 PST 2008;  
root:xnu-1228.9.59~1/RELEASE_I386
System model name: MacBook1,1 (Mac-F4208CC8)

Regards,
Chris.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (Darwin)

iD8DBQFJTB/1p+9ff145KVIRAkJnAJ9DDhnUj9s7K0HBz28G/lkU9CkhEQCfQCfP
UbNbwThBjfpx4N389JqfcD4=
=/qFD
-----END PGP SIGNATURE-----

From ksh at ironsoftware.de  Fri Dec 19 14:31:46 2008
From: ksh at ironsoftware.de (Christian Kendi)
Date: Fri, 19 Dec 2008 17:31:46 -0500
Subject: [zfs-discuss] snapshots
Message-ID: <7B02C308-334C-4F2D-A31C-1CE993C40670@ironsoftware.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi,

1. I was wondering how the progress is on the .snapshots feature is?

2. I'm using ZFS as my home directory now. There are certain directories
which i dont like to snapshot. For example, Movies and Music makes no  
sense.

Is anyone aware of a possiblity to exclude certain directories from  
getting
snapshotted?

Regards,
Chris.
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (Darwin)

iD8DBQFJTCDSp+9ff145KVIRAvLJAJ9gR0LQ3Lfg+f7/6uslgMIVjgXfCQCeLNWR
NGy8Od9CoIMeQS4Vj9I4VkU=
=aKhf
-----END PGP SIGNATURE-----

From macrbg at mac.com  Fri Dec 19 15:18:52 2008
From: macrbg at mac.com (Robert Gordon)
Date: Fri, 19 Dec 2008 17:18:52 -0600
Subject: [zfs-discuss] snapshots
In-Reply-To: <7B02C308-334C-4F2D-A31C-1CE993C40670@ironsoftware.de>
References: <7B02C308-334C-4F2D-A31C-1CE993C40670@ironsoftware.de>
Message-ID: <B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>


On Dec 19, 2008, at 4:31 PM, Christian Kendi wrote:

> Hi,
>
> 1. I was wondering how the progress is on the .snapshots feature is?
>
> 2. I'm using ZFS as my home directory now. There are certain  
> directories
> which i dont like to snapshot. For example, Movies and Music makes  
> no sense.
>
> Is anyone aware of a possiblity to exclude certain directories from  
> getting
> snapshotted?

Make Movies and Music zfs filesystems.

;-)

Robert.

From ndellofano at apple.com  Fri Dec 19 18:54:58 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 19 Dec 2008 18:54:58 -0800
Subject: [zfs-discuss] panic()
In-Reply-To: <D057934A-25DA-444F-A9C1-8C12628AD436@ironsoftware.de>
References: <D057934A-25DA-444F-A9C1-8C12628AD436@ironsoftware.de>
Message-ID: <C64A3A5A-50B0-49F7-B853-B48D0978B723@apple.com>

looks like you've either ripped out a drive prematurely, your drive  
went to sleep, or your drive had a hardware failure.  At any rate, ZFS  
can't find it to write to it so panic ensues since they were  
ansynchronous writes we've already returned success on and this would  
make disk state inconsistent.

In Snow Leopard we are thinking of changing this so that we will  
actually freeze the pipleline if this occurs and be alerted you've  
lost data.

Noel

On Dec 19, 2008, at 2:28 PM, Christian Kendi wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hi,
>
> im just off to report a kernel panic. The website wont send me an  
> activation mail, so i just do it here.
>
> Thu Dec 18 15:28:26 2008
> panic(cpu 0 caller 0x00C08D21): "ZFS: I/O failure (write on  
> <unknown> off 0: zio 0x4a2eab0 [L0 ZFS plain file] 1800L/1800P  
> DVA[0]=<0:144b278800:1800> fletcher2 uncompressed LE contiguous  
> birth=26685 fill=1  
> cksum=758849a10b2fd5b6:4bfc8c90062499f1:c0fc06747bfaa98b: 
> 1cf76936dedc736d): error " "5"@/Volumes/pixie_dust/home/ndellofano/ 
> zfs-work/zfs-119/zfs_kext/zfs/zio.c:918
> Backtrace (CPU 0), Frame : Return Address (4 potential args on stack)
> 0x38893e38 : 0x12b4f3 (0x45b13c 0x38893e6c 0x1335e4 0x0)
> 0x38893e88 : 0xc08d21 (0xc786b0 0xc786a4 0xc74400 0xca5670)
> 0x38893f08 : 0xc053fb (0x4a2eab0 0x16f5 0x38893f28 0xc4c18a)
> 0x38893f48 : 0xc644a6 (0x4a2eab0 0x47f8800 0x271356 0xc8d5f4)
> 0x38893fc8 : 0x1a017c (0x481c500 0x0 0x1a30b5 0xd2036b0)
> Backtrace terminated-invalid frame pointer 0
>      Kernel loadable modules in backtrace (with dependencies):
>         com.apple.filesystems.zfs(8.0)@0xbf1000->0xcbcfff
>
> BSD process name corresponding to current thread: kernel_task
>
> Mac OS version:
> 9G55
>
> Kernel version:
> Darwin Kernel Version 9.6.0: Mon Nov 24 17:37:00 PST 2008;  
> root:xnu-1228.9.59~1/RELEASE_I386
> System model name: MacBook1,1 (Mac-F4208CC8)
>
> Regards,
> Chris.
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.7 (Darwin)
>
> iD8DBQFJTB/1p+9ff145KVIRAkJnAJ9DDhnUj9s7K0HBz28G/lkU9CkhEQCfQCfP
> UbNbwThBjfpx4N389JqfcD4=
> =/qFD
> -----END PGP SIGNATURE-----
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Fri Dec 19 18:59:37 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 19 Dec 2008 18:59:37 -0800
Subject: [zfs-discuss] snapshots
In-Reply-To: <B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
References: <7B02C308-334C-4F2D-A31C-1CE993C40670@ironsoftware.de>
	<B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
Message-ID: <590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>

I second that.  Thats why ZFS filesystems have low overhead, they are  
closer to directories then traditional filesystems.  Plus it gives you  
a lot more fine grained control of your stuff.  For the future we're  
considering possibly doing something in the line of excluding certain  
directories, especially for backups and privacy concerns.  However for  
now, multiple zfs filesystems is the best way to go.

Noel

On Dec 19, 2008, at 3:18 PM, Robert Gordon wrote:

>
> On Dec 19, 2008, at 4:31 PM, Christian Kendi wrote:
>
>> Hi,
>>
>> 1. I was wondering how the progress is on the .snapshots feature is?
>>
>> 2. I'm using ZFS as my home directory now. There are certain  
>> directories
>> which i dont like to snapshot. For example, Movies and Music makes  
>> no sense.
>>
>> Is anyone aware of a possiblity to exclude certain directories from  
>> getting
>> snapshotted?
>
> Make Movies and Music zfs filesystems.
>
> ;-)
>
> Robert.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Fri Dec 19 19:25:11 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 19 Dec 2008 19:25:11 -0800
Subject: [zfs-discuss] Wishlist
In-Reply-To: <64378FED-4268-443B-92E0-48FE3AEDA2E8@sbod.at>
References: <64378FED-4268-443B-92E0-48FE3AEDA2E8@sbod.at>
Message-ID: <0A9D28CB-661B-42C8-9C90-E2673ED89D16@apple.com>

hehe I luuuuv the message :)


> -Encrypted ZFS ( Very important )
Sun is working on it, once they are done (in code review last I  
checked) we'll see about porting it over.

>
> -Bootable ZFS  ( Also very important )

Possibly in the future but not for right now.  We're concentrating  
efforts on stability  and boot-ability would take some time.

>
> -Resizable Zpools

We already have this ability, not sure what you're looking for here?   
Do I get a cookie for this one since it works? :)

>
> -A nice ZFS GUI ( klickibunti! )

A GUI is being worked on currently but only for Server

>
> -Time Machine patch to handle snapshots and do my backups for me.

This one is  important and likely will come sometime, but not for SL

>
> - Peace on Earth

   This is pretty big and there's no way they'd let me squeeze it into  
SL cause its a featureless release :) So you'll have to wait to the  
next major release.  Sorry :)


and be careful with the cookie offers I luuuuuv cookies!! (especially  
at Christmas time, I mean who doesn't?)

Happy Holidays!
Noel :)


On Dec 18, 2008, at 10:32 AM, Franz Schmalzl wrote:

> [ To north-pole ]
>
> Dear Mother Noelmas!
>
> My Wishes for this Christmas:
>
> Please bring me :
>
> -Encrypted ZFS ( Very important )
> -Bootable ZFS  ( Also very important )
> -Resizable Zpools
> -A nice ZFS GUI ( klickibunti! )
> -Time Machine patch to handle snapshots and do my backups for me.
> - Peace on Earth
>
>
> Milk and Cookies will be waiting for you in Salzburg, since my  
> chimney is not big enough you are welcome to use the front door.
> I hope my wishes will be fulfilled, since i was a very nice little  
> boy this year ( sort of ).
>
> Kind regards,
>
> Franz Schmalzl
>
>
> ;-)
>
>
>
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From richmc at gmail.com  Fri Dec 19 19:44:45 2008
From: richmc at gmail.com (Richard McClellan)
Date: Fri, 19 Dec 2008 19:44:45 -0800
Subject: [zfs-discuss] Wishlist
In-Reply-To: <0A9D28CB-661B-42C8-9C90-E2673ED89D16@apple.com>
References: <64378FED-4268-443B-92E0-48FE3AEDA2E8@sbod.at>
	<0A9D28CB-661B-42C8-9C90-E2673ED89D16@apple.com>
Message-ID: <E3A8B458-2102-4DAB-B34E-21A444DF511C@gmail.com>


>>
>
>>
>> -Resizable Zpools
>
> We already have this ability, not sure what you're looking for  
> here?  Do I get a cookie for this one since it works? :)

A lot of people think of resizable as adding (removing) a disk to  
(from) a vdev (not possible now) instead of adding another equivalent  
vdev to the pool.



>
>
>>
>> -A nice ZFS GUI ( klickibunti! )
>
> A GUI is being worked on currently but only for Server
>
>>
>> -Time Machine patch to handle snapshots and do my backups for me.
>
> This one is  important and likely will come sometime, but not for SL
>
>>
>> - Peace on Earth
>
>  This is pretty big and there's no way they'd let me squeeze it into  
> SL cause its a featureless release :) So you'll have to wait to the  
> next major release.  Sorry :)
>
>
> and be careful with the cookie offers I luuuuuv cookies!!  
> (especially at Christmas time, I mean who doesn't?)
>
> Happy Holidays!
> Noel :)
>
>
> On Dec 18, 2008, at 10:32 AM, Franz Schmalzl wrote:
>
>> [ To north-pole ]
>>
>> Dear Mother Noelmas!
>>
>> My Wishes for this Christmas:
>>
>> Please bring me :
>>
>> -Encrypted ZFS ( Very important )
>> -Bootable ZFS  ( Also very important )
>> -Resizable Zpools
>> -A nice ZFS GUI ( klickibunti! )
>> -Time Machine patch to handle snapshots and do my backups for me.
>> - Peace on Earth
>>
>>
>> Milk and Cookies will be waiting for you in Salzburg, since my  
>> chimney is not big enough you are welcome to use the front door.
>> I hope my wishes will be fulfilled, since i was a very nice little  
>> boy this year ( sort of ).
>>
>> Kind regards,
>>
>> Franz Schmalzl
>>
>>
>> ;-)
>>
>>
>>
>>
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From lhunath at gmail.com  Sat Dec 20 03:36:00 2008
From: lhunath at gmail.com (Maarten Billemont)
Date: Sat, 20 Dec 2008 12:36:00 +0100
Subject: [zfs-discuss] Panic On Simple Read Ops
Message-ID: <251E440E-4543-4FEF-BAE2-06DA518BC911@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

While Jaikoz was fingerprinting my music collection; the following  
panic occurred:

Sat Dec 20 12:32:10 2008
panic(cpu 0 caller 0x34D0BDD9): "zap->zap_u.zap_fat.zap_phys- 
 >zap_magic == 0x2F52AB2ABULL failed, 0 == -181751125"@/Volumes/ 
pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/zap.c:569
Backtrace (CPU 0), Frame : Return Address (4 potential args on stack)
0x344b35d8 : 0x12b0fa (0x459234 0x344b360c 0x133243 0x0)
0x344b3628 : 0x34d0bdd9 (0x34d72a28 0x34d729ec 0x0 0x34d49d7c)
0x344b3688 : 0x34d0c1cf (0x0 0x1 0x344b37e8 0x34d09f65)
0x344b3708 : 0x34d0aede (0x10529700 0x344b37e0 0x9505e00 0x0)
0x344b37a8 : 0x34d0c0e6 (0x344b37e0 0x9505e00 0x3c042 0x0)
0x344b3818 : 0x34cd2e0b (0xc1fa9b0 0x3c042 0x0 0x3add1)
0x344b38a8 : 0x1f2844 (0x344b38cc 0x77487e0 0x344b38e8 0x426a004)
0x344b38f8 : 0x1f5296 (0x7070f40 0x344b3d20 0x87bf784 0x12fa53)
0x344b3958 : 0x1c4f47 (0x7070f40 0x344b3d20 0x87bf784 0x87bf784)
0x344b3f78 : 0x3ddd6e (0x59609d0 0x87bf680 0x87bf6c4 0xbfffec04)
0x344b3fc8 : 0x19f3b3 (0x6507360 0x0 0x1a20b5 0xba867a0)
No mapping exists for frame pointer
Backtrace terminated-invalid frame pointer 0xbfffd1b8
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0x34ccd000->0x34d98fff

BSD process name corresponding to current thread: mipcore

Mac OS version:
9F33

Kernel version:
Darwin Kernel Version 9.5.0: Wed Sep  3 11:29:43 PDT 2008;  
root:xnu-1228.7.58~1/RELEASE_I386
System model name: MacBookPro3,1 (Mac-F4238BC8)

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.9 (Darwin)

iEYEARECAAYFAklM2KMACgkQKjXvaR+/rRvhYwCgnxYP4QYm4yD79aqm/jZWM+57
n3EAoMpUe5FPRGaVucQJuWAPUgAL/naE
=KCYD
-----END PGP SIGNATURE-----

From zfs at sbod.at  Sat Dec 20 05:02:53 2008
From: zfs at sbod.at (Franz Schmalzl)
Date: Sat, 20 Dec 2008 14:02:53 +0100
Subject: [zfs-discuss] Wishlist
In-Reply-To: <0A9D28CB-661B-42C8-9C90-E2673ED89D16@apple.com>
References: <64378FED-4268-443B-92E0-48FE3AEDA2E8@sbod.at>
	<0A9D28CB-661B-42C8-9C90-E2673ED89D16@apple.com>
Message-ID: <FA6B5E30-2B11-491F-B59A-ED1D38BA14B5@sbod.at>

>>
>> -Resizable Zpools
>
> We already have this ability, not sure what you're looking for  
> here?  Do I get a cookie for this one since it works? :)


By resizable zpools i mean i have a 500gb partition with one zpool on  
it, and i want to shrink this one down to 400gb to create another  
partition after it.



is that possible ?



cheers

franz









On 20.12.2008, at 04:25, No?l Dellofano wrote:

> hehe I luuuuv the message :)
>
>
>> -Encrypted ZFS ( Very important )
> Sun is working on it, once they are done (in code review last I  
> checked) we'll see about porting it over.
>
>>
>> -Bootable ZFS  ( Also very important )
>
> Possibly in the future but not for right now.  We're concentrating  
> efforts on stability  and boot-ability would take some time.
>
>>
>> -Resizable Zpools
>
> We already have this ability, not sure what you're looking for  
> here?  Do I get a cookie for this one since it works? :)
>
>>
>> -A nice ZFS GUI ( klickibunti! )
>
> A GUI is being worked on currently but only for Server
>
>>
>> -Time Machine patch to handle snapshots and do my backups for me.
>
> This one is  important and likely will come sometime, but not for SL
>
>>
>> - Peace on Earth
>
>  This is pretty big and there's no way they'd let me squeeze it into  
> SL cause its a featureless release :) So you'll have to wait to the  
> next major release.  Sorry :)
>
>
> and be careful with the cookie offers I luuuuuv cookies!!  
> (especially at Christmas time, I mean who doesn't?)
>
> Happy Holidays!
> Noel :)
>
>
> On Dec 18, 2008, at 10:32 AM, Franz Schmalzl wrote:
>
>> [ To north-pole ]
>>
>> Dear Mother Noelmas!
>>
>> My Wishes for this Christmas:
>>
>> Please bring me :
>>
>> -Encrypted ZFS ( Very important )
>> -Bootable ZFS  ( Also very important )
>> -Resizable Zpools
>> -A nice ZFS GUI ( klickibunti! )
>> -Time Machine patch to handle snapshots and do my backups for me.
>> - Peace on Earth
>>
>>
>> Milk and Cookies will be waiting for you in Salzburg, since my  
>> chimney is not big enough you are welcome to use the front door.
>> I hope my wishes will be fulfilled, since i was a very nice little  
>> boy this year ( sort of ).
>>
>> Kind regards,
>>
>> Franz Schmalzl
>>
>>
>> ;-)
>>
>>
>>
>>
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From mailinglists at MailNewsRSS.com  Sat Dec 20 07:34:45 2008
From: mailinglists at MailNewsRSS.com (Jason Todd Slack-Moehrle)
Date: Sat, 20 Dec 2008 07:34:45 -0800
Subject: [zfs-discuss] snapshots
In-Reply-To: <590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
References: <7B02C308-334C-4F2D-A31C-1CE993C40670@ironsoftware.de>
	<B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
Message-ID: <11B52BED-BE06-48E8-9A50-F3515D123FA3@MailNewsRSS.com>

Hi All,

Can you explain a little more?

How would we accomplish this?

I have a 250gb ZFS partition that is dedicated for my home directory.  
I would like to start taking snapshots, but things like Music, Movies,  
etc I would like to exclude.

Would I have to wipe the 250gb partition and re-partition it to do this.

Sorry I dont quite follow what you are saying.

-Jason

On Dec 19, 2008, at 6:59 PM, No?l Dellofano wrote:

> I second that.  Thats why ZFS filesystems have low overhead, they  
> are closer to directories then traditional filesystems.  Plus it  
> gives you a lot more fine grained control of your stuff.  For the  
> future we're considering possibly doing something in the line of  
> excluding certain directories, especially for backups and privacy  
> concerns.  However for now, multiple zfs filesystems is the best way  
> to go.
>
> Noel
>
> On Dec 19, 2008, at 3:18 PM, Robert Gordon wrote:
>
>>
>> On Dec 19, 2008, at 4:31 PM, Christian Kendi wrote:
>>
>>> Hi,
>>>
>>> 1. I was wondering how the progress is on the .snapshots feature is?
>>>
>>> 2. I'm using ZFS as my home directory now. There are certain  
>>> directories
>>> which i dont like to snapshot. For example, Movies and Music makes  
>>> no sense.
>>>
>>> Is anyone aware of a possiblity to exclude certain directories  
>>> from getting
>>> snapshotted?
>>
>> Make Movies and Music zfs filesystems.
>>
>> ;-)
>>
>> Robert.
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From lists at loveturtle.net  Sat Dec 20 07:41:34 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Sat, 20 Dec 2008 10:41:34 -0500
Subject: [zfs-discuss] snapshots
In-Reply-To: <11B52BED-BE06-48E8-9A50-F3515D123FA3@MailNewsRSS.com>
References: <7B02C308-334C-4F2D-A31C-1CE993C40670@ironsoftware.de>	<B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
	<11B52BED-BE06-48E8-9A50-F3515D123FA3@MailNewsRSS.com>
Message-ID: <494D122E.20508@loveturtle.net>

lets use my laptop as an example.

My home dir is a zfs filesystem.
turtle at ramuh ~ $ df -h .
Filesystem          Size   Used  Avail Capacity  Mounted on
ramuh/home/turtle   55Gi   28Gi   27Gi    52%    /Users/turtle

the zfs filesystem for my home directory is ramuh/home/turtle

my /Users/turtle/Library directory is a separate zfs filesystem, if I 
snapshot ramuh/home/turtle the contents of library are excluded from the 
snapshot because it's a different filesystem. consequently, I can also 
snapshot my /Users/turtle/Library directory independantly from the rest 
of my home dir.

turtle at ramuh ~ $ zfs list -t filesystem -r ramuh/home/turtle
NAME                        USED  AVAIL  REFER  MOUNTPOINT
ramuh/home/turtle          32.5G  26.9G  28.1G  /Users/turtle
ramuh/home/turtle/Library  3.51G  26.9G  3.33G  /Users/turtle/Library

lets say I want to exclude /Users/turtle/nosnapshot/ directory from 
snapshots, just simply create a new fs.

ramuh turtle # zfs create ramuh/home/turtle/nosnapshot

ramuh turtle # df -h /Users/turtle/nosnapshot
Filesystem                     Size   Used  Avail Capacity  Mounted on
ramuh/home/turtle/nosnapshot   27Gi   18Ki   27Gi     1%    
/Users/turtle/nosnapshot

ramuh turtle # zfs list -t filesystem -r ramuh/home/turtle
NAME                           USED  AVAIL  REFER  MOUNTPOINT
ramuh/home/turtle             32.5G  26.9G  28.1G  /Users/turtle
ramuh/home/turtle/Library     3.51G  26.9G  3.33G  /Users/turtle/Library
ramuh/home/turtle/nosnapshot  21.5K  26.9G  21.5K  /Users/turtle/nosnapshot

see?

now If i create a snapshot ramuh/home/turtle at snapshot1 the contents of 
the Library & nosnapshot directory are ignored because they're a 
different zfs filesystem.

I hope that made sense.

Cheers,
Dillon

On 12/20/08 10:34 AM, Jason Todd Slack-Moehrle wrote:
> Hi All,
>
> Can you explain a little more?
>
> How would we accomplish this?
>
> I have a 250gb ZFS partition that is dedicated for my home directory. 
> I would like to start taking snapshots, but things like Music, Movies, 
> etc I would like to exclude.
>
> Would I have to wipe the 250gb partition and re-partition it to do this.
>
> Sorry I dont quite follow what you are saying.
>
> -Jason
>
> On Dec 19, 2008, at 6:59 PM, No?l Dellofano wrote:
>
>> I second that.  Thats why ZFS filesystems have low overhead, they are 
>> closer to directories then traditional filesystems.  Plus it gives 
>> you a lot more fine grained control of your stuff.  For the future 
>> we're considering possibly doing something in the line of excluding 
>> certain directories, especially for backups and privacy concerns.  
>> However for now, multiple zfs filesystems is the best way to go.
>>
>> Noel
>>
>> On Dec 19, 2008, at 3:18 PM, Robert Gordon wrote:
>>
>>>
>>> On Dec 19, 2008, at 4:31 PM, Christian Kendi wrote:
>>>
>>>> Hi,
>>>>
>>>> 1. I was wondering how the progress is on the .snapshots feature is?
>>>>
>>>> 2. I'm using ZFS as my home directory now. There are certain 
>>>> directories
>>>> which i dont like to snapshot. For example, Movies and Music makes 
>>>> no sense.
>>>>
>>>> Is anyone aware of a possiblity to exclude certain directories from 
>>>> getting
>>>> snapshotted?
>>>
>>> Make Movies and Music zfs filesystems.
>>>
>>> ;-)
>>>
>>> Robert.
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From mailinglists at MailNewsRSS.com  Sat Dec 20 08:02:02 2008
From: mailinglists at MailNewsRSS.com (Jason Todd Slack-Moehrle)
Date: Sat, 20 Dec 2008 08:02:02 -0800
Subject: [zfs-discuss] snapshots
In-Reply-To: <494D122E.20508@loveturtle.net>
References: <7B02C308-334C-4F2D-A31C-1CE993C40670@ironsoftware.de>	<B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
	<11B52BED-BE06-48E8-9A50-F3515D123FA3@MailNewsRSS.com>
	<494D122E.20508@loveturtle.net>
Message-ID: <D7A64948-626A-4136-A391-C3A3272D116E@MailNewsRSS.com>

Hi Dillon,

So, you can create multiple ZFS filesystems inside the one large ZFS  
partition. And really, the one ZFS partition that I have is just one  
big ZFS filesystem....

And to be more clear, You have a ZFS partition that is really one  
large ZFS filesystem and I create a second ZFS filesystem inside the  
partition they share the entire space allocated so that is why you did  
not specify a size, etc when you created the new filesystem below?

And I starting to grok this?

-Jason

On Dec 20, 2008, at 7:41 AM, Dillon Kass wrote:

> lets use my laptop as an example.
>
> My home dir is a zfs filesystem.
> turtle at ramuh ~ $ df -h .
> Filesystem          Size   Used  Avail Capacity  Mounted on
> ramuh/home/turtle   55Gi   28Gi   27Gi    52%    /Users/turtle
>
> the zfs filesystem for my home directory is ramuh/home/turtle
>
> my /Users/turtle/Library directory is a separate zfs filesystem, if  
> I snapshot ramuh/home/turtle the contents of library are excluded  
> from the snapshot because it's a different filesystem. consequently,  
> I can also snapshot my /Users/turtle/Library directory independantly  
> from the rest of my home dir.
>
> turtle at ramuh ~ $ zfs list -t filesystem -r ramuh/home/turtle
> NAME                        USED  AVAIL  REFER  MOUNTPOINT
> ramuh/home/turtle          32.5G  26.9G  28.1G  /Users/turtle
> ramuh/home/turtle/Library  3.51G  26.9G  3.33G  /Users/turtle/Library
>
> lets say I want to exclude /Users/turtle/nosnapshot/ directory from  
> snapshots, just simply create a new fs.
>
> ramuh turtle # zfs create ramuh/home/turtle/nosnapshot
>
> ramuh turtle # df -h /Users/turtle/nosnapshot
> Filesystem                     Size   Used  Avail Capacity  Mounted on
> ramuh/home/turtle/nosnapshot   27Gi   18Ki   27Gi     1%    /Users/ 
> turtle/nosnapshot
>
> ramuh turtle # zfs list -t filesystem -r ramuh/home/turtle
> NAME                           USED  AVAIL  REFER  MOUNTPOINT
> ramuh/home/turtle             32.5G  26.9G  28.1G  /Users/turtle
> ramuh/home/turtle/Library     3.51G  26.9G  3.33G  /Users/turtle/ 
> Library
> ramuh/home/turtle/nosnapshot  21.5K  26.9G  21.5K  /Users/turtle/ 
> nosnapshot
>
> see?
>
> now If i create a snapshot ramuh/home/turtle at snapshot1 the contents  
> of the Library & nosnapshot directory are ignored because they're a  
> different zfs filesystem.
>
> I hope that made sense.
>
> Cheers,
> Dillon
>
> On 12/20/08 10:34 AM, Jason Todd Slack-Moehrle wrote:
>> Hi All,
>>
>> Can you explain a little more?
>>
>> How would we accomplish this?
>>
>> I have a 250gb ZFS partition that is dedicated for my home  
>> directory. I would like to start taking snapshots, but things like  
>> Music, Movies, etc I would like to exclude.
>>
>> Would I have to wipe the 250gb partition and re-partition it to do  
>> this.
>>
>> Sorry I dont quite follow what you are saying.
>>
>> -Jason
>>
>> On Dec 19, 2008, at 6:59 PM, No?l Dellofano wrote:
>>
>>> I second that.  Thats why ZFS filesystems have low overhead, they  
>>> are closer to directories then traditional filesystems.  Plus it  
>>> gives you a lot more fine grained control of your stuff.  For the  
>>> future we're considering possibly doing something in the line of  
>>> excluding certain directories, especially for backups and privacy  
>>> concerns.  However for now, multiple zfs filesystems is the best  
>>> way to go.
>>>
>>> Noel
>>>
>>> On Dec 19, 2008, at 3:18 PM, Robert Gordon wrote:
>>>
>>>>
>>>> On Dec 19, 2008, at 4:31 PM, Christian Kendi wrote:
>>>>
>>>>> Hi,
>>>>>
>>>>> 1. I was wondering how the progress is on the .snapshots feature  
>>>>> is?
>>>>>
>>>>> 2. I'm using ZFS as my home directory now. There are certain  
>>>>> directories
>>>>> which i dont like to snapshot. For example, Movies and Music  
>>>>> makes no sense.
>>>>>
>>>>> Is anyone aware of a possiblity to exclude certain directories  
>>>>> from getting
>>>>> snapshotted?
>>>>
>>>> Make Movies and Music zfs filesystems.
>>>>
>>>> ;-)
>>>>
>>>> Robert.
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ksh at ironsoftware.de  Sat Dec 20 08:23:55 2008
From: ksh at ironsoftware.de (Christian Kendi)
Date: Sat, 20 Dec 2008 11:23:55 -0500
Subject: [zfs-discuss] snapshots
Message-ID: <5E6EA587-7948-4745-8E3B-C0511F6062E5@ironsoftware.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hey,

thanks for your answers. That was my solution for now. Funny ;)
Well, its a proper way to exclude from snapshots.

Lets say i make a new zfs fs in tank/user/Movies.
I was curious if you snapshot tank/user, the snapshot tank/user/Movies  
would
be recursive or is it independed from tank/user ?

Regards,
Chris.

 >
 > Hi Dillon,
 >
 > So, you can create multiple ZFS filesystems inside the one large ZFS
 > partition. And really, the one ZFS partition that I have is just one
 > big ZFS filesystem....
 >
 > And to be more clear, You have a ZFS partition that is really one
 > large ZFS filesystem and I create a second ZFS filesystem inside the
 > partition they share the entire space allocated so that is why you  
did
 > not specify a size, etc when you created the new filesystem below?
 >
 > And I starting to grok this?
 >
 > -Jason
 >
 > On Dec 20, 2008, at 7:41 AM, Dillon Kass wrote:
 >
 > lets use my laptop as an example.
 >
 > My home dir is a zfs filesystem.
 > turtle at ramuh ~ $ df -h .
 > Filesystem          Size   Used  Avail Capacity  Mounted on
 > ramuh/home/turtle   55Gi   28Gi   27Gi    52%    /Users/turtle
 >
 > the zfs filesystem for my home directory is ramuh/home/turtle
 >
 > my /Users/turtle/Library directory is a separate zfs filesystem, if
 > I snapshot ramuh/home/turtle the contents of library are excluded
 > from the snapshot because it's a different filesystem. consequently,
 > I can also snapshot my /Users/turtle/Library directory independantly
 > from the rest of my home dir.
 >
 > turtle at ramuh ~ $ zfs list -t filesystem -r ramuh/home/turtle
 > NAME                        USED  AVAIL  REFER  MOUNTPOINT
 > ramuh/home/turtle          32.5G  26.9G  28.1G  /Users/turtle
 > ramuh/home/turtle/Library  3.51G  26.9G  3.33G  /Users/turtle/Library
 >
 > lets say I want to exclude /Users/turtle/nosnapshot/ directory from
 > snapshots, just simply create a new fs.
 >
 > ramuh turtle # zfs create ramuh/home/turtle/nosnapshot
 >
 > ramuh turtle # df -h /Users/turtle/nosnapshot
 > Filesystem                     Size   Used  Avail Capacity  Mounted  
on
 > ramuh/home/turtle/nosnapshot   27Gi   18Ki   27Gi     1%    /Users/
 > turtle/nosnapshot
 >
 > ramuh turtle # zfs list -t filesystem -r ramuh/home/turtle
 > NAME                           USED  AVAIL  REFER  MOUNTPOINT
 > ramuh/home/turtle             32.5G  26.9G  28.1G  /Users/turtle
 > ramuh/home/turtle/Library     3.51G  26.9G  3.33G  /Users/turtle/
 > Library
 > ramuh/home/turtle/nosnapshot  21.5K  26.9G  21.5K  /Users/turtle/
 > nosnapshot
 >
 > see?
 >
 > now If i create a snapshot ramuh/home/turtle at snapshot1 the  
contents
 > of the Library & nosnapshot directory are ignored because they're a
 > different zfs filesystem.
 >
 > I hope that made sense.
 >
 > Cheers,
 > Dillon
 >
 > On 12/20/08 10:34 AM, Jason Todd Slack-Moehrle wrote:
 >> Hi All,
 >>
 >> Can you explain a little more?
 >>
 >> How would we accomplish this?
 >>
 >> I have a 250gb ZFS partition that is dedicated for my home
 >> directory. I would like to start taking snapshots, but things like
 >> Music, Movies, etc I would like to exclude.
 >>
 >> Would I have to wipe the 250gb partition and re-partition it to do
 >> this.
 >>
 >> Sorry I dont quite follow what you are saying.
 >>
 >> -Jason
 >>
 >> On Dec 19, 2008, at 6:59 PM, No?l Dellofano wrote:
 >>
 >>> I second that.  Thats why ZFS filesystems have low overhead, they
 >>> are closer to directories then traditional filesystems.  Plus it
 >>> gives you a lot more fine grained control of your stuff.  For the
 >>> future we're considering possibly doing something in the line of
 >>> excluding certain directories, especially for backups and privacy
 >>> concerns.  However for now, multiple zfs filesystems is the best
 >>> way to go.
 >>>
 >>> Noel
 >>>
 >>> On Dec 19, 2008, at 3:18 PM, Robert Gordon wrote:
 >>>
 >>>>
 >>>> On Dec 19, 2008, at 4:31 PM, Christian Kendi wrote:
 >>>>
 >>>>> Hi,
 >>>>>
 >>>>> 1. I was wondering how the progress is on the .snapshots feature
 >>>>> is?
 >>>>>
 >>>>> 2. I'm using ZFS as my home directory now. There are certain
 >>>>> directories
 >>>>> which i dont like to snapshot. For example, Movies and Music
 >>>>> makes no sense.
 >>>>>
 >>>>> Is anyone aware of a possiblity to exclude certain directories
 >>>>> from getting
 >>>>> snapshotted?
 >>>>
 >>>> Make Movies and Music zfs filesystems.
 >>>>
 >>>> ;-)
 >>>>
 >>>> Robert.
 >>>> _______________________________________________
 >>>> zfs-discuss mailing list
 >>>> zfs-discuss at lists.macosforge.org
 >>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
 >>>
 >>> _______________________________________________
 >>> zfs-discuss mailing list
 >>> zfs-discuss at lists.macosforge.org
 >>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
 >>
 >> _______________________________________________
 >> zfs-discuss mailing list
 >> zfs-discuss at lists.macosforge.org
 >> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
 >
 > _______________________________________________
 > zfs-discuss mailing list
 > zfs-discuss at lists.macosforge.org
 > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

ksh at kshs-Computer:~ $

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (Darwin)

iD8DBQFJTRwbp+9ff145KVIRAv78AKCjhKj1S7KI4hUczkW46GLKofXUUQCfdkNU
A1Vbiz1BSApj+Hy3v/QPsE8=
=8+Xs
-----END PGP SIGNATURE-----

From ksh at ironsoftware.de  Sat Dec 20 08:35:15 2008
From: ksh at ironsoftware.de (Christian Kendi)
Date: Sat, 20 Dec 2008 11:35:15 -0500
Subject: [zfs-discuss] panic()
In-Reply-To: <D057934A-25DA-444F-A9C1-8C12628AD436@ironsoftware.de>
References: <D057934A-25DA-444F-A9C1-8C12628AD436@ironsoftware.de>
Message-ID: <645FB96F-145F-4ED2-AAFA-6A7D5645FA7F@ironsoftware.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hi Noel,

this was my laptop HD. As this occurs i was writing in Word, therefore
no sleep or remove.

The hardware failiure is possible, but will this happen every time on  
a non-disasterous
hardware failure ? It was nothing dramatic, as the SMART of the HD is  
fine this time.
What would happen if i get continous read/write errors due to damaged  
sectors.
How will ZFS react on this?

Greets,
Chris.

 >
 >
 > looks like you've either ripped out a drive prematurely, your drive
 > went to sleep, or your drive had a hardware failure.  At any rate,  
ZFS
 > can't find it to write to it so panic ensues since they were
 > ansynchronous writes we've already returned success on and this would
 > make disk state inconsistent.
 >
 > In Snow Leopard we are thinking of changing this so that we will
 > actually freeze the pipleline if this occurs and be alerted you've
 > lost data.
 >
 > Noel

On Dec 19, 2008, at 2:28 PM, Christian Kendi wrote:

 > -----BEGIN PGP SIGNED MESSAGE-----
 > Hash: SHA1
 >
 > Hi,
 >
 > im just off to report a kernel panic. The website wont send me an
 > activation mail, so i just do it here.
 >
 > Thu Dec 18 15:28:26 2008
 > panic(cpu 0 caller 0x00C08D21): "ZFS: I/O failure (write on
 > <unknown> off 0: zio 0x4a2eab0 [L0 ZFS plain file] 1800L/1800P
 > DVA[0]=<0:144b278800:1800> fletcher2 uncompressed LE contiguous
 > birth=26685 fill=1
 > cksum=758849a10b2fd5b6:4bfc8c90062499f1:c0fc06747bfaa98b:
 > 1cf76936dedc736d): error " "5"@/Volumes/pixie_dust/home/ndellofano/
 > zfs-work/zfs-119/zfs_kext/zfs/zio.c:918
 > Backtrace (CPU 0), Frame : Return Address (4 potential args on stack)
 > 0x38893e38 : 0x12b4f3 (0x45b13c 0x38893e6c 0x1335e4 0x0)
 > 0x38893e88 : 0xc08d21 (0xc786b0 0xc786a4 0xc74400 0xca5670)
 > 0x38893f08 : 0xc053fb (0x4a2eab0 0x16f5 0x38893f28 0xc4c18a)
 > 0x38893f48 : 0xc644a6 (0x4a2eab0 0x47f8800 0x271356 0xc8d5f4)
 > 0x38893fc8 : 0x1a017c (0x481c500 0x0 0x1a30b5 0xd2036b0)
 > Backtrace terminated-invalid frame pointer 0
 >      Kernel loadable modules in backtrace (with dependencies):
 >         com.apple.filesystems.zfs(8.0)@0xbf1000->0xcbcfff
 >
 > BSD process name corresponding to current thread: kernel_task
 >
 > Mac OS version:
 > 9G55
 >
 > Kernel version:
 > Darwin Kernel Version 9.6.0: Mon Nov 24 17:37:00 PST 2008;
 > root:xnu-1228.9.59~1/RELEASE_I386
 > System model name: MacBook1,1 (Mac-F4208CC8)
 >
 > Regards,
 > Chris.
 > -----BEGIN PGP SIGNATURE-----
 > Version: GnuPG v1.4.7 (Darwin)
 >
 > iD8DBQFJTB/1p+9ff145KVIRAkJnAJ9DDhnUj9s7K0HBz28G/lkU9CkhEQCfQCfP
 > UbNbwThBjfpx4N389JqfcD4=
 > =/qFD
 > -----END PGP SIGNATURE-----
 > _______________________________________________
 > zfs-discuss mailing list
 > zfs-discuss at lists.macosforge.org
 > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.7 (Darwin)

iD8DBQFJTR7Dp+9ff145KVIRApysAJ9I9U1oKnpWR6wh4EOFeu1tBWt74wCdH5rL
4sE/J26J/tK1m8BacbA7czg=
=lMRB
-----END PGP SIGNATURE-----

From richmc at gmail.com  Sat Dec 20 09:31:05 2008
From: richmc at gmail.com (Richard McClellan)
Date: Sat, 20 Dec 2008 09:31:05 -0800
Subject: [zfs-discuss] snapshots
In-Reply-To: <11B52BED-BE06-48E8-9A50-F3515D123FA3@MailNewsRSS.com>
References: <7B02C308-334C-4F2D-A31C-1CE993C40670@ironsoftware.de>
	<B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
	<11B52BED-BE06-48E8-9A50-F3515D123FA3@MailNewsRSS.com>
Message-ID: <F467A3CF-07E3-426F-BB0B-D302FD2F73FC@gmail.com>


On Dec 20, 2008, at 07:34 , Jason Todd Slack-Moehrle wrote:

> Hi All,
>
> Can you explain a little more?
>
> How would we accomplish this?
>
> I have a 250gb ZFS partition that is dedicated for my home  
> directory. I would like to start taking snapshots, but things like  
> Music, Movies, etc I would like to exclude.
>

Snapshots don't use any space until you modify/delete a file in the  
snapshot. I can't imagine frequently deleting music and movies, but  
perhaps some people do. :-)

If you really, really don't want to include music and movies you can  
(as Robert suggested) put them on a different ZFS filesystem, under  
the *same* ZFS pool.

For example, if you have a zpool named mypool with a filesystem named  
jason (mypool/jason) then simply do(*):
zfs create mypool/music
zfs create mypool/movies

Now you can snapshot mypool/jason (`zfs snapshot mypool/ 
jason at 20081220`) and it won't include your music and movies.

(*) this is not the best or recommended layout of filesystems, just an  
example.

> Would I have to wipe the 250gb partition and re-partition it to do  
> this.
>

No.  See above.

> Sorry I dont quite follow what you are saying.

You should give the ZFS docs a read. Sun has online versions of them  
and they are quite good.

		Rich


>
>
> -Jason
>
> On Dec 19, 2008, at 6:59 PM, No?l Dellofano wrote:
>
>> I second that.  Thats why ZFS filesystems have low overhead, they  
>> are closer to directories then traditional filesystems.  Plus it  
>> gives you a lot more fine grained control of your stuff.  For the  
>> future we're considering possibly doing something in the line of  
>> excluding certain directories, especially for backups and privacy  
>> concerns.  However for now, multiple zfs filesystems is the best  
>> way to go.
>>
>> Noel
>>
>> On Dec 19, 2008, at 3:18 PM, Robert Gordon wrote:
>>
>>>
>>> On Dec 19, 2008, at 4:31 PM, Christian Kendi wrote:
>>>
>>>> Hi,
>>>>
>>>> 1. I was wondering how the progress is on the .snapshots feature  
>>>> is?
>>>>
>>>> 2. I'm using ZFS as my home directory now. There are certain  
>>>> directories
>>>> which i dont like to snapshot. For example, Movies and Music  
>>>> makes no sense.
>>>>
>>>> Is anyone aware of a possiblity to exclude certain directories  
>>>> from getting
>>>> snapshotted?
>>>
>>> Make Movies and Music zfs filesystems.
>>>
>>> ;-)
>>>
>>> Robert.
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From richmc at gmail.com  Sat Dec 20 09:39:29 2008
From: richmc at gmail.com (Richard McClellan)
Date: Sat, 20 Dec 2008 09:39:29 -0800
Subject: [zfs-discuss] snapshots
In-Reply-To: <D7A64948-626A-4136-A391-C3A3272D116E@MailNewsRSS.com>
References: <7B02C308-334C-4F2D-A31C-1CE993C40670@ironsoftware.de>
	<B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
	<11B52BED-BE06-48E8-9A50-F3515D123FA3@MailNewsRSS.com>
	<494D122E.20508@loveturtle.net>
	<D7A64948-626A-4136-A391-C3A3272D116E@MailNewsRSS.com>
Message-ID: <3567E51A-C7B7-47FF-98A5-DDC3322862A0@gmail.com>


On Dec 20, 2008, at 08:02 , Jason Todd Slack-Moehrle wrote:

> Hi Dillon,
>
> So, you can create multiple ZFS filesystems inside the one large ZFS  
> partition. And really, the one ZFS partition that I have is just one  
> big ZFS filesystem....

Kinda.  Think of the original ZFS filesystem (the one created when you  
do `zpool create mypool <vdev>`) as a pool of storage. One big pool  
that all of the filesystems share. (You can set quotas, reservations,  
reference quotas, etc to guarantee that space is available for  
particular filesystems and that no filesystem grows too large.)

>
>
> And to be more clear, You have a ZFS partition that is really one  
> large ZFS filesystem and I create a second ZFS filesystem inside the  
> partition they share the entire space allocated so that is why you  
> did not specify a size, etc when you created the new filesystem below?
>
> And I starting to grok this?

Yes! Sharing the storage pool is one of the big strengths of ZFS: you  
no longer have to guess how much storage you'll need for a particular  
"partition" when it is created.  (Except for "volume" ZFS filesystems  
which must have a size specified when they're created, but that's a  
story for another email and can be read about in the Sun's excellent  
online docs.)

		Rich


>
>
> -Jason
>
> On Dec 20, 2008, at 7:41 AM, Dillon Kass wrote:
>
>> lets use my laptop as an example.
>>
>> My home dir is a zfs filesystem.
>> turtle at ramuh ~ $ df -h .
>> Filesystem          Size   Used  Avail Capacity  Mounted on
>> ramuh/home/turtle   55Gi   28Gi   27Gi    52%    /Users/turtle
>>
>> the zfs filesystem for my home directory is ramuh/home/turtle
>>
>> my /Users/turtle/Library directory is a separate zfs filesystem, if  
>> I snapshot ramuh/home/turtle the contents of library are excluded  
>> from the snapshot because it's a different filesystem.  
>> consequently, I can also snapshot my /Users/turtle/Library  
>> directory independantly from the rest of my home dir.
>>
>> turtle at ramuh ~ $ zfs list -t filesystem -r ramuh/home/turtle
>> NAME                        USED  AVAIL  REFER  MOUNTPOINT
>> ramuh/home/turtle          32.5G  26.9G  28.1G  /Users/turtle
>> ramuh/home/turtle/Library  3.51G  26.9G  3.33G  /Users/turtle/Library
>>
>> lets say I want to exclude /Users/turtle/nosnapshot/ directory from  
>> snapshots, just simply create a new fs.
>>
>> ramuh turtle # zfs create ramuh/home/turtle/nosnapshot
>>
>> ramuh turtle # df -h /Users/turtle/nosnapshot
>> Filesystem                     Size   Used  Avail Capacity  Mounted  
>> on
>> ramuh/home/turtle/nosnapshot   27Gi   18Ki   27Gi     1%    /Users/ 
>> turtle/nosnapshot
>>
>> ramuh turtle # zfs list -t filesystem -r ramuh/home/turtle
>> NAME                           USED  AVAIL  REFER  MOUNTPOINT
>> ramuh/home/turtle             32.5G  26.9G  28.1G  /Users/turtle
>> ramuh/home/turtle/Library     3.51G  26.9G  3.33G  /Users/turtle/ 
>> Library
>> ramuh/home/turtle/nosnapshot  21.5K  26.9G  21.5K  /Users/turtle/ 
>> nosnapshot
>>
>> see?
>>
>> now If i create a snapshot ramuh/home/turtle at snapshot1 the contents  
>> of the Library & nosnapshot directory are ignored because they're a  
>> different zfs filesystem.
>>
>> I hope that made sense.
>>
>> Cheers,
>> Dillon
>>
>> On 12/20/08 10:34 AM, Jason Todd Slack-Moehrle wrote:
>>> Hi All,
>>>
>>> Can you explain a little more?
>>>
>>> How would we accomplish this?
>>>
>>> I have a 250gb ZFS partition that is dedicated for my home  
>>> directory. I would like to start taking snapshots, but things like  
>>> Music, Movies, etc I would like to exclude.
>>>
>>> Would I have to wipe the 250gb partition and re-partition it to do  
>>> this.
>>>
>>> Sorry I dont quite follow what you are saying.
>>>
>>> -Jason
>>>
>>> On Dec 19, 2008, at 6:59 PM, No?l Dellofano wrote:
>>>
>>>> I second that.  Thats why ZFS filesystems have low overhead, they  
>>>> are closer to directories then traditional filesystems.  Plus it  
>>>> gives you a lot more fine grained control of your stuff.  For the  
>>>> future we're considering possibly doing something in the line of  
>>>> excluding certain directories, especially for backups and privacy  
>>>> concerns.  However for now, multiple zfs filesystems is the best  
>>>> way to go.
>>>>
>>>> Noel
>>>>
>>>> On Dec 19, 2008, at 3:18 PM, Robert Gordon wrote:
>>>>
>>>>>
>>>>> On Dec 19, 2008, at 4:31 PM, Christian Kendi wrote:
>>>>>
>>>>>> Hi,
>>>>>>
>>>>>> 1. I was wondering how the progress is on the .snapshots  
>>>>>> feature is?
>>>>>>
>>>>>> 2. I'm using ZFS as my home directory now. There are certain  
>>>>>> directories
>>>>>> which i dont like to snapshot. For example, Movies and Music  
>>>>>> makes no sense.
>>>>>>
>>>>>> Is anyone aware of a possiblity to exclude certain directories  
>>>>>> from getting
>>>>>> snapshotted?
>>>>>
>>>>> Make Movies and Music zfs filesystems.
>>>>>
>>>>> ;-)
>>>>>
>>>>> Robert.
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From richmc at gmail.com  Sat Dec 20 09:45:56 2008
From: richmc at gmail.com (Richard McClellan)
Date: Sat, 20 Dec 2008 09:45:56 -0800
Subject: [zfs-discuss] snapshots
In-Reply-To: <5E6EA587-7948-4745-8E3B-C0511F6062E5@ironsoftware.de>
References: <5E6EA587-7948-4745-8E3B-C0511F6062E5@ironsoftware.de>
Message-ID: <D1B628CC-A32E-4A51-83DD-4FD220B1839B@gmail.com>

snapshots are not recursive unless the -r flag is specified.  In your  
case, `zfs snapshot tank/user at now` will grab the files in tank/user  
and nothing from tank/user/Movies.  If you want to get the Movies (and  
other) "sub" filesystems, then do `zfs snapshot -r tank/users at now`.

		Rich


On Dec 20, 2008, at 08:23 , Christian Kendi wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hey,
>
> thanks for your answers. That was my solution for now. Funny ;)
> Well, its a proper way to exclude from snapshots.
>
> Lets say i make a new zfs fs in tank/user/Movies.
> I was curious if you snapshot tank/user, the snapshot tank/user/ 
> Movies would
> be recursive or is it independed from tank/user ?
>
> Regards,
> Chris.
>
> >
> > Hi Dillon,
> >
> > So, you can create multiple ZFS filesystems inside the one large ZFS
> > partition. And really, the one ZFS partition that I have is just one
> > big ZFS filesystem....
> >
> > And to be more clear, You have a ZFS partition that is really one
> > large ZFS filesystem and I create a second ZFS filesystem inside the
> > partition they share the entire space allocated so that is why you  
> did
> > not specify a size, etc when you created the new filesystem below?
> >
> > And I starting to grok this?
> >
> > -Jason
> >
> > On Dec 20, 2008, at 7:41 AM, Dillon Kass wrote:
> >
> > lets use my laptop as an example.
> >
> > My home dir is a zfs filesystem.
> > turtle at ramuh ~ $ df -h .
> > Filesystem          Size   Used  Avail Capacity  Mounted on
> > ramuh/home/turtle   55Gi   28Gi   27Gi    52%    /Users/turtle
> >
> > the zfs filesystem for my home directory is ramuh/home/turtle
> >
> > my /Users/turtle/Library directory is a separate zfs filesystem, if
> > I snapshot ramuh/home/turtle the contents of library are excluded
> > from the snapshot because it's a different filesystem. consequently,
> > I can also snapshot my /Users/turtle/Library directory independantly
> > from the rest of my home dir.
> >
> > turtle at ramuh ~ $ zfs list -t filesystem -r ramuh/home/turtle
> > NAME                        USED  AVAIL  REFER  MOUNTPOINT
> > ramuh/home/turtle          32.5G  26.9G  28.1G  /Users/turtle
> > ramuh/home/turtle/Library  3.51G  26.9G  3.33G  /Users/turtle/ 
> Library
> >
> > lets say I want to exclude /Users/turtle/nosnapshot/ directory from
> > snapshots, just simply create a new fs.
> >
> > ramuh turtle # zfs create ramuh/home/turtle/nosnapshot
> >
> > ramuh turtle # df -h /Users/turtle/nosnapshot
> > Filesystem                     Size   Used  Avail Capacity   
> Mounted on
> > ramuh/home/turtle/nosnapshot   27Gi   18Ki   27Gi     1%    /Users/
> > turtle/nosnapshot
> >
> > ramuh turtle # zfs list -t filesystem -r ramuh/home/turtle
> > NAME                           USED  AVAIL  REFER  MOUNTPOINT
> > ramuh/home/turtle             32.5G  26.9G  28.1G  /Users/turtle
> > ramuh/home/turtle/Library     3.51G  26.9G  3.33G  /Users/turtle/
> > Library
> > ramuh/home/turtle/nosnapshot  21.5K  26.9G  21.5K  /Users/turtle/
> > nosnapshot
> >
> > see?
> >
> > now If i create a snapshot ramuh/home/turtle at snapshot1 the  
> contents
> > of the Library & nosnapshot directory are ignored because they're a
> > different zfs filesystem.
> >
> > I hope that made sense.
> >
> > Cheers,
> > Dillon
> >
> > On 12/20/08 10:34 AM, Jason Todd Slack-Moehrle wrote:
> >> Hi All,
> >>
> >> Can you explain a little more?
> >>
> >> How would we accomplish this?
> >>
> >> I have a 250gb ZFS partition that is dedicated for my home
> >> directory. I would like to start taking snapshots, but things like
> >> Music, Movies, etc I would like to exclude.
> >>
> >> Would I have to wipe the 250gb partition and re-partition it to do
> >> this.
> >>
> >> Sorry I dont quite follow what you are saying.
> >>
> >> -Jason
> >>
> >> On Dec 19, 2008, at 6:59 PM, No?l Dellofano wrote:
> >>
> >>> I second that.  Thats why ZFS filesystems have low overhead, they
> >>> are closer to directories then traditional filesystems.  Plus it
> >>> gives you a lot more fine grained control of your stuff.  For the
> >>> future we're considering possibly doing something in the line of
> >>> excluding certain directories, especially for backups and privacy
> >>> concerns.  However for now, multiple zfs filesystems is the best
> >>> way to go.
> >>>
> >>> Noel
> >>>
> >>> On Dec 19, 2008, at 3:18 PM, Robert Gordon wrote:
> >>>
> >>>>
> >>>> On Dec 19, 2008, at 4:31 PM, Christian Kendi wrote:
> >>>>
> >>>>> Hi,
> >>>>>
> >>>>> 1. I was wondering how the progress is on the .snapshots feature
> >>>>> is?
> >>>>>
> >>>>> 2. I'm using ZFS as my home directory now. There are certain
> >>>>> directories
> >>>>> which i dont like to snapshot. For example, Movies and Music
> >>>>> makes no sense.
> >>>>>
> >>>>> Is anyone aware of a possiblity to exclude certain directories
> >>>>> from getting
> >>>>> snapshotted?
> >>>>
> >>>> Make Movies and Music zfs filesystems.
> >>>>
> >>>> ;-)
> >>>>
> >>>> Robert.
> >>>> _______________________________________________
> >>>> zfs-discuss mailing list
> >>>> zfs-discuss at lists.macosforge.org
> >>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >>>
> >>> _______________________________________________
> >>> zfs-discuss mailing list
> >>> zfs-discuss at lists.macosforge.org
> >>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >>
> >> _______________________________________________
> >> zfs-discuss mailing list
> >> zfs-discuss at lists.macosforge.org
> >> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss at lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> ksh at kshs-Computer:~ $
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.7 (Darwin)
>
> iD8DBQFJTRwbp+9ff145KVIRAv78AKCjhKj1S7KI4hUczkW46GLKofXUUQCfdkNU
> A1Vbiz1BSApj+Hy3v/QPsE8=
> =8+Xs
> -----END PGP SIGNATURE-----
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From richmc at gmail.com  Sat Dec 20 09:56:45 2008
From: richmc at gmail.com (Richard McClellan)
Date: Sat, 20 Dec 2008 09:56:45 -0800
Subject: [zfs-discuss] panic()
In-Reply-To: <645FB96F-145F-4ED2-AAFA-6A7D5645FA7F@ironsoftware.de>
References: <D057934A-25DA-444F-A9C1-8C12628AD436@ironsoftware.de>
	<645FB96F-145F-4ED2-AAFA-6A7D5645FA7F@ironsoftware.de>
Message-ID: <14D057D1-8DAA-44AB-B62E-271D39E2DD3F@gmail.com>


On Dec 20, 2008, at 08:35 , Christian Kendi wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Hi Noel,
>
> this was my laptop HD. As this occurs i was writing in Word, therefore
> no sleep or remove.
>

Don't laptops spin down/sleep hard drives if they're inactive? If you  
stop for a long moment and think while you're writing, it may be long  
enough (especially if on battery) for the OS to do something to the  
drive that ZFS doesn't like.

> The hardware failiure is possible, but will this happen every time  
> on a non-disasterous
> hardware failure ? It was nothing dramatic, as the SMART of the HD  
> is fine this time.
> What would happen if i get continous read/write errors due to  
> damaged sectors.
> How will ZFS react on this?

If ZFS encounters read errors (comparing checksums from different  
copies of the same block when reading or doing a scrub) it will flag  
the drive, but it won't take it offline.  But... your laptop probably  
only has the one drive, so unless you set the ZFS copies property > 1  
you won't know there's a problem.

		Rich

>
>
> Greets,
> Chris.
>
> >
> >
> > looks like you've either ripped out a drive prematurely, your drive
> > went to sleep, or your drive had a hardware failure.  At any rate,  
> ZFS
> > can't find it to write to it so panic ensues since they were
> > ansynchronous writes we've already returned success on and this  
> would
> > make disk state inconsistent.
> >
> > In Snow Leopard we are thinking of changing this so that we will
> > actually freeze the pipleline if this occurs and be alerted you've
> > lost data.
> >
> > Noel
>
> On Dec 19, 2008, at 2:28 PM, Christian Kendi wrote:
>
> > -----BEGIN PGP SIGNED MESSAGE-----
> > Hash: SHA1
> >
> > Hi,
> >
> > im just off to report a kernel panic. The website wont send me an
> > activation mail, so i just do it here.
> >
> > Thu Dec 18 15:28:26 2008
> > panic(cpu 0 caller 0x00C08D21): "ZFS: I/O failure (write on
> > <unknown> off 0: zio 0x4a2eab0 [L0 ZFS plain file] 1800L/1800P
> > DVA[0]=<0:144b278800:1800> fletcher2 uncompressed LE contiguous
> > birth=26685 fill=1
> > cksum=758849a10b2fd5b6:4bfc8c90062499f1:c0fc06747bfaa98b:
> > 1cf76936dedc736d): error " "5"@/Volumes/pixie_dust/home/ndellofano/
> > zfs-work/zfs-119/zfs_kext/zfs/zio.c:918
> > Backtrace (CPU 0), Frame : Return Address (4 potential args on  
> stack)
> > 0x38893e38 : 0x12b4f3 (0x45b13c 0x38893e6c 0x1335e4 0x0)
> > 0x38893e88 : 0xc08d21 (0xc786b0 0xc786a4 0xc74400 0xca5670)
> > 0x38893f08 : 0xc053fb (0x4a2eab0 0x16f5 0x38893f28 0xc4c18a)
> > 0x38893f48 : 0xc644a6 (0x4a2eab0 0x47f8800 0x271356 0xc8d5f4)
> > 0x38893fc8 : 0x1a017c (0x481c500 0x0 0x1a30b5 0xd2036b0)
> > Backtrace terminated-invalid frame pointer 0
> >      Kernel loadable modules in backtrace (with dependencies):
> >         com.apple.filesystems.zfs(8.0)@0xbf1000->0xcbcfff
> >
> > BSD process name corresponding to current thread: kernel_task
> >
> > Mac OS version:
> > 9G55
> >
> > Kernel version:
> > Darwin Kernel Version 9.6.0: Mon Nov 24 17:37:00 PST 2008;
> > root:xnu-1228.9.59~1/RELEASE_I386
> > System model name: MacBook1,1 (Mac-F4208CC8)
> >
> > Regards,
> > Chris.
> > -----BEGIN PGP SIGNATURE-----
> > Version: GnuPG v1.4.7 (Darwin)
> >
> > iD8DBQFJTB/1p+9ff145KVIRAkJnAJ9DDhnUj9s7K0HBz28G/lkU9CkhEQCfQCfP
> > UbNbwThBjfpx4N389JqfcD4=
> > =/qFD
> > -----END PGP SIGNATURE-----
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss at lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.7 (Darwin)
>
> iD8DBQFJTR7Dp+9ff145KVIRApysAJ9I9U1oKnpWR6wh4EOFeu1tBWt74wCdH5rL
> 4sE/J26J/tK1m8BacbA7czg=
> =lMRB
> -----END PGP SIGNATURE-----
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From bwaters at nrao.edu  Sat Dec 20 11:17:14 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Sat, 20 Dec 2008 12:17:14 -0700
Subject: [zfs-discuss] snapshots
In-Reply-To: <590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
References: <7B02C308-334C-4F2D-A31C-1CE993C40670@ironsoftware.de>
	<B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
Message-ID: <0F79DE0F-328E-4033-8EED-CA65951C5703@nrao.edu>

Except when it stops working:

$ snapshot
cannot create snapshot 'pool/bwaters at 2008-12-20T12:15:14': dataset is  
busy


I've exported/imported, scrubbed, rebooted. I've tried logging in as a  
console (root) user and scrubbing and then snapshot from that user. No  
joy.

What else should I try?




On Dec 19, 2008, at 7:59 PM, No?l Dellofano wrote:

> I second that.  Thats why ZFS filesystems have low overhead, they  
> are closer to directories then traditional filesystems.  Plus it  
> gives you a lot more fine grained control of your stuff.  For the  
> future we're considering possibly doing something in the line of  
> excluding certain directories, especially for backups and privacy  
> concerns.  However for now, multiple zfs filesystems is the best way  
> to go.
>
> Noel
>
> On Dec 19, 2008, at 3:18 PM, Robert Gordon wrote:
>
>>
>> On Dec 19, 2008, at 4:31 PM, Christian Kendi wrote:
>>
>>> Hi,
>>>
>>> 1. I was wondering how the progress is on the .snapshots feature is?
>>>
>>> 2. I'm using ZFS as my home directory now. There are certain  
>>> directories
>>> which i dont like to snapshot. For example, Movies and Music makes  
>>> no sense.
>>>
>>> Is anyone aware of a possiblity to exclude certain directories  
>>> from getting
>>> snapshotted?
>>
>> Make Movies and Music zfs filesystems.
>>
>> ;-)
>>
>> Robert.
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From richmc at gmail.com  Sat Dec 20 11:23:24 2008
From: richmc at gmail.com (Richard McClellan)
Date: Sat, 20 Dec 2008 11:23:24 -0800
Subject: [zfs-discuss] snapshots
In-Reply-To: <0F79DE0F-328E-4033-8EED-CA65951C5703@nrao.edu>
References: <7B02C308-334C-4F2D-A31C-1CE993C40670@ironsoftware.de>
	<B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
	<0F79DE0F-328E-4033-8EED-CA65951C5703@nrao.edu>
Message-ID: <CB583DFA-E2E8-4F86-9579-C457603D3E14@gmail.com>

What is your snapshot command aliased to?

On Dec 20, 2008, at 11:17 , Boyd Waters wrote:

> Except when it stops working:
>
> $ snapshot
> cannot create snapshot 'pool/bwaters at 2008-12-20T12:15:14': dataset  
> is busy
>
>
> I've exported/imported, scrubbed, rebooted. I've tried logging in as  
> a console (root) user and scrubbing and then snapshot from that  
> user. No joy.
>
> What else should I try?
>
>
>
>
> On Dec 19, 2008, at 7:59 PM, No?l Dellofano wrote:
>
>> I second that.  Thats why ZFS filesystems have low overhead, they  
>> are closer to directories then traditional filesystems.  Plus it  
>> gives you a lot more fine grained control of your stuff.  For the  
>> future we're considering possibly doing something in the line of  
>> excluding certain directories, especially for backups and privacy  
>> concerns.  However for now, multiple zfs filesystems is the best  
>> way to go.
>>
>> Noel
>>
>> On Dec 19, 2008, at 3:18 PM, Robert Gordon wrote:
>>
>>>
>>> On Dec 19, 2008, at 4:31 PM, Christian Kendi wrote:
>>>
>>>> Hi,
>>>>
>>>> 1. I was wondering how the progress is on the .snapshots feature  
>>>> is?
>>>>
>>>> 2. I'm using ZFS as my home directory now. There are certain  
>>>> directories
>>>> which i dont like to snapshot. For example, Movies and Music  
>>>> makes no sense.
>>>>
>>>> Is anyone aware of a possiblity to exclude certain directories  
>>>> from getting
>>>> snapshotted?
>>>
>>> Make Movies and Music zfs filesystems.
>>>
>>> ;-)
>>>
>>> Robert.
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From hanche at math.ntnu.no  Sat Dec 20 11:29:20 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Sat, 20 Dec 2008 20:29:20 +0100 (CET)
Subject: [zfs-discuss] snapshots
In-Reply-To: <0F79DE0F-328E-4033-8EED-CA65951C5703@nrao.edu>
References: <B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
	<0F79DE0F-328E-4033-8EED-CA65951C5703@nrao.edu>
Message-ID: <20081220.202920.185448097.hanche@math.ntnu.no>

+ Boyd Waters <bwaters at nrao.edu>:

> Except when it stops working:
>
> $ snapshot
> cannot create snapshot 'pool/bwaters at 2008-12-20T12:15:14': dataset 
> is busy
>
>
> I've exported/imported, scrubbed, rebooted. I've tried logging in 
> as a console (root) user and scrubbing and then snapshot from that 
> user. No joy.
>
> What else should I try?

Is the scrub still in progress? That might be the reason for the 
trouble. If all else fails, you might try unmounting the file system 
and then taking a snapshot of it.

- Harald

From bwaters at nrao.edu  Sat Dec 20 11:46:25 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Sat, 20 Dec 2008 12:46:25 -0700
Subject: [zfs-discuss] snapshots
In-Reply-To: <20081220.202920.185448097.hanche@math.ntnu.no>
References: <B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
	<0F79DE0F-328E-4033-8EED-CA65951C5703@nrao.edu>
	<20081220.202920.185448097.hanche@math.ntnu.no>
Message-ID: <F8071BC9-E5FC-428B-BCC7-2460C9EBD1E2@nrao.edu>

Thanks all for the replies!

This list never lets you down! :-)



On Dec 20, 2008, at 12:29 PM, Harald Hanche-Olsen wrote:

> Is the scrub still in progress? That might be the reason for the  
> trouble. If all else fails, you might try unmounting the file system  
> and then taking a snapshot of it.


Good ideas -- so good that I've tried :-(  The scrub is completed, no  
errors. unmounted and can't take a snapshot.


On Dec 20, 2008, at 12:23 PM, Richard McClellan wrote:

> What is your snapshot command aliased to?


Very good question, very sorry I didn't show that:

$ cat $(which snapshot)
#! /bin/bash

zfs snapshot pool/bwaters@$(date +%FT%T)


From alex.blewitt at gmail.com  Sat Dec 20 11:48:46 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Sat, 20 Dec 2008 19:48:46 +0000
Subject: [zfs-discuss] snapshots
In-Reply-To: <F8071BC9-E5FC-428B-BCC7-2460C9EBD1E2@nrao.edu>
References: <B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
	<0F79DE0F-328E-4033-8EED-CA65951C5703@nrao.edu>
	<20081220.202920.185448097.hanche@math.ntnu.no>
	<F8071BC9-E5FC-428B-BCC7-2460C9EBD1E2@nrao.edu>
Message-ID: <636fd28e0812201148w719e397cg18f3b634af53e23d@mail.gmail.com>

On Sat, Dec 20, 2008 at 7:46 PM, Boyd Waters <bwaters at nrao.edu> wrote:
> Good ideas -- so good that I've tried :-(  The scrub is completed, no
> errors. unmounted and can't take a snapshot.

Can you zfs export -f the pool, then zfs import it again?

Alex

From richmc at gmail.com  Sat Dec 20 12:02:33 2008
From: richmc at gmail.com (Richard McClellan)
Date: Sat, 20 Dec 2008 12:02:33 -0800
Subject: [zfs-discuss] snapshots
In-Reply-To: <F8071BC9-E5FC-428B-BCC7-2460C9EBD1E2@nrao.edu>
References: <B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
	<0F79DE0F-328E-4033-8EED-CA65951C5703@nrao.edu>
	<20081220.202920.185448097.hanche@math.ntnu.no>
	<F8071BC9-E5FC-428B-BCC7-2460C9EBD1E2@nrao.edu>
Message-ID: <5792952B-8C55-4D43-A8A4-362C1F7FCFCA@gmail.com>


On Dec 20, 2008, at 11:46 , Boyd Waters wrote:
>
>
>
> On Dec 20, 2008, at 12:23 PM, Richard McClellan wrote:
>
>> What is your snapshot command aliased to?
>
>
> Very good question, very sorry I didn't show that:
>
> $ cat $(which snapshot)
> #! /bin/bash
>
> zfs snapshot pool/bwaters@$(date +%FT%T)
>

Hmm.  The command looks fine and works for me.

Others that have encountered this error (dataset busy) manually umount/ 
mount the filesystem and snapshots started working again.  Check this  
out: http://malsserver.blogspot.com/2008/11/zfs-auto-snapshot-and-thus-time-slider.html

		Rich

From johnemac at tekserve.com  Sat Dec 20 12:34:04 2008
From: johnemac at tekserve.com (John McAdams)
Date: Sat, 20 Dec 2008 15:34:04 -0500
Subject: [zfs-discuss] snapshots
In-Reply-To: <5792952B-8C55-4D43-A8A4-362C1F7FCFCA@gmail.com>
References: <B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
	<0F79DE0F-328E-4033-8EED-CA65951C5703@nrao.edu>
	<20081220.202920.185448097.hanche@math.ntnu.no>
	<F8071BC9-E5FC-428B-BCC7-2460C9EBD1E2@nrao.edu>
	<5792952B-8C55-4D43-A8A4-362C1F7FCFCA@gmail.com>
Message-ID: <6FD43096-3580-4B92-8A36-9ACFA446950E@tekserve.com>

Could someone remind me how to get the ZFS FS to mount? I can create  
one just fine using zfs create reflection/Music.

I see my Music FS under the reflection pool, but clicking on it and  
issuing zfs mount commands don't work. I believe this has been covered  
before but I can't find the thread.

Thanks,
-- jmca

On Dec 20, 2008, at 3:02 PM, Richard McClellan wrote:

>
> On Dec 20, 2008, at 11:46 , Boyd Waters wrote:
>>
>>
>>
>> On Dec 20, 2008, at 12:23 PM, Richard McClellan wrote:
>>
>>> What is your snapshot command aliased to?
>>
>>
>> Very good question, very sorry I didn't show that:
>>
>> $ cat $(which snapshot)
>> #! /bin/bash
>>
>> zfs snapshot pool/bwaters@$(date +%FT%T)
>>
>
> Hmm.  The command looks fine and works for me.
>
> Others that have encountered this error (dataset busy) manually  
> umount/mount the filesystem and snapshots started working again.   
> Check this out: http://malsserver.blogspot.com/2008/11/zfs-auto-snapshot-and-thus-time-slider.html
>
> 		Rich
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From zfs at sbod.at  Sat Dec 20 12:47:36 2008
From: zfs at sbod.at (Franz Schmalzl)
Date: Sat, 20 Dec 2008 21:47:36 +0100
Subject: [zfs-discuss] snapshots
In-Reply-To: <6FD43096-3580-4B92-8A36-9ACFA446950E@tekserve.com>
References: <B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
	<0F79DE0F-328E-4033-8EED-CA65951C5703@nrao.edu>
	<20081220.202920.185448097.hanche@math.ntnu.no>
	<F8071BC9-E5FC-428B-BCC7-2460C9EBD1E2@nrao.edu>
	<5792952B-8C55-4D43-A8A4-362C1F7FCFCA@gmail.com>
	<6FD43096-3580-4B92-8A36-9ACFA446950E@tekserve.com>
Message-ID: <5B5B61CC-65D9-4804-921C-AF367C2F18F3@sbod.at>

try adjusting the mountpoint property

zfs set mountpoint=mountpoint pool/filesystem


franz



On 20.12.2008, at 21:34, John McAdams wrote:

> Could someone remind me how to get the ZFS FS to mount? I can create  
> one just fine using zfs create reflection/Music.
>
> I see my Music FS under the reflection pool, but clicking on it and  
> issuing zfs mount commands don't work. I believe this has been  
> covered before but I can't find the thread.
>
> Thanks,
> -- jmca
>
> On Dec 20, 2008, at 3:02 PM, Richard McClellan wrote:
>
>>
>> On Dec 20, 2008, at 11:46 , Boyd Waters wrote:
>>>
>>>
>>>
>>> On Dec 20, 2008, at 12:23 PM, Richard McClellan wrote:
>>>
>>>> What is your snapshot command aliased to?
>>>
>>>
>>> Very good question, very sorry I didn't show that:
>>>
>>> $ cat $(which snapshot)
>>> #! /bin/bash
>>>
>>> zfs snapshot pool/bwaters@$(date +%FT%T)
>>>
>>
>> Hmm.  The command looks fine and works for me.
>>
>> Others that have encountered this error (dataset busy) manually  
>> umount/mount the filesystem and snapshots started working again.   
>> Check this out: http://malsserver.blogspot.com/2008/11/zfs-auto-snapshot-and-thus-time-slider.html
>>
>> 		Rich
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From johnemac at tekserve.com  Sat Dec 20 13:19:10 2008
From: johnemac at tekserve.com (John McAdams)
Date: Sat, 20 Dec 2008 16:19:10 -0500
Subject: [zfs-discuss] snapshots
In-Reply-To: <5B5B61CC-65D9-4804-921C-AF367C2F18F3@sbod.at>
References: <B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
	<0F79DE0F-328E-4033-8EED-CA65951C5703@nrao.edu>
	<20081220.202920.185448097.hanche@math.ntnu.no>
	<F8071BC9-E5FC-428B-BCC7-2460C9EBD1E2@nrao.edu>
	<5792952B-8C55-4D43-A8A4-362C1F7FCFCA@gmail.com>
	<6FD43096-3580-4B92-8A36-9ACFA446950E@tekserve.com>
	<5B5B61CC-65D9-4804-921C-AF367C2F18F3@sbod.at>
Message-ID: <7FCA3F04-8BF5-45E9-A663-6D4D85A80E3C@tekserve.com>

I gave that a try

sparky:~ johnemac$ zfs set mountpoint=/Volumes/AVClub reflection/AVClub
sparky:~ johnemac$ ls -la /Volumes/
total 14
drwxrwxrwt@  7 root      admin   238 Dec 20 16:16 .
drwxrwxr-t  33 root      admin  1190 Dec 16 08:45 ..
drwxr-xr-x   3 johnemac  staff     3 Dec 20 16:14 AVClub
drwxrwxr-t  29 root      admin  1054 Dec 17 09:10 Snow Leopard
lrwxr-xr-x   1 root      admin     1 Dec 20 15:13 Sparky HD -> /
drwxrwxr-x@ 16 johnemac  staff   612 Dec 18 09:54 macie320
drwxr-xr-x@  5 johnemac  staff     6 Dec 20 16:16 reflection


I can write to it if I go to /Volumes, but it still doesn't appear on  
the Desktop or in the Sidebar.

Thanks for the help. I can use if from here.

-- jmca

On Dec 20, 2008, at 3:47 PM, Franz Schmalzl wrote:

> try adjusting the mountpoint property
>
> zfs set mountpoint=mountpoint pool/filesystem
>
>
> franz
>
>
>
> On 20.12.2008, at 21:34, John McAdams wrote:
>
>> Could someone remind me how to get the ZFS FS to mount? I can  
>> create one just fine using zfs create reflection/Music.
>>
>> I see my Music FS under the reflection pool, but clicking on it and  
>> issuing zfs mount commands don't work. I believe this has been  
>> covered before but I can't find the thread.
>>
>> Thanks,
>> -- jmca
>>
>> On Dec 20, 2008, at 3:02 PM, Richard McClellan wrote:
>>
>>>
>>> On Dec 20, 2008, at 11:46 , Boyd Waters wrote:
>>>>
>>>>
>>>>
>>>> On Dec 20, 2008, at 12:23 PM, Richard McClellan wrote:
>>>>
>>>>> What is your snapshot command aliased to?
>>>>
>>>>
>>>> Very good question, very sorry I didn't show that:
>>>>
>>>> $ cat $(which snapshot)
>>>> #! /bin/bash
>>>>
>>>> zfs snapshot pool/bwaters@$(date +%FT%T)
>>>>
>>>
>>> Hmm.  The command looks fine and works for me.
>>>
>>> Others that have encountered this error (dataset busy) manually  
>>> umount/mount the filesystem and snapshots started working again.   
>>> Check this out: http://malsserver.blogspot.com/2008/11/zfs-auto-snapshot-and-thus-time-slider.html
>>>
>>> 		Rich
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081220/28ef9ac8/attachment.html>

From zfs at sbod.at  Sat Dec 20 14:01:16 2008
From: zfs at sbod.at (Franz Schmalzl)
Date: Sat, 20 Dec 2008 23:01:16 +0100
Subject: [zfs-discuss] snapshots
In-Reply-To: <7FCA3F04-8BF5-45E9-A663-6D4D85A80E3C@tekserve.com>
References: <B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
	<0F79DE0F-328E-4033-8EED-CA65951C5703@nrao.edu>
	<20081220.202920.185448097.hanche@math.ntnu.no>
	<F8071BC9-E5FC-428B-BCC7-2460C9EBD1E2@nrao.edu>
	<5792952B-8C55-4D43-A8A4-362C1F7FCFCA@gmail.com>
	<6FD43096-3580-4B92-8A36-9ACFA446950E@tekserve.com>
	<5B5B61CC-65D9-4804-921C-AF367C2F18F3@sbod.at>
	<7FCA3F04-8BF5-45E9-A663-6D4D85A80E3C@tekserve.com>
Message-ID: <91E67E40-834D-4C52-8763-09E55FC278F8@sbod.at>

Yes I have the same issue,

My zpool ist mounted at /Users

altough /Users does not show up in the finder anymore...
when issuing a ls -al it's there



noel ?




cheers

franz




On 20.12.2008, at 22:19, John McAdams wrote:

> I gave that a try
>
> sparky:~ johnemac$ zfs set mountpoint=/Volumes/AVClub reflection/ 
> AVClub
> sparky:~ johnemac$ ls -la /Volumes/
> total 14
> drwxrwxrwt@  7 root      admin   238 Dec 20 16:16 .
> drwxrwxr-t  33 root      admin  1190 Dec 16 08:45 ..
> drwxr-xr-x   3 johnemac  staff     3 Dec 20 16:14 AVClub
> drwxrwxr-t  29 root      admin  1054 Dec 17 09:10 Snow Leopard
> lrwxr-xr-x   1 root      admin     1 Dec 20 15:13 Sparky HD -> /
> drwxrwxr-x@ 16 johnemac  staff   612 Dec 18 09:54 macie320
> drwxr-xr-x@  5 johnemac  staff     6 Dec 20 16:16 reflection
>
>
> I can write to it if I go to /Volumes, but it still doesn't appear  
> on the Desktop or in the Sidebar.
>
> Thanks for the help. I can use if from here.
>
> -- jmca
>
> On Dec 20, 2008, at 3:47 PM, Franz Schmalzl wrote:
>
>> try adjusting the mountpoint property
>>
>> zfs set mountpoint=mountpoint pool/filesystem
>>
>>
>> franz
>>
>>
>>
>> On 20.12.2008, at 21:34, John McAdams wrote:
>>
>>> Could someone remind me how to get the ZFS FS to mount? I can  
>>> create one just fine using zfs create reflection/Music.
>>>
>>> I see my Music FS under the reflection pool, but clicking on it  
>>> and issuing zfs mount commands don't work. I believe this has been  
>>> covered before but I can't find the thread.
>>>
>>> Thanks,
>>> -- jmca
>>>
>>> On Dec 20, 2008, at 3:02 PM, Richard McClellan wrote:
>>>
>>>>
>>>> On Dec 20, 2008, at 11:46 , Boyd Waters wrote:
>>>>>
>>>>>
>>>>>
>>>>> On Dec 20, 2008, at 12:23 PM, Richard McClellan wrote:
>>>>>
>>>>>> What is your snapshot command aliased to?
>>>>>
>>>>>
>>>>> Very good question, very sorry I didn't show that:
>>>>>
>>>>> $ cat $(which snapshot)
>>>>> #! /bin/bash
>>>>>
>>>>> zfs snapshot pool/bwaters@$(date +%FT%T)
>>>>>
>>>>
>>>> Hmm.  The command looks fine and works for me.
>>>>
>>>> Others that have encountered this error (dataset busy) manually  
>>>> umount/mount the filesystem and snapshots started working again.   
>>>> Check this out: http://malsserver.blogspot.com/2008/11/zfs-auto-snapshot-and-thus-time-slider.html
>>>>
>>>> 		Rich
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From macrbg at mac.com  Sat Dec 20 17:43:20 2008
From: macrbg at mac.com (Robert Gordon)
Date: Sat, 20 Dec 2008 19:43:20 -0600
Subject: [zfs-discuss] snapshots
In-Reply-To: <91E67E40-834D-4C52-8763-09E55FC278F8@sbod.at>
References: <B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
	<0F79DE0F-328E-4033-8EED-CA65951C5703@nrao.edu>
	<20081220.202920.185448097.hanche@math.ntnu.no>
	<F8071BC9-E5FC-428B-BCC7-2460C9EBD1E2@nrao.edu>
	<5792952B-8C55-4D43-A8A4-362C1F7FCFCA@gmail.com>
	<6FD43096-3580-4B92-8A36-9ACFA446950E@tekserve.com>
	<5B5B61CC-65D9-4804-921C-AF367C2F18F3@sbod.at>
	<7FCA3F04-8BF5-45E9-A663-6D4D85A80E3C@tekserve.com>
	<91E67E40-834D-4C52-8763-09E55FC278F8@sbod.at>
Message-ID: <1E557182-D5EC-4AEB-A502-DB4928AB299B@mac.com>


IIRC this is a known issue that the Apple Z-Team are addressing; there  
are issues (in the version of ZFS we are using) since this is a beta  
preview... If i had to guess, i would say that 1st QTR next year we  
will see something more integrated and stable... but again this would  
be a guess, since i work for storspeed not apple .. ;-)


Robert..


On Dec 20, 2008, at 4:01 PM, Franz Schmalzl wrote:

> Yes I have the same issue,
>
> My zpool ist mounted at /Users
>
> altough /Users does not show up in the finder anymore...
> when issuing a ls -al it's there
>
>
>
> noel ?
>
>
>
>
> cheers
>
> franz
>
>
>
>
> On 20.12.2008, at 22:19, John McAdams wrote:
>
>> I gave that a try
>>
>> sparky:~ johnemac$ zfs set mountpoint=/Volumes/AVClub reflection/ 
>> AVClub
>> sparky:~ johnemac$ ls -la /Volumes/
>> total 14
>> drwxrwxrwt@  7 root      admin   238 Dec 20 16:16 .
>> drwxrwxr-t  33 root      admin  1190 Dec 16 08:45 ..
>> drwxr-xr-x   3 johnemac  staff     3 Dec 20 16:14 AVClub
>> drwxrwxr-t  29 root      admin  1054 Dec 17 09:10 Snow Leopard
>> lrwxr-xr-x   1 root      admin     1 Dec 20 15:13 Sparky HD -> /
>> drwxrwxr-x@ 16 johnemac  staff   612 Dec 18 09:54 macie320
>> drwxr-xr-x@  5 johnemac  staff     6 Dec 20 16:16 reflection
>>
>>
>> I can write to it if I go to /Volumes, but it still doesn't appear  
>> on the Desktop or in the Sidebar.
>>
>> Thanks for the help. I can use if from here.
>>
>> -- jmca
>>
>> On Dec 20, 2008, at 3:47 PM, Franz Schmalzl wrote:
>>
>>> try adjusting the mountpoint property
>>>
>>> zfs set mountpoint=mountpoint pool/filesystem
>>>
>>>
>>> franz
>>>
>>>
>>>
>>> On 20.12.2008, at 21:34, John McAdams wrote:
>>>
>>>> Could someone remind me how to get the ZFS FS to mount? I can  
>>>> create one just fine using zfs create reflection/Music.
>>>>
>>>> I see my Music FS under the reflection pool, but clicking on it  
>>>> and issuing zfs mount commands don't work. I believe this has  
>>>> been covered before but I can't find the thread.
>>>>
>>>> Thanks,
>>>> -- jmca
>>>>
>>>> On Dec 20, 2008, at 3:02 PM, Richard McClellan wrote:
>>>>
>>>>>
>>>>> On Dec 20, 2008, at 11:46 , Boyd Waters wrote:
>>>>>>
>>>>>>
>>>>>>
>>>>>> On Dec 20, 2008, at 12:23 PM, Richard McClellan wrote:
>>>>>>
>>>>>>> What is your snapshot command aliased to?
>>>>>>
>>>>>>
>>>>>> Very good question, very sorry I didn't show that:
>>>>>>
>>>>>> $ cat $(which snapshot)
>>>>>> #! /bin/bash
>>>>>>
>>>>>> zfs snapshot pool/bwaters@$(date +%FT%T)
>>>>>>
>>>>>
>>>>> Hmm.  The command looks fine and works for me.
>>>>>
>>>>> Others that have encountered this error (dataset busy) manually  
>>>>> umount/mount the filesystem and snapshots started working  
>>>>> again.  Check this out: http://malsserver.blogspot.com/2008/11/zfs-auto-snapshot-and-thus-time-slider.html
>>>>>
>>>>> 		Rich
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From zfs-discuss at stomer.net  Mon Dec 22 02:23:45 2008
From: zfs-discuss at stomer.net (Paul Grave)
Date: Mon, 22 Dec 2008 10:23:45 +0000
Subject: [zfs-discuss] Expanding a Raidz
Message-ID: <6DCAAE7D-F4AD-4A40-9131-DEE3CE3C8A71@stomer.net>

Hi,

I came across this post while looking for solutions to the problem of  
expanding a raidz:
http://mail.opensolaris.org/pipermail/zfs-discuss/2005-November/029698.html

I followed the instructions like so:

mkfile 80m disk1 disk2 disk3 disk4
mkfile 160m disk5 disk6 disk7 disk8
zpool create -f pool raidz /Users/paul/disk1 /Users/paul/disk2 /Users/ 
paul/disk3 /Users/paul/disk4
zpool upgrade pool # I tried without upgrading and had the same results
zpool list pool

NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
pool                    302M    534K    301M     0%  ONLINE     -

then:

zpool replace pool /Users/paul/disk1 /Users/paul/disk5
zpool replace pool /Users/paul/disk2 /Users/paul/disk6
zpool replace pool /Users/paul/disk3 /Users/paul/disk7
zpool replace pool /Users/paul/disk4 /Users/paul/disk8
zpool list pool

NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
pool                    302M    543K    301M     0%  ONLINE     -

According to the above mention mailing list post, I should be seeing:
NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
pool                    622M    262K    622M     0%  ONLINE     -

Am I missing something?  Or is ZFS on the Mac missing something?  Or  
is the author of the original mailing-list post mistaken?

--
Paul Grave
email: mailto:paul at stomer.net
web: http://paulgrave.eu

--
Paul Grave
email: mailto:paul at stomer.net
web: http://paulgrave.eu


From dirkschelfhout at mac.com  Mon Dec 22 03:00:45 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Mon, 22 Dec 2008 12:00:45 +0100
Subject: [zfs-discuss] Expanding a Raidz
In-Reply-To: <6DCAAE7D-F4AD-4A40-9131-DEE3CE3C8A71@stomer.net>
References: <6DCAAE7D-F4AD-4A40-9131-DEE3CE3C8A71@stomer.net>
Message-ID: <68671770-62AA-4CEB-89DB-ED65DE6582ED@mac.com>

Did you let it resilver after each replace ?
I believe I did a scrub after each replace. can;t remember.
On 22 Dec 2008, at 11:23, Paul Grave wrote:

> Hi,
>
> I came across this post while looking for solutions to the problem  
> of expanding a raidz:
> http://mail.opensolaris.org/pipermail/zfs-discuss/2005-November/029698.html
>
> I followed the instructions like so:
>
> mkfile 80m disk1 disk2 disk3 disk4
> mkfile 160m disk5 disk6 disk7 disk8
> zpool create -f pool raidz /Users/paul/disk1 /Users/paul/disk2 / 
> Users/paul/disk3 /Users/paul/disk4
> zpool upgrade pool # I tried without upgrading and had the same  
> results
> zpool list pool
>
> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
> ALTROOT
> pool                    302M    534K    301M     0%  ONLINE     -
>
> then:
>
> zpool replace pool /Users/paul/disk1 /Users/paul/disk5
> zpool replace pool /Users/paul/disk2 /Users/paul/disk6
> zpool replace pool /Users/paul/disk3 /Users/paul/disk7
> zpool replace pool /Users/paul/disk4 /Users/paul/disk8
> zpool list pool
>
> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
> ALTROOT
> pool                    302M    543K    301M     0%  ONLINE     -
>
> According to the above mention mailing list post, I should be seeing:
> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
> ALTROOT
> pool                    622M    262K    622M     0%  ONLINE     -
>
> Am I missing something?  Or is ZFS on the Mac missing something?  Or  
> is the author of the original mailing-list post mistaken?
>
> --
> Paul Grave
> email: mailto:paul at stomer.net
> web: http://paulgrave.eu
>
> --
> Paul Grave
> email: mailto:paul at stomer.net
> web: http://paulgrave.eu
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From zfs-discuss at stomer.net  Mon Dec 22 03:07:10 2008
From: zfs-discuss at stomer.net (Paul Grave)
Date: Mon, 22 Dec 2008 11:07:10 +0000
Subject: [zfs-discuss] Expanding a Raidz
In-Reply-To: <68671770-62AA-4CEB-89DB-ED65DE6582ED@mac.com>
References: <6DCAAE7D-F4AD-4A40-9131-DEE3CE3C8A71@stomer.net>
	<68671770-62AA-4CEB-89DB-ED65DE6582ED@mac.com>
Message-ID: <3E701E8E-E045-4325-A53B-4972FF4E2763@stomer.net>

Thanks. That's all it needed:

zpool scrub pool
zpool list pool
NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
pool                    622M    621K    621M     0%  ONLINE     -

Nice!

--
Paul Grave
email: mailto:paul at stomer.net
web: http://paulgrave.eu

On 22 Dec 2008, at 11:00, Dirk Schelfhout wrote:

> Did you let it resilver after each replace ?
> I believe I did a scrub after each replace. can;t remember.
> On 22 Dec 2008, at 11:23, Paul Grave wrote:
>
>> Hi,
>>
>> I came across this post while looking for solutions to the problem  
>> of expanding a raidz:
>> http://mail.opensolaris.org/pipermail/zfs-discuss/2005-November/029698.html
>>
>> I followed the instructions like so:
>>
>> mkfile 80m disk1 disk2 disk3 disk4
>> mkfile 160m disk5 disk6 disk7 disk8
>> zpool create -f pool raidz /Users/paul/disk1 /Users/paul/disk2 / 
>> Users/paul/disk3 /Users/paul/disk4
>> zpool upgrade pool # I tried without upgrading and had the same  
>> results
>> zpool list pool
>>
>> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
>> ALTROOT
>> pool                    302M    534K    301M     0%  ONLINE     -
>>
>> then:
>>
>> zpool replace pool /Users/paul/disk1 /Users/paul/disk5
>> zpool replace pool /Users/paul/disk2 /Users/paul/disk6
>> zpool replace pool /Users/paul/disk3 /Users/paul/disk7
>> zpool replace pool /Users/paul/disk4 /Users/paul/disk8
>> zpool list pool
>>
>> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
>> ALTROOT
>> pool                    302M    543K    301M     0%  ONLINE     -
>>
>> According to the above mention mailing list post, I should be seeing:
>> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
>> ALTROOT
>> pool                    622M    262K    622M     0%  ONLINE     -
>>
>> Am I missing something?  Or is ZFS on the Mac missing something?   
>> Or is the author of the original mailing-list post mistaken?
>>
>> --
>> Paul Grave
>> email: mailto:paul at stomer.net
>> web: http://paulgrave.eu
>>
>> --
>> Paul Grave
>> email: mailto:paul at stomer.net
>> web: http://paulgrave.eu
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dirkschelfhout at mac.com  Mon Dec 22 03:16:46 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Mon, 22 Dec 2008 12:16:46 +0100
Subject: [zfs-discuss] Expanding a Raidz
In-Reply-To: <080526AF-5208-42FB-8BCE-32A7B3A9E948@stomer.net>
References: <6DCAAE7D-F4AD-4A40-9131-DEE3CE3C8A71@stomer.net>
	<68671770-62AA-4CEB-89DB-ED65DE6582ED@mac.com>
	<080526AF-5208-42FB-8BCE-32A7B3A9E948@stomer.net>
Message-ID: <C33D988C-9184-4C35-B1A8-09FCA44848B0@mac.com>

remember if you do this with disks with real data on them,
you have to do a scrub after each replace.!!!!
Dirk
On 22 Dec 2008, at 12:06, Paul Grave wrote:

> Thanks. That's all it needed:
>
> zpool scrub pool
> zpool list pool
> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
> ALTROOT
> pool                    622M    621K    621M     0%  ONLINE     -
>
> Nice!
>
> --
> Paul Grave
> email: mailto:paul at stomer.net
> web: http://paulgrave.eu
>
> On 22 Dec 2008, at 11:00, Dirk Schelfhout wrote:
>
>> Did you let it resilver after each replace ?
>> I believe I did a scrub after each replace. can;t remember.
>> On 22 Dec 2008, at 11:23, Paul Grave wrote:
>>
>>> Hi,
>>>
>>> I came across this post while looking for solutions to the problem  
>>> of expanding a raidz:
>>> http://mail.opensolaris.org/pipermail/zfs-discuss/2005-November/029698.html
>>>
>>> I followed the instructions like so:
>>>
>>> mkfile 80m disk1 disk2 disk3 disk4
>>> mkfile 160m disk5 disk6 disk7 disk8
>>> zpool create -f pool raidz /Users/paul/disk1 /Users/paul/disk2 / 
>>> Users/paul/disk3 /Users/paul/disk4
>>> zpool upgrade pool # I tried without upgrading and had the same  
>>> results
>>> zpool list pool
>>>
>>> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
>>> ALTROOT
>>> pool                    302M    534K    301M     0%  ONLINE     -
>>>
>>> then:
>>>
>>> zpool replace pool /Users/paul/disk1 /Users/paul/disk5
>>> zpool replace pool /Users/paul/disk2 /Users/paul/disk6
>>> zpool replace pool /Users/paul/disk3 /Users/paul/disk7
>>> zpool replace pool /Users/paul/disk4 /Users/paul/disk8
>>> zpool list pool
>>>
>>> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
>>> ALTROOT
>>> pool                    302M    543K    301M     0%  ONLINE     -
>>>
>>> According to the above mention mailing list post, I should be  
>>> seeing:
>>> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
>>> ALTROOT
>>> pool                    622M    262K    622M     0%  ONLINE     -
>>>
>>> Am I missing something?  Or is ZFS on the Mac missing something?   
>>> Or is the author of the original mailing-list post mistaken?
>>>
>>> --
>>> Paul Grave
>>> email: mailto:paul at stomer.net
>>> web: http://paulgrave.eu
>>>
>>> --
>>> Paul Grave
>>> email: mailto:paul at stomer.net
>>> web: http://paulgrave.eu
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From canadrian at electricteaparty.net  Mon Dec 22 06:37:55 2008
From: canadrian at electricteaparty.net (Adrian Thornton)
Date: Mon, 22 Dec 2008 07:37:55 -0700
Subject: [zfs-discuss] Expanding a Raidz
In-Reply-To: <68671770-62AA-4CEB-89DB-ED65DE6582ED@mac.com>
References: <6DCAAE7D-F4AD-4A40-9131-DEE3CE3C8A71@stomer.net>
	<68671770-62AA-4CEB-89DB-ED65DE6582ED@mac.com>
Message-ID: <72feba2a0812220637s44356acey4347d168de1974b1@mail.gmail.com>

I have done this with actual disks in my RAID-Z before. I upgraded from
320gb to 750gb drives. If I recall, after doing the replace on all disks, I
had to do a zpool export and then a zpool import before the greater capacity
was shown.

On Mon, Dec 22, 2008 at 04:00, Dirk Schelfhout <dirkschelfhout at mac.com>wrote:

> Did you let it resilver after each replace ?
> I believe I did a scrub after each replace. can;t remember.
>
> On 22 Dec 2008, at 11:23, Paul Grave wrote:
>
>  Hi,
>>
>> I came across this post while looking for solutions to the problem of
>> expanding a raidz:
>>
>> http://mail.opensolaris.org/pipermail/zfs-discuss/2005-November/029698.html
>>
>> I followed the instructions like so:
>>
>> mkfile 80m disk1 disk2 disk3 disk4
>> mkfile 160m disk5 disk6 disk7 disk8
>> zpool create -f pool raidz /Users/paul/disk1 /Users/paul/disk2
>> /Users/paul/disk3 /Users/paul/disk4
>> zpool upgrade pool # I tried without upgrading and had the same results
>> zpool list pool
>>
>> NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
>> pool                    302M    534K    301M     0%  ONLINE     -
>>
>> then:
>>
>> zpool replace pool /Users/paul/disk1 /Users/paul/disk5
>> zpool replace pool /Users/paul/disk2 /Users/paul/disk6
>> zpool replace pool /Users/paul/disk3 /Users/paul/disk7
>> zpool replace pool /Users/paul/disk4 /Users/paul/disk8
>> zpool list pool
>>
>> NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
>> pool                    302M    543K    301M     0%  ONLINE     -
>>
>> According to the above mention mailing list post, I should be seeing:
>> NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
>> pool                    622M    262K    622M     0%  ONLINE     -
>>
>> Am I missing something?  Or is ZFS on the Mac missing something?  Or is
>> the author of the original mailing-list post mistaken?
>>
>> --
>> Paul Grave
>> email: mailto:paul at stomer.net
>> web: http://paulgrave.eu
>>
>> --
>> Paul Grave
>> email: mailto:paul at stomer.net
>> web: http://paulgrave.eu
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081222/8d2a70bb/attachment.html>

From dirkschelfhout at mac.com  Mon Dec 22 06:46:58 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Mon, 22 Dec 2008 15:46:58 +0100
Subject: [zfs-discuss] Expanding a Raidz
In-Reply-To: <72feba2a0812220637s44356acey4347d168de1974b1@mail.gmail.com>
References: <6DCAAE7D-F4AD-4A40-9131-DEE3CE3C8A71@stomer.net>
	<68671770-62AA-4CEB-89DB-ED65DE6582ED@mac.com>
	<72feba2a0812220637s44356acey4347d168de1974b1@mail.gmail.com>
Message-ID: <72483BDC-C199-4A28-93D7-7A514FF94A5A@mac.com>

I did the same but didn't do this.
This is what zpool history shows me what I did, apparently it  
resilvered on its own.
the import lines must be from a reboot. :
2008-10-10.21:10:04 zpool offline backup disk1s3
2008-10-10.21:12:18 zpool replace backup /dev/disk1s3 /dev/disk1s2
2008-10-11.01:16:11 zpool offline backup disk2s3
2008-10-11.01:17:03 zpool replace backup /dev/disk2s3 /dev/disk2s2
2008-10-11.04:43:59 zpool import -f 8894140276474256651
2008-10-11.04:47:06 zpool offline backup disk3s3
2008-10-11.04:48:38 zpool replace backup /dev/disk3s3 /dev/disk3s2
2008-10-11.13:55:43 zpool import -f 8894140276474256651

Not to confuse. The disks I replaced are the same, but after the  
replace zfs gets the whole disk.
Before the replace part off it was used for an hfplus stripe.

Dirk

Dirk
On 22 Dec 2008, at 15:37, Adrian Thornton wrote:

> I have done this with actual disks in my RAID-Z before. I upgraded  
> from 320gb to 750gb drives. If I recall, after doing the replace on  
> all disks, I had to do a zpool export and then a zpool import before  
> the greater capacity was shown.
>
> On Mon, Dec 22, 2008 at 04:00, Dirk Schelfhout  
> <dirkschelfhout at mac.com> wrote:
> Did you let it resilver after each replace ?
> I believe I did a scrub after each replace. can;t remember.
>
> On 22 Dec 2008, at 11:23, Paul Grave wrote:
>
> Hi,
>
> I came across this post while looking for solutions to the problem  
> of expanding a raidz:
> http://mail.opensolaris.org/pipermail/zfs-discuss/2005-November/029698.html
>
> I followed the instructions like so:
>
> mkfile 80m disk1 disk2 disk3 disk4
> mkfile 160m disk5 disk6 disk7 disk8
> zpool create -f pool raidz /Users/paul/disk1 /Users/paul/disk2 / 
> Users/paul/disk3 /Users/paul/disk4
> zpool upgrade pool # I tried without upgrading and had the same  
> results
> zpool list pool
>
> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
> ALTROOT
> pool                    302M    534K    301M     0%  ONLINE     -
>
> then:
>
> zpool replace pool /Users/paul/disk1 /Users/paul/disk5
> zpool replace pool /Users/paul/disk2 /Users/paul/disk6
> zpool replace pool /Users/paul/disk3 /Users/paul/disk7
> zpool replace pool /Users/paul/disk4 /Users/paul/disk8
> zpool list pool
>
> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
> ALTROOT
> pool                    302M    543K    301M     0%  ONLINE     -
>
> According to the above mention mailing list post, I should be seeing:
> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
> ALTROOT
> pool                    622M    262K    622M     0%  ONLINE     -
>
> Am I missing something?  Or is ZFS on the Mac missing something?  Or  
> is the author of the original mailing-list post mistaken?
>
> --
> Paul Grave
> email: mailto:paul at stomer.net
> web: http://paulgrave.eu
>
> --
> Paul Grave
> email: mailto:paul at stomer.net
> web: http://paulgrave.eu
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081222/6f3dcb9d/attachment-0001.html>

From ndellofano at apple.com  Mon Dec 22 23:57:38 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon, 22 Dec 2008 23:57:38 -0800
Subject: [zfs-discuss] A feature request
In-Reply-To: <fbaadd320812171310j657784a3mbfe3af4a5df029f7@mail.gmail.com>
References: <fbaadd320812171310j657784a3mbfe3af4a5df029f7@mail.gmail.com>
Message-ID: <03DD928D-F418-4678-B289-C302389A3B82@apple.com>

Hmm, I can check with the hardware guys but i'm not sure we have any  
control of this sort of thing via the filesystem layer.  Likely it's  
out of our control...

Noel

On Dec 17, 2008, at 1:10 PM, Mr. Zorg ... wrote:

> Noel, would it be possible to implement a feature that would cause the
> drive light to blink on a faulted/offline/whatever disk?  Or is there
> a tool already that can do this?  Assuming the drive still has some
> power, that is.  Using external drives, it's *extremely* difficult to
> tell which drive is dead/dying since their drive numbers change every
> time you boot...
>
> Thanks!
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From eableson at mac.com  Tue Dec 23 00:07:11 2008
From: eableson at mac.com (Erik Ableson)
Date: Tue, 23 Dec 2008 09:07:11 +0100
Subject: [zfs-discuss] ZFS Hybrid Storage
Message-ID: <C6438682-2D62-4F4E-81F2-6E2A20D76AED@mac.com>

Hi there, just wondering if there are plans to integrate the HSP  
features to be able to add read and write caching devices to the Apple  
version?

I'm looking at an optibay and an Intel SSD... and something similar  
for my server.

Erik Ableson
+33 6 80 83 58 28
(envoy? par iPhone)


From zorg at sogeeky.net  Tue Dec 23 00:13:07 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Tue, 23 Dec 2008 00:13:07 -0800
Subject: [zfs-discuss] A feature request
In-Reply-To: <03DD928D-F418-4678-B289-C302389A3B82@apple.com>
References: <fbaadd320812171310j657784a3mbfe3af4a5df029f7@mail.gmail.com>
	<03DD928D-F418-4678-B289-C302389A3B82@apple.com>
Message-ID: <180B883C-3ED7-48AE-BE6F-84CA3DF90AF9@sogeeky.net>

That would be really handy. It doesn't have to be at the filesystem  
level. Just something like "diskutil blink disk3" would be useful. If  
I know what disk number the faulted volume is, cool. If not, I can  
find it by making the *other* drives blink. :)

I swear I saw a *nix program to do that once, but I'll be darned if I  
can find it now.

Thanks for listening!  Keep up the good work!

On Dec 22, 2008, at 11:57 PM, No?l Dellofano <ndellofano at apple.com>  
wrote:

> Hmm, I can check with the hardware guys but i'm not sure we have any  
> control of this sort of thing via the filesystem layer.  Likely it's  
> out of our control...
>
> Noel
>
> On Dec 17, 2008, at 1:10 PM, Mr. Zorg ... wrote:
>
>> Noel, would it be possible to implement a feature that would cause  
>> the
>> drive light to blink on a faulted/offline/whatever disk?  Or is there
>> a tool already that can do this?  Assuming the drive still has some
>> power, that is.  Using external drives, it's *extremely* difficult to
>> tell which drive is dead/dying since their drive numbers change every
>> time you boot...
>>
>> Thanks!
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

From alex.blewitt at gmail.com  Tue Dec 23 02:14:48 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Tue, 23 Dec 2008 10:14:48 +0000
Subject: [zfs-discuss] A feature request
In-Reply-To: <180B883C-3ED7-48AE-BE6F-84CA3DF90AF9@sogeeky.net>
References: <fbaadd320812171310j657784a3mbfe3af4a5df029f7@mail.gmail.com>
	<03DD928D-F418-4678-B289-C302389A3B82@apple.com>
	<180B883C-3ED7-48AE-BE6F-84CA3DF90AF9@sogeeky.net>
Message-ID: <636fd28e0812230214n5404d9a2yba34bda269f5cc05@mail.gmail.com>

> I swear I saw a *nix program to do that once, but I'll be darned if I can
> find it now.

If you've got a disk activity light, you could do something like:

while true
do
dd if=/dev/disk1 of=/dev/null bs=1024 count=10
sleep 1
done

That would pulse the disk activity light every second. Bonus points to
those who have a variation on 'count' to make it pulse SOS in morse
code.

Of course, in a heavily loaded disk, that's not going to show up much
- but if you've got a faulted disk that isn't otherwise being used,
should make it somewhat obvious.

Alex

From tnn at t86.dk  Tue Dec 23 02:34:32 2008
From: tnn at t86.dk (=?ISO-8859-1?Q?Troels_N=F8rgaard_Nielsen?=)
Date: Tue, 23 Dec 2008 11:34:32 +0100
Subject: [zfs-discuss] A feature request
In-Reply-To: <636fd28e0812230214n5404d9a2yba34bda269f5cc05@mail.gmail.com>
References: <fbaadd320812171310j657784a3mbfe3af4a5df029f7@mail.gmail.com>
	<03DD928D-F418-4678-B289-C302389A3B82@apple.com>
	<180B883C-3ED7-48AE-BE6F-84CA3DF90AF9@sogeeky.net>
	<636fd28e0812230214n5404d9a2yba34bda269f5cc05@mail.gmail.com>
Message-ID: <BAC89226-62E1-4A12-BB11-4012F0C6CA3C@t86.dk>

The SOS would be..

int c = 10, int n = 0;
if(n >= 3 && n <= 5)    tc = c*3;
else   tc = bs;
if(n == 8)   sleep 7;
else   sleep 1;
n++;

Good holiday every one and second Franz' whistlist. ;-)

Best regards
Troels N?rgaard

Den 23/12/2008 kl. 11.14 skrev Alex Blewitt:

>> I swear I saw a *nix program to do that once, but I'll be darned if  
>> I can
>> find it now.
>
> If you've got a disk activity light, you could do something like:
>
> while true
> do
> dd if=/dev/disk1 of=/dev/null bs=1024 count=10
> sleep 1
> done
>
> That would pulse the disk activity light every second. Bonus points to
> those who have a variation on 'count' to make it pulse SOS in morse
> code.
>
> Of course, in a heavily loaded disk, that's not going to show up much
> - but if you've got a faulted disk that isn't otherwise being used,
> should make it somewhat obvious.
>
> Alex
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Tue Dec 23 10:25:56 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue, 23 Dec 2008 10:25:56 -0800
Subject: [zfs-discuss] ZFS Hybrid Storage
In-Reply-To: <C6438682-2D62-4F4E-81F2-6E2A20D76AED@mac.com>
References: <C6438682-2D62-4F4E-81F2-6E2A20D76AED@mac.com>
Message-ID: <2784B24C-370B-4BAD-8E8B-1CE532E20DAB@apple.com>

We're still discussing pros and cons of that.  We could possibly do it  
in the future but it won't happen immediately since we're  
concentrating on stability.

Noel
On Dec 23, 2008, at 12:07 AM, Erik Ableson wrote:

> Hi there, just wondering if there are plans to integrate the HSP  
> features to be able to add read and write caching devices to the  
> Apple version?
>
> I'm looking at an optibay and an Intel SSD... and something similar  
> for my server.
>
> Erik Ableson
> +33 6 80 83 58 28
> (envoy? par iPhone)
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From eableson at mac.com  Tue Dec 23 10:34:48 2008
From: eableson at mac.com (Erik Ableson)
Date: Tue, 23 Dec 2008 19:34:48 +0100
Subject: [zfs-discuss] ZFS Hybrid Storage
In-Reply-To: <2784B24C-370B-4BAD-8E8B-1CE532E20DAB@apple.com>
References: <C6438682-2D62-4F4E-81F2-6E2A20D76AED@mac.com>
	<2784B24C-370B-4BAD-8E8B-1CE532E20DAB@apple.com>
Message-ID: <5A91D786-7BFA-41DA-BF34-826106DEF1B9@mac.com>

Fair enough. You've already got a lot on your plate.

But I do want to see it in 10.7!

Erik Ableson
+33 6 80 83 58 28
(envoy? par iPhone)


On 23 d?c. 08, at 19:25, No?l Dellofano <ndellofano at apple.com> wrote:

> We're still discussing pros and cons of that.  We could possibly do  
> it in the future but it won't happen immediately since we're  
> concentrating on stability.
>
> Noel
> On Dec 23, 2008, at 12:07 AM, Erik Ableson wrote:
>
>> Hi there, just wondering if there are plans to integrate the HSP  
>> features to be able to add read and write caching devices to the  
>> Apple version?
>>
>> I'm looking at an optibay and an Intel SSD... and something similar  
>> for my server.
>>
>> Erik Ableson
>> +33 6 80 83 58 28
>> (envoy? par iPhone)
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

From zfs at sbod.at  Tue Dec 23 13:27:40 2008
From: zfs at sbod.at (Franz Schmalzl)
Date: Tue, 23 Dec 2008 22:27:40 +0100
Subject: [zfs-discuss] Merry Christmas!
Message-ID: <86CFCB71-1C4E-49A4-A197-C89DBDD2E90B@sbod.at>

Ahoy!

Marry Christmas and Frohe Weihnachten to all of you !
;-)

Cheers,

Franz 
  

From jeff at jeffawaddell.com  Tue Dec 23 13:37:06 2008
From: jeff at jeffawaddell.com (Jeff Waddell)
Date: Tue, 23 Dec 2008 14:37:06 -0700
Subject: [zfs-discuss] rsync backups to ZFS volume
In-Reply-To: <mailman.5.1229007607.6882.zfs-discuss@lists.macosforge.org>
References: <mailman.5.1229007607.6882.zfs-discuss@lists.macosforge.org>
Message-ID: <5BED75AF-63C8-4167-89F9-ED879194FFFB@jeffawaddell.com>

I've been wondering about the effectivness of an rsync backup of an HFS 
+ volume to ZFS as well. For a while now, I've been exploring  
different options to replace Time Machine as a backup server for my  
Mac systems. Time Machine just hasn't been efficient backing up to my  
AirDisk, taking over 30 minutes for small hourly incrementals and no  
delta-transfer. Rsync would be a perfect solution, but then the  
question remains about snapshots. I know there are many rsync-related  
snapshot scripts that make use of hard-links, but they are also  
incredibly slow to an AirDisk.

For efficiency purposes, I keep coming back to ZFS for whole system  
backups. However, I have reservations on how well ZFS can backup all  
of the different aspects of an HFS+ volume (metadata, xattr,  
permissions, fileflags, resource forks. etc.). I've briefly run the  
backup bouncer test script with ZFS, and the results haven't been very  
good, but I'm also not sure if all HFS+ data needs to be preserved to  
have an effective backup (since I'm not looking for bootability). I'm  
especially interested with using OpenSolaris ZFS as a backup server  
(since my Macs are portable). Any personal experiences or advice would  
be greatly appreciated! Thanks!

-Jeff Waddell

On Dec 11, 2008, at 8:00 AM, zfs-discuss-request at lists.macosforge.org  
wrote:
>
> Message: 1
> Date: Thu, 11 Dec 2008 10:59:01 +1100
> From: Nathan <zfs at encode.mine.nu>
> Subject: [zfs-discuss] rsync backups to ZFS volume
> To: zfs-discuss at lists.macosforge.org
> Message-ID: <7f2931472cea820a55908c7656df0872 at localhost>
> Content-Type: text/plain; charset="utf-8"
>
> Hi,
>
> I recently got a new MBP, and thought I should do something about
> backing it up regularly.  I had an external drive formatted as zfs,
> and I figured I'd use rsync to copy files over, then do a ZFS
> snapshot.
>
> I'm wondering what people's thoughts are on the effectiveness of
> rsync to backup a HFS+ volume to a ZFS volume.  I'm using rsync 3.0.4,
> installed via darwinports, with the following flags (which I got from
> http://www.jeffawaddell.com/2008/05/rsync-solaris-problems.html).
> /Volumes/backup is my ZFS volume, I'm using the ZFS-119 binaries.
>
> sudo rsync -avxXHAN --fileflags --protect-args --force-change /
> /Volumes/backup
>
> I plan on excluding the following files and folders, although I
> haven't included that in the above command:
>
> /tmp/*
> /Network/*
> /cores/*
> */.Trash
> /afs/*
> /automount/*
> /private/tmp/*
> /private/var/run/*
> /private/var/spool/postfix/*
> /private/var/vm/*
> /Previous Systems.localized
> .Spotlight-*/
> Can anyone think of any issues doing this?
> I
> notice applications seem to have the correct icons in
> /Volumes/backup/Applications, and I can run some of them from there
> with no issues.  Photoshop CS3 quits straight away, not sure if thats
> due to the application itself or something I'm overlooking.
>
> Thanks!
>
> Regards,
> Nathan.

From zfs at encode.mine.nu  Tue Dec 23 14:08:23 2008
From: zfs at encode.mine.nu (Nathan)
Date: Wed, 24 Dec 2008 09:08:23 +1100
Subject: [zfs-discuss] rsync backups to ZFS volume
In-Reply-To: <5BED75AF-63C8-4167-89F9-ED879194FFFB@jeffawaddell.com>
References: <mailman.5.1229007607.6882.zfs-discuss@lists.macosforge.org>
	<5BED75AF-63C8-4167-89F9-ED879194FFFB@jeffawaddell.com>
Message-ID: <d2e544ca412cfb3332b4e8e82977a954@localhost>

I've been using that rsync command ever since I posted it to the list, with
the addition of the --delete option

It seems to backup important aspects of HFS+, but I admit I've only checked
it by running some applications from their backups on the ZFS volume. Most
seem to run ok, but some don't, for example Photoshop CS3. This turns out
to be at least partially due to the case-sensitivity of ZFS - CS3 would try
opening a library using a path that includes capital letters, but the
directories weren't capitalised correctly on the ZFS volume.

Given that I can open data, and at least some applications, I'm not sure
what more is necessary for a non-bootable backup solution. 

Nathan. 

On Tue, 23 Dec 2008 14:37:06 -0700, Jeff Waddell <jeff at jeffawaddell.com>
wrote:
> I've been wondering about the effectivness of an rsync backup of an HFS
> + volume to ZFS as well. For a while now, I've been exploring
> different options to replace Time Machine as a backup server for my
> Mac systems. Time Machine just hasn't been efficient backing up to my
> AirDisk, taking over 30 minutes for small hourly incrementals and no
> delta-transfer. Rsync would be a perfect solution, but then the
> question remains about snapshots. I know there are many rsync-related
> snapshot scripts that make use of hard-links, but they are also
> incredibly slow to an AirDisk.
> 
> For efficiency purposes, I keep coming back to ZFS for whole system
> backups. However, I have reservations on how well ZFS can backup all
> of the different aspects of an HFS+ volume (metadata, xattr,
> permissions, fileflags, resource forks. etc.). I've briefly run the
> backup bouncer test script with ZFS, and the results haven't been very
> good, but I'm also not sure if all HFS+ data needs to be preserved to
> have an effective backup (since I'm not looking for bootability). I'm
> especially interested with using OpenSolaris ZFS as a backup server
> (since my Macs are portable). Any personal experiences or advice would
> be greatly appreciated! Thanks!
> 
> -Jeff Waddell
> 
> On Dec 11, 2008, at 8:00 AM, zfs-discuss-request at lists.macosforge.org
> wrote:
>>
>> Message: 1
>> Date: Thu, 11 Dec 2008 10:59:01 +1100
>> From: Nathan <zfs at encode.mine.nu>
>> Subject: [zfs-discuss] rsync backups to ZFS volume
>> To: zfs-discuss at lists.macosforge.org
>> Message-ID: <7f2931472cea820a55908c7656df0872 at localhost>
>> Content-Type: text/plain; charset="utf-8"
>>
>> Hi,
>>
>> I recently got a new MBP, and thought I should do something about
>> backing it up regularly.  I had an external drive formatted as zfs,
>> and I figured I'd use rsync to copy files over, then do a ZFS
>> snapshot.
>>
>> I'm wondering what people's thoughts are on the effectiveness of
>> rsync to backup a HFS+ volume to a ZFS volume.  I'm using rsync 3.0.4,
>> installed via darwinports, with the following flags (which I got from
>> http://www.jeffawaddell.com/2008/05/rsync-solaris-problems.html).
>> /Volumes/backup is my ZFS volume, I'm using the ZFS-119 binaries.
>>
>> sudo rsync -avxXHAN --fileflags --protect-args --force-change /
>> /Volumes/backup
>>
>> I plan on excluding the following files and folders, although I
>> haven't included that in the above command:
>>
>> /tmp/*
>> /Network/*
>> /cores/*
>> */.Trash
>> /afs/*
>> /automount/*
>> /private/tmp/*
>> /private/var/run/*
>> /private/var/spool/postfix/*
>> /private/var/vm/*
>> /Previous Systems.localized
>> .Spotlight-*/
>> Can anyone think of any issues doing this?
>> I
>> notice applications seem to have the correct icons in
>> /Volumes/backup/Applications, and I can run some of them from there
>> with no issues.  Photoshop CS3 quits straight away, not sure if thats
>> due to the application itself or something I'm overlooking.
>>
>> Thanks!
>>
>> Regards,
>> Nathan.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From cshei at cs.indiana.edu  Tue Dec 23 15:23:33 2008
From: cshei at cs.indiana.edu (Chun-Yu Shei)
Date: Tue, 23 Dec 2008 18:23:33 -0500
Subject: [zfs-discuss] rsync backups to ZFS volume
In-Reply-To: <5BED75AF-63C8-4167-89F9-ED879194FFFB@jeffawaddell.com>
References: <mailman.5.1229007607.6882.zfs-discuss@lists.macosforge.org>
	<5BED75AF-63C8-4167-89F9-ED879194FFFB@jeffawaddell.com>
Message-ID: <495172F5.2010506@cs.indiana.edu>

This isn't directly related, but I recently moved my home directory on 
my MacBook Pro onto ZFS, leaving the rest of the system on HFS+.  I've 
got an OpenSolaris server sharing a ZVol that I connect to with the 
globalSAN iSCSI initiator 
(http://www.studionetworksolutions.com/products/product_detail.php?pi=11) that's 
formatted as HFS+ for Time Machine to back up to.  This takes care of 
backing up all my applications, and since those don't change very often, 
I only bother "attaching" the Time Machine disk occasionally.

For my home directory, I have a cron job on my server that tries to ssh 
to my laptop at 4:30am, take a snapshot, and send it over the network 
with zfs send/recv.  Since it does an incremental send from snapshot to 
snapshot, there is absolutely no time spent locating what files changed, 
etc.  This of course could result in problems should you disconnect the 
network while it's in progress, but that's why I chose to run it at 
4:30am.  So now I basically have all the benefits of having a Time 
Capsule and more... automatic backups whenever my laptop is on the 
network, compression and checksumming built into the filesystem, and 
snapshots.  I really look forward to official ZFS support in Snow 
Leopard! :)  (although I'm a bit sad to hear that there won't be a 
pretty GUI for it to make it accessible to the masses)

Chun-Yu

On 12/23/08 4:37 PM, Jeff Waddell wrote:
> I've been wondering about the effectivness of an rsync backup of an HFS+
> volume to ZFS as well. For a while now, I've been exploring different
> options to replace Time Machine as a backup server for my Mac systems.
> Time Machine just hasn't been efficient backing up to my AirDisk, taking
> over 30 minutes for small hourly incrementals and no delta-transfer.
> Rsync would be a perfect solution, but then the question remains about
> snapshots. I know there are many rsync-related snapshot scripts that
> make use of hard-links, but they are also incredibly slow to an AirDisk.
>
> For efficiency purposes, I keep coming back to ZFS for whole system
> backups. However, I have reservations on how well ZFS can backup all of
> the different aspects of an HFS+ volume (metadata, xattr, permissions,
> fileflags, resource forks. etc.). I've briefly run the backup bouncer
> test script with ZFS, and the results haven't been very good, but I'm
> also not sure if all HFS+ data needs to be preserved to have an
> effective backup (since I'm not looking for bootability). I'm especially
> interested with using OpenSolaris ZFS as a backup server (since my Macs
> are portable). Any personal experiences or advice would be greatly
> appreciated! Thanks!
>
> -Jeff Waddell
>
> On Dec 11, 2008, at 8:00 AM, zfs-discuss-request at lists.macosforge.org
> wrote:
>>
>> Message: 1
>> Date: Thu, 11 Dec 2008 10:59:01 +1100
>> From: Nathan <zfs at encode.mine.nu>
>> Subject: [zfs-discuss] rsync backups to ZFS volume
>> To: zfs-discuss at lists.macosforge.org
>> Message-ID: <7f2931472cea820a55908c7656df0872 at localhost>
>> Content-Type: text/plain; charset="utf-8"
>>
>> Hi,
>>
>> I recently got a new MBP, and thought I should do something about
>> backing it up regularly. I had an external drive formatted as zfs,
>> and I figured I'd use rsync to copy files over, then do a ZFS
>> snapshot.
>>
>> I'm wondering what people's thoughts are on the effectiveness of
>> rsync to backup a HFS+ volume to a ZFS volume. I'm using rsync 3.0.4,
>> installed via darwinports, with the following flags (which I got from
>> http://www.jeffawaddell.com/2008/05/rsync-solaris-problems.html).
>> /Volumes/backup is my ZFS volume, I'm using the ZFS-119 binaries.
>>
>> sudo rsync -avxXHAN --fileflags --protect-args --force-change /
>> /Volumes/backup
>>
>> I plan on excluding the following files and folders, although I
>> haven't included that in the above command:
>>
>> /tmp/*
>> /Network/*
>> /cores/*
>> */.Trash
>> /afs/*
>> /automount/*
>> /private/tmp/*
>> /private/var/run/*
>> /private/var/spool/postfix/*
>> /private/var/vm/*
>> /Previous Systems.localized
>> .Spotlight-*/
>> Can anyone think of any issues doing this?
>> I
>> notice applications seem to have the correct icons in
>> /Volumes/backup/Applications, and I can run some of them from there
>> with no issues. Photoshop CS3 quits straight away, not sure if thats
>> due to the application itself or something I'm overlooking.
>>
>> Thanks!
>>
>> Regards,
>> Nathan.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From hanche at math.ntnu.no  Wed Dec 24 01:00:05 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Wed, 24 Dec 2008 10:00:05 +0100 (CET)
Subject: [zfs-discuss] rsync backups to ZFS volume
In-Reply-To: <495172F5.2010506@cs.indiana.edu>
References: <mailman.5.1229007607.6882.zfs-discuss@lists.macosforge.org>
	<5BED75AF-63C8-4167-89F9-ED879194FFFB@jeffawaddell.com>
	<495172F5.2010506@cs.indiana.edu>
Message-ID: <20081224.100005.82372709.hanche@math.ntnu.no>

+ Chun-Yu Shei <cshei at cs.indiana.edu>:

> For my home directory, I have a cron job on my server that tries to
> ssh to my laptop at 4:30am, take a snapshot, and send it over the
> network with zfs send/recv.  Since it does an incremental send from
> snapshot to snapshot, there is absolutely no time spent locating what
> files changed, etc.  This of course could result in problems should
> you disconnect the network while it's in progress, but that's why I
> chose to run it at 4:30am.

For your peace of mind, I can assure you that, in my experience (on
freebsd at least) zfs receive does the reasonable thing if it is
receives an incomplete snapshot: Namely, it reverts the target to its
previous state. OTOH, if the receive process should be interrupted,
e.g. by a power failure, you will probably have to roll back the
target filesystem manually before you can send another increment.
Still, no data is likely to be lost (other than your backup for that
night, of course).

- Harald

From cshei at cs.indiana.edu  Wed Dec 24 09:13:56 2008
From: cshei at cs.indiana.edu (Chun-Yu Shei)
Date: Wed, 24 Dec 2008 12:13:56 -0500
Subject: [zfs-discuss] rsync backups to ZFS volume
In-Reply-To: <20081224.100005.82372709.hanche@math.ntnu.no>
References: <mailman.5.1229007607.6882.zfs-discuss@lists.macosforge.org>	<5BED75AF-63C8-4167-89F9-ED879194FFFB@jeffawaddell.com>	<495172F5.2010506@cs.indiana.edu>
	<20081224.100005.82372709.hanche@math.ntnu.no>
Message-ID: <49526DD4.3060307@cs.indiana.edu>

On 12/24/08 4:00 AM, Harald Hanche-Olsen wrote:
> + Chun-Yu Shei<cshei at cs.indiana.edu>:
>
>> For my home directory, I have a cron job on my server that tries to
>> ssh to my laptop at 4:30am, take a snapshot, and send it over the
>> network with zfs send/recv.  Since it does an incremental send from
>> snapshot to snapshot, there is absolutely no time spent locating what
>> files changed, etc.  This of course could result in problems should
>> you disconnect the network while it's in progress, but that's why I
>> chose to run it at 4:30am.
>
> For your peace of mind, I can assure you that, in my experience (on
> freebsd at least) zfs receive does the reasonable thing if it is
> receives an incomplete snapshot: Namely, it reverts the target to its
> previous state. OTOH, if the receive process should be interrupted,
> e.g. by a power failure, you will probably have to roll back the
> target filesystem manually before you can send another increment.
> Still, no data is likely to be lost (other than your backup for that
> night, of course).

Yep, getting the snapshots back in sync is what I was referring to. 
 From what I understand, ZFS creates a temporary clone that it performs 
the receive into, then switches it in once the receive is complete. 
There was a recent blog post by one of the Sun ZFS guys that mentioned 
the process at http://blogs.sun.com/ahrens/entry/new_scrub_code .  So 
data loss is not my concern... it's the fact that the "from" snapshots 
have to match on both sides for the incremental send to work.

Chun-Yu

From zfs at sbod.at  Sun Dec 28 06:53:25 2008
From: zfs at sbod.at (Franz Schmalzl)
Date: Sun, 28 Dec 2008 15:53:25 +0100
Subject: [zfs-discuss] TimeMachine (Again)
Message-ID: <54E34407-3FFD-4B95-87E1-D030056DAA03@sbod.at>

Hey,

maybe some of you remember my old post about a shared zfs filesystem  
over afp over an ssh tunnel ;)
I used this to trick time machine to backup into a "local"  
sparsebundle residing on my raidz.
I never really used this, because it sucked... I just wanted to see if  
it was possible.

Now a new approach came to my mind, i created a 500gb Sparsebundle and  
mounted it via hdid -netremovable
So it looks like an internal disk when mounted, but still Time Machine  
won't let me choose it as the backup target.

Any Ideas how to proceed further ?

Help would be appreciated


Cheers,

Franz 

From michael at barryfamily.org  Sun Dec 28 10:25:20 2008
From: michael at barryfamily.org (Michael William Barry)
Date: Sun, 28 Dec 2008 12:25:20 -0600
Subject: [zfs-discuss]  TimeMachine (Again)
Message-ID: <41E7C461-E79A-4850-A0D2-441D94DA7CD8@barryfamily.org>

Have you tried this:

defaults write com.apple.systempreferences  
TMShowUnsupportedNetworkVolumes 1
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081228/3be250db/attachment.html>

From zfs at sbod.at  Sun Dec 28 10:28:31 2008
From: zfs at sbod.at (Franz Schmalzl)
Date: Sun, 28 Dec 2008 19:28:31 +0100
Subject: [zfs-discuss] TimeMachine (Again)
In-Reply-To: <41E7C461-E79A-4850-A0D2-441D94DA7CD8@barryfamily.org>
References: <41E7C461-E79A-4850-A0D2-441D94DA7CD8@barryfamily.org>
Message-ID: <0424A847-3779-4DEB-AC91-9A30B6114A7C@sbod.at>

Jup, but ( hence the name ) this only affects network Volumes

thanks for your answer


regards


franz



On 28.12.2008, at 19:25, Michael William Barry wrote:

> Have you tried this:
>
> defaults write com.apple.systempreferences  
> TMShowUnsupportedNetworkVolumes 1
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From bwaters at nrao.edu  Sun Dec 28 12:41:08 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Sun, 28 Dec 2008 13:41:08 -0700
Subject: [zfs-discuss] A Glorious Hack
Message-ID: <2b0225fb0812281241q2c1fa480xd7b2b89c311e7a3d@mail.gmail.com>

I have been unable to create snapshots since October. Until today.

I exported the raidz1 pool, then I tried to boot my Mac Pro with the
OpenSolaris 2008.11 CD. didn't work, got stuck on an unrecognized UART
value at 0x3f8 (I think).

But did you know that VMWare Fusion can see non-Windows disks? I created
VMWare "raw" disks for each device in the pool, then added them to an
OpenSolaris virtual machine. Before I started the virtual machine, I made
*sure* that my pool was exported (zpool export) and unmounted. Started it
up. It worked.

I got different complaints from OpenSolaris, which helped me troubleshoot
my problem. I have ZFS filesystems nested under other, top-level ZFS
filesystems. Fine. BUT the mountpoint directories for those
sub-filesystems were not empty. "zpool import" on OpenSolaris complained
about that; I don't think zpool on Mac OS X said anything about it.

If the mountpoint of a ZFS filesystem isn't empty, and is a directory on
another ZFS filesystem, then everything will work rock-solid UNTIL you try
to create a snapshot. The snapshot will fail with a completely cryptic
"dataset is busy" message.


Once I figured that out, I simply mounted my filesystems elsewhere,
snapshot no problem.
Also I could see the offending files: usually an empty ".automounted"
file. My home-dir mountpoint had a new user template (e.g., the "About
Stacks.PDF" in the Documents folder).

I cleared up some of that inside the OpenSolaris VM, unmounted everything,
another zpool export, shut down the VM. It seemed to get stuck shutting
down. I waited a bit then turned it off from vmware.

On a Mac OS X Terminal, I did a zpool import and zpool status...

Oops. My raidz1 is degraded, somehow it's looking for the BSD-style
"dsk/c0t0s3p1" instead of "/dev/disk3s2". For the last element; an
off-by-1 bug? Anyway, I did a "zpool replace" to explain that to the
system. Now it's resilvering, 5 hours to go.



So yeah, this is insanely dangerous, perhaps. But if you want to examine a
ZFS setup on OpenSolaris, you can use VMWare running on the Mac!

I will post the detailed commands I used to set up the VM when I get back
to a working computer. I'm typing all this on my iPhone, sorry. The
auto-correction is driving me crazy...


~ boyd

Boyd Waters
National Radio Astronomy Observatory
Socorro, New Mexico
http://www.aoc.nrao.edu/~bwaters

From bwaters at nrao.edu  Sun Dec 28 12:42:07 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Sun, 28 Dec 2008 13:42:07 -0700
Subject: [zfs-discuss] A Glorious Hack
In-Reply-To: <2b0225fb0812281241q2c1fa480xd7b2b89c311e7a3d@mail.gmail.com>
References: <2b0225fb0812281241q2c1fa480xd7b2b89c311e7a3d@mail.gmail.com>
Message-ID: <2b0225fb0812281242s40120261j68e061806df8659d@mail.gmail.com>

> I created VMWare "raw" disks for each device in the pool, then added
them to an OpenSolaris virtual machine.


My RAIDZ is on the four internal SATA bays of my Mac Pro; that's
/dev/disk{0,1,2,3}

To let a VMWare virtual machine see these, it'll need to have a way to
translate a virtual disk (vmdk) to a real disk. This is called "raw"
access.

Like this:

 /Library/Application\ Support/VMware\ Fusion/vmware-rawdiskCreator \
   create \
   /dev/disk0 \
   1 \
   ~/Documents/Virtual\ Machines.localized/Solaris.vmwarevm/rawdisk0 \
   ide


That command will create a text file that maps the raw disk to the virtual
machine, and a binary file that is just an MBR (master boot record) that
might make Winders happy. I don't think that MBR is needed, but
whatever...

I can't add more than 4 IDE drives to a virtual machine.
vmware-rawdiskCreator claims that it can create SCSI drives as well, but
it lies. Fortunately, the vmdk text file that it spits out is dead simple
to edit; I replaced this line

   ddb.adapterType = "ide"

with this one

   ddb.adapterType = "buslogic"


For each of the four hard disks in my RAIDZ.

Then I added these vmdk disks to the VM via the VMWare Fusion GUI. Fine.
Fired it up, those four disks show up fine in the OpenSolaris VM, and away
you go!

From denis at h3q.com  Wed Dec 10 20:42:18 2008
From: denis at h3q.com (Denis Ahrens)
Date: Thu, 11 Dec 2008 04:42:18 -0000
Subject: [zfs-discuss] rsync backups to ZFS volume
Message-ID: <0DCB982E-6699-4890-BC46-353C3329BBE7@h3q.com>

Hi

You should add the option --inplace because by default
rsync uses tmp files which is not good for the zfs
snapshot feature because of COW (copy-on-write).

Denis


From bwaters at nrao.edu  Sat Dec 20 17:00:30 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Sat, 20 Dec 2008 18:00:30 -0700
Subject: [zfs-discuss] snapshots
In-Reply-To: <5792952B-8C55-4D43-A8A4-362C1F7FCFCA@gmail.com>
References: <B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
	<0F79DE0F-328E-4033-8EED-CA65951C5703@nrao.edu>
	<20081220.202920.185448097.hanche@math.ntnu.no>
	<F8071BC9-E5FC-428B-BCC7-2460C9EBD1E2@nrao.edu>
	<5792952B-8C55-4D43-A8A4-362C1F7FCFCA@gmail.com>
Message-ID: <2b0225fb0812201700j65853f7bud1f49fd631f6b2d4@mail.gmail.com>

> On Dec 20, 2008, at 12:23 PM, Richard McClellan wrote:
>
> Others that have encountered this error (dataset busy) manually umount/mount
> the filesystem and snapshots started working again.  Check this out:
> http://malsserver.blogspot.com/2008/11/zfs-auto-snapshot-and-thus-time-slider.html

Thanks much, but that doesn't work for me....

Here is a transcript of a long dialog between me and ZFS:

======
Last login: Sat Dec 20 17:31:41 on console
me at galatea ~ $ sudo -i
Password:
root at galatea ~ # man zfs
root at galatea ~ # zfs unmount /Volumes/pool
cannot unmount '/Volumes/pool/backup/galatea': Resource busy
cannot unmount '/Volumes/pool/backup': Resource busy
cannot unmount '/Volumes/pool': Resource busy
root at galatea ~ # lsof | grep /Volumes/pool
Finder    14688        galatea   15r      DIR      45,17        35
    2 /Volumes/pool/backup/galatea
root at galatea ~ # osascript -e "='tell application "Finder" to quit'
> C
root at galatea ~ # osascript -e 'tell application "Finder" to quit'
root at galatea ~ # lsof | grep /Volumes/pool
fseventsd    55           root   10u      REG       45,5         0
  262 /Volumes/pool/.fseventsd/00000000015d4794
root at galatea ~ # killall fseventsd
root at galatea ~ # lsof | grep /Volumes/pool
mds          49           root  txt       REG       45,5        52
  517 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/0.indexGroups
mds          49           root  txt       REG       45,5         8
  529 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/0.indexCompactDirectory
mds          49           root  txt       REG       45,5      2731
  147 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexGroups
mds          49           root  txt       REG       45,5      1024
  153 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexCompactDirectory
mds          49           root  txt       REG       45,5      2056
  527 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/0.indexDirectory
mds          49           root  txt       REG       45,5      8224
  152 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexDirectory
mds          49           root  txt       REG       45,5       624
  510 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/0.indexIds
mds          49           root  txt       REG       45,5     32768
  146 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexIds
mds          49           root   41u      REG       45,5     53248
   40 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/store.db
mds          49           root   53u      REG       45,5     53248
   41 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/.store.db
mds          49           root   56r      DIR       45,5         9
    2 /Volumes/pool
mds          49           root   57u      REG       45,5     16384
   16 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/psid.db
mds          49           root   58u      REG       45,5         0
   43 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/journalLive
mds          49           root   59u      REG       45,5        28
   17 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/indexState
mds          49           root   61r      DIR       45,5        40
   14 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F
mds          49           root   62u      REG       45,5         0
   44 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/journalExclusion
mds          49           root   73u      REG       45,5         0
   45 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/journalSync
root at galatea ~ # kill 49
root at galatea ~ # lsof | grep /Volumes/pool
mds          49           root  twd       DIR       45,5        41
   14 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F
mds          49           root  txt       REG       45,5        52
  517 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/0.indexGroups
mds          49           root  txt       REG       45,5         8
  529 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/0.indexCompactDirectory
mds          49           root  txt       REG       45,5      2731
  147 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexGroups
mds          49           root  txt       REG       45,5      1024
  153 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexCompactDirectory
mds          49           root  txt       REG       45,5      2056
  527 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/0.indexDirectory
mds          49           root  txt       REG       45,5      8224
  152 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexDirectory
mds          49           root  txt       REG       45,5       624
  510 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/0.indexIds
mds          49           root  txt       REG       45,5     32768
  146 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexIds
mds          49           root    6u      REG       45,5        14
  145 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexUpdates
mds          49           root    8u      REG       45,5      4096
  144 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexHead
mds          49           root   26u      REG       45,5        52
  517 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/0.indexGroups
mds          49           root   27r      REG       45,5       624
  510 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/0.indexIds
mds          49           root   28u      REG       45,5      4096
  503 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/0.indexHead
mds          49           root   30r      REG       45,5      1401
  519 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/0.indexPostings
mds          49           root   31r      REG       45,5         8
  521 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/0.indexPositions
mds          49           root   33r      REG       45,5      2056
  527 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/0.indexDirectory
mds          49           root   35r      REG       45,5         8
  529 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/0.indexCompactDirectory
mds          49           root   37r      REG       45,5     70200
  536 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/0.indexArrays
mds          49           root   41u      REG       45,5     53248
   40 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/store.db
mds          49           root   44u      REG       45,5      2731
  147 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexGroups
mds          49           root   49u      REG       45,5     32768
  146 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexIds
mds          49           root   52u      REG       45,5      4096
  148 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexPostings
mds          49           root   53u      REG       45,5     53248
   41 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/.store.db
mds          49           root   54u      REG       45,5      8192
  149 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexTermIds
mds          49           root   55u      REG       45,5      4096
  150 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexPositions
mds          49           root   56r      DIR       45,5         9
    2 /Volumes/pool
mds          49           root   57u      REG       45,5     16384
   16 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/psid.db
mds          49           root   58u      REG       45,5         0
   43 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/journalLive
mds          49           root   59u      REG       45,5        28
   17 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/indexState
mds          49           root   60u      REG       45,5      8192
  151 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexPositionTable
mds          49           root   61r      DIR       45,5        41
   14 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F
mds          49           root   62u      REG       45,5      8224
  152 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexDirectory
mds          49           root   63u      REG       45,5      1024
  153 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexCompactDirectory
mds          49           root   64u      REG       45,5     65536
  154 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/live.0.indexArrays
mds          49           root   73u      REG       45,5         0
   45 /Volumes/pool/.Spotlight-V100/Store-V1/Stores/169E9FFA-03A7-4B1F-A966-9DCCFFF3AB2F/journalSync
fseventsd 14844           root    7u      REG       45,5         0
  267 /Volumes/pool/.fseventsd/00000000015d4d2a
root at galatea ~ # zfs unmount -f /Volumes/pool
root at galatea ~ # date +%Ft%T
2008-12-20t17:42:34
root at galatea ~ # zfs snapshot pool/bwaters@$(date +%Ft%T)
cannot create snapshot 'pool/bwaters at 2008-12-20t17:42:50': dataset is busy
root at galatea ~ # monut
-sh: monut: command not found
root at galatea ~ # mount
/dev/disk4s3 on / (hfs, local, journaled)
devfs on /dev (devfs, local)
fdesc on /dev (fdesc, union)
map -hosts on /net (autofs, automounted)
map auto_home on /home (autofs, automounted)
/dev/disk4s2 on /Volumes/galatea-tiger (hfs, local, journaled)
tank on /Volumes/tank (zfs, local)
tank/bwaters on /Volumes/tank/bwaters (zfs, local)
tank/media on /Volumes/tank/media (zfs, local)
tank/opt on /Volumes/tank/opt (zfs, local)
tank/opt_leopard on /Volumes/tank/opt_leopard (zfs, local)
tank/opt_tiger on /Volumes/tank/opt_tiger (zfs, local)
tank/staging on /Volumes/tank/staging (zfs, local)
/dev/disk10s2 on /Users/galatea (hfs, local, nodev, nosuid, journaled)
root at galatea ~ # zfs list
NAME                                          USED  AVAIL  REFER  MOUNTPOINT
pool                                         1.92T   774G  7.66M  /Volumes/pool
pool/backup                                   889G   774G   283G
/Volumes/pool/backup
pool/backup/cameron                          8.06G   774G  8.06G
/Volumes/pool/backup/cameron
pool/backup/eurythion                        81.5G   774G  65.2G
/Volumes/pool/backup/eurythion
pool/backup/eurythion-tiger                  9.63G   774G  9.63G
/Volumes/pool/backup/eurythion-tiger
pool/backup/eurythion/opt                    16.3G   774G  16.3G
/Volumes/pool/backup/eurythion/opt
pool/backup/galatea                           505G   774G   111G
/Volumes/pool/backup/galatea
pool/backup/galatea at 2008-04-10T15:52:23       349G      -   370G  -
pool/backup/galatea at 2008-10-15T16:10:29       630M      -   112G  -
pool/backup/galatea at 2008-10-16T16:10:28       517M      -   112G  -
pool/backup/galatea at 2008-10-17T16:10:43       534M      -   112G  -
pool/backup/galatea at 2008-10-26T16:12:47      49.4K      -   110G  -
pool/backup/galatea at 2008-10-26T21:15:32      49.4K      -   110G  -
pool/backup/galatea at 2008-10-27T16:10:50       249M      -   111G  -
pool/backup/galatea at 2008-10-27T20:03:05       400K      -   111G  -
pool/backup/galatea-opt_2008-06-24T16:58:07   295K   774G  34.7G
/Volumes/pool/backup/galatea-opt_2008-06-24T16:58:07
pool/backup/galatea/opt                      38.2G   774G   184K
/Volumes/pool/backup/galatea/opt
pool/backup/galatea/opt at 2008-03-31T16:50:00  1.30G      -  10.3G  -
pool/backup/galatea/opt at 2008-04-10T15:52:23  1.87G      -  21.9G  -
pool/backup/galatea/opt at 2008-06-24T16:58:07  15.0G      -  34.7G  -
pool/backup/galatea_2008-04-10T15:52:23       142M   774G   348G
/Volumes/pool/backup/galatea_2008-04-10T15:52:23
pool/backup/ibook_10_3_9                     1.87G   774G  1.87G
/Volumes/pool/backup/ibook_10_3_9
pool/backup/mochi                            84.0M   774G  84.0M
/Volumes/pool/backup/mochi
pool/backup/sedna                             646M   774G   646M
/Volumes/pool/backup/sedna
pool/backup/vmware-winxp                     49.4K   774G  49.4K
/Volumes/pool/backup/vmware-winxp
pool/bwaters                                  878G   774G   671G
/Volumes/pool/bwaters
pool/bwaters at 2008-04-11T14:16:10             3.63G      -   271G  -
pool/bwaters at 2008-04-20T14:20:43             96.0M      -   281G  -
pool/bwaters at 2008-04-24T19:19:55              110M      -   302G  -
pool/bwaters at 2008-05-14T13:52:41              152M      -   432G  -
pool/bwaters at 2008-05-29T16:47:59              129M      -   466G  -
pool/bwaters at 2008-06-13T16:22:27             20.4M      -   530G  -
pool/bwaters at 2008-06-27T16:11:45              202M      -   545G  -
pool/bwaters at 2008-07-07T18:54:03             7.71M      -   551G  -
pool/bwaters at 2008-07-29T17:13:16             1.52M      -   559G  -
pool/bwaters at 2008-09-22T07:24:43             7.64M      -   562G  -
pool/bwaters at 2008-10-01T12:24:09             24.8M      -   574G  -
pool/bwaters at 2008-10-28T17:14:54              222M      -   615G  -
pool/bwaters at 2008-10-29T13:33:52             18.2M      -   438G  -
pool/casa                                    34.5G   774G  28.0G
/Volumes/pool/casa
pool/casa at 2008-11-17T12:32:46                 425K      -  27.6G  -
pool/casa/data                               6.52G   774G  6.52G
/Volumes/pool/casa/data
pool/casa/data at 2008-11-17T12:35:13           45.6K      -  6.52G  -
pool/pictures                                23.5G   774G  53.9K
/Volumes/pool/pictures
pool/pictures/staging                        23.5G   774G  23.5G
/Volumes/pool/pictures/staging
pool/vm                                       142G   774G  23.5G
/Volumes/pool/vm
pool/vm/nexenta                              1.96G   774G  1.96G
/Volumes/pool/vm/nexenta
pool/vm/nrao-rhel4                           29.9G   774G  29.9G
/Volumes/pool/vm/nrao-rhel4
pool/vm/rhel5                                19.2G   774G  15.3G
/Volumes/pool/vm/rhel5
pool/vm/rhel5 at 2008-04-17T13:25:54            3.94G      -  3.94G  -
pool/vm/solaris                              5.20G   774G  5.20G
/Volumes/pool/vm/solaris
pool/vm/windowsXP                            62.4G   774G  62.4G
/Volumes/pool/vm/windowsXP
tank                                         1.33T  9.53G   106K  /Volumes/tank
tank/bwaters                                 46.4K  9.53G  46.4K
/Volumes/tank/bwaters
tank/media                                    435G  9.53G   420G
/Volumes/tank/media
tank/media at samsara                           74.8M      -  6.26G  -
tank/media at silver                            1.54G      -  40.0G  -
tank/media at nessus                             732M      -   163G  -
tank/media at nessus2                           49.5M      -   200G  -
tank/media at nessus_snapshot                    163K      -   206G  -
tank/media at nessus_snapshot2                  8.35G      -   299G  -
tank/opt                                     23.7G  9.53G  13.5G
/Volumes/tank/opt
tank/opt at galatea-tiger                       10.1G      -  13.8G  -
tank/opt at galatea-leopard                     66.3M      -  13.5G  -
tank/opt_leopard                             71.8K  9.53G  13.5G
/Volumes/tank/opt_leopard
tank/opt_tiger                               48.9M  9.53G  13.8G
/Volumes/tank/opt_tiger
tank/staging                                  901G  9.53G   684G
/Volumes/tank/staging
tank/staging at 2006-01-25__samsara             13.3G      -  55.1G  -
tank/staging at 2006-03-01__samsara             12.8G      -  57.1G  -
tank/staging at 2006-04-08__eurythion           16.4G      -  33.5G  -
tank/staging at 2006-06-08__eurythion           26.6G      -  52.7G  -
tank/staging at 2006-07-08__eurythion           42.1M      -  51.8G  -
tank/staging at 2006-07-11__canonical           3.43G      -   231G  -
tank/staging at canonical_photos                3.37G      -   277G  -
tank/staging at canonical_maxtor                 300K      -   565G  -
tank/staging at canonical_littleLprd             340K      -   593G  -
tank/staging at canonical_warranty              83.4M      -   620G  -
root at galatea ~ # ls /Volumes/
.DS_Store     galatea       galatea-tiger tank
root at galatea ~ # zfs snapshot tank/bwaters@$(date +%Ft%T)
cannot create snapshot 'tank/bwaters at 2008-12-20t17:44:04': dataset is busy
root at galatea ~ # zfs unmount -f tank
root at galatea ~ # mount
/dev/disk4s3 on / (hfs, local, journaled)
devfs on /dev (devfs, local)
fdesc on /dev (fdesc, union)
map -hosts on /net (autofs, automounted)
map auto_home on /home (autofs, automounted)
/dev/disk4s2 on /Volumes/galatea-tiger (hfs, local, journaled)
/dev/disk10s2 on /Users/galatea (hfs, local, nodev, nosuid, journaled)
root at galatea ~ # ls /Volumes/
.DS_Store     galatea       galatea-tiger
root at galatea ~ # zfs snapshot tank/bwaters@$(date +%Ft%T)
cannot create snapshot 'tank/bwaters at 2008-12-20t17:44:39': dataset is busy
root at galatea ~ # zfs export pool/bwaters
unrecognized command 'export'
usage: zfs command args ...
where 'command' is one of the following:

	create [-p] [-o property=value] ... <filesystem>
	create [-ps] [-b blocksize] [-o property=value] ... -V <size> <volume>
	destroy [-rRf] <filesystem|volume|snapshot>

	snapshot [-r] <filesystem at snapname|volume at snapname>
	rollback [-rRf] <snapshot>
	clone [-p] <snapshot> <filesystem|volume>
	promote <clone-filesystem>
	rename <filesystem|volume|snapshot> <filesystem|volume|snapshot>
	rename -p <filesystem|volume> <filesystem|volume>
	rename -r <snapshot> <snapshot>
	list [-rH] [-o property[,...]] [-t type[,...]] [-s property] ...
	    [-S property] ... [filesystem|volume|snapshot] ...

	set <property=value> <filesystem|volume> ...
	get [-rHp] [-o field[,...]] [-s source[,...]]
	    <"all" | property[,...]> [filesystem|volume|snapshot] ...
	inherit [-r] <property> <filesystem|volume> ...
	upgrade [-v]
	upgrade [-r] [-V version] <-a | filesystem ...>

	mount
	mount [-vO] [-o opts] <-a | filesystem>
	unmount [-f] <-a | filesystem|mountpoint>
	share <-a | filesystem>
	unshare [-f] <-a | filesystem|mountpoint>

	send [-i snapshot] <snapshot>
	receive [-vnF] <filesystem|volume|snapshot>
	receive [-vnF] -d <filesystem>

	allow [-ldug] <"everyone"|user|group>[,...] <perm|@setname>[,...]
	    <filesystem|volume>
	allow [-ld] -e <perm|@setname>[,...] <filesystem|volume>
	allow -c <perm|@setname>[,...] <filesystem|volume>
	allow -s @setname <perm|@setname>[,...] <filesystem|volume>

	unallow [-rldug] <"everyone"|user|group>[,...]
	    [<perm|@setname>[,...]] <filesystem|volume>
	unallow [-rld] -e [<perm|@setname>[,...]] <filesystem|volume>
	unallow [-r] -c [<perm|@setname>[,...]] <filesystem|volume>
	unallow [-r] -s @setname [<perm|@setname>[,...]] <filesystem|volume>

Each dataset is of the form: pool/[dataset/]*dataset[@name]

For the property list, run: zfs set|get
root at galatea ~ # zpool export pool
root at galatea ~ # zpool status
  pool: tank
 state: ONLINE
 scrub: none requested
config:

	NAME         STATE     READ WRITE CKSUM
	tank         ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk7s2  ONLINE       0     0     0
	    disk5s2  ONLINE       0     0     0
	    disk6s2  ONLINE       0     0     0
	    disk8s2  ONLINE       0     0     0

errors: No known data errors
root at galatea ~ # zpool import pool
root at galatea ~ # zpool status
  pool: pool
 state: ONLINE
 scrub: none requested
config:

	NAME         STATE     READ WRITE CKSUM
	pool         ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk0s2  ONLINE       0     0     0
	    disk3s2  ONLINE       0     0     0
	    disk2s2  ONLINE       0     0     0
	    disk1s2  ONLINE       0     0     0

errors: No known data errors

  pool: tank
 state: ONLINE
 scrub: none requested
config:

	NAME         STATE     READ WRITE CKSUM
	tank         ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk7s2  ONLINE       0     0     0
	    disk5s2  ONLINE       0     0     0
	    disk6s2  ONLINE       0     0     0
	    disk8s2  ONLINE       0     0     0

errors: No known data errors
root at galatea ~ # zfs snapshot pool/bwaters@$(date +%Ft%T)
cannot create snapshot 'pool/bwaters at 2008-12-20t17:47:31': dataset is busy
root at galatea ~ # zpool export tank
root at galatea ~ # zfs snapshot pool/bwaters@$(date +%Ft%T)
cannot create snapshot 'pool/bwaters at 2008-12-20t17:48:10': dataset is busy
root at galatea ~ # man zpool
root at galatea ~ # zpool upgrade pool
This system is currently running ZFS pool version 8.

Pool 'pool' is already formatted using the current version.
root at galatea ~ # zpool clear pool
root at galatea ~ # zpool status
  pool: pool
 state: ONLINE
 scrub: none requested
config:

	NAME         STATE     READ WRITE CKSUM
	pool         ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk0s2  ONLINE       0     0     0
	    disk3s2  ONLINE       0     0     0
	    disk2s2  ONLINE       0     0     0
	    disk1s2  ONLINE       0     0     0

errors: No known data errors
root at galatea ~ # zfs snapshot pool/bwaters@$(date +%Ft%T)
cannot create snapshot 'pool/bwaters at 2008-12-20t17:52:46': dataset is busy
root at galatea ~ # lsof | grep pool/bwaters
root at galatea ~ # zpool history pool
History for 'pool':
2008-02-25.18:40:24 zpool create pool raidz /dev/disk0s2 /dev/disk1s2
/dev/disk2s2 /dev/disk3s2
2008-02-25.18:42:13 zfs create pool/backup
2008-02-25.20:12:40 zpool import -f 17751373224295458742
2008-03-05.17:44:48 zpool import -f 17751373224295458742
2008-03-05.17:48:24 zpool upgrade pool
2008-03-06.15:50:58 zfs create pool/opt
2008-03-06.16:02:07 zpool import -f 17751373224295458742
2008-03-06.16:07:12 zfs destroy pool/opt
2008-03-07.13:22:42 zpool import -f 17751373224295458742
2008-03-10.09:17:37 zpool import -f 17751373224295458742
2008-03-24.17:52:31 zpool import -f 17751373224295458742
2008-03-31.16:17:49 zfs create pool/backup/galatea
2008-03-31.16:28:50 zfs snapshot pool/backup/galatea at opt_active
2008-04-01.12:00:11 zfs destroy pool/backup/galatea at opt_active
2008-04-01.12:11:25 zfs snapshot pool/backup/galatea at opt
2008-04-01.12:20:37 zpool import -f 17751373224295458742
2008-04-01.18:21:40 zfs snapshot pool/backup/galatea at 2008-04-01T18:21:38
2008-04-01.18:35:28 zfs snapshot pool/backup/galatea at 2008-04-01T18:35:27
2008-04-01.19:44:12 zfs snapshot pool/backup/galatea at 2008-04-01T19:44:12
2008-04-01.20:55:41 zfs snapshot pool/backup/galatea at 2008-04-01T20:55:40
2008-04-01.22:08:57 zfs snapshot pool/backup/galatea at 2008-04-01T22:08:56
2008-04-01.23:21:50 zfs snapshot pool/backup/galatea at 2008-04-01T23:21:49
2008-04-02.00:35:04 zfs snapshot pool/backup/galatea at 2008-04-02T00:35:03
2008-04-02.01:47:47 zfs snapshot pool/backup/galatea at 2008-04-02T01:47:46
2008-04-02.03:00:52 zfs snapshot pool/backup/galatea at 2008-04-02T03:00:51
2008-04-02.04:13:56 zfs snapshot pool/backup/galatea at 2008-04-02T04:13:55
2008-04-02.05:27:19 zfs snapshot pool/backup/galatea at 2008-04-02T05:27:18
2008-04-02.06:39:53 zfs snapshot pool/backup/galatea at 2008-04-02T06:39:52
2008-04-02.07:52:54 zfs snapshot pool/backup/galatea at 2008-04-02T07:52:52
2008-04-02.09:06:06 zfs snapshot pool/backup/galatea at 2008-04-02T09:06:04
2008-04-02.10:19:04 zfs snapshot pool/backup/galatea at 2008-04-02T10:19:03
2008-04-02.11:32:13 zfs snapshot pool/backup/galatea at 2008-04-02T11:32:12
2008-04-02.12:45:06 zfs snapshot pool/backup/galatea at 2008-04-02T12:45:05
2008-04-02.13:58:07 zfs snapshot pool/backup/galatea at 2008-04-02T13:58:05
2008-04-02.15:11:06 zfs snapshot pool/backup/galatea at 2008-04-02T15:11:05
2008-04-02.16:23:51 zfs snapshot pool/backup/galatea at 2008-04-02T16:23:50
2008-04-02.17:36:47 zfs snapshot pool/backup/galatea at 2008-04-02T17:36:45
2008-04-02.18:49:45 zfs snapshot pool/backup/galatea at 2008-04-02T18:49:43
2008-04-02.20:03:00 zfs snapshot pool/backup/galatea at 2008-04-02T20:02:59
2008-04-02.21:15:51 zfs snapshot pool/backup/galatea at 2008-04-02T21:15:50
2008-04-02.22:29:11 zfs snapshot pool/backup/galatea at 2008-04-02T22:29:10
2008-04-02.23:42:09 zfs snapshot pool/backup/galatea at 2008-04-02T23:42:08
2008-04-03.00:54:54 zfs snapshot pool/backup/galatea at 2008-04-03T00:54:53
2008-04-03.02:07:53 zfs snapshot pool/backup/galatea at 2008-04-03T02:07:52
2008-04-03.03:21:03 zfs snapshot pool/backup/galatea at 2008-04-03T03:21:02
2008-04-03.04:34:01 zfs snapshot pool/backup/galatea at 2008-04-03T04:34:00
2008-04-03.05:46:56 zfs snapshot pool/backup/galatea at 2008-04-03T05:46:55
2008-04-03.06:59:55 zfs snapshot pool/backup/galatea at 2008-04-03T06:59:54
2008-04-03.08:13:06 zfs snapshot pool/backup/galatea at 2008-04-03T08:13:05
2008-04-03.09:26:21 zfs snapshot pool/backup/galatea at 2008-04-03T09:26:20
2008-04-03.10:38:47 zfs snapshot pool/backup/galatea at 2008-04-03T10:38:46
2008-04-03.11:51:51 zfs snapshot pool/backup/galatea at 2008-04-03T11:51:50
2008-04-03.13:05:19 zfs snapshot pool/backup/galatea at 2008-04-03T13:05:18
2008-04-03.13:16:29 zfs destroy pool/backup/galatea at 2008-04-01T18:21:38
2008-04-03.13:16:30 zfs destroy pool/backup/galatea at 2008-04-01T18:35:27
2008-04-03.13:16:31 zfs destroy pool/backup/galatea at 2008-04-01T19:44:12
2008-04-03.13:16:32 zfs destroy pool/backup/galatea at 2008-04-01T20:55:40
2008-04-03.13:16:34 zfs destroy pool/backup/galatea at 2008-04-01T22:08:56
2008-04-03.13:16:35 zfs destroy pool/backup/galatea at 2008-04-01T23:21:49
2008-04-03.13:16:36 zfs destroy pool/backup/galatea at 2008-04-02T00:35:03
2008-04-03.13:16:37 zfs destroy pool/backup/galatea at 2008-04-02T01:47:46
2008-04-03.13:16:38 zfs destroy pool/backup/galatea at 2008-04-02T03:00:51
2008-04-03.13:16:39 zfs destroy pool/backup/galatea at 2008-04-02T04:13:55
2008-04-03.13:16:40 zfs destroy pool/backup/galatea at 2008-04-02T05:27:18
2008-04-03.13:16:41 zfs destroy pool/backup/galatea at 2008-04-02T06:39:52
2008-04-03.13:16:42 zfs destroy pool/backup/galatea at 2008-04-02T07:52:52
2008-04-03.13:16:43 zfs destroy pool/backup/galatea at 2008-04-02T09:06:04
2008-04-03.13:16:44 zfs destroy pool/backup/galatea at 2008-04-02T10:19:03
2008-04-03.13:16:45 zfs destroy pool/backup/galatea at 2008-04-02T11:32:12
2008-04-03.13:16:46 zfs destroy pool/backup/galatea at 2008-04-02T12:45:05
2008-04-03.13:16:47 zfs destroy pool/backup/galatea at 2008-04-02T13:58:05
2008-04-03.13:16:48 zfs destroy pool/backup/galatea at 2008-04-02T15:11:05
2008-04-03.13:16:49 zfs destroy pool/backup/galatea at 2008-04-02T16:23:50
2008-04-03.13:16:50 zfs destroy pool/backup/galatea at 2008-04-02T17:36:45
2008-04-03.13:16:51 zfs destroy pool/backup/galatea at 2008-04-02T18:49:43
2008-04-03.13:16:52 zfs destroy pool/backup/galatea at 2008-04-02T20:02:59
2008-04-03.13:16:54 zfs destroy pool/backup/galatea at 2008-04-02T21:15:50
2008-04-03.13:16:55 zfs destroy pool/backup/galatea at 2008-04-02T22:29:10
2008-04-03.13:16:56 zfs destroy pool/backup/galatea at 2008-04-02T23:42:08
2008-04-03.13:16:57 zfs destroy pool/backup/galatea at 2008-04-03T00:54:53
2008-04-03.13:16:58 zfs destroy pool/backup/galatea at 2008-04-03T02:07:52
2008-04-03.13:16:59 zfs destroy pool/backup/galatea at 2008-04-03T03:21:02
2008-04-03.13:17:01 zfs destroy pool/backup/galatea at 2008-04-03T04:34:00
2008-04-03.13:17:02 zfs destroy pool/backup/galatea at 2008-04-03T05:46:55
2008-04-03.13:17:03 zfs destroy pool/backup/galatea at 2008-04-03T06:59:54
2008-04-03.14:18:41 zfs snapshot pool/backup/galatea at 2008-04-03T14:18:40
2008-04-03.14:18:42 zfs destroy pool/backup/galatea at 2008-04-03T08:13:05
2008-04-03.14:18:44 zfs destroy pool/backup/galatea at 2008-04-03T09:26:20
2008-04-03.15:31:09 zfs snapshot pool/backup/galatea at 2008-04-03T15:31:08
2008-04-03.15:31:11 zfs destroy pool/backup/galatea at 2008-04-03T10:38:46
2008-04-03.16:43:46 zfs snapshot pool/backup/galatea at 2008-04-03T16:43:45
2008-04-03.16:43:47 zfs destroy pool/backup/galatea at 2008-04-03T11:51:50
2008-04-03.17:57:14 zfs snapshot pool/backup/galatea at 2008-04-03T17:57:13
2008-04-03.17:57:16 zfs destroy pool/backup/galatea at 2008-04-03T13:05:18
2008-04-03.19:11:58 zfs snapshot pool/backup/galatea at 2008-04-03T19:11:57
2008-04-03.19:12:00 zfs destroy pool/backup/galatea at 2008-04-03T14:18:40
2008-04-03.20:23:25 zfs snapshot pool/backup/galatea at 2008-04-03T20:23:24
2008-04-03.20:23:27 zfs destroy pool/backup/galatea at 2008-04-03T15:31:08
2008-04-03.21:36:11 zfs snapshot pool/backup/galatea at 2008-04-03T21:36:10
2008-04-03.21:36:14 zfs destroy pool/backup/galatea at 2008-04-03T16:43:45
2008-04-03.22:48:53 zfs snapshot pool/backup/galatea at 2008-04-03T22:48:52
2008-04-03.22:48:56 zfs destroy pool/backup/galatea at 2008-04-03T17:57:13
2008-04-04.00:02:09 zfs snapshot pool/backup/galatea at 2008-04-04T00:02:08
2008-04-04.00:02:11 zfs destroy pool/backup/galatea at 2008-04-03T19:11:57
2008-04-04.01:14:57 zfs snapshot pool/backup/galatea at 2008-04-04T01:14:56
2008-04-04.01:14:58 zfs destroy pool/backup/galatea at 2008-04-03T20:23:24
2008-04-04.02:28:14 zfs snapshot pool/backup/galatea at 2008-04-04T02:28:13
2008-04-04.02:28:16 zfs destroy pool/backup/galatea at 2008-04-03T21:36:10
2008-04-04.03:41:08 zfs snapshot pool/backup/galatea at 2008-04-04T03:41:06
2008-04-04.03:41:09 zfs destroy pool/backup/galatea at 2008-04-03T22:48:52
2008-04-04.04:54:01 zfs snapshot pool/backup/galatea at 2008-04-04T04:54:00
2008-04-04.04:54:03 zfs destroy pool/backup/galatea at 2008-04-04T00:02:08
2008-04-04.06:07:07 zfs snapshot pool/backup/galatea at 2008-04-04T06:07:06
2008-04-04.06:07:09 zfs destroy pool/backup/galatea at 2008-04-04T01:14:56
2008-04-04.07:20:08 zfs snapshot pool/backup/galatea at 2008-04-04T07:20:07
2008-04-04.07:20:11 zfs destroy pool/backup/galatea at 2008-04-04T02:28:13
2008-04-04.08:33:16 zfs snapshot pool/backup/galatea at 2008-04-04T08:33:15
2008-04-04.08:33:18 zfs destroy pool/backup/galatea at 2008-04-04T03:41:06
2008-04-04.09:46:19 zfs snapshot pool/backup/galatea at 2008-04-04T09:46:17
2008-04-04.09:46:21 zfs destroy pool/backup/galatea at 2008-04-04T04:54:00
2008-04-04.10:59:16 zfs snapshot pool/backup/galatea at 2008-04-04T10:59:15
2008-04-04.10:59:18 zfs destroy pool/backup/galatea at 2008-04-04T06:07:06
2008-04-04.12:13:39 zfs snapshot pool/backup/galatea at 2008-04-04T12:13:38
2008-04-04.12:13:41 zfs destroy pool/backup/galatea at 2008-04-04T07:20:07
2008-04-04.13:26:32 zfs snapshot pool/backup/galatea at 2008-04-04T13:26:31
2008-04-04.13:26:34 zfs destroy pool/backup/galatea at 2008-04-04T08:33:15
2008-04-04.14:38:45 zfs snapshot pool/backup/galatea at 2008-04-04T14:38:44
2008-04-04.14:38:47 zfs destroy pool/backup/galatea at 2008-04-04T09:46:17
2008-04-04.15:53:21 zfs snapshot pool/backup/galatea at 2008-04-04T15:53:20
2008-04-04.15:53:24 zfs destroy pool/backup/galatea at 2008-04-04T10:59:15
2008-04-04.16:41:08 zpool import -f 17751373224295458742
2008-04-05.15:23:35 zpool import -f 17751373224295458742
2008-04-05.15:32:08 zfs clone pool/backup/galatea at 2008-04-04T15:53:20
pool/backup/galatea-10.5.2-2008-04-04
2008-04-05.15:32:32 zfs promote pool/backup/galatea-10.5.2-2008-04-04
2008-04-05.15:43:34 zfs promote pool/backup/galatea
2008-04-05.17:11:33 zfs snapshot pool/backup/galatea at 2008-04-05T17:11:31
2008-04-05.17:11:34 zfs destroy pool/backup/galatea at 2008-04-04T12:13:38
2008-04-05.18:31:14 zfs snapshot pool/backup/galatea at 2008-04-05T18:31:13
2008-04-05.18:31:16 zfs destroy pool/backup/galatea at 2008-04-04T13:26:31
2008-04-05.19:23:07 zfs snapshot pool/backup/galatea at 2008-04-05T19:23:06
2008-04-05.19:23:09 zfs destroy pool/backup/galatea at 2008-04-04T14:38:44
2008-04-05.20:35:23 zfs snapshot pool/backup/galatea at 2008-04-05T20:35:23
2008-04-05.21:48:11 zfs snapshot pool/backup/galatea at 2008-04-05T21:48:10
2008-04-05.21:48:14 zfs destroy pool/backup/galatea at 2008-04-05T17:11:31
2008-04-05.23:01:15 zfs snapshot pool/backup/galatea at 2008-04-05T23:01:14
2008-04-05.23:01:16 zfs destroy pool/backup/galatea at 2008-04-05T18:31:13
2008-04-06.00:14:14 zfs snapshot pool/backup/galatea at 2008-04-06T00:14:13
2008-04-06.00:14:16 zfs destroy pool/backup/galatea at 2008-04-05T19:23:06
2008-04-06.01:27:17 zfs snapshot pool/backup/galatea at 2008-04-06T01:27:16
2008-04-06.01:27:19 zfs destroy pool/backup/galatea at 2008-04-05T20:35:23
2008-04-06.02:40:17 zfs snapshot pool/backup/galatea at 2008-04-06T02:40:15
2008-04-06.02:40:18 zfs destroy pool/backup/galatea at 2008-04-05T21:48:10
2008-04-06.03:53:12 zfs snapshot pool/backup/galatea at 2008-04-06T03:53:11
2008-04-06.03:53:13 zfs destroy pool/backup/galatea at 2008-04-05T23:01:14
2008-04-06.05:06:14 zfs snapshot pool/backup/galatea at 2008-04-06T05:06:13
2008-04-06.05:06:16 zfs destroy pool/backup/galatea at 2008-04-06T00:14:13
2008-04-06.06:19:10 zfs snapshot pool/backup/galatea at 2008-04-06T06:19:08
2008-04-06.06:19:12 zfs destroy pool/backup/galatea at 2008-04-06T01:27:16
2008-04-06.07:32:01 zfs snapshot pool/backup/galatea at 2008-04-06T07:32:00
2008-04-06.07:32:03 zfs destroy pool/backup/galatea at 2008-04-06T02:40:15
2008-04-06.08:45:16 zfs snapshot pool/backup/galatea at 2008-04-06T08:45:14
2008-04-06.08:45:18 zfs destroy pool/backup/galatea at 2008-04-06T03:53:11
2008-04-06.09:58:18 zfs snapshot pool/backup/galatea at 2008-04-06T09:58:16
2008-04-06.09:58:19 zfs destroy pool/backup/galatea at 2008-04-06T05:06:13
2008-04-06.11:11:17 zfs snapshot pool/backup/galatea at 2008-04-06T11:11:16
2008-04-06.11:11:19 zfs destroy pool/backup/galatea at 2008-04-06T06:19:08
2008-04-06.12:24:09 zfs snapshot pool/backup/galatea at 2008-04-06T12:24:08
2008-04-06.12:24:11 zfs destroy pool/backup/galatea at 2008-04-06T07:32:00
2008-04-06.13:37:21 zfs snapshot pool/backup/galatea at 2008-04-06T13:37:20
2008-04-06.13:37:23 zfs destroy pool/backup/galatea at 2008-04-06T08:45:14
2008-04-06.14:50:20 zfs snapshot pool/backup/galatea at 2008-04-06T14:50:19
2008-04-06.14:50:22 zfs destroy pool/backup/galatea at 2008-04-06T09:58:16
2008-04-06.16:03:12 zfs snapshot pool/backup/galatea at 2008-04-06T16:03:11
2008-04-06.16:03:13 zfs destroy pool/backup/galatea at 2008-04-06T11:11:16
2008-04-06.17:16:19 zfs snapshot pool/backup/galatea at 2008-04-06T17:16:18
2008-04-06.17:16:21 zfs destroy pool/backup/galatea at 2008-04-06T12:24:08
2008-04-06.18:29:19 zfs snapshot pool/backup/galatea at 2008-04-06T18:29:18
2008-04-06.18:29:21 zfs destroy pool/backup/galatea at 2008-04-06T13:37:20
2008-04-06.19:42:19 zfs snapshot pool/backup/galatea at 2008-04-06T19:42:17
2008-04-06.19:42:21 zfs destroy pool/backup/galatea at 2008-04-06T14:50:19
2008-04-06.20:55:20 zfs snapshot pool/backup/galatea at 2008-04-06T20:55:19
2008-04-06.20:55:22 zfs destroy pool/backup/galatea at 2008-04-06T16:03:11
2008-04-06.22:08:25 zfs snapshot pool/backup/galatea at 2008-04-06T22:08:24
2008-04-06.22:08:27 zfs destroy pool/backup/galatea at 2008-04-06T17:16:18
2008-04-06.23:21:18 zfs snapshot pool/backup/galatea at 2008-04-06T23:21:17
2008-04-06.23:21:21 zfs destroy pool/backup/galatea at 2008-04-06T18:29:18
2008-04-07.00:34:29 zfs snapshot pool/backup/galatea at 2008-04-07T00:34:28
2008-04-07.00:34:31 zfs destroy pool/backup/galatea at 2008-04-06T19:42:17
2008-04-07.01:47:31 zfs snapshot pool/backup/galatea at 2008-04-07T01:47:29
2008-04-07.01:47:32 zfs destroy pool/backup/galatea at 2008-04-06T20:55:19
2008-04-07.03:00:11 zfs snapshot pool/backup/galatea at 2008-04-07T03:00:10
2008-04-07.03:00:13 zfs destroy pool/backup/galatea at 2008-04-06T22:08:24
2008-04-07.04:13:16 zfs snapshot pool/backup/galatea at 2008-04-07T04:13:15
2008-04-07.04:13:18 zfs destroy pool/backup/galatea at 2008-04-06T23:21:17
2008-04-07.05:26:19 zfs snapshot pool/backup/galatea at 2008-04-07T05:26:18
2008-04-07.05:26:21 zfs destroy pool/backup/galatea at 2008-04-07T00:34:28
2008-04-07.06:39:21 zfs snapshot pool/backup/galatea at 2008-04-07T06:39:20
2008-04-07.06:39:23 zfs destroy pool/backup/galatea at 2008-04-07T01:47:29
2008-04-07.07:52:27 zfs snapshot pool/backup/galatea at 2008-04-07T07:52:26
2008-04-07.07:52:29 zfs destroy pool/backup/galatea at 2008-04-07T03:00:10
2008-04-07.09:05:23 zfs snapshot pool/backup/galatea at 2008-04-07T09:05:22
2008-04-07.09:05:25 zfs destroy pool/backup/galatea at 2008-04-07T04:13:15
2008-04-07.10:19:14 zfs snapshot pool/backup/galatea at 2008-04-07T10:19:13
2008-04-07.10:19:16 zfs destroy pool/backup/galatea at 2008-04-07T05:26:18
2008-04-07.11:34:35 zfs snapshot pool/backup/galatea at 2008-04-07T11:34:34
2008-04-07.11:34:37 zfs destroy pool/backup/galatea at 2008-04-07T06:39:20
2008-04-07.12:47:30 zfs snapshot pool/backup/galatea at 2008-04-07T12:47:29
2008-04-07.12:47:32 zfs destroy pool/backup/galatea at 2008-04-07T07:52:26
2008-04-07.14:01:31 zfs snapshot pool/backup/galatea at 2008-04-07T14:01:30
2008-04-07.14:01:33 zfs destroy pool/backup/galatea at 2008-04-07T09:05:22
2008-04-07.15:12:32 zfs snapshot pool/backup/galatea at 2008-04-07T15:12:31
2008-04-07.15:12:34 zfs destroy pool/backup/galatea at 2008-04-07T10:19:13
2008-04-07.16:24:15 zfs snapshot pool/backup/galatea at 2008-04-07T16:24:14
2008-04-07.16:24:17 zfs destroy pool/backup/galatea at 2008-04-07T11:34:34
2008-04-07.17:37:34 zfs snapshot pool/backup/galatea at 2008-04-07T17:37:33
2008-04-07.17:37:37 zfs destroy pool/backup/galatea at 2008-04-07T12:47:29
2008-04-07.18:50:04 zfs snapshot pool/backup/galatea at 2008-04-07T18:50:03
2008-04-07.18:50:07 zfs destroy pool/backup/galatea at 2008-04-07T14:01:30
2008-04-07.20:02:29 zfs snapshot pool/backup/galatea at 2008-04-07T20:02:28
2008-04-07.20:02:31 zfs destroy pool/backup/galatea at 2008-04-07T15:12:31
2008-04-07.21:15:31 zfs snapshot pool/backup/galatea at 2008-04-07T21:15:30
2008-04-07.21:15:33 zfs destroy pool/backup/galatea at 2008-04-07T16:24:14
2008-04-07.22:28:30 zfs snapshot pool/backup/galatea at 2008-04-07T22:28:29
2008-04-07.22:28:33 zfs destroy pool/backup/galatea at 2008-04-07T17:37:33
2008-04-07.23:41:26 zfs snapshot pool/backup/galatea at 2008-04-07T23:41:24
2008-04-07.23:41:27 zfs destroy pool/backup/galatea at 2008-04-07T18:50:03
2008-04-08.00:54:11 zfs snapshot pool/backup/galatea at 2008-04-08T00:54:10
2008-04-08.00:54:13 zfs destroy pool/backup/galatea at 2008-04-07T20:02:28
2008-04-08.02:07:25 zfs snapshot pool/backup/galatea at 2008-04-08T02:07:23
2008-04-08.02:07:26 zfs destroy pool/backup/galatea at 2008-04-07T21:15:30
2008-04-08.03:20:18 zfs snapshot pool/backup/galatea at 2008-04-08T03:20:17
2008-04-08.03:20:20 zfs destroy pool/backup/galatea at 2008-04-07T22:28:29
2008-04-08.04:33:36 zfs snapshot pool/backup/galatea at 2008-04-08T04:33:34
2008-04-08.04:33:38 zfs destroy pool/backup/galatea at 2008-04-07T23:41:24
2008-04-08.05:46:26 zfs snapshot pool/backup/galatea at 2008-04-08T05:46:25
2008-04-08.05:46:28 zfs destroy pool/backup/galatea at 2008-04-08T00:54:10
2008-04-08.06:59:18 zfs snapshot pool/backup/galatea at 2008-04-08T06:59:17
2008-04-08.06:59:19 zfs destroy pool/backup/galatea at 2008-04-08T02:07:23
2008-04-08.08:12:34 zfs snapshot pool/backup/galatea at 2008-04-08T08:12:33
2008-04-08.08:12:36 zfs destroy pool/backup/galatea at 2008-04-08T03:20:17
2008-04-08.09:25:28 zfs snapshot pool/backup/galatea at 2008-04-08T09:25:26
2008-04-08.09:25:29 zfs destroy pool/backup/galatea at 2008-04-08T04:33:34
2008-04-08.10:38:45 zfs snapshot pool/backup/galatea at 2008-04-08T10:38:44
2008-04-08.10:38:46 zfs destroy pool/backup/galatea at 2008-04-08T05:46:25
2008-04-08.11:51:34 zfs snapshot pool/backup/galatea at 2008-04-08T11:51:33
2008-04-08.11:51:36 zfs destroy pool/backup/galatea at 2008-04-08T06:59:17
2008-04-08.13:04:24 zfs snapshot pool/backup/galatea at 2008-04-08T13:04:23
2008-04-08.13:04:26 zfs destroy pool/backup/galatea at 2008-04-08T08:12:33
2008-04-08.14:17:19 zfs snapshot pool/backup/galatea at 2008-04-08T14:17:18
2008-04-08.14:17:21 zfs destroy pool/backup/galatea at 2008-04-08T09:25:26
2008-04-08.15:30:38 zfs snapshot pool/backup/galatea at 2008-04-08T15:30:36
2008-04-08.15:30:39 zfs destroy pool/backup/galatea at 2008-04-08T10:38:44
2008-04-08.16:46:12 zfs snapshot pool/backup/galatea at 2008-04-08T16:46:11
2008-04-08.16:46:14 zfs destroy pool/backup/galatea at 2008-04-08T11:51:33
2008-04-08.18:02:07 zfs snapshot pool/backup/galatea at 2008-04-08T18:02:06
2008-04-08.18:02:09 zfs destroy pool/backup/galatea at 2008-04-08T13:04:23
2008-04-08.19:10:33 zfs snapshot pool/backup/galatea at 2008-04-08T19:10:32
2008-04-08.19:10:35 zfs destroy pool/backup/galatea at 2008-04-08T14:17:18
2008-04-09.10:30:15 zpool import -f 17751373224295458742
2008-04-09.11:38:09 zfs snapshot pool/backup/galatea at 2008-04-09T11:38:08
2008-04-09.11:38:12 zfs destroy pool/backup/galatea at 2008-04-08T15:30:36
2008-04-09.11:52:16 zfs snapshot pool/backup/galatea at 2008-04-09T11:52:15
2008-04-09.11:52:19 zfs destroy pool/backup/galatea at 2008-04-08T16:46:11
2008-04-09.13:04:09 zfs snapshot pool/backup/galatea at 2008-04-09T13:04:07
2008-04-09.13:04:11 zfs destroy pool/backup/galatea at 2008-04-08T18:02:06
2008-04-09.14:14:58 zfs snapshot pool/backup/galatea at 2008-04-09T14:14:56
2008-04-09.14:15:00 zfs destroy pool/backup/galatea at 2008-04-08T19:10:32
2008-04-09.15:28:06 zfs snapshot pool/backup/galatea at 2008-04-09T15:28:05
2008-04-09.15:28:09 zfs destroy pool/backup/galatea at 2008-04-09T11:38:08
2008-04-09.16:43:17 zfs snapshot pool/backup/galatea at 2008-04-09T16:43:16
2008-04-09.16:43:21 zfs destroy pool/backup/galatea at 2008-04-09T11:52:15
2008-04-09.17:54:09 zfs snapshot pool/backup/galatea at 2008-04-09T17:54:08
2008-04-09.17:54:11 zfs destroy pool/backup/galatea at 2008-04-09T13:04:07
2008-04-09.19:06:55 zfs snapshot pool/backup/galatea at 2008-04-09T19:06:53
2008-04-09.19:06:57 zfs destroy pool/backup/galatea at 2008-04-09T14:14:56
2008-04-09.20:19:55 zfs snapshot pool/backup/galatea at 2008-04-09T20:19:54
2008-04-09.20:19:58 zfs destroy pool/backup/galatea at 2008-04-09T15:28:05
2008-04-09.21:32:55 zfs snapshot pool/backup/galatea at 2008-04-09T21:32:54
2008-04-09.21:32:57 zfs destroy pool/backup/galatea at 2008-04-09T16:43:16
2008-04-09.22:46:01 zfs snapshot pool/backup/galatea at 2008-04-09T22:46:00
2008-04-09.22:46:03 zfs destroy pool/backup/galatea at 2008-04-09T17:54:08
2008-04-09.23:59:00 zfs snapshot pool/backup/galatea at 2008-04-09T23:58:58
2008-04-09.23:59:02 zfs destroy pool/backup/galatea at 2008-04-09T19:06:53
2008-04-10.01:11:51 zfs snapshot pool/backup/galatea at 2008-04-10T01:11:50
2008-04-10.01:11:54 zfs destroy pool/backup/galatea at 2008-04-09T20:19:54
2008-04-10.02:24:39 zfs snapshot pool/backup/galatea at 2008-04-10T02:24:38
2008-04-10.02:24:42 zfs destroy pool/backup/galatea at 2008-04-09T21:32:54
2008-04-10.03:37:55 zfs snapshot pool/backup/galatea at 2008-04-10T03:37:54
2008-04-10.03:37:57 zfs destroy pool/backup/galatea at 2008-04-09T22:46:00
2008-04-10.04:50:54 zfs snapshot pool/backup/galatea at 2008-04-10T04:50:52
2008-04-10.04:50:56 zfs destroy pool/backup/galatea at 2008-04-09T23:58:58
2008-04-10.06:03:47 zfs snapshot pool/backup/galatea at 2008-04-10T06:03:45
2008-04-10.06:03:48 zfs destroy pool/backup/galatea at 2008-04-10T01:11:50
2008-04-10.07:16:52 zfs snapshot pool/backup/galatea at 2008-04-10T07:16:51
2008-04-10.07:16:55 zfs destroy pool/backup/galatea at 2008-04-10T02:24:38
2008-04-10.08:29:58 zfs snapshot pool/backup/galatea at 2008-04-10T08:29:56
2008-04-10.08:30:00 zfs destroy pool/backup/galatea at 2008-04-10T03:37:54
2008-04-10.09:42:51 zfs snapshot pool/backup/galatea at 2008-04-10T09:42:50
2008-04-10.09:42:53 zfs destroy pool/backup/galatea at 2008-04-10T04:50:52
2008-04-10.10:55:49 zfs snapshot pool/backup/galatea at 2008-04-10T10:55:48
2008-04-10.10:55:51 zfs destroy pool/backup/galatea at 2008-04-10T06:03:45
2008-04-10.12:08:56 zfs snapshot pool/backup/galatea at 2008-04-10T12:08:55
2008-04-10.12:08:58 zfs destroy pool/backup/galatea at 2008-04-10T07:16:51
2008-04-10.13:21:50 zfs snapshot pool/backup/galatea at 2008-04-10T13:21:49
2008-04-10.13:21:53 zfs destroy pool/backup/galatea at 2008-04-10T08:29:56
2008-04-10.14:32:20 zpool import -f 17751373224295458742
2008-04-10.15:42:18 zfs snapshot pool/backup/galatea at 2008-04-10T15:42:18
2008-04-10.15:52:25 zfs snapshot pool/backup/galatea at 2008-04-10T15:52:23
2008-04-10.16:21:43 zfs create pool/bwaters
2008-04-11.14:16:11 zfs snapshot pool/bwaters at 2008-04-11T14:16:10
2008-04-11.20:01:59 zpool import -f 17751373224295458742
2008-04-11.20:03:20 zpool import -f 17751373224295458742
2008-04-11.20:09:18 zpool import -f 17751373224295458742
2008-04-14.12:52:23 zfs clone pool/backup/galatea at opt pool/backup/galatea_opt
2008-04-16.12:56:15 zpool import -f 17751373224295458742
2008-04-16.12:59:00 zpool export pool
2008-04-16.13:01:06 zpool import -f 17751373224295458742
2008-04-16.13:37:01 zpool import -f 17751373224295458742
2008-04-16.15:18:26 zpool import -f 17751373224295458742
2008-04-17.11:17:51 zfs create pool/vm
2008-04-17.11:18:04 zfs create pool/vm/nrao-rhel4
2008-04-17.11:51:13 zfs create pool/vm/rhel5
2008-04-17.12:31:06 zfs create pool/vm/solaris
2008-04-17.12:32:12 zfs create pool/vm/nexenta
2008-04-17.13:25:54 zfs snapshot pool/vm/rhel5 at 2008-04-17T13:25:54
2008-04-20.14:20:44 zfs snapshot pool/bwaters at 2008-04-20T14:20:43
2008-04-20.14:22:25 zfs create pool/vm/windowsXP
2008-04-22.10:56:57 zpool import -f 17751373224295458742
2008-04-24.16:50:02 zpool import -f 17751373224295458742
2008-04-24.17:06:52 zpool import -f 17751373224295458742
2008-04-24.19:19:56 zfs snapshot pool/bwaters at 2008-04-24T19:19:55
2008-04-25.10:14:58 zpool import -f 17751373224295458742
2008-05-08.18:53:57 zfs create pool/pictures
2008-05-08.18:54:01 zfs create pool/pictures/staging
2008-05-09.13:57:22 zfs create pool/backup/mochi
2008-05-09.15:53:01 zpool import -f 17751373224295458742
2008-05-14.13:52:41 zfs snapshot pool/bwaters at 2008-05-14T13:52:41
2008-05-14.15:07:53 zpool import -f 17751373224295458742
2008-05-14.16:18:09 zfs create pool/vm/kubuntu
2008-05-14.16:18:36 zfs create pool/vm/fedora
2008-05-15.14:14:59 zpool import -f 17751373224295458742
2008-05-15.16:08:59 zpool import -f 17751373224295458742
2008-05-23.09:00:24 zpool import -f 17751373224295458742
2008-05-23.09:10:25 zfs create pool/backup/cameron
2008-05-23.10:09:47 zfs create pool/backup/ibook_10_3_9
2008-05-29.16:47:59 zfs snapshot pool/bwaters at 2008-05-29T16:47:59
2008-05-29.17:03:20 zpool import -f 17751373224295458742
2008-05-29.17:23:27 zpool import -f 17751373224295458742
2008-06-04.14:22:35 zfs create pool/backup/eurythion
2008-06-04.14:22:55 zfs create pool/backup/eurythion/opt
2008-06-04.14:23:09 zfs create pool/backup/galatea/opt
2008-06-04.15:08:31 zfs destroy pool/backup/galatea_opt
2008-06-04.15:28:10 zfs clone pool/backup/galatea at opt pool/backup/galatea_opt
2008-06-04.15:28:53 zfs destroy pool/backup/galatea_opt
2008-06-04.15:29:10 zfs destroy pool/backup/galatea at opt
2008-06-04.15:32:50 zfs snapshot pool/backup/galatea/opt at 2008-03-31T16:50:00
2008-06-04.16:11:56 zfs snapshot pool/backup/galatea/opt at 2008-04-04T15:53:20
2008-06-04.16:38:53 zfs clone pool/backup/galatea at 2008-04-10T15:52:23
pool/backup/galatea_2008-04-10T15:52:23
2008-06-04.17:00:14 zfs snapshot pool/backup/galatea/opt at 2008-04-10T15:52:23
2008-06-04.17:13:27 zfs destroy pool/backup/galatea at 2008-04-10T09:42:50
2008-06-04.17:13:40 zfs destroy pool/backup/galatea at 2008-04-10T10:55:48
2008-06-04.17:14:46 zfs destroy pool/backup/galatea at 2008-04-10T12:08:55
2008-06-04.17:16:33 zfs destroy -R pool/backup/galatea at 2008-04-04T15:53:20
2008-06-04.17:16:56 zfs destroy pool/backup/galatea at 2008-04-10T13:21:49
2008-06-04.17:17:38 zfs destroy pool/backup/galatea at 2008-04-10T15:42:18
2008-06-10.14:39:00 zpool import -f 17751373224295458742
2008-06-10.14:56:17 zpool import -f 17751373224295458742
2008-06-10.15:02:35 zfs snapshot pool/backup/galatea at 2008-06-10T15:02:33
2008-06-10.15:04:42 zpool import -f 17751373224295458742
2008-06-10.15:08:06 zfs snapshot pool/backup/galatea at 2008-06-10T15:08:05
2008-06-10.16:10:46 zfs snapshot pool/backup/galatea at 2008-06-10T16:10:45
2008-06-11.16:03:39 zfs snapshot pool/backup/galatea at 2008-06-11T16:03:38
2008-06-12.17:11:02 zfs snapshot pool/backup/galatea at 2008-06-12T17:11:00
2008-06-13.16:22:31 zfs snapshot pool/bwaters at 2008-06-13T16:22:27
2008-06-13.16:37:54 zfs snapshot pool/backup/galatea at 2008-06-13T16:37:52
2008-06-14.16:14:02 zfs snapshot pool/backup/galatea at 2008-06-14T16:14:00
2008-06-15.16:13:17 zfs snapshot pool/backup/galatea at 2008-06-15T16:13:15
2008-06-15.16:13:20 zfs destroy pool/backup/galatea at 2008-06-10T15:02:33
2008-06-16.11:55:34 zpool import -f 17751373224295458742
2008-06-16.14:13:20 zfs snapshot pool/backup/galatea at 2008-06-16T14:13:19
2008-06-16.14:13:25 zfs destroy pool/backup/galatea at 2008-06-10T15:08:05
2008-06-17.16:03:26 zpool import -f 17751373224295458742
2008-06-18.16:31:45 zpool import -f 17751373224295458742
2008-06-19.16:16:36 zfs snapshot pool/backup/galatea at 2008-06-19T16:16:34
2008-06-19.16:16:42 zfs destroy pool/backup/galatea at 2008-06-10T16:10:45
2008-06-20.16:16:24 zfs snapshot pool/backup/galatea at 2008-06-20T16:16:23
2008-06-20.16:16:43 zfs destroy pool/backup/galatea at 2008-06-11T16:03:38
2008-06-21.16:16:07 zfs snapshot pool/backup/galatea at 2008-06-21T16:16:06
2008-06-23.11:39:53 zpool import -f 17751373224295458742
2008-06-23.16:16:20 zfs snapshot pool/backup/galatea at 2008-06-23T16:16:19
2008-06-23.16:16:22 zfs destroy pool/backup/galatea at 2008-06-13T16:37:52
2008-06-24.16:35:45 zfs snapshot pool/backup/galatea at 2008-06-24T16:35:44
2008-06-24.16:35:48 zfs destroy pool/backup/galatea at 2008-06-14T16:14:00
2008-06-24.16:58:08 zfs snapshot pool/backup/galatea/opt at 2008-06-24T16:58:07
2008-06-25.16:16:12 zfs snapshot pool/backup/galatea at 2008-06-25T16:16:11
2008-06-25.16:16:15 zfs destroy pool/backup/galatea at 2008-06-15T16:13:15
2008-06-26.16:12:37 zfs snapshot pool/backup/galatea at 2008-06-26T16:12:36
2008-06-26.16:12:41 zfs destroy pool/backup/galatea at 2008-06-16T14:13:19
2008-06-27.16:11:47 zfs snapshot pool/bwaters at 2008-06-27T16:11:45
2008-06-27.16:20:32 zfs snapshot pool/backup/galatea at 2008-06-27T16:20:31
2008-06-27.16:20:34 zfs destroy pool/backup/galatea at 2008-06-19T16:16:34
2008-06-28.16:21:50 zfs snapshot pool/backup/galatea at 2008-06-28T16:21:50
2008-06-28.16:21:53 zfs destroy pool/backup/galatea at 2008-06-20T16:16:23
2008-06-29.16:13:46 zfs snapshot pool/backup/galatea at 2008-06-29T16:13:45
2008-06-29.16:13:49 zfs destroy pool/backup/galatea at 2008-06-21T16:16:06
2008-06-30.16:26:36 zfs snapshot pool/backup/galatea at 2008-06-30T16:26:36
2008-06-30.16:26:39 zfs destroy pool/backup/galatea at 2008-06-23T16:16:19
2008-07-01.16:17:28 zfs snapshot pool/backup/galatea at 2008-07-01T16:17:28
2008-07-01.16:17:31 zfs destroy pool/backup/galatea at 2008-06-24T16:35:44
2008-07-02.16:14:31 zfs snapshot pool/backup/galatea at 2008-07-02T16:14:30
2008-07-02.16:14:34 zfs destroy pool/backup/galatea at 2008-06-25T16:16:11
2008-07-03.16:14:30 zfs snapshot pool/backup/galatea at 2008-07-03T16:14:29
2008-07-03.16:14:33 zfs destroy pool/backup/galatea at 2008-06-26T16:12:36
2008-07-04.16:21:12 zfs snapshot pool/backup/galatea at 2008-07-04T16:21:11
2008-07-04.16:21:16 zfs destroy pool/backup/galatea at 2008-06-27T16:20:31
2008-07-05.16:14:16 zfs snapshot pool/backup/galatea at 2008-07-05T16:14:15
2008-07-05.16:14:20 zfs destroy pool/backup/galatea at 2008-06-28T16:21:50
2008-07-06.16:13:55 zfs snapshot pool/backup/galatea at 2008-07-06T16:13:54
2008-07-06.16:13:57 zfs destroy pool/backup/galatea at 2008-06-29T16:13:45
2008-07-07.16:09:04 zfs snapshot pool/backup/galatea at 2008-07-07T16:09:04
2008-07-07.16:09:07 zfs destroy pool/backup/galatea at 2008-06-30T16:26:36
2008-07-07.18:42:50 zfs clone pool/bwaters at 2008-06-27T16:11:45
pool/bwaters_lastsnap
2008-07-07.18:43:08 zfs destroy pool/bwaters_lastsnap
2008-07-07.18:43:31 zfs clone pool/bwaters at 2008-06-27T16:11:45
pool/bwaters_lastsnap
2008-07-07.18:54:04 zfs snapshot pool/bwaters at 2008-07-07T18:54:03
2008-07-07.18:54:23 zfs destroy pool/bwaters_lastsnap
2008-07-08.16:09:00 zfs snapshot pool/backup/galatea at 2008-07-08T16:08:59
2008-07-08.16:09:02 zfs destroy pool/backup/galatea at 2008-07-01T16:17:28
2008-07-09.16:15:52 zfs snapshot pool/backup/galatea at 2008-07-09T16:15:50
2008-07-09.16:15:55 zfs destroy pool/backup/galatea at 2008-07-02T16:14:30
2008-07-10.16:14:53 zfs snapshot pool/backup/galatea at 2008-07-10T16:14:52
2008-07-10.16:14:57 zfs destroy pool/backup/galatea at 2008-07-03T16:14:29
2008-07-11.16:15:34 zfs snapshot pool/backup/galatea at 2008-07-11T16:15:33
2008-07-11.16:15:37 zfs destroy pool/backup/galatea at 2008-07-04T16:21:11
2008-07-12.16:14:24 zfs snapshot pool/backup/galatea at 2008-07-12T16:14:23
2008-07-12.16:14:26 zfs destroy pool/backup/galatea at 2008-07-05T16:14:15
2008-07-13.16:14:20 zfs snapshot pool/backup/galatea at 2008-07-13T16:14:19
2008-07-13.16:14:23 zfs destroy pool/backup/galatea at 2008-07-06T16:13:54
2008-07-14.16:14:40 zfs snapshot pool/backup/galatea at 2008-07-14T16:14:39
2008-07-14.16:14:43 zfs destroy pool/backup/galatea at 2008-07-07T16:09:04
2008-07-15.16:14:29 zfs snapshot pool/backup/galatea at 2008-07-15T16:14:28
2008-07-15.16:14:32 zfs destroy pool/backup/galatea at 2008-07-08T16:08:59
2008-07-16.16:14:52 zfs snapshot pool/backup/galatea at 2008-07-16T16:14:51
2008-07-16.16:14:55 zfs destroy pool/backup/galatea at 2008-07-09T16:15:50
2008-07-17.16:14:30 zfs snapshot pool/backup/galatea at 2008-07-17T16:14:29
2008-07-17.16:14:33 zfs destroy pool/backup/galatea at 2008-07-10T16:14:52
2008-07-17.17:25:14 zpool import -f 17751373224295458742
2008-07-17.19:43:07 zfs snapshot pool/backup/galatea at 2008-07-17T19:43:07
2008-07-17.19:43:10 zfs destroy pool/backup/galatea at 2008-07-11T16:15:33
2008-07-18.16:17:25 zfs snapshot pool/backup/galatea at 2008-07-18T16:17:24
2008-07-18.16:17:28 zfs destroy pool/backup/galatea at 2008-07-12T16:14:23
2008-07-19.16:14:10 zfs snapshot pool/backup/galatea at 2008-07-19T16:14:09
2008-07-19.16:14:13 zfs destroy pool/backup/galatea at 2008-07-13T16:14:19
2008-07-20.16:14:15 zfs snapshot pool/backup/galatea at 2008-07-20T16:14:14
2008-07-20.16:14:17 zfs destroy pool/backup/galatea at 2008-07-14T16:14:39
2008-07-21.16:14:44 zfs snapshot pool/backup/galatea at 2008-07-21T16:14:43
2008-07-21.16:14:47 zfs destroy pool/backup/galatea at 2008-07-15T16:14:28
2008-07-22.16:14:18 zfs snapshot pool/backup/galatea at 2008-07-22T16:14:16
2008-07-22.16:14:20 zfs destroy pool/backup/galatea at 2008-07-16T16:14:51
2008-07-23.16:13:38 zfs snapshot pool/backup/galatea at 2008-07-23T16:13:37
2008-07-23.16:13:41 zfs destroy pool/backup/galatea at 2008-07-17T16:14:29
2008-07-24.16:14:41 zfs snapshot pool/backup/galatea at 2008-07-24T16:14:41
2008-07-24.16:14:45 zfs destroy pool/backup/galatea at 2008-07-17T19:43:07
2008-07-25.16:13:58 zfs snapshot pool/backup/galatea at 2008-07-25T16:13:55
2008-07-25.16:14:03 zfs destroy pool/backup/galatea at 2008-07-18T16:17:24
2008-07-26.16:14:01 zfs snapshot pool/backup/galatea at 2008-07-26T16:14:00
2008-07-26.16:14:05 zfs destroy pool/backup/galatea at 2008-07-19T16:14:09
2008-07-27.16:13:43 zfs snapshot pool/backup/galatea at 2008-07-27T16:13:42
2008-07-27.16:13:48 zfs destroy pool/backup/galatea at 2008-07-20T16:14:14
2008-07-28.16:13:43 zfs snapshot pool/backup/galatea at 2008-07-28T16:13:42
2008-07-28.16:13:46 zfs destroy pool/backup/galatea at 2008-07-21T16:14:43
2008-07-29.16:13:49 zfs snapshot pool/backup/galatea at 2008-07-29T16:13:48
2008-07-29.16:13:52 zfs destroy pool/backup/galatea at 2008-07-22T16:14:16
2008-07-29.17:13:17 zfs snapshot pool/bwaters at 2008-07-29T17:13:16
2008-07-30.16:15:22 zfs snapshot pool/backup/galatea at 2008-07-30T16:15:21
2008-07-30.16:15:24 zfs destroy pool/backup/galatea at 2008-07-23T16:13:37
2008-07-31.16:14:29 zfs snapshot pool/backup/galatea at 2008-07-31T16:14:28
2008-07-31.16:14:32 zfs destroy pool/backup/galatea at 2008-07-24T16:14:41
2008-08-01.16:14:29 zfs snapshot pool/backup/galatea at 2008-08-01T16:14:28
2008-08-01.16:14:31 zfs destroy pool/backup/galatea at 2008-07-25T16:13:55
2008-08-02.16:14:19 zfs snapshot pool/backup/galatea at 2008-08-02T16:14:18
2008-08-02.16:14:22 zfs destroy pool/backup/galatea at 2008-07-26T16:14:00
2008-08-03.16:14:33 zfs snapshot pool/backup/galatea at 2008-08-03T16:14:32
2008-08-03.16:14:36 zfs destroy pool/backup/galatea at 2008-07-27T16:13:42
2008-08-04.16:14:20 zfs snapshot pool/backup/galatea at 2008-08-04T16:14:19
2008-08-04.16:14:23 zfs destroy pool/backup/galatea at 2008-07-28T16:13:42
2008-08-05.16:14:22 zfs snapshot pool/backup/galatea at 2008-08-05T16:14:21
2008-08-05.16:14:25 zfs destroy pool/backup/galatea at 2008-07-29T16:13:48
2008-08-06.16:14:36 zfs snapshot pool/backup/galatea at 2008-08-06T16:14:35
2008-08-06.16:14:40 zfs destroy pool/backup/galatea at 2008-07-30T16:15:21
2008-08-07.16:14:37 zfs snapshot pool/backup/galatea at 2008-08-07T16:14:36
2008-08-07.16:14:40 zfs destroy pool/backup/galatea at 2008-07-31T16:14:28
2008-08-08.16:14:21 zfs snapshot pool/backup/galatea at 2008-08-08T16:14:21
2008-08-08.16:14:24 zfs destroy pool/backup/galatea at 2008-08-01T16:14:28
2008-08-09.16:14:11 zfs snapshot pool/backup/galatea at 2008-08-09T16:14:10
2008-08-09.16:14:16 zfs destroy pool/backup/galatea at 2008-08-02T16:14:18
2008-08-12.14:22:50 zpool import -f 17751373224295458742
2008-08-12.15:00:43 zfs snapshot pool/backup/galatea at 2008-08-12T15:00:42
2008-08-12.15:00:46 zfs destroy pool/backup/galatea at 2008-08-03T16:14:32
2008-08-12.16:23:56 zfs snapshot pool/backup/galatea at 2008-08-12T16:23:55
2008-08-12.16:23:59 zfs destroy pool/backup/galatea at 2008-08-04T16:14:19
2008-08-13.16:08:27 zfs snapshot pool/backup/galatea at 2008-08-13T16:08:25
2008-08-13.16:08:30 zfs destroy pool/backup/galatea at 2008-08-05T16:14:21
2008-08-14.16:08:23 zfs snapshot pool/backup/galatea at 2008-08-14T16:08:23
2008-08-14.16:08:26 zfs destroy pool/backup/galatea at 2008-08-06T16:14:35
2008-08-15.16:08:43 zfs snapshot pool/backup/galatea at 2008-08-15T16:08:42
2008-08-15.16:08:46 zfs destroy pool/backup/galatea at 2008-08-07T16:14:36
2008-08-16.16:08:35 zfs snapshot pool/backup/galatea at 2008-08-16T16:08:35
2008-08-16.16:08:38 zfs destroy pool/backup/galatea at 2008-08-08T16:14:21
2008-08-17.16:08:20 zfs snapshot pool/backup/galatea at 2008-08-17T16:08:19
2008-08-17.16:08:23 zfs destroy pool/backup/galatea at 2008-08-09T16:14:10
2008-08-18.16:08:39 zfs snapshot pool/backup/galatea at 2008-08-18T16:08:38
2008-08-18.16:08:43 zfs destroy pool/backup/galatea at 2008-08-12T15:00:42
2008-08-19.16:08:42 zfs snapshot pool/backup/galatea at 2008-08-19T16:08:40
2008-08-19.16:08:45 zfs destroy pool/backup/galatea at 2008-08-12T16:23:55
2008-08-20.16:08:49 zfs snapshot pool/backup/galatea at 2008-08-20T16:08:48
2008-08-20.16:08:51 zfs destroy pool/backup/galatea at 2008-08-13T16:08:25
2008-08-21.16:08:40 zfs snapshot pool/backup/galatea at 2008-08-21T16:08:39
2008-08-21.16:08:42 zfs destroy pool/backup/galatea at 2008-08-14T16:08:23
2008-08-22.16:08:45 zfs snapshot pool/backup/galatea at 2008-08-22T16:08:44
2008-08-22.16:08:48 zfs destroy pool/backup/galatea at 2008-08-15T16:08:42
2008-08-22.18:31:47 zfs clone pool/backup/galatea at 2008-08-16T16:08:35
pool/backup/galatea_2008-08-16T16:08:35
2008-08-22.18:32:06 zfs destroy pool/backup/galatea_2008-08-16T16:08:35
2008-08-22.18:33:11 zfs clone
pool/backup/galatea/opt at 2008-06-24T16:58:07
pool/backup/galatea-opt_2008-06-24T16:58:07
2008-09-21.20:04:58 zpool import -f 17751373224295458742
2008-09-22.07:24:43 zfs snapshot pool/bwaters at 2008-09-22T07:24:43
2008-09-22.07:31:06 zpool import -f 17751373224295458742
2008-09-22.07:47:14 zfs snapshot pool/backup/galatea at 2008-09-22T07:47:13
2008-09-22.07:47:17 zfs destroy pool/backup/galatea at 2008-08-16T16:08:35
2008-09-22.13:11:20 zpool import -f 17751373224295458742
2008-09-22.16:14:03 zpool import -f 17751373224295458742
2008-09-22.16:34:16 zfs snapshot pool/backup/galatea at 2008-09-22T16:34:15
2008-09-22.16:34:20 zfs destroy pool/backup/galatea at 2008-08-17T16:08:19
2008-09-22.16:50:34 zpool import -f 17751373224295458742
2008-09-22.17:01:41 zfs snapshot pool/backup/galatea at 2008-09-22T17:01:40
2008-09-22.17:01:43 zfs destroy pool/backup/galatea at 2008-08-18T16:08:38
2008-09-22.18:30:17 zpool import -f 17751373224295458742
2008-09-23.16:13:40 zfs snapshot pool/backup/galatea at 2008-09-23T16:13:38
2008-09-23.16:13:42 zfs destroy pool/backup/galatea at 2008-08-19T16:08:40
2008-09-24.16:10:32 zfs snapshot pool/backup/galatea at 2008-09-24T16:10:30
2008-09-24.16:10:34 zfs destroy pool/backup/galatea at 2008-08-20T16:08:48
2008-09-25.16:09:12 zfs snapshot pool/backup/galatea at 2008-09-25T16:09:11
2008-09-25.16:09:15 zfs destroy pool/backup/galatea at 2008-08-21T16:08:39
2008-09-26.16:11:12 zfs snapshot pool/backup/galatea at 2008-09-26T16:11:03
2008-09-26.16:11:14 zfs destroy pool/backup/galatea at 2008-08-22T16:08:44
2008-09-27.16:10:58 zfs snapshot pool/backup/galatea at 2008-09-27T16:10:57
2008-09-27.16:11:01 zfs destroy pool/backup/galatea at 2008-09-22T07:47:13
2008-09-28.16:09:45 zfs snapshot pool/backup/galatea at 2008-09-28T16:09:44
2008-09-28.16:09:48 zfs destroy pool/backup/galatea at 2008-09-22T16:34:15
2008-09-29.16:10:35 zfs snapshot pool/backup/galatea at 2008-09-29T16:10:33
2008-09-29.16:10:39 zfs destroy pool/backup/galatea at 2008-09-22T17:01:40
2008-09-30.16:10:07 zfs snapshot pool/backup/galatea at 2008-09-30T16:10:06
2008-09-30.16:10:10 zfs destroy pool/backup/galatea at 2008-09-23T16:13:38
2008-10-01.12:24:10 zfs snapshot pool/bwaters at 2008-10-01T12:24:09
2008-10-06.19:57:31 zpool import -f 17751373224295458742
2008-10-10.14:24:42 zpool import -f 17751373224295458742
2008-10-10.14:42:03 zfs snapshot pool/backup/galatea at 2008-10-10T14:42:02
2008-10-10.14:42:06 zfs destroy pool/backup/galatea at 2008-09-24T16:10:30
2008-10-10.15:06:38 zpool import -f 17751373224295458742
2008-10-10.15:17:08 zfs snapshot pool/backup/galatea at 2008-10-10T15:17:07
2008-10-10.15:17:11 zfs destroy pool/backup/galatea at 2008-09-25T16:09:11
2008-10-10.16:12:20 zfs snapshot pool/backup/galatea at 2008-10-10T16:12:19
2008-10-10.16:12:23 zfs destroy pool/backup/galatea at 2008-09-26T16:11:03
2008-10-11.16:10:17 zfs snapshot pool/backup/galatea at 2008-10-11T16:10:15
2008-10-11.16:10:20 zfs destroy pool/backup/galatea at 2008-09-27T16:10:57
2008-10-12.16:10:02 zfs snapshot pool/backup/galatea at 2008-10-12T16:10:01
2008-10-12.16:10:05 zfs destroy pool/backup/galatea at 2008-09-28T16:09:44
2008-10-13.15:29:19 zpool import -f 17751373224295458742
2008-10-13.15:38:36 zpool import -f 17751373224295458742
2008-10-13.16:09:44 zfs snapshot pool/backup/galatea at 2008-10-13T16:09:43
2008-10-13.16:09:47 zfs destroy pool/backup/galatea at 2008-09-29T16:10:33
2008-10-14.16:13:22 zfs snapshot pool/backup/galatea at 2008-10-14T16:13:22
2008-10-14.16:13:25 zfs destroy pool/backup/galatea at 2008-09-30T16:10:06
2008-10-15.16:10:30 zfs snapshot pool/backup/galatea at 2008-10-15T16:10:29
2008-10-15.16:10:32 zfs destroy pool/backup/galatea at 2008-10-10T14:42:02
2008-10-16.16:10:28 zfs snapshot pool/backup/galatea at 2008-10-16T16:10:28
2008-10-16.16:10:32 zfs destroy pool/backup/galatea at 2008-10-10T15:17:07
2008-10-17.16:10:43 zfs snapshot pool/backup/galatea at 2008-10-17T16:10:43
2008-10-17.16:10:46 zfs destroy pool/backup/galatea at 2008-10-10T16:12:19
2008-10-26.13:13:10 zpool import -f 17751373224295458742
2008-10-26.16:12:48 zfs snapshot pool/backup/galatea at 2008-10-26T16:12:47
2008-10-26.16:12:51 zfs destroy pool/backup/galatea at 2008-10-11T16:10:15
2008-10-26.20:07:45 zfs create pool/backup/eurythion-tiger
2008-10-26.20:55:32 zpool import -f 17751373224295458742
2008-10-26.21:15:33 zfs snapshot pool/backup/galatea at 2008-10-26T21:15:32
2008-10-26.21:15:39 zfs destroy pool/backup/galatea at 2008-10-12T16:10:01
2008-10-27.16:10:51 zfs snapshot pool/backup/galatea at 2008-10-27T16:10:50
2008-10-27.16:10:55 zfs destroy pool/backup/galatea at 2008-10-13T16:09:43
2008-10-27.20:03:06 zfs snapshot pool/backup/galatea at 2008-10-27T20:03:05
2008-10-27.20:03:11 zfs destroy pool/backup/galatea at 2008-10-14T16:13:22
2008-10-27.22:05:03 zpool import -f 17751373224295458742
2008-10-27.22:19:24 zpool import -f 17751373224295458742
2008-10-28.10:32:20 zpool import -f 17751373224295458742
2008-10-28.13:12:49 zpool import -f 17751373224295458742
2008-10-28.17:14:55 zfs snapshot pool/bwaters at 2008-10-28T17:14:54
2008-10-28.18:44:07 zpool import -f 17751373224295458742
2008-10-29.13:26:15 zpool import -f 17751373224295458742
2008-10-29.13:33:53 zfs snapshot pool/bwaters at 2008-10-29T13:33:52
2008-10-31.12:27:10 zpool import -f 17751373224295458742
2008-10-31.14:25:52 zpool import -f 17751373224295458742
2008-11-01.14:04:45 zpool import -f 17751373224295458742
2008-11-01.14:48:27 zpool import -f 17751373224295458742
2008-11-01.15:11:46 zpool import -f 17751373224295458742
2008-11-02.15:17:58 zpool import -f 17751373224295458742
2008-11-02.15:55:59 zpool import -f 17751373224295458742
2008-11-02.16:04:35 zpool import -f 17751373224295458742
2008-11-02.16:13:20 zpool import -f 17751373224295458742
2008-11-02.16:22:00 zpool import -f 17751373224295458742
2008-11-02.23:03:58 zpool import -f 17751373224295458742
2008-11-03.14:07:54 zpool import -f 17751373224295458742
2008-11-03.15:06:35 zpool import -f 17751373224295458742
2008-11-03.17:16:04 zpool import -f 17751373224295458742
2008-11-03.17:30:18 zpool import -f 17751373224295458742
2008-11-04.13:28:04 zpool import -f 17751373224295458742
2008-11-04.15:54:47 zfs create pool/backup/vmware-winxp
2008-11-05.10:29:45 zfs create pool/casa
2008-11-05.12:40:55 zfs create pool/casa/data
2008-11-12.15:01:07 zfs create pool/vm/gentoo
2008-11-15.19:54:13 zfs create pool/backup/sedna
2008-11-17.12:32:46 zfs snapshot pool/casa at 2008-11-17T12:32:46
2008-11-17.12:35:14 zfs snapshot pool/casa/data at 2008-11-17T12:35:13
2008-11-21.16:59:51 zpool import -f 17751373224295458742
2008-11-21.17:03:27 zpool import -f 17751373224295458742
2008-11-25.19:07:11 zpool import -f 17751373224295458742
2008-11-30.14:38:02 zfs destroy pool/vm/gentoo
2008-12-02.13:03:28 zfs destroy pool/vm/fedora
2008-12-02.13:03:42 zfs destroy pool/vm/kubuntu
2008-12-02.16:31:14 zpool import -f 17751373224295458742
2008-12-02.16:49:39 zpool import -f 17751373224295458742
2008-12-02.16:55:54 zpool export -f pool
2008-12-02.16:57:37 zpool import pool
2008-12-02.16:58:25 zpool scrub pool
2008-12-15.19:35:03 zpool import -f 17751373224295458742
2008-12-20.17:45:08 zpool export pool
2008-12-20.17:45:40 zpool import pool
2008-12-20.17:52:20 zpool clear pool

From alex.blewitt at gmail.com  Mon Dec 29 02:11:34 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Mon, 29 Dec 2008 10:11:34 +0000
Subject: [zfs-discuss] Mirror of a Stripe or raidz?
In-Reply-To: <D3D8D91A-D327-4278-985A-F00289A6D2A0@mac.com>
References: <D3D8D91A-D327-4278-985A-F00289A6D2A0@mac.com>
Message-ID: <636fd28e0812290211m7fa3a9fam1169201f5cc8ae54@mail.gmail.com>

On Mon, Dec 1, 2008 at 12:08 AM, Aaron <ax2ron at mac.com> wrote:
> Since snapshots aren't fully baked in OS X yet

Really? Why do you think that? I've got over 1000 snapshots on my ZFS
filesystems on one pool. True, the .zfs autodir doesn't work, and the
send/recv you can't pipe directly, but those don't impact the utility
of snapshots.

> I would like to pickup a 1TB drive and increase my storage space and
> redundancy by:
> a. converting the raidz pool to a striped pool and mirror it onto the 1TB
> drive (3x320 stripe + 1TB mirror of the stripe)

Not sure what you mean here; a raidz pool is already striped. Do you
mean just lose the redundancy data? Is this for speed?

> b. Keep the raidz pool as is and do replication of that pool onto the 1TB
> drive as a separate 1 drive zfs "pool"

I'd be tempted to use the 1TB drive as a pair of drives (most of the
enclosures are in fact 2x500G drives) and then address them
independently as ZFS mirrors. That way, you'd get reliability and
access the spools independently (though the transfer rate is probably
the limiting feature here, rather than the speed). If you want to
increase speed, get two of them and mirror complementary disks.

I wrote up about the Iomega Ultramax, which I'm using as  an external
FW HD pair:

http://alblue.blogspot.com/2008/04/review-iomega-ultramax-and-hfz-vs-zfs.html

In fact, these drives have a small partition at the front of the drive
for HFS+ (for the OS) and the rest is ZFS, so I'd be tempted to throw
all the drives at ZFS or ZFS partitions.

Alex

From caronni at gmail.com  Mon Dec 29 07:57:17 2008
From: caronni at gmail.com (Germano Caronni)
Date: Mon, 29 Dec 2008 16:57:17 +0100
Subject: [zfs-discuss] A Glorious Hack
In-Reply-To: <2b0225fb0812281242s40120261j68e061806df8659d@mail.gmail.com>
References: <2b0225fb0812281241q2c1fa480xd7b2b89c311e7a3d@mail.gmail.com>
	<2b0225fb0812281242s40120261j68e061806df8659d@mail.gmail.com>
Message-ID: <327b821f0812290757u1141628dx1975a3b54b2eac88@mail.gmail.com>

the thing about non-empty mount points blocking snapshots is *very* useful
to know.
thanks for sharing!


On Sun, Dec 28, 2008 at 21:42, Boyd Waters <bwaters at nrao.edu> wrote:

> > I created VMWare "raw" disks for each device in the pool, then added
> them to an OpenSolaris virtual machine.
>
>
> My RAIDZ is on the four internal SATA bays of my Mac Pro; that's
> /dev/disk{0,1,2,3}
>
> To let a VMWare virtual machine see these, it'll need to have a way to
> translate a virtual disk (vmdk) to a real disk. This is called "raw"
> access.
>
> Like this:
>
>  /Library/Application\ Support/VMware\ Fusion/vmware-rawdiskCreator \
>   create \
>   /dev/disk0 \
>   1 \
>   ~/Documents/Virtual\ Machines.localized/Solaris.vmwarevm/rawdisk0 \
>   ide
>
>
> That command will create a text file that maps the raw disk to the virtual
> machine, and a binary file that is just an MBR (master boot record) that
> might make Winders happy. I don't think that MBR is needed, but
> whatever...
>
> I can't add more than 4 IDE drives to a virtual machine.
> vmware-rawdiskCreator claims that it can create SCSI drives as well, but
> it lies. Fortunately, the vmdk text file that it spits out is dead simple
> to edit; I replaced this line
>
>   ddb.adapterType = "ide"
>
> with this one
>
>   ddb.adapterType = "buslogic"
>
>
> For each of the four hard disks in my RAIDZ.
>
> Then I added these vmdk disks to the VM via the VMWare Fusion GUI. Fine.
> Fired it up, those four disks show up fine in the OpenSolaris VM, and away
> you go!
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081229/960a1dfc/attachment.html>

From sean at crosscreekentertain.com  Sun Dec 28 23:49:02 2008
From: sean at crosscreekentertain.com (Sean O'Neil)
Date: Sun, 28 Dec 2008 23:49:02 -0800
Subject: [zfs-discuss] Kernel Panic
Message-ID: <2E3D01C9-FC71-4B06-99A7-7A798ED837D0@crosscreekentertain.com>

Just tested this out on a G5 PPC, 10.5.6.  I set up a zpool with two  
raid-z (like a RAID 5+0).  6 sata disks each (12 total).  Getting a  
kernel panic every time I do a large write operation or if I run a  
disk performance benchmark.

I couldn't create additional filesystems.  "zfs create" gives me an  
error.

From bwaters at nrao.edu  Mon Dec 29 11:15:12 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Mon, 29 Dec 2008 12:15:12 -0700
Subject: [zfs-discuss] snapshots
In-Reply-To: <2b0225fb0812201700j65853f7bud1f49fd631f6b2d4@mail.gmail.com>
References: <B349067A-3B41-4F37-93AC-7C2E9971FAA1@mac.com>
	<590879AE-ACCD-4870-9A9B-730E4EB27CFA@apple.com>
	<0F79DE0F-328E-4033-8EED-CA65951C5703@nrao.edu>
	<20081220.202920.185448097.hanche@math.ntnu.no>
	<F8071BC9-E5FC-428B-BCC7-2460C9EBD1E2@nrao.edu>
	<5792952B-8C55-4D43-A8A4-362C1F7FCFCA@gmail.com>
	<2b0225fb0812201700j65853f7bud1f49fd631f6b2d4@mail.gmail.com>
Message-ID: <292397E9-9BEC-4042-9159-1D7FB2293BC4@nrao.edu>


On Dec 20, 2008, at 6:00 PM, Boyd Waters wrote:

>> On Dec 20, 2008, at 12:23 PM, Richard McClellan wrote:
>>
>> Others that have encountered this error (dataset busy) manually  
>> umount/mount
>> the filesystem and snapshots started working again.  Check this out:
>> http://malsserver.blogspot.com/2008/11/zfs-auto-snapshot-and-thus-time-slider.html


Just so we know about this... I figured out my "dataset busy" problem  
yesterday, it was the result of mounting a ZFS filesystem on a non- 
empty directory. The ZFS filesystem would shadow the contents of that  
directory. And would refuse to snapshot.

I made sure that my mountpoints were empty directories, and I was able  
to create snapshots again.

Hope this helps!

  - boyd


From darwinskernel at gmail.com  Tue Dec 30 04:09:43 2008
From: darwinskernel at gmail.com (Charles Darwin)
Date: Tue, 30 Dec 2008 07:09:43 -0500
Subject: [zfs-discuss] Permissions are not enabled on the disk (-9973)
Message-ID: <9E9379C7-E489-48A0-88D1-DCCA4CE4CAB4@gmail.com>

$ diskutil list |grep ZFS
    3:                        ZFS neo                     81.3 Gi     
disk0s3
$ diskutil repairPermissions /dev/disk0s3
Permissions are not enabled on the disk.
Error encountered attempting to verify/repair permissions on disk0s3  
neo: Permissions are not enabled on the disk (-9973)

Any help is highly appreciated.

From zfs at sbod.at  Tue Dec 30 04:57:40 2008
From: zfs at sbod.at (Franz Schmalzl)
Date: Tue, 30 Dec 2008 13:57:40 +0100
Subject: [zfs-discuss] Permissions are not enabled on the disk (-9973)
In-Reply-To: <9E9379C7-E489-48A0-88D1-DCCA4CE4CAB4@gmail.com>
References: <9E9379C7-E489-48A0-88D1-DCCA4CE4CAB4@gmail.com>
Message-ID: <663122D5-97A0-4D9B-B16D-858A5A974423@sbod.at>

Permissions should be ok!

Trying to repair permissions using diskutil only works with recipe  
files which are created when something is installed via an installer  
process.

 From my point of view, you should be fine...



franz






On 30.12.2008, at 13:09, Charles Darwin wrote:

> $ diskutil list |grep ZFS
>   3:                        ZFS neo                     81.3 Gi     
> disk0s3
> $ diskutil repairPermissions /dev/disk0s3
> Permissions are not enabled on the disk.
> Error encountered attempting to verify/repair permissions on disk0s3  
> neo: Permissions are not enabled on the disk (-9973)
>
> Any help is highly appreciated.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dirkschelfhout at mac.com  Wed Dec 31 01:24:33 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Wed, 31 Dec 2008 10:24:33 +0100
Subject: [zfs-discuss] A Glorious Hack
In-Reply-To: <327b821f0812290757u1141628dx1975a3b54b2eac88@mail.gmail.com>
References: <2b0225fb0812281241q2c1fa480xd7b2b89c311e7a3d@mail.gmail.com>
	<2b0225fb0812281242s40120261j68e061806df8659d@mail.gmail.com>
	<327b821f0812290757u1141628dx1975a3b54b2eac88@mail.gmail.com>
Message-ID: <EC3F4528-DEFB-47A6-83A5-6D1BBAE8E0D6@mac.com>

Could I do the same with solaris running in virtualbox ?
unmount my zfs raidz in osx
create hardlinks to the rdisk slices into a shared folder with solaris
and then import the zfs in solaris ?
Can something go wrong ?
I'll try testing this
Dirk
On 29 Dec 2008, at 16:57, Germano Caronni wrote:

> the thing about non-empty mount points blocking snapshots is *very*  
> useful to know.
> thanks for sharing!
>
>
> On Sun, Dec 28, 2008 at 21:42, Boyd Waters <bwaters at nrao.edu> wrote:
> > I created VMWare "raw" disks for each device in the pool, then added
> them to an OpenSolaris virtual machine.
>
>
> My RAIDZ is on the four internal SATA bays of my Mac Pro; that's
> /dev/disk{0,1,2,3}
>
> To let a VMWare virtual machine see these, it'll need to have a way to
> translate a virtual disk (vmdk) to a real disk. This is called "raw"
> access.
>
> Like this:
>
>  /Library/Application\ Support/VMware\ Fusion/vmware-rawdiskCreator \
>   create \
>   /dev/disk0 \
>   1 \
>   ~/Documents/Virtual\ Machines.localized/Solaris.vmwarevm/rawdisk0 \
>   ide
>
>
> That command will create a text file that maps the raw disk to the  
> virtual
> machine, and a binary file that is just an MBR (master boot record)  
> that
> might make Winders happy. I don't think that MBR is needed, but
> whatever...
>
> I can't add more than 4 IDE drives to a virtual machine.
> vmware-rawdiskCreator claims that it can create SCSI drives as well,  
> but
> it lies. Fortunately, the vmdk text file that it spits out is dead  
> simple
> to edit; I replaced this line
>
>   ddb.adapterType = "ide"
>
> with this one
>
>   ddb.adapterType = "buslogic"
>
>
> For each of the four hard disks in my RAIDZ.
>
> Then I added these vmdk disks to the VM via the VMWare Fusion GUI.  
> Fine.
> Fired it up, those four disks show up fine in the OpenSolaris VM,  
> and away
> you go!
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081231/44b57c83/attachment.html>

From pmau at me.com  Wed Dec 31 06:14:39 2008
From: pmau at me.com (Patrick Mau)
Date: Wed, 31 Dec 2008 15:14:39 +0100
Subject: [zfs-discuss] Question about ZFS status
Message-ID: <97880A58-AF3A-485E-A626-8141C4B065A8@me.com>

Hallo everyone

First, I'm not yet subscribed to the list, please include my address  
in the reply, thanks a lot.

I've recently started using ZFS (zfs-119) on my MacPro. It all worked  
fine, ignoring all the open issues that are still work in progress.
Looking at the source makes me wonder if there will be additionall  
releases before the inclusion in MacOS 10.6.

Since Apple published the source, there have been very little changes  
to the source, so I'd like to ask if it's better to wait for 10.6  
before actively converting my existing filesystems. The last change  
seems to be 5 month ago.

Does anyone have any information wether Apple will continue publishing  
changes here, or is all the development on ZFS targetting the next OSX  
release and will only be included in upcoming seeds for Snow Leopard?

Don't get me wrong. I'm just wondering if it's better to wait for  
official releases, because there's little visible activity with  
regards to the cdebase.

Thanks, and have a Happy New Year
Patrick

