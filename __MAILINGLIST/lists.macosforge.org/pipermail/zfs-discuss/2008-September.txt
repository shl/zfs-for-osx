From zfs at hessmann.de  Wed Sep  3 08:16:22 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Wed, 3 Sep 2008 17:16:22 +0200
Subject: [zfs-discuss] Kernel Panic after zpool status on (damaged) USB stick
Message-ID: <E02D5E8D-FD93-46DD-9BA2-7935390BBA9B@hessmann.de>

Hello,


there seems to be something wrong with a ZFS formated 2GB USB stick.

ls -la results in partly correct directory listing, partly error  
messages.

A zpool status results in reproducible Kernel Panics:

=====
Tue Sep  2 12:24:39 2008
panic(cpu 0 caller 0x001A8CD4): Kernel trap at 0x0036271b, type  
14=page fault, registers:
CR0: 0x80010033, CR2: 0x0000000c, CR3: 0x011b9000, CR4: 0x00000660
EAX: 0x00000000, EBX: 0x34d7fde8, ECX: 0x1f000000, EDX: 0x00000000
CR2: 0x0000000c, EBP: 0x34d7fd18, ESI: 0x00000200, EDI: 0x041d2000
EFL: 0x00010202, EIP: 0x0036271b, CS:  0x00000008, DS:  0x04a20010
Error code: 0x00000000

Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x34d7fb38 : 0x12b0fa (0x4592a4 0x34d7fb6c 0x133243 0x0)
0x34d7fb88 : 0x1a8cd4 (0x46280c 0x36271b 0xe 0x461fbc)
0x34d7fc68 : 0x19ede5 (0x34d7fc80 0x60125d0 0x34d7fd18 0x36271b)
0x34d7fc78 : 0x36271b (0xe 0x48 0x63890010 0x190010)
0x34d7fd18 : 0xcbedf8 (0x0 0x69949 0x3000 0x0)
0x34d7fd38 : 0xcc6935 (0x0 0x0 0x34d7fd58 0x1a236f)
0x34d7fd78 : 0x20300f (0x1f000000 0xcf1c5a20 0x41d2000 0x3)
0x34d7fdb8 : 0x1f63a2 (0x34d7fde8 0x246 0x34d7fe18 0x1da567)
0x34d7fe18 : 0x1ec67c (0x49cae10 0xcf1c5a20 0x41d2000 0x3)
0x34d7fe78 : 0x3661ff (0x4a0a8a0 0xcf1c5a20 0x41d2000 0x34d7ff50)
0x34d7fe98 : 0x38cb8f (0x4a0a8a0 0xcf1c5a20 0x41d2000 0x34d7ff50)
0x34d7ff78 : 0x3ddde2 (0x8b87750 0x61503c0 0x6150404 0x0)
0x34d7ffc8 : 0x19f2c3 (0x8fdb5e0 0x0 0x1a20b5 0x8fdb5e0)
No mapping exists for frame pointer
Backtrace terminated-invalid frame pointer 0xbfffa888
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0xc63000->0xd2efff

BSD process name corresponding to current thread: zpool

Mac OS version:
9E17

Kernel version:
Darwin Kernel Version 9.4.0: Mon Jun  9 19:30:53 PDT 2008;  
root:xnu-1228.5.20~1/RELEASE_I386
System model name: MacBook4,1 (Mac-F22788A9)
====

zfs-119 on newest 10.5.


Best regards,

Christian


From riscky at gmail.com  Wed Sep  3 12:55:48 2008
From: riscky at gmail.com (Riscky Abacus)
Date: Wed, 3 Sep 2008 15:55:48 -0400
Subject: [zfs-discuss] Kernel Panic after zpool status on (damaged) USB
	stick
In-Reply-To: <E02D5E8D-FD93-46DD-9BA2-7935390BBA9B@hessmann.de>
References: <E02D5E8D-FD93-46DD-9BA2-7935390BBA9B@hessmann.de>
Message-ID: <51cfc2260809031255u341cae2fq116ebf24dda94320@mail.gmail.com>

This has been an issue for me for some time... I think I sent a few
emails to the list sometime in March/April. I basically gave up using
zfs on flash drives.... :(

On Wed, Sep 3, 2008 at 11:16 AM, Christian He?mann <zfs at hessmann.de> wrote:
> Hello,
>
>
> there seems to be something wrong with a ZFS formated 2GB USB stick.
>
> ls -la results in partly correct directory listing, partly error
> messages.
>
> A zpool status results in reproducible Kernel Panics:
>
> =====
> Tue Sep  2 12:24:39 2008
> panic(cpu 0 caller 0x001A8CD4): Kernel trap at 0x0036271b, type
> 14=page fault, registers:
> CR0: 0x80010033, CR2: 0x0000000c, CR3: 0x011b9000, CR4: 0x00000660
> EAX: 0x00000000, EBX: 0x34d7fde8, ECX: 0x1f000000, EDX: 0x00000000
> CR2: 0x0000000c, EBP: 0x34d7fd18, ESI: 0x00000200, EDI: 0x041d2000
> EFL: 0x00010202, EIP: 0x0036271b, CS:  0x00000008, DS:  0x04a20010
> Error code: 0x00000000
>
> Backtrace, Format - Frame : Return Address (4 potential args on stack)
> 0x34d7fb38 : 0x12b0fa (0x4592a4 0x34d7fb6c 0x133243 0x0)
> 0x34d7fb88 : 0x1a8cd4 (0x46280c 0x36271b 0xe 0x461fbc)
> 0x34d7fc68 : 0x19ede5 (0x34d7fc80 0x60125d0 0x34d7fd18 0x36271b)
> 0x34d7fc78 : 0x36271b (0xe 0x48 0x63890010 0x190010)
> 0x34d7fd18 : 0xcbedf8 (0x0 0x69949 0x3000 0x0)
> 0x34d7fd38 : 0xcc6935 (0x0 0x0 0x34d7fd58 0x1a236f)
> 0x34d7fd78 : 0x20300f (0x1f000000 0xcf1c5a20 0x41d2000 0x3)
> 0x34d7fdb8 : 0x1f63a2 (0x34d7fde8 0x246 0x34d7fe18 0x1da567)
> 0x34d7fe18 : 0x1ec67c (0x49cae10 0xcf1c5a20 0x41d2000 0x3)
> 0x34d7fe78 : 0x3661ff (0x4a0a8a0 0xcf1c5a20 0x41d2000 0x34d7ff50)
> 0x34d7fe98 : 0x38cb8f (0x4a0a8a0 0xcf1c5a20 0x41d2000 0x34d7ff50)
> 0x34d7ff78 : 0x3ddde2 (0x8b87750 0x61503c0 0x6150404 0x0)
> 0x34d7ffc8 : 0x19f2c3 (0x8fdb5e0 0x0 0x1a20b5 0x8fdb5e0)
> No mapping exists for frame pointer
> Backtrace terminated-invalid frame pointer 0xbfffa888
>       Kernel loadable modules in backtrace (with dependencies):
>          com.apple.filesystems.zfs(8.0)@0xc63000->0xd2efff
>
> BSD process name corresponding to current thread: zpool
>
> Mac OS version:
> 9E17
>
> Kernel version:
> Darwin Kernel Version 9.4.0: Mon Jun  9 19:30:53 PDT 2008;
> root:xnu-1228.5.20~1/RELEASE_I386
> System model name: MacBook4,1 (Mac-F22788A9)
> ====
>
> zfs-119 on newest 10.5.
>
>
> Best regards,
>
> Christian
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

From craig at mainstream.net  Thu Sep  4 07:36:42 2008
From: craig at mainstream.net (Craig Peterson)
Date: Thu, 4 Sep 2008 10:36:42 -0400
Subject: [zfs-discuss] Problems with ZFS on Intel Mac Pro Copying Hundreds
	of Files
Message-ID: <CF1FE6B9-A158-42C7-9A76-5F15960F673E@mainstream.net>

I'm running build 119 on an up-to-date Mac Pro.  I've set up a three- 
disk zfs raid-z:

$ zpool status
   pool: Internal
  state: ONLINE
status: The pool is formatted using an older on-disk format.  The pool  
can
         still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
         pool will no longer be accessible on older software versions.
  scrub: none requested
config:

         NAME         STATE     READ WRITE CKSUM
         Internal     ONLINE       0     0     0
           raidz1     ONLINE       0     0     0
             disk1s2  ONLINE       0     0     0
             disk2s2  ONLINE       0     0     0
             disk3s2  ONLINE       0     0     0

errors: No known data errors

I'm copying my music collection over to the pool, and it locks up  
trying to write to the zfs partition after an indeterminate amount of  
time.  At this point I have to reboot the machine by physically power  
cycling it in order to get back at the zfs partition and anything that  
tries to write to it gets locked up:

iTunes/iTunes Music/Dr. Jeffrey D. Thompson/Music for Brainwave  
Massage CD2/
iTunes/iTunes Music/Dr. Jeffrey D. Thompson/Music for Brainwave  
Massage CD2/03 Awakened Focus.m4a
    138083516 100%   70.69MB/s    0:00:01 (xfer#55201, to- 
check=12475/116940)
iTunes/iTunes Music/Dr. Jeffrey D. Thompson/Music for Brainwave  
Massage CD2/04 Tranquil Awareness.m4a
^C
rsync error: received SIGINT, SIGTERM, or SIGHUP (code 20) at / 
SourceCache/rsync/rsync-35.2/rsync/rsync.c(244) [sender=2.6.9]
rsync: writefd_unbuffered failed to write 133 bytes [generator]:  
Broken pipe (32)
bash-3.2# rsync -av --progress iTunes /Volumes/Internal/
building file list ...
116941 files to consider

At this point it hangs, as do any write operations to the partition.

I've only tried using rsync to copy the files, and rebooting the  
machine every hour or so isn't much of an option :-)

Suggestions?  I found a bug report of something similar that was said  
to be fixed in build 119... but it doesn't appear to be.

Thanks,

Craig.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080904/3acb3647/attachment.html 

From alan.knell at mastigo4.demon.co.uk  Wed Sep  3 09:59:11 2008
From: alan.knell at mastigo4.demon.co.uk (Alan Knell)
Date: Wed, 3 Sep 2008 17:59:11 +0100
Subject: [zfs-discuss] Where are my zfs filesystems?
Message-ID: <9EC3287C-9F46-4965-8552-A7F91BD2AAC1@mastigo4.demon.co.uk>

Hallo. I'm a newcomer to zfs. I have a problem and I can't find the  
answer.

As su I created a 3 disk raidz [on my Intel MacPro]. I named the pool  
Media.zfs.

I then zfs created 3 file systems: Media.zfs/Audio, Media.zfs/Images,  
and Media.zfs/Video. I set the mountpoints to be directories created  
in my home fs: ~/Audio, ~/Images and ~/Video respectively. Mounted, I  
checked ownership and permissions - all well.

I copied files to the zfs filesystems, all well, everything worked  
fine [and I was feeling pleased with myself].

The pool mounted on /Volumes and showed up in the finder as expected.  
My files were visible in the finder at the filesystem mountpoints in  
my home directory, but no filesystems were visible under /Volumes/ 
Media.zfs - normal behaviour, I think.

Thinking it would be nice to make the files visible under Media.zfs,  
and in the home directory, I made these changes.
- I created directories Audio, Images and Video under /Volumes/ 
Media.zfs.
- I reset the mountpoints of the file systems to these new  
directories, and remounted them: no problem.
- I created symbolic links from my home directory, eg ~/Movies --> / 
Volumes/Media.zfs/Video.
- I rechecked ownership and permissions - all as expected.

Now I could access my files from home, and under /Volumes/Media.zfs.
In the finder the mountpoints in /Volumes/Media.zfs showed as links -  
normal behaviour, I believe.

This happy state lasted several days and at least one reboot. Then  
this morning I copied over some image files to Media.zfs images.
The copy appeared to go as planned, but something changed.

Now my files are not listed under the /Volumes/Media.zfs mountpoints,  
nor under the symbolic links in my home directory.
The Media.zfs directory mountpoints now appear in Finder with normal  
directory icons - no longer as links.

zfs get all shows the expected values - the filesystems and data are  
still there.

Attempts to reset the mountpoints, to unmount or remount, all end in  
error messages, done as su or as myself.

It may be I am in error naming the file systems the same as their  
mountpoints - eg mounting Media.zfs/Audio on /Volumes/Media.zfs/Audio  
- created with a mkdir command.

I would much value comments and advice.
Where did I go wrong?
How can I recover my filesystems?

TNX

Alan





From lists at loveturtle.net  Thu Sep  4 10:40:27 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Thu, 04 Sep 2008 13:40:27 -0400
Subject: [zfs-discuss] Problems with ZFS on Intel Mac Pro Copying
 Hundreds of Files
In-Reply-To: <CF1FE6B9-A158-42C7-9A76-5F15960F673E@mainstream.net>
References: <CF1FE6B9-A158-42C7-9A76-5F15960F673E@mainstream.net>
Message-ID: <48C01D8B.8050209@loveturtle.net>

try using rsync 3.x

From chris.shuman at gmail.com  Fri Sep  5 19:39:00 2008
From: chris.shuman at gmail.com (Chris Shuman)
Date: Fri, 5 Sep 2008 21:39:00 -0500
Subject: [zfs-discuss] Help Please! /Volumes is completely gone!
Message-ID: <ac78d4820809051939i57da9d03hcee29005b51f4a46@mail.gmail.com>

Machine 1.6Ghz G5 w/ 2GB
10.5 up to date running 119 bits.

Changed 'canmount' property a while back on all my zfs filesystems.
Decided to switch them back to the way they were. I messed up and this
is what it looks like now.

NAME                                  USED  AVAIL  REFER  MOUNTPOINT
zdata                                 218G  5.37G  1.12M  /Volumes/zdata
zdata/home                            218G  5.37G  49.5K  /Volumes/
zdata/home/anika                      181M  5.37G   181M  /Volumes/
zdata/home/chris                     4.00G  5.37G  4.00G  /Volumes/
zdata/home/jaren                      216M  5.37G   216M  /Volumes/
zdata/home/severin                    437M  5.37G   437M  /Volumes/
zdata/home/test                       213G  5.37G  3.75M  /Volumes/
zdata/home/test/Desktop              31.9M  5.37G  31.9M  /Volumes/
zdata/home/test/Documents            4.13G  5.37G  4.13G  /Volumes/
zdata/home/test/Documents at 20080614a   239K      -  4.13G  -
zdata/home/test/Downloads              82K  5.37G    82K  /Volumes/
zdata/home/test/Library              3.05G  5.37G  2.92G  /Volumes/
zdata/home/test/Library at 20080524a     135M      -  2.57G  -
zdata/home/test/Movies                184G  5.37G   184G  /Volumes/
zdata/home/test/Music                31.9M  5.37G  31.9M  /Volumes/
zdata/home/test/Pictures             22.1G  5.37G  22.0G  /Volumes/
zdata/home/test/Pictures at 20080524a   18.2M      -  21.7G  -
zdata/home/test/Pictures at 20080614a   31.7M      -  21.9G  -
zdata/home/test/Public                 48K  5.37G    48K  /Volumes/
zdata/home/test/Sites                  52K  5.37G    52K  /Volumes/
zdata/junk                             19K  5.37G    19K  /Volumes/zdata/junk
LeopardG5:~ root# zfs set mountpoint=/Volumes/ zdata/home/test/Sites
cannot unmount '/Volumes': Resource busy
LeopardG5:~ root# zfs set mountpoint=/Volumes/zdata/home/test/Sites
zdata/home/test/Sites
cannot unmount '/Volumes': Resource busy
LeopardG5:~ root#

As you can see I get a 'Resource busy' message when I try to fix it.
At this point the only volume I have is my system volume. Not sure
what to do to fix the issue. HFS volumes are gone as well as ZFS
volumes.

Thanks,

Chris

From quension at mac.com  Fri Sep  5 21:55:34 2008
From: quension at mac.com (Trevor Talbot)
Date: Fri, 05 Sep 2008 21:55:34 -0700
Subject: [zfs-discuss] ZFS and Samba do not get along
In-Reply-To: <3FDE7512-BE6D-4E66-97A2-7A303A258914@sun.com>
References: <C4DB3D37.6F24%qapf@qapf.com>
	<ca3c5ef60808291518g7ddec061ib7921a16df82d097@mail.gmail.com>
	<3FDE7512-BE6D-4E66-97A2-7A303A258914@sun.com>
Message-ID: <DA0BB5E6-E9AC-4870-B7F5-2600BBB52D2E@mac.com>

On Aug 30, 2008, at 4:08 AM, Jonathan Edwards wrote:

> you need a more recent version of Samba, minimally 3.0.26 along with  
> the right build parameters to get the ZFS ACL support (ZFS uses a  
> variant of NFSv4 style ACLs)  .. we put this in around snv_70 in  
> opensolaris last year see:
> http://mail.opensolaris.org/pipermail/zfs-discuss/2007-September/042552.html

I'm pretty sure it's failing while trying to copy named streams  
(extended attributes), specifically the ones for HFS fork simulation  
on NTFS. I haven't had time to confirm or investigate why, though I  
have some vague notion that it didn't happen in 102A.

Is that one of the symptoms of samba's misunderstanding of ZFS ACLs?  
(It should be a little different on OS X, since the filesystem is  
using the kernel's ACL engine for access checks, not ZFS's.)

> On Aug 29, 2008, at 6:18 PM, Marko Milisavljevic wrote:
>
>> No idea how to fix it, but I can confirm the same problem form Vista.
>>
>> On Wed, Aug 27, 2008 at 5:19 PM, Alex P. Frangis <qapf at qapf.com>  
>> wrote:
>>> Using the built in Samba sharing in OSX I have a folder shared on  
>>> a 3 Drive Raid-Z connected via firewire, and when accessing the  
>>> samba share from windows hosts I can not copy files, all file  
>>> copies end with a host is unreachable error after transfering 99%  
>>> of their content. You can open files, you can view files in  
>>> notepad, you can mount ISO's across the network, but copying a  
>>> file from point A to point B fails.
>>>
>>> Anyone have any idea how to fix this one?


From hanche at math.ntnu.no  Fri Sep  5 22:54:53 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Sat, 06 Sep 2008 07:54:53 +0200 (CEST)
Subject: [zfs-discuss] Help Please! /Volumes is completely gone!
In-Reply-To: <ac78d4820809051939i57da9d03hcee29005b51f4a46@mail.gmail.com>
References: <ac78d4820809051939i57da9d03hcee29005b51f4a46@mail.gmail.com>
Message-ID: <20080906.075453.242734032.hanche@math.ntnu.no>

+ "Chris Shuman" <chris.shuman at gmail.com>:

> LeopardG5:~ root# zfs set mountpoint=/Volumes/zdata/home/test/Sites
> zdata/home/test/Sites
> cannot unmount '/Volumes': Resource busy

I wonder if you could reboot into single user mode and fix it?
Just a thought ...

- Harald

From chris.shuman at gmail.com  Sat Sep  6 09:15:31 2008
From: chris.shuman at gmail.com (Chris Shuman)
Date: Sat, 6 Sep 2008 11:15:31 -0500
Subject: [zfs-discuss] zfs-discuss Digest, Vol 9, Issue 3
In-Reply-To: <mailman.7.1220709632.47536.zfs-discuss@lists.macosforge.org>
References: <mailman.7.1220709632.47536.zfs-discuss@lists.macosforge.org>
Message-ID: <ac78d4820809060915k52628474j3be70b6c3e6aafbf@mail.gmail.com>

Harald, thanks for the idea, but I don't have any zfs tools available
from single user mode. I really am at a loss here. Definitely in over
my head.

Interesting that it does not affect external drives, just internal
drives. Flash drive and external USB drives mount fine.

If there is someone out there that would not mind giving me their
number so we can discuss it in real time it would be greatly
appreciated.

Thanks,

Chris

>
> Machine 1.6Ghz G5 w/ 2GB
> 10.5 up to date running 119 bits.
>
> Changed 'canmount' property a while back on all my zfs filesystems.
> Decided to switch them back to the way they were. I messed up and this
> is what it looks like now.
>
> NAME                                  USED  AVAIL  REFER  MOUNTPOINT
> zdata                                 218G  5.37G  1.12M  /Volumes/zdata
> zdata/home                            218G  5.37G  49.5K  /Volumes/
> zdata/home/anika                      181M  5.37G   181M  /Volumes/
> zdata/home/chris                     4.00G  5.37G  4.00G  /Volumes/
> zdata/home/jaren                      216M  5.37G   216M  /Volumes/
> zdata/home/severin                    437M  5.37G   437M  /Volumes/
> zdata/home/test                       213G  5.37G  3.75M  /Volumes/
> zdata/home/test/Desktop              31.9M  5.37G  31.9M  /Volumes/
> zdata/home/test/Documents            4.13G  5.37G  4.13G  /Volumes/
> zdata/home/test/Documents at 20080614a   239K      -  4.13G  -
> zdata/home/test/Downloads              82K  5.37G    82K  /Volumes/
> zdata/home/test/Library              3.05G  5.37G  2.92G  /Volumes/
> zdata/home/test/Library at 20080524a     135M      -  2.57G  -
> zdata/home/test/Movies                184G  5.37G   184G  /Volumes/
> zdata/home/test/Music                31.9M  5.37G  31.9M  /Volumes/
> zdata/home/test/Pictures             22.1G  5.37G  22.0G  /Volumes/
> zdata/home/test/Pictures at 20080524a   18.2M      -  21.7G  -
> zdata/home/test/Pictures at 20080614a   31.7M      -  21.9G  -
> zdata/home/test/Public                 48K  5.37G    48K  /Volumes/
> zdata/home/test/Sites                  52K  5.37G    52K  /Volumes/
> zdata/junk                             19K  5.37G    19K  /Volumes/zdata/junk
> LeopardG5:~ root# zfs set mountpoint=/Volumes/ zdata/home/test/Sites
> cannot unmount '/Volumes': Resource busy
> LeopardG5:~ root# zfs set mountpoint=/Volumes/zdata/home/test/Sites
> zdata/home/test/Sites
> cannot unmount '/Volumes': Resource busy
> LeopardG5:~ root#
>
> As you can see I get a 'Resource busy' message when I try to fix it.
> At this point the only volume I have is my system volume. Not sure
> what to do to fix the issue. HFS volumes are gone as well as ZFS
> volumes.
>
> Thanks,
>
> Chris
>
>
> ------------------------------
>

>
> Message: 3
> Date: Sat, 06 Sep 2008 07:54:53 +0200 (CEST)
> From: Harald Hanche-Olsen <hanche at math.ntnu.no>
> Subject: Re: [zfs-discuss] Help Please! /Volumes is completely gone!
> To: chris.shuman at gmail.com
> Cc: zfs-discuss at lists.macosforge.org
> Message-ID: <20080906.075453.242734032.hanche at math.ntnu.no>
> Content-Type: Text/Plain; charset=us-ascii
>
> + "Chris Shuman" <chris.shuman at gmail.com>:
>
>> LeopardG5:~ root# zfs set mountpoint=/Volumes/zdata/home/test/Sites
>> zdata/home/test/Sites
>> cannot unmount '/Volumes': Resource busy
>
> I wonder if you could reboot into single user mode and fix it?
> Just a thought ...
>
> - Harald
>
>
> ------------------------------
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
> End of zfs-discuss Digest, Vol 9, Issue 3
> *****************************************
>

From hanche at math.ntnu.no  Sun Sep  7 01:31:37 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Sun, 07 Sep 2008 10:31:37 +0200 (CEST)
Subject: [zfs-discuss] Help Please! /Volumes is completely gone!
In-Reply-To: <ac78d4820809060915k52628474j3be70b6c3e6aafbf@mail.gmail.com>
References: <mailman.7.1220709632.47536.zfs-discuss@lists.macosforge.org>
	<ac78d4820809060915k52628474j3be70b6c3e6aafbf@mail.gmail.com>
Message-ID: <20080907.103137.88362333.hanche@math.ntnu.no>

+ "Chris Shuman" <chris.shuman at gmail.com>:

> Harald, thanks for the idea, but I don't have any zfs tools
> available from single user mode.

Why not? I suppose it's possible you need to load the zfs extension
(using kextload), and you may need to run  mount -uw /  in case the
zpool and/or zfs programs need write access to /etc/zfs/ to work
right, but other than that I see no problem there.  But I haven't
tried it myself, and would very much like to know what could go wrong
with it.

(My other idea is more complicated, namely to disable the automounter
using  sudo launchtcl unload -w com.apple.automountd  and then
rebooting.  This might inhibit the mounting of your zfs volumes and
allow you to change them.  Unfortunately, it might inhibit the
mounting of other critical volumes as well, and so just compound the
problem.  So I wouldn't recommend this unless you really have a firm
grasp of what you're doing.)

- Harald

From hanche at math.ntnu.no  Sun Sep  7 03:25:20 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Sun, 07 Sep 2008 12:25:20 +0200 (CEST)
Subject: [zfs-discuss] Help Please! /Volumes is completely gone!
In-Reply-To: <20080907.103137.88362333.hanche@math.ntnu.no>
References: <mailman.7.1220709632.47536.zfs-discuss@lists.macosforge.org>
	<ac78d4820809060915k52628474j3be70b6c3e6aafbf@mail.gmail.com>
	<20080907.103137.88362333.hanche@math.ntnu.no>
Message-ID: <20080907.122520.131644342.hanche@math.ntnu.no>

+ Harald Hanche-Olsen <hanche at math.ntnu.no>:

Wait, wait, here is another idea:

Diagnosis: You have multiple zfs filesystems all mounted on
/Volumes. They will be mounted on top of eacher, which means that all
of them will be busy and can be unmounted - with the exception of the
one mounted on top. You should be able to unmount that one: Just go
through the motions of trying to unmount the whole pile until one
unmount succeeds. Repeat, and repeat again util hopefully they are all
unmounted.  (At this point, your mounted HFS volumes should magically
reappear).  Then go fix the mountpoints before remounting the whole
pile.

- Harald

From lists at loveturtle.net  Sun Sep  7 07:45:15 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Sun, 07 Sep 2008 10:45:15 -0400
Subject: [zfs-discuss] Help Please! /Volumes is completely gone!
In-Reply-To: <20080907.122520.131644342.hanche@math.ntnu.no>
References: <mailman.7.1220709632.47536.zfs-discuss@lists.macosforge.org>	<ac78d4820809060915k52628474j3be70b6c3e6aafbf@mail.gmail.com>	<20080907.103137.88362333.hanche@math.ntnu.no>
	<20080907.122520.131644342.hanche@math.ntnu.no>
Message-ID: <48C3E8FB.4060602@loveturtle.net>

zfs unmount -a may help.

there is no reason at all that you can't play with your zpools in single 
user mode except that maybe your hostid wont be set and then you will 
have to manually import the pool again once you exit single user mode 
but other than that i can't see any problems.

Harald Hanche-Olsen wrote:
> + Harald Hanche-Olsen<hanche at math.ntnu.no>:
>
> Wait, wait, here is another idea:
>
> Diagnosis: You have multiple zfs filesystems all mounted on
> /Volumes. They will be mounted on top of eacher, which means that all
> of them will be busy and can be unmounted - with the exception of the
> one mounted on top. You should be able to unmount that one: Just go
> through the motions of trying to unmount the whole pile until one
> unmount succeeds. Repeat, and repeat again util hopefully they are all
> unmounted.  (At this point, your mounted HFS volumes should magically
> reappear).  Then go fix the mountpoints before remounting the whole
> pile.
>
> - Harald
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>    


From ndellofano at apple.com  Sun Sep  7 23:14:37 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Sun, 7 Sep 2008 23:14:37 -0700
Subject: [zfs-discuss] Help Please! /Volumes is completely gone!
In-Reply-To: <ac78d4820809051939i57da9d03hcee29005b51f4a46@mail.gmail.com>
References: <ac78d4820809051939i57da9d03hcee29005b51f4a46@mail.gmail.com>
Message-ID: <B5D16B4B-2E5F-4AB2-9018-2CBE73C89824@apple.com>

Hey Chris

Can you send me a 'zpool history' for your pool?   what's an 'ls / 
Volumes' give you?  Also what's the output of 'zfs mount'?  Once I get  
a handle on where your machine is we can hopefully get it straightened  
out :)

Noel

On Sep 5, 2008, at 7:39 PM, Chris Shuman wrote:

> Machine 1.6Ghz G5 w/ 2GB
> 10.5 up to date running 119 bits.
>
> Changed 'canmount' property a while back on all my zfs filesystems.
> Decided to switch them back to the way they were. I messed up and this
> is what it looks like now.
>
> NAME                                  USED  AVAIL  REFER  MOUNTPOINT
> zdata                                 218G  5.37G  1.12M  /Volumes/ 
> zdata
> zdata/home                            218G  5.37G  49.5K  /Volumes/
> zdata/home/anika                      181M  5.37G   181M  /Volumes/
> zdata/home/chris                     4.00G  5.37G  4.00G  /Volumes/
> zdata/home/jaren                      216M  5.37G   216M  /Volumes/
> zdata/home/severin                    437M  5.37G   437M  /Volumes/
> zdata/home/test                       213G  5.37G  3.75M  /Volumes/
> zdata/home/test/Desktop              31.9M  5.37G  31.9M  /Volumes/
> zdata/home/test/Documents            4.13G  5.37G  4.13G  /Volumes/
> zdata/home/test/Documents at 20080614a   239K      -  4.13G  -
> zdata/home/test/Downloads              82K  5.37G    82K  /Volumes/
> zdata/home/test/Library              3.05G  5.37G  2.92G  /Volumes/
> zdata/home/test/Library at 20080524a     135M      -  2.57G  -
> zdata/home/test/Movies                184G  5.37G   184G  /Volumes/
> zdata/home/test/Music                31.9M  5.37G  31.9M  /Volumes/
> zdata/home/test/Pictures             22.1G  5.37G  22.0G  /Volumes/
> zdata/home/test/Pictures at 20080524a   18.2M      -  21.7G  -
> zdata/home/test/Pictures at 20080614a   31.7M      -  21.9G  -
> zdata/home/test/Public                 48K  5.37G    48K  /Volumes/
> zdata/home/test/Sites                  52K  5.37G    52K  /Volumes/
> zdata/junk                             19K  5.37G    19K  /Volumes/ 
> zdata/junk
> LeopardG5:~ root# zfs set mountpoint=/Volumes/ zdata/home/test/Sites
> cannot unmount '/Volumes': Resource busy
> LeopardG5:~ root# zfs set mountpoint=/Volumes/zdata/home/test/Sites
> zdata/home/test/Sites
> cannot unmount '/Volumes': Resource busy
> LeopardG5:~ root#
>
> As you can see I get a 'Resource busy' message when I try to fix it.
> At this point the only volume I have is my system volume. Not sure
> what to do to fix the issue. HFS volumes are gone as well as ZFS
> volumes.
>
> Thanks,
>
> Chris
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Sun Sep  7 23:15:42 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Sun, 7 Sep 2008 23:15:42 -0700
Subject: [zfs-discuss] Help Please! /Volumes is completely gone!
In-Reply-To: <ac78d4820809051939i57da9d03hcee29005b51f4a46@mail.gmail.com>
References: <ac78d4820809051939i57da9d03hcee29005b51f4a46@mail.gmail.com>
Message-ID: <A424830A-5D92-464D-A90E-6E9EB0632649@apple.com>

Hey Chris

Can you send me a 'zpool history' for your pool?   what's an 'ls / 
Volumes' give you?  Also what's the output of 'zfs mount'?  Once I get  
a handle on where your machine is we can hopefully get it straightened  
out :)

Noel

On Sep 5, 2008, at 7:39 PM, Chris Shuman wrote:

> Machine 1.6Ghz G5 w/ 2GB
> 10.5 up to date running 119 bits.
>
> Changed 'canmount' property a while back on all my zfs filesystems.
> Decided to switch them back to the way they were. I messed up and this
> is what it looks like now.
>
> NAME                                  USED  AVAIL  REFER  MOUNTPOINT
> zdata                                 218G  5.37G  1.12M  /Volumes/ 
> zdata
> zdata/home                            218G  5.37G  49.5K  /Volumes/
> zdata/home/anika                      181M  5.37G   181M  /Volumes/
> zdata/home/chris                     4.00G  5.37G  4.00G  /Volumes/
> zdata/home/jaren                      216M  5.37G   216M  /Volumes/
> zdata/home/severin                    437M  5.37G   437M  /Volumes/
> zdata/home/test                       213G  5.37G  3.75M  /Volumes/
> zdata/home/test/Desktop              31.9M  5.37G  31.9M  /Volumes/
> zdata/home/test/Documents            4.13G  5.37G  4.13G  /Volumes/
> zdata/home/test/Documents at 20080614a   239K      -  4.13G  -
> zdata/home/test/Downloads              82K  5.37G    82K  /Volumes/
> zdata/home/test/Library              3.05G  5.37G  2.92G  /Volumes/
> zdata/home/test/Library at 20080524a     135M      -  2.57G  -
> zdata/home/test/Movies                184G  5.37G   184G  /Volumes/
> zdata/home/test/Music                31.9M  5.37G  31.9M  /Volumes/
> zdata/home/test/Pictures             22.1G  5.37G  22.0G  /Volumes/
> zdata/home/test/Pictures at 20080524a   18.2M      -  21.7G  -
> zdata/home/test/Pictures at 20080614a   31.7M      -  21.9G  -
> zdata/home/test/Public                 48K  5.37G    48K  /Volumes/
> zdata/home/test/Sites                  52K  5.37G    52K  /Volumes/
> zdata/junk                             19K  5.37G    19K  /Volumes/ 
> zdata/junk
> LeopardG5:~ root# zfs set mountpoint=/Volumes/ zdata/home/test/Sites
> cannot unmount '/Volumes': Resource busy
> LeopardG5:~ root# zfs set mountpoint=/Volumes/zdata/home/test/Sites
> zdata/home/test/Sites
> cannot unmount '/Volumes': Resource busy
> LeopardG5:~ root#
>
> As you can see I get a 'Resource busy' message when I try to fix it.
> At this point the only volume I have is my system volume. Not sure
> what to do to fix the issue. HFS volumes are gone as well as ZFS
> volumes.
>
> Thanks,
>
> Chris
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From cshuman at nordam.com  Mon Sep  8 10:08:07 2008
From: cshuman at nordam.com (Shuman, Chris)
Date: Mon, 8 Sep 2008 12:08:07 -0500
Subject: [zfs-discuss] zfs-discuss Digest, Vol 9, Issue 5
In-Reply-To: <mailman.9.1220882411.30496.zfs-discuss@lists.macosforge.org>
References: <mailman.9.1220882411.30496.zfs-discuss@lists.macosforge.org>
Message-ID: <4AC87E46E30CE84EA59AD3E5DE7D3F3502829E4F@PCINSEX1.nordam.com>

Before I show the history I will let you all know I did get this issue
fixed last night based off of Harald's suggestion to try and reverse
everything. The history does not show all that happened as none of the "
cannot unmount '/Volumes': Resource busy" errors are shown.

I'm positive by now that the screw up had nothing to do with ZFS and
everything to do with me :)  If that is not the case and there is other
information I can get from my system to help, just let me know.

Thanks for helping me through this everybody.

History:
2008-08-06.18:48:40 zfs set canmount=off zdata/home
2008-08-06.18:53:39 zfs set canmount=off zdata/home/anika
2008-08-06.18:53:45 zfs set canmount=off zdata/home/chris
2008-08-06.18:53:50 zfs set canmount=off zdata/home/jaren
2008-08-06.18:53:57 zfs set canmount=off zdata/home/severin
2008-08-06.18:54:04 zfs set canmount=off zdata/home/test
2008-08-06.18:54:11 zfs set canmount=off zdata/home/test/Desktop
2008-08-06.18:54:17 zfs set canmount=off zdata/home/test/Documents
2008-08-06.18:54:25 zfs set canmount=off zdata/home/test/Downloads
2008-08-06.18:54:32 zfs set canmount=off zdata/home/test/Library
2008-08-06.18:54:39 zfs set canmount=off zdata/home/test/Movies
2008-08-06.18:54:44 zfs set canmount=off zdata/home/test/Pictures
2008-08-06.18:54:52 zfs set canmount=off zdata/home/test/Public
2008-08-06.18:54:57 zfs set canmount=off zdata/home/test/Sites
2008-08-07.19:53:56 zfs set canmount=on zdata/home
2008-08-07.19:55:04 zfs set mountpoint=/Volumes/ zdata/home
2008-08-07.19:56:00 zfs set canmount=off zdata/home
2008-08-10.21:30:10 zpool scrub zdata
2008-08-16.16:54:47 zpool import -f 14165958316872700719
2008-08-17.06:57:25 zpool import -f 14165958316872700719
2008-08-18.11:36:44 zpool import -f 14165958316872700719
2008-08-21.20:09:19 zpool import -f 14165958316872700719
2008-08-27.16:34:24 zpool import -f 14165958316872700719
2008-09-05.19:49:39 zfs set canmount=on zdata/home
2008-09-05.19:51:28 zfs set canmount=on zdata/home/severin
2008-09-05.19:51:45 zfs set canmount=on zdata/home
2008-09-05.19:51:50 zfs set canmount=on zdata/home/anika
2008-09-05.19:51:55 zfs set canmount=on zdata/home/chris
2008-09-05.19:51:59 zfs set canmount=on zdata/home/jaren
2008-09-05.19:52:03 zfs set canmount=on zdata/home/severin
2008-09-05.19:52:09 zfs set canmount=on zdata/home/test
2008-09-05.19:52:15 zfs set canmount=on zdata/home/test/Desktop
2008-09-05.19:52:20 zfs set canmount=on zdata/home/test/Documents
2008-09-05.19:52:29 zfs set canmount=on zdata/home/test/Downloads
2008-09-05.19:52:36 zfs set canmount=on zdata/home/test/Library
2008-09-05.19:52:43 zfs set canmount=on zdata/home/test/Movies
2008-09-05.19:53:01 zfs set canmount=on zdata/home/test/Music
2008-09-05.19:53:07 zfs set canmount=on zdata/home/test/Pictures
2008-09-05.19:53:13 zfs set canmount=on zdata/home/test/Public
2008-09-05.19:53:17 zfs set canmount=on zdata/home/test/Sites
2008-09-05.19:54:36 zfs set mountpoint=/Volumes/ zdata/home
2008-09-05.19:54:46 zfs set mountpoint=/Volumes/ zdata/home/anika
2008-09-05.19:54:53 zfs set mountpoint=/Volumes/ zdata/home/chris
2008-09-05.19:54:56 zfs set mountpoint=/Volumes/ zdata/home/jaren
2008-09-05.19:55:00 zfs set mountpoint=/Volumes/ zdata/home/severin
2008-09-05.19:55:04 zfs set mountpoint=/Volumes/ zdata/home/test
2008-09-05.19:55:12 zfs set mountpoint=/Volumes/ zdata/home/test/Desktop
2008-09-05.19:55:16 zfs set mountpoint=/Volumes/
zdata/home/test/Documents
2008-09-05.19:55:24 zfs set mountpoint=/Volumes/
zdata/home/test/Downloads
2008-09-05.19:55:31 zfs set mountpoint=/Volumes/ zdata/home/test/Library
2008-09-05.19:55:37 zfs set mountpoint=/Volumes/ zdata/home/test/Movies
2008-09-05.19:55:44 zfs set mountpoint=/Volumes/ zdata/home/test/Music
2008-09-05.19:55:49 zfs set mountpoint=/Volumes/
zdata/home/test/Pictures
2008-09-05.19:55:55 zfs set mountpoint=/Volumes/ zdata/home/test/Public
2008-09-05.19:56:01 zfs set mountpoint=/Volumes/ zdata/home/test/Sites
2008-09-05.20:07:08 zpool import -f 14165958316872700719
2008-09-05.21:10:37 zfs create zdata/junk
2008-09-06.09:02:51 zpool import -f 14165958316872700719
2008-09-06.09:12:18 zpool import -f 14165958316872700719
2008-09-06.22:42:01 zpool import -f 14165958316872700719
2008-09-07.16:49:26 zfs set canmount=off zdata/junk
2008-09-07.16:50:05 zfs set mountpoint=/Volumes/zdata/junk zdata/junk
2008-09-07.21:54:32 zfs set mountpoint=/Volumes/zdata/home/test/Sites
zdata/home/test/Sites
2008-09-07.21:55:18 zfs set mountpoint=/Volumes/zdata/home/test/Public
zdata/home/test/Public
2008-09-07.21:55:58 zfs set mountpoint=/Volumes/zdata/home/test/Pictures
zdata/home/test/Pictures
2008-09-07.21:56:29 zfs set mountpoint=/Volumes/zdata/home/test/Music
zdata/home/test/Music
2008-09-07.21:56:55 zfs set mountpoint=/Volumes/zdata/home/test/Movies
zdata/home/test/Movies
2008-09-07.21:57:16 zfs set mountpoint=/Volumes/zdata/home/test/Library
zdata/home/test/Library
2008-09-07.21:57:46 zfs set
mountpoint=/Volumes/zdata/home/test/Downloads zdata/home/test/Downloads
2008-09-07.22:03:19 zpool import -f 14165958316872700719
2008-09-07.22:06:46 zfs set canmount=off zdata/home/test/Downloads
2008-09-07.22:06:57 zfs set canmount=off zdata/home/test/Library
2008-09-07.22:07:03 zfs set canmount=off zdata/home/test/Movies
2008-09-07.22:07:09 zfs set canmount=off zdata/home/test/Music
2008-09-07.22:07:18 zfs set canmount=off zdata/home/test/Pictures
2008-09-07.22:07:22 zfs set canmount=off zdata/home/test/Public
2008-09-07.22:07:27 zfs set canmount=off zdata/home/test/Sites
2008-09-07.22:07:34 zfs set canmount=off zdata/junk
2008-09-07.22:09:36 zfs set mountpoint=/Volumes/zdata/home zdata/home
2008-09-07.22:09:47 zfs set mountpoint=/Volumes/zdata/home
zdata/home/anika
2008-09-07.22:09:52 zfs set mountpoint=/Volumes/zdata/home
zdata/home/chris
2008-09-07.22:09:55 zfs set mountpoint=/Volumes/zdata/home
zdata/home/jaren
2008-09-07.22:10:01 zfs set mountpoint=/Volumes/zdata/home
zdata/home/severin
2008-09-07.22:10:07 zfs set mountpoint=/Volumes/zdata/home
zdata/home/test
2008-09-07.22:10:14 zfs set mountpoint=/Volumes/zdata/home
zdata/home/test/Desktop
2008-09-07.22:10:46 zfs set
mountpoint=/Volumes/zdata/home/test/Documents zdata/home/test/Documents
2008-09-07.22:11:10 zfs set mountpoint=/Volumes/zdata/home/test/Desktop
zdata/home/test/Desktop
2008-09-07.22:12:16 zfs set mountpoint=/Volumes/zdata/home/anika
zdata/home/anika
2008-09-07.22:12:31 zfs set mountpoint=/Volumes/zdata/home/chris
zdata/home/chris
2008-09-07.22:12:49 zfs set mountpoint=/Volumes/zdata/home/jaren
zdata/home/jaren
2008-09-07.22:13:06 zfs set mountpoint=/Volumes/zdata/home/severin
zdata/home/severin
2008-09-07.22:13:46 zfs set mountpoint=/Volumes/zdata/home/test
zdata/home/test
2008-09-07.22:15:22 zfs set canmount=on zdata/home
2008-09-07.22:15:27 zfs set canmount=on zdata/home/anika
2008-09-07.22:15:31 zfs set canmount=on zdata/home/chris
2008-09-07.22:15:36 zfs set canmount=on zdata/home/jaren
2008-09-07.22:15:42 zfs set canmount=on zdata/home/severin
2008-09-07.22:15:48 zfs set canmount=on zdata/home/test
2008-09-07.22:15:52 zfs set canmount=on zdata/home/test/Desktop
2008-09-07.22:15:58 zfs set canmount=on zdata/home/test/Documents
2008-09-07.22:16:09 zfs set canmount=on zdata/home/test/Downloads
2008-09-07.22:16:24 zfs set canmount=on zdata/home/test/Library
2008-09-07.22:16:30 zfs set canmount=on zdata/home/test/Movies
2008-09-07.22:16:51 zfs set canmount=on zdata/home/test/Music
2008-09-07.22:16:56 zfs set canmount=on zdata/home/test/Pictures
2008-09-07.22:17:04 zfs set canmount=on zdata/home/test/Public
2008-09-07.22:17:09 zfs set canmount=on zdata/home/test/Sites
2008-09-07.22:17:19 zfs set canmount=on zdata/junk
2008-09-07.22:18:42 zpool import -f 14165958316872700719


This message is intended only for the named recipient(s). If you are not one of the named recipients, you have received this message in error. Please notify the sender immediately and delete the message (along with its attachments) in a way so that no image of it is retrievable or accessible from the computer where this message has been displayed or from any network to which such computer is connected. Any use or disclosure of this message or its contents/attachments by anyone other than the intended recipient(s) is expressly prohibited and could result in civil or criminal liability.

From craig at mainstream.net  Mon Sep  8 13:34:41 2008
From: craig at mainstream.net (Craig Peterson)
Date: Mon, 8 Sep 2008 16:34:41 -0400
Subject: [zfs-discuss] Serious Problems with ZFS on Intel Mac Pro Copying
	Hundreds of Files
Message-ID: <21852A79-8C7D-4793-9F03-70311B1BFA1B@mainstream.net>

[Not sure if this made it through last time...  Craig.]

I'm running build 119 on an up-to-date Mac Pro (10.5.4).  I've set up  
a three-disk zfs raid-z:

$ zpool status
   pool: Internal
  state: ONLINE
status: The pool is formatted using an older on-disk format.  The pool  
can
         still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
         pool will no longer be accessible on older software versions.
  scrub: none requested
config:

         NAME         STATE     READ WRITE CKSUM
         Internal     ONLINE       0     0     0
           raidz1     ONLINE       0     0     0
             disk1s2  ONLINE       0     0     0
             disk2s2  ONLINE       0     0     0
             disk3s2  ONLINE       0     0     0

errors: No known data errors

I'm copying my music collection over to the pool, and it locks up  
trying to write to the zfs partition after an indeterminate amount of  
time.  At this point I have to reboot the machine by physically power  
cycling it in order to get back at the zfs partition and anything that  
tries to write to it gets locked up:

iTunes/iTunes Music/Dr. Jeffrey D. Thompson/Music for Brainwave  
Massage CD2/
iTunes/iTunes Music/Dr. Jeffrey D. Thompson/Music for Brainwave  
Massage CD2/03 Awakened Focus.m4a
    138083516 100%   70.69MB/s    0:00:01 (xfer#55201, to- 
check=12475/116940)
iTunes/iTunes Music/Dr. Jeffrey D. Thompson/Music for Brainwave  
Massage CD2/04 Tranquil Awareness.m4a
^C
rsync error: received SIGINT, SIGTERM, or SIGHUP (code 20) at / 
SourceCache/rsync/rsync-35.2/rsync/rsync.c(244) [sender=2.6.9]
rsync: writefd_unbuffered failed to write 133 bytes [generator]:  
Broken pipe (32)
bash-3.2# rsync -av --progress iTunes /Volumes/Internal/
building file list ...
116941 files to consider

At this point it hangs, as do any write operations to the partition.

I've only tried using rsync to copy the files, and rebooting the  
machine every hour or so isn't much of an option :-)

Suggestions?  I found a bug report of something similar that was said  
to be fixed in build 119... but it doesn't appear to be.

In order to make sure that the problem wasn't hardware related, I  
created a three-disk RAID-0 using Diskutil and MAC's filesystem and  
have been performing the same copy -- with no problems.  It really  
does appear to be a zfs problem.

Thanks,

Craig.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080908/5c5b9646/attachment.html 

From craig at mainstream.net  Mon Sep  8 13:45:24 2008
From: craig at mainstream.net (Craig Peterson)
Date: Mon, 8 Sep 2008 16:45:24 -0400
Subject: [zfs-discuss] Problems with ZFS on Intel Mac Pro Copying Hundreds
	of Files
In-Reply-To: <mailman.5.1220623211.4849.zfs-discuss@lists.macosforge.org>
References: <mailman.5.1220623211.4849.zfs-discuss@lists.macosforge.org>
Message-ID: <B726AAC2-E1E4-48EE-9B36-5C73F795937E@mainstream.net>

I had the same problem with pax...  Not sure why rsync 3.x would help,  
as that solution is usually aimed at a problem with the caching of  
copy tables.

I did verify that it wasn't a hardware problem by re-formatting the  
disks into a striped volume with Apple's FS on it and have been able  
to both pax and rsync over the entire contents of the drive I was  
trying to copy to zfs, but which kept getting locked-up hanging on  
writes,

Craig.

On Sep 5, 2008, at 10:00 AM, zfs-discuss-request at lists.macosforge.org  
wrote:

> Date: Thu, 04 Sep 2008 13:40:27 -0400
> From: Dillon Kass <lists at loveturtle.net>
> Subject: Re: [zfs-discuss] Problems with ZFS on Intel Mac Pro Copying
> 	Hundreds of Files
> To: zfs-discuss at lists.macosforge.org
> Message-ID: <48C01D8B.8050209 at loveturtle.net>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
> try using rsync 3.x
>
>
> ------------------------------

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080908/f12af411/attachment.html 

From alex.blewitt at gmail.com  Mon Sep  8 13:47:50 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Mon, 8 Sep 2008 21:47:50 +0100
Subject: [zfs-discuss] Problems with ZFS on Intel Mac Pro Copying
	Hundreds of Files
In-Reply-To: <B726AAC2-E1E4-48EE-9B36-5C73F795937E@mainstream.net>
References: <mailman.5.1220623211.4849.zfs-discuss@lists.macosforge.org>
	<B726AAC2-E1E4-48EE-9B36-5C73F795937E@mainstream.net>
Message-ID: <636fd28e0809081347t38efab80o9426d65accdce693@mail.gmail.com>

Use rsync 3. There are problems with rsync 2.x, possibly due to the
way it copies files. I've used rsync 3 and it works fine.

Alex

On Mon, Sep 8, 2008 at 9:45 PM, Craig Peterson <craig at mainstream.net> wrote:
> I had the same problem with pax...  Not sure why rsync 3.x would help, as
> that solution is usually aimed at a problem with the caching of copy tables.
> I did verify that it wasn't a hardware problem by re-formatting the disks
> into a striped volume with Apple's FS on it and have been able to both pax
> and rsync over the entire contents of the drive I was trying to copy to zfs,
> but which kept getting locked-up hanging on writes,
>
> Craig.
> On Sep 5, 2008, at 10:00 AM, zfs-discuss-request at lists.macosforge.org wrote:
>
> Date: Thu, 04 Sep 2008 13:40:27 -0400
> From: Dillon Kass <lists at loveturtle.net>
> Subject: Re: [zfs-discuss] Problems with ZFS on Intel Mac Pro Copying
> Hundreds of Files
> To: zfs-discuss at lists.macosforge.org
> Message-ID: <48C01D8B.8050209 at loveturtle.net>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
> try using rsync 3.x
>
>
> ------------------------------
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>

From bplist at thinkpink.com  Mon Sep  8 14:23:27 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Mon, 8 Sep 2008 14:23:27 -0700
Subject: [zfs-discuss] Problems with ZFS on Intel Mac Pro Copying
	Hundreds of Files
In-Reply-To: <636fd28e0809081347t38efab80o9426d65accdce693@mail.gmail.com>
References: <mailman.5.1220623211.4849.zfs-discuss@lists.macosforge.org>
	<B726AAC2-E1E4-48EE-9B36-5C73F795937E@mainstream.net>
	<636fd28e0809081347t38efab80o9426d65accdce693@mail.gmail.com>
Message-ID: <84B0B0A3-0C7B-4294-90E4-0B50507D292A@thinkpink.com>

I think the main point is that a user-level program (be it rsync 2, 3,  
pax, tar or whatever) shouldn't be able to hang the system.  I could  
easily panic the system with earlier versions of zfs by doing  
operations involving large numbers of small-ish files (like a big  
iPhoto Library), but I haven't had such problems recently.

bri

On Sep 8, 2008, at 1:47 PM, Alex Blewitt wrote:

> Use rsync 3. There are problems with rsync 2.x, possibly due to the
> way it copies files. I've used rsync 3 and it works fine.
>
> Alex
>
> On Mon, Sep 8, 2008 at 9:45 PM, Craig Peterson  
> <craig at mainstream.net> wrote:
>> I had the same problem with pax...  Not sure why rsync 3.x would  
>> help, as
>> that solution is usually aimed at a problem with the caching of  
>> copy tables.
>> I did verify that it wasn't a hardware problem by re-formatting the  
>> disks
>> into a striped volume with Apple's FS on it and have been able to  
>> both pax
>> and rsync over the entire contents of the drive I was trying to  
>> copy to zfs,
>> but which kept getting locked-up hanging on writes,
>>
>> Craig.
>> On Sep 5, 2008, at 10:00 AM, zfs-discuss- 
>> request at lists.macosforge.org wrote:
>>
>> Date: Thu, 04 Sep 2008 13:40:27 -0400
>> From: Dillon Kass <lists at loveturtle.net>
>> Subject: Re: [zfs-discuss] Problems with ZFS on Intel Mac Pro Copying
>> Hundreds of Files
>> To: zfs-discuss at lists.macosforge.org
>> Message-ID: <48C01D8B.8050209 at loveturtle.net>
>> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>>
>> try using rsync 3.x
>>
>>
>> ------------------------------
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From alex.blewitt at gmail.com  Mon Sep  8 15:06:53 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Mon, 8 Sep 2008 23:06:53 +0100
Subject: [zfs-discuss] Problems with ZFS on Intel Mac Pro Copying
	Hundreds of Files
In-Reply-To: <84B0B0A3-0C7B-4294-90E4-0B50507D292A@thinkpink.com>
References: <mailman.5.1220623211.4849.zfs-discuss@lists.macosforge.org>
	<B726AAC2-E1E4-48EE-9B36-5C73F795937E@mainstream.net>
	<636fd28e0809081347t38efab80o9426d65accdce693@mail.gmail.com>
	<84B0B0A3-0C7B-4294-90E4-0B50507D292A@thinkpink.com>
Message-ID: <636fd28e0809081506t241648f1mf578c4b2d659409d@mail.gmail.com>

Right, that's because there were problems with ZFS and mmap'ing files
in earlier releases, and those programs that used mmap didn't work
well. I believe that the current and previous release had mmap support
working OK, but there are still a few bugs (after all, it is
pre-release software). So some software tickles bugs, and some don't.
The ones that tickle bugs cause hangs :-)

Alex

On Mon, Sep 8, 2008 at 10:23 PM, Brian Pinkerton <bplist at thinkpink.com> wrote:
> I think the main point is that a user-level program (be it rsync 2, 3,
> pax, tar or whatever) shouldn't be able to hang the system.  I could
> easily panic the system with earlier versions of zfs by doing
> operations involving large numbers of small-ish files (like a big
> iPhoto Library), but I haven't had such problems recently.
>
> bri
>
> On Sep 8, 2008, at 1:47 PM, Alex Blewitt wrote:
>
>> Use rsync 3. There are problems with rsync 2.x, possibly due to the
>> way it copies files. I've used rsync 3 and it works fine.
>>
>> Alex
>>
>> On Mon, Sep 8, 2008 at 9:45 PM, Craig Peterson
>> <craig at mainstream.net> wrote:
>>> I had the same problem with pax...  Not sure why rsync 3.x would
>>> help, as
>>> that solution is usually aimed at a problem with the caching of
>>> copy tables.
>>> I did verify that it wasn't a hardware problem by re-formatting the
>>> disks
>>> into a striped volume with Apple's FS on it and have been able to
>>> both pax
>>> and rsync over the entire contents of the drive I was trying to
>>> copy to zfs,
>>> but which kept getting locked-up hanging on writes,
>>>
>>> Craig.
>>> On Sep 5, 2008, at 10:00 AM, zfs-discuss-
>>> request at lists.macosforge.org wrote:
>>>
>>> Date: Thu, 04 Sep 2008 13:40:27 -0400
>>> From: Dillon Kass <lists at loveturtle.net>
>>> Subject: Re: [zfs-discuss] Problems with ZFS on Intel Mac Pro Copying
>>> Hundreds of Files
>>> To: zfs-discuss at lists.macosforge.org
>>> Message-ID: <48C01D8B.8050209 at loveturtle.net>
>>> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>>>
>>> try using rsync 3.x
>>>
>>>
>>> ------------------------------
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

From caronni at gmail.com  Mon Sep  8 23:37:27 2008
From: caronni at gmail.com (Germano Caronni)
Date: Tue, 9 Sep 2008 08:37:27 +0200
Subject: [zfs-discuss] Serious Problems with ZFS on Intel Mac Pro
	Copying Hundreds of Files
In-Reply-To: <21852A79-8C7D-4793-9F03-70311B1BFA1B@mainstream.net>
References: <21852A79-8C7D-4793-9F03-70311B1BFA1B@mainstream.net>
Message-ID: <327b821f0809082337j7f753026o40f5b9d5161d8799@mail.gmail.com>

You are seeing what I saw. Still waiting for a version after -119 which
might fix this.
Germano

On Mon, Sep 8, 2008 at 22:34, Craig Peterson <craig at mainstream.net> wrote:

> [Not sure if this made it through last time...  Craig.]
>
> I'm running build 119 on an up-to-date Mac Pro (10.5.4).  I've set up a
> three-disk zfs raid-z:
> $ zpool status
>   pool: Internal
>  state: ONLINE
> status: The pool is formatted using an older on-disk format.  The pool can
>         still be used, but some features are unavailable.
> action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
>         pool will no longer be accessible on older software versions.
>  scrub: none requested
> config:
>
>         NAME         STATE     READ WRITE CKSUM
>         Internal     ONLINE       0     0     0
>           raidz1     ONLINE       0     0     0
>             disk1s2  ONLINE       0     0     0
>             disk2s2  ONLINE       0     0     0
>             disk3s2  ONLINE       0     0     0
>
> errors: No known data errors
>
> I'm copying my music collection over to the pool, and it locks up trying to
> write to the zfs partition after an indeterminate amount of time.  At this
> point I have to reboot the machine by physically power cycling it in order
> to get back at the zfs partition and anything that tries to write to it gets
> locked up:
>
> iTunes/iTunes Music/Dr. Jeffrey D. Thompson/Music for Brainwave Massage
> CD2/
> iTunes/iTunes Music/Dr. Jeffrey D. Thompson/Music for Brainwave Massage
> CD2/03 Awakened Focus.m4a
>    138083516 100%   70.69MB/s    0:00:01 (xfer#55201,
> to-check=12475/116940)
> iTunes/iTunes Music/Dr. Jeffrey D. Thompson/Music for Brainwave Massage
> CD2/04 Tranquil Awareness.m4a
> ^C
> rsync error: received SIGINT, SIGTERM, or SIGHUP (code 20) at
> /SourceCache/rsync/rsync-35.2/rsync/rsync.c(244) [sender=2.6.9]
> rsync: writefd_unbuffered failed to write 133 bytes [generator]: Broken
> pipe (32)
> bash-3.2# rsync -av --progress iTunes /Volumes/Internal/
> building file list ...
> 116941 files to consider
>
> At this point it hangs, as do any write operations to the partition.
>
> I've only tried using rsync to copy the files, and rebooting the machine
> every hour or so isn't much of an option :-)
>
> Suggestions?  I found a bug report of something similar that was said to be
> fixed in build 119... but it doesn't appear to be.
>
> In order to make sure that the problem wasn't hardware related, I created a
> three-disk RAID-0 using Diskutil and MAC's filesystem and have been
> performing the same copy -- with no problems.  It really does appear to be a
> zfs problem.
>
> Thanks,
>
> Craig.
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080909/26830c24/attachment.html 

From werner.donne at re.be  Tue Sep  9 00:07:55 2008
From: werner.donne at re.be (=?ISO-8859-1?Q?Werner_Donn=E9?=)
Date: Tue, 9 Sep 2008 09:07:55 +0200
Subject: [zfs-discuss] Initialization of a pool during boot
Message-ID: <52204649-1CCD-4FD5-8D5C-863DD09032BE@re.be>

Hi,

There is something strange about the initialization of a raidz pool on
a set of three USB disks. I use ZFS 119 on a Mac Mini with Mac OS X  
10.5.4.
During the boot sequence the disks go on and off several times. I  
usually
have to boot two or three times before the pool works properly. In most
cases the pool is in a degraded state with one disk that is not  
available.
Strangely enough it is not always the same disk.

In other cases the pool is completely online, but very slow. Normally
there is always activity on all disks at the same time, but in this
case when two of them are very active the third is not or much less and
vice versa.

Once the pool is online in a normal way it is very fast and shows no
problems for the rest of the day.

Best regards,

Werner.
--
Werner Donn?  --  Re                                     http://www.pincette.biz
Engelbeekstraat 8                                               http://www.re.be
BE-3300 Tienen
tel: (+32) 486 425803	e-mail: werner.donne at re.be






From craig at mainstream.net  Tue Sep  9 08:38:14 2008
From: craig at mainstream.net (Craig Peterson)
Date: Tue, 9 Sep 2008 11:38:14 -0400
Subject: [zfs-discuss] Problems with ZFS on Intel Mac Pro Copying
	Hundreds of Files
In-Reply-To: <636fd28e0809081347t38efab80o9426d65accdce693@mail.gmail.com>
References: <mailman.5.1220623211.4849.zfs-discuss@lists.macosforge.org>
	<B726AAC2-E1E4-48EE-9B36-5C73F795937E@mainstream.net>
	<636fd28e0809081347t38efab80o9426d65accdce693@mail.gmail.com>
Message-ID: <01A92A06-8C2B-4636-ACF7-F7B086BCA49D@mainstream.net>

I reformatted the drives, re-created the pool, used the latest rsync,  
and voi-la!  Kind of an odd bug...

Thanks for your help!

Craig.

On Sep 8, 2008, at 4:47 PM, Alex Blewitt wrote:

> Use rsync 3. There are problems with rsync 2.x, possibly due to the
> way it copies files. I've used rsync 3 and it works fine.
>
> Alex
>
> On Mon, Sep 8, 2008 at 9:45 PM, Craig Peterson  
> <craig at mainstream.net> wrote:
>> I had the same problem with pax...  Not sure why rsync 3.x would  
>> help, as
>> that solution is usually aimed at a problem with the caching of  
>> copy tables.
>> I did verify that it wasn't a hardware problem by re-formatting the  
>> disks
>> into a striped volume with Apple's FS on it and have been able to  
>> both pax
>> and rsync over the entire contents of the drive I was trying to  
>> copy to zfs,
>> but which kept getting locked-up hanging on writes,
>>
>> Craig.
>> On Sep 5, 2008, at 10:00 AM, zfs-discuss- 
>> request at lists.macosforge.org wrote:
>>
>> Date: Thu, 04 Sep 2008 13:40:27 -0400
>> From: Dillon Kass <lists at loveturtle.net>
>> Subject: Re: [zfs-discuss] Problems with ZFS on Intel Mac Pro Copying
>> Hundreds of Files
>> To: zfs-discuss at lists.macosforge.org
>> Message-ID: <48C01D8B.8050209 at loveturtle.net>
>> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>>
>> try using rsync 3.x
>>
>>
>> ------------------------------
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>>


From oliver.oli+0815 at gmail.com  Thu Sep 11 05:45:16 2008
From: oliver.oli+0815 at gmail.com (Oliver Oli)
Date: Thu, 11 Sep 2008 14:45:16 +0200
Subject: [zfs-discuss] copies=2 striped pool. one disk died. rescue possible?
Message-ID: <66f535320809110545l587a5119h577c59d12d25b039@mail.gmail.com>

Luckily I had not experienced any data loss with ZFS, but sooner or
later hard disks die and I would like to be prepared.

My question is:

I have a striped pool

zpool create Tank disk1 disk2

then created a filesystem with the copies=2 option

zfs create -o copies=2 Tank/safedir

When one of the disks dies, can I recover the files in Tank/safedir?

From alex.blewitt at gmail.com  Thu Sep 11 05:57:54 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Thu, 11 Sep 2008 13:57:54 +0100
Subject: [zfs-discuss] copies=2 striped pool. one disk died. rescue
	possible?
In-Reply-To: <66f535320809110545l587a5119h577c59d12d25b039@mail.gmail.com>
References: <66f535320809110545l587a5119h577c59d12d25b039@mail.gmail.com>
Message-ID: <636fd28e0809110557k5439ea10p30469d4aad6ccd1e@mail.gmail.com>

Probably not. You're much better off, with two disks, creating a
mirror of the pool and using that. That way, you don't need to set
copies=2 for the file system.

Consider a big file, striped across A and B:

ABABABABA

If you write this twice you get:

ABABABABA
ABABABABA

So if you lose A, you lose exactly half of your file, despite still
having two copies of the B bits.

On the other hand, if you have a mirror, you get:
AAAAAAAAA AAAAAAAAA

So if you lose one disk, you're still good to go. Note that the amount
of space taken up is the same (roughly) if all file systems have pools
on.

If you want mixed (i.e. some striped, some mirrored) you can always
partition your disks and have one partition mirrored and the other
striped.

Alex

On Thu, Sep 11, 2008 at 1:45 PM, Oliver Oli <oliver.oli+0815 at gmail.com> wrote:
> Luckily I had not experienced any data loss with ZFS, but sooner or
> later hard disks die and I would like to be prepared.
>
> My question is:
>
> I have a striped pool
>
> zpool create Tank disk1 disk2
>
> then created a filesystem with the copies=2 option
>
> zfs create -o copies=2 Tank/safedir
>
> When one of the disks dies, can I recover the files in Tank/safedir?
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

From oliver.oli+0815 at gmail.com  Thu Sep 11 06:04:16 2008
From: oliver.oli+0815 at gmail.com (Oliver Oli)
Date: Thu, 11 Sep 2008 15:04:16 +0200
Subject: [zfs-discuss] copies=2 striped pool. one disk died. rescue
	possible?
In-Reply-To: <636fd28e0809110557k5439ea10p30469d4aad6ccd1e@mail.gmail.com>
References: <66f535320809110545l587a5119h577c59d12d25b039@mail.gmail.com>
	<636fd28e0809110557k5439ea10p30469d4aad6ccd1e@mail.gmail.com>
Message-ID: <66f535320809110604l12020023ye0dff0343b551f64@mail.gmail.com>

On Thu, Sep 11, 2008 at 2:57 PM, Alex Blewitt <alex.blewitt at gmail.com> wrote:
> If you want mixed (i.e. some striped, some mirrored) you can always
> partition your disks and have one partition mirrored and the other
> striped.
>
> Alex

Which I would do, if ZFS had the option to resize. I just cannot
predict how many GB I have to mirror and how much of the data is
non-critical. I'm looking for some flexible schema.

From alex.blewitt at gmail.com  Thu Sep 11 06:13:13 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Thu, 11 Sep 2008 14:13:13 +0100
Subject: [zfs-discuss] copies=2 striped pool. one disk died. rescue
	possible?
In-Reply-To: <66f535320809110604l12020023ye0dff0343b551f64@mail.gmail.com>
References: <66f535320809110545l587a5119h577c59d12d25b039@mail.gmail.com>
	<636fd28e0809110557k5439ea10p30469d4aad6ccd1e@mail.gmail.com>
	<66f535320809110604l12020023ye0dff0343b551f64@mail.gmail.com>
Message-ID: <636fd28e0809110613q12c46de7t632acac0725b33cd@mail.gmail.com>

Just mirror it all. Disk is cheap. That's the I in RAID, after all ...

Alex

On Thu, Sep 11, 2008 at 2:04 PM, Oliver Oli <oliver.oli+0815 at gmail.com> wrote:
> On Thu, Sep 11, 2008 at 2:57 PM, Alex Blewitt <alex.blewitt at gmail.com> wrote:
>> If you want mixed (i.e. some striped, some mirrored) you can always
>> partition your disks and have one partition mirrored and the other
>> striped.
>>
>> Alex
>
> Which I would do, if ZFS had the option to resize. I just cannot
> predict how many GB I have to mirror and how much of the data is
> non-critical. I'm looking for some flexible schema.
>

From bplist at thinkpink.com  Thu Sep 11 10:54:57 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Thu, 11 Sep 2008 10:54:57 -0700
Subject: [zfs-discuss] Finder eject vs. multiple pools
Message-ID: <8A9BC02F-4014-4518-9147-29689BFA7249@thinkpink.com>

After doing some backups today from a couple of ZFS file systems on a  
"permanent" pool, I tried using the Finder to eject the backup disks  
(a couple of 1TB drives with a single striped ZFS pool) and got the  
Finder message:

	The device containing "Backup2" also contains 5 other volumes that  
will not be ejected.  Are you sure...

Looks like the Finder somehow considers all ZFS filesystems to be on  
the same device, even if they're on completely separate devices/pools.

I wasn't brave enough to just say "yes", but "zfs export -f Backup2"  
worked just fine. :)

bri


From jason at jasonrm.net  Fri Sep 12 02:42:47 2008
From: jason at jasonrm.net (Jason Richard McNeil)
Date: Fri, 12 Sep 2008 02:42:47 -0700
Subject: [zfs-discuss] copies=2 striped pool. one disk died. rescue
	possible?
In-Reply-To: <66f535320809110604l12020023ye0dff0343b551f64@mail.gmail.com>
References: <66f535320809110545l587a5119h577c59d12d25b039@mail.gmail.com>
	<636fd28e0809110557k5439ea10p30469d4aad6ccd1e@mail.gmail.com>
	<66f535320809110604l12020023ye0dff0343b551f64@mail.gmail.com>
Message-ID: <3D16C89B-9A1E-424A-A6C5-F2AF2316BD03@jasonrm.net>

Just to follow up on this idea of using copies=2 instead of mirroring.

I just did a small test using local files using the exact scenario you  
suggested and in my opinion, this is a very, very bad idea.

If you have parts of one drive that fail, maybe, just maybe you will  
be lucky and recover what you need, but if you have a complete drive  
failure... at least in the test I did this evening, your data will be  
gone. ZFS will treat it as a severe failure and will halt the OS to  
prevent further damage. There really just isn't any guarantee that  
your data can survive a hard disk failure except when using mirroring  
or raidz. The copies option seems best suited to handle cases where a  
single drive might have read/write failures, but if the drive is  
pulled out of the system or dies completely, I don't expect the copies  
option to help you out at all. You might be able to get this to work,  
but as far as I've ever seen, this is not a good idea.

I could be wrong in what i've said so far, but here is the basic  
point... it's a very bad idea if this is any sort of data you care  
about. What I've defined as non-critical data in the past always turns  
out to be more critical later on down the road. Maybe that is just me...

Here is my recommendation, not knowing much more about your situation.
Use two drives of about equal size, mirror them and start your pool  
with those two drives. Now if you need more storage you can either add  
more drives, two at a time, mirroring each and adding it to the pool.  
When you can't add more sets of drives, replace the single smallest  
drive with a larger one and resilver. Once the resilver is complete,  
replace the second drive in that same mirror set with a larger drive  
and resilver again. Upon completion your pool will resize as it now  
realizes there is more space available in that mirror set. Usually you  
want to add more mirror sets before replacing smaller drives, but that  
is of course your call based upon your circumstances.

This is what I am running now, I used to run raidz, but have found a  
set of three two-way mirrors provide me with better data security.
If luck is on my side I could even have up to three drives fail (one  
drive out of each mirror) and not suffer any data loss.

mypool
   mirror
     disk6s2
     disk8s2
   mirror
     disk5s2
     disk7s2
   mirror
     disk0s2
     disk3s2

Jason R. McNeil

On Sep 11, 2008, at 6:04 AM, Oliver Oli wrote:

> On Thu, Sep 11, 2008 at 2:57 PM, Alex Blewitt  
> <alex.blewitt at gmail.com> wrote:
>> If you want mixed (i.e. some striped, some mirrored) you can always
>> partition your disks and have one partition mirrored and the other
>> striped.
>>
>> Alex
>
> Which I would do, if ZFS had the option to resize. I just cannot
> predict how many GB I have to mirror and how much of the data is
> non-critical. I'm looking for some flexible schema.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From William.Winnett at Sun.COM  Fri Sep 12 04:32:20 2008
From: William.Winnett at Sun.COM (Bill Winnett)
Date: Fri, 12 Sep 2008 07:32:20 -0400
Subject: [zfs-discuss] A new Panic ""mutex_enter: locking against
 myself!"@/Volumes/pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/zfs_context.c:449"
Message-ID: <CCF853FF-B0B4-4747-9156-E868B3321BC0@sun.com>



I was "ditto"ing all the files from a hfs drive to a new zfs drive.   
That ditto process had been running for about an hour(it is a  
300Gig).   I had already run over night a ditto to move all my photo's  
from a hfs drive to another new zfs drive.  I had done some directory  
pruning(delete's) and here were about 8000 individual files that  
needed to be deleted.  So, I opened a terminal and went to .Trashes/ 
501 and ran rm -rf *, after a minute or so, my system paniced.

---
Fri Sep 12 07:21:07 2008
panic(cpu 0 caller 0x00C75103): "mutex_enter: locking against  
myself!"@/Volumes/pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/ 
zfs/zfs_context.c:449
Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x5c44b948 : 0x12b0fa (0x4592a4 0x5c44b97c 0x133243 0x0)
0x5c44b998 : 0xc75103 (0xccfff8 0x7c0886c 0x5c44b9d8 0x174672)
0x5c44b9b8 : 0xc220fc (0x89752b38 0x1 0x5c44b9e8 0xb7fa94c)
0x5c44b9f8 : 0x1f4170 (0x5c44ba14 0x8 0x5c44ba1c 0x0)
0x5c44ba38 : 0x1db7f4 (0xcb2fb00 0xe754964 0x0 0x2)
0x5c44ba88 : 0x1dba10 (0xcb2fb00 0x1 0x5c44bad8 0x1f6057)
0x5c44bad8 : 0x1dbcb8 (0x0 0xe754964 0x5c44bb18 0x88791c18)
0x5c44baf8 : 0x1dbcee (0xcb2fb00 0x1710008 0x5c44bb28 0x7c08800)
0x5c44bb18 : 0xc79881 (0xcb2fb00 0x13120 0x0 0x0)
0x5c44bbc8 : 0xc94e2c (0x5c44be1c 0x0 0x0 0x5c44be1c)
0x5c44be38 : 0x1f4065 (0x5c44be54 0xb730240 0x5c44be98 0x1d8c06)
0x5c44be98 : 0x1e853f (0xb730240 0x5c44bf2c 0x5c44bee8 0x14)
0x5c44bf78 : 0x3ddde2 (0x7130970 0xe754860 0xe7548a4 0x0)
0x5c44bfc8 : 0x19f2c3 (0xe6395e0 0x0 0x1a20b5 0x76364f0)
No mapping exists for frame pointer
Backtrace terminated-invalid frame pointer 0xb0e8e858
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0xc1a000->0xce5fff

BSD process name corresponding to current thread: Finder

Mac OS version:
9E17

Kernel version:
Darwin Kernel Version 9.4.0: Mon Jun  9 19:30:53 PDT 2008;  
root:xnu-1228.5.20~1/RELEASE_I386
System model name: iMac7,1 (Mac-F4238CC8)
----
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080912/fb3f1c9c/attachment.html 

From William.Winnett at Sun.COM  Fri Sep 12 05:03:06 2008
From: William.Winnett at Sun.COM (Bill Winnett)
Date: Fri, 12 Sep 2008 08:03:06 -0400
Subject: [zfs-discuss] ditto process hang from hfs to a zfs drive
Message-ID: <BAC5EA10-D315-4A76-BC46-5B1603556AE8@sun.com>



I recently had a ditto process hang my system.    The process was  
moving a photo collection from a old hfs drive to a new ZFS drive.

Here is all the info I could collect before the inevitable end and  
power-off.

---
Sampling process 290 for 3 seconds with 1 millisecond of run time  
between samples
Sampling completed, processing symbols...
Analysis of sampling ditto (pid 290) every 1 millisecond
Call graph:
     2712 Thread_2503
       2712 0x1b52
         2712 0x2ee4
           2712 BOMCopierCopyWithOptions
             2712 _copyFromDirToDir
               2712 _copyDir
                 2712 _copyFromDirToDir
                   2712 _copyDir
                     2712 _copyFromDirToDir
                       2712 _copyDir
                         2712 _copyFromDirToDir
                           2712 _copyFile
                             2712 _executeBomCopySpecification
                               2712 _BOMFileDirectWrite
                                 2712 write$UNIX2003
                                   2712 write$UNIX2003

Total number in stack (recursive counted multiple, when >=5):

Sort by top of stack, same collapsed (when >= 5):
         write$UNIX2003        2712
Sample analysis of process 290 written to file /dev/stdout



open files

/Volumes/Photos_232G
/usr/bin/ditto
/usr/lib/dyld
/private/var/db/dyld/dyld_shared_cache_i386
/dev/ttys000
/dev/ttys000
/dev/ttys000
/Volumes/Photos-e
/Volumes/Photos-e/Home Photo's
/Volumes/Photos-e/Home Photo's/2005
/Volumes/Photos-e/Home Photo's/2005/2005-12-25 - Christmas
/Volumes/Photos-e/Home Photo's/2005/2005-12-25 - Christmas/CRW_8192.CRW
/Volumes/Photos_232G/Home Photo's/2005/2005-12-25 - Christmas/.BC.Si7KIz

statistics

threads:1
ports:13
CPU Time:2.24.90
Context Switches:1196561
faults:612928
page ins:41
Mach Messages In:41
Mach Messages Out:64
Mach System Calls:64
Unix System Calls:1487774



memory

Real Memory Size:628.00 KB
Virtual Memory Size:17.64 MB
Shared memory Size:184.00 KB
Private Memory Size:200.00 KB
Virtual Private Memory:9.16 MB


what shows in the ditto terminel

copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8182.jpg ... 162865 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8182.THM ... 9549 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8183.CRW ... 7177486 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8183.jpg ... 151578 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8183.THM ... 10069 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8184.CRW ... 6871302 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8184.jpg ... 85452 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8184.THM ... 7789 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8185.CRW ... 7176994 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8185.jpg ... 125237 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8185.THM ... 9181 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8186.CRW ... 8875164 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8186.jpg ... 238386 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8186.THM ... 13311 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8187.CRW ... 7382040 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8187.jpg ... 180311 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8187.THM ... 11341 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8188.CRW ... 7535602 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8188.jpg ... 222185 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8188.THM ... 11964 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8189.CRW ... 7065884 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8189.jpg ... 166191 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8189.THM ... 10229 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8190.CRW ... 6787606 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8190.jpg ... 136645 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8190.THM ... 10299 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8191.CRW ... 8307548 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8191.jpg ... 204408 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
CRW_8191.THM ... 11013 bytes
copying file ./Home Photo's/2005/2005-12-25 - Christmas/CRW_8192.CRW ...
-----



Once the process hung, access to the zfs drive was blocked, any  
process including finder that attempted access would hang.  I could  
not kill/restart finder.  Kill -9 did not work.  System shutdown was  
blocked, etc.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080912/9638c521/attachment-0001.html 

From ndellofano at apple.com  Fri Sep 12 09:03:12 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 12 Sep 2008 09:03:12 -0700
Subject: [zfs-discuss] ditto process hang from hfs to a zfs drive
In-Reply-To: <BAC5EA10-D315-4A76-BC46-5B1603556AE8@sun.com>
References: <BAC5EA10-D315-4A76-BC46-5B1603556AE8@sun.com>
Message-ID: <C11883C7-DEDE-4436-AD3D-4BD6CC1A8E4D@apple.com>

hey Bill,
What Leopard and zfs build are you running?

Noel

On Sep 12, 2008, at 5:03 AM, Bill Winnett wrote:

>
>
> I recently had a ditto process hang my system.    The process was  
> moving a photo collection from a old hfs drive to a new ZFS drive.
>
> Here is all the info I could collect before the inevitable end and  
> power-off.
>
> ---
> Sampling process 290 for 3 seconds with 1 millisecond of run time  
> between samples
> Sampling completed, processing symbols...
> Analysis of sampling ditto (pid 290) every 1 millisecond
> Call graph:
>     2712 Thread_2503
>       2712 0x1b52
>         2712 0x2ee4
>           2712 BOMCopierCopyWithOptions
>             2712 _copyFromDirToDir
>               2712 _copyDir
>                 2712 _copyFromDirToDir
>                   2712 _copyDir
>                     2712 _copyFromDirToDir
>                       2712 _copyDir
>                         2712 _copyFromDirToDir
>                           2712 _copyFile
>                             2712 _executeBomCopySpecification
>                               2712 _BOMFileDirectWrite
>                                 2712 write$UNIX2003
>                                   2712 write$UNIX2003
>
> Total number in stack (recursive counted multiple, when >=5):
>
> Sort by top of stack, same collapsed (when >= 5):
>         write$UNIX2003        2712
> Sample analysis of process 290 written to file /dev/stdout
>
>
>
> open files
>
> /Volumes/Photos_232G
> /usr/bin/ditto
> /usr/lib/dyld
> /private/var/db/dyld/dyld_shared_cache_i386
> /dev/ttys000
> /dev/ttys000
> /dev/ttys000
> /Volumes/Photos-e
> /Volumes/Photos-e/Home Photo's
> /Volumes/Photos-e/Home Photo's/2005
> /Volumes/Photos-e/Home Photo's/2005/2005-12-25 - Christmas
> /Volumes/Photos-e/Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8192.CRW
> /Volumes/Photos_232G/Home Photo's/2005/2005-12-25 -  
> Christmas/.BC.Si7KIz
>
> statistics
>
> threads:1
> ports:13
> CPU Time:2.24.90
> Context Switches:1196561
> faults:612928
> page ins:41
> Mach Messages In:41
> Mach Messages Out:64
> Mach System Calls:64
> Unix System Calls:1487774
>
>
>
> memory
>
> Real Memory Size:628.00 KB
> Virtual Memory Size:17.64 MB
> Shared memory Size:184.00 KB
> Private Memory Size:200.00 KB
> Virtual Private Memory:9.16 MB
>
>
> what shows in the ditto terminel
>
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8182.jpg ... 162865 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8182.THM ... 9549 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8183.CRW ... 7177486 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8183.jpg ... 151578 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8183.THM ... 10069 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8184.CRW ... 6871302 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8184.jpg ... 85452 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8184.THM ... 7789 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8185.CRW ... 7176994 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8185.jpg ... 125237 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8185.THM ... 9181 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8186.CRW ... 8875164 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8186.jpg ... 238386 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8186.THM ... 13311 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8187.CRW ... 7382040 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8187.jpg ... 180311 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8187.THM ... 11341 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8188.CRW ... 7535602 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8188.jpg ... 222185 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8188.THM ... 11964 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8189.CRW ... 7065884 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8189.jpg ... 166191 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8189.THM ... 10229 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8190.CRW ... 6787606 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8190.jpg ... 136645 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8190.THM ... 10299 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8191.CRW ... 8307548 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8191.jpg ... 204408 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8191.THM ... 11013 bytes
> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
> CRW_8192.CRW ...
> -----
>
>
>
> Once the process hung, access to the zfs drive was blocked, any  
> process including finder that attempted access would hang.  I could  
> not kill/restart finder.  Kill -9 did not work.  System shutdown was  
> blocked, etc.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080912/a7f68967/attachment.html 

From William.Winnett at Sun.COM  Fri Sep 12 09:11:32 2008
From: William.Winnett at Sun.COM (Bill Winnett)
Date: Fri, 12 Sep 2008 12:11:32 -0400
Subject: [zfs-discuss] ditto process hang from hfs to a zfs drive
In-Reply-To: <C11883C7-DEDE-4436-AD3D-4BD6CC1A8E4D@apple.com>
References: <BAC5EA10-D315-4A76-BC46-5B1603556AE8@sun.com>
	<C11883C7-DEDE-4436-AD3D-4BD6CC1A8E4D@apple.com>
Message-ID: <B1F54A30-522F-42C6-853A-F205AC900AE4@Sun.COM>


On Sep 12, 2008, at 12:03 PM, No?l Dellofano wrote:

> hey Bill,
> What Leopard and zfs build are you running?
10.5.4  build 9E17 of Leopard and build 119 of ZFS
>
> Noel
>
> On Sep 12, 2008, at 5:03 AM, Bill Winnett wrote:
>
>>
>>
>> I recently had a ditto process hang my system.    The process was  
>> moving a photo collection from a old hfs drive to a new ZFS drive.
>>
>> Here is all the info I could collect before the inevitable end and  
>> power-off.
>>
>> ---
>> Sampling process 290 for 3 seconds with 1 millisecond of run time  
>> between samples
>> Sampling completed, processing symbols...
>> Analysis of sampling ditto (pid 290) every 1 millisecond
>> Call graph:
>>     2712 Thread_2503
>>       2712 0x1b52
>>         2712 0x2ee4
>>           2712 BOMCopierCopyWithOptions
>>             2712 _copyFromDirToDir
>>               2712 _copyDir
>>                 2712 _copyFromDirToDir
>>                   2712 _copyDir
>>                     2712 _copyFromDirToDir
>>                       2712 _copyDir
>>                         2712 _copyFromDirToDir
>>                           2712 _copyFile
>>                             2712 _executeBomCopySpecification
>>                               2712 _BOMFileDirectWrite
>>                                 2712 write$UNIX2003
>>                                   2712 write$UNIX2003
>>
>> Total number in stack (recursive counted multiple, when >=5):
>>
>> Sort by top of stack, same collapsed (when >= 5):
>>         write$UNIX2003        2712
>> Sample analysis of process 290 written to file /dev/stdout
>>
>>
>>
>> open files
>>
>> /Volumes/Photos_232G
>> /usr/bin/ditto
>> /usr/lib/dyld
>> /private/var/db/dyld/dyld_shared_cache_i386
>> /dev/ttys000
>> /dev/ttys000
>> /dev/ttys000
>> /Volumes/Photos-e
>> /Volumes/Photos-e/Home Photo's
>> /Volumes/Photos-e/Home Photo's/2005
>> /Volumes/Photos-e/Home Photo's/2005/2005-12-25 - Christmas
>> /Volumes/Photos-e/Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8192.CRW
>> /Volumes/Photos_232G/Home Photo's/2005/2005-12-25 -  
>> Christmas/.BC.Si7KIz
>>
>> statistics
>>
>> threads:1
>> ports:13
>> CPU Time:2.24.90
>> Context Switches:1196561
>> faults:612928
>> page ins:41
>> Mach Messages In:41
>> Mach Messages Out:64
>> Mach System Calls:64
>> Unix System Calls:1487774
>>
>>
>>
>> memory
>>
>> Real Memory Size:628.00 KB
>> Virtual Memory Size:17.64 MB
>> Shared memory Size:184.00 KB
>> Private Memory Size:200.00 KB
>> Virtual Private Memory:9.16 MB
>>
>>
>> what shows in the ditto terminel
>>
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8182.jpg ... 162865 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8182.THM ... 9549 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8183.CRW ... 7177486 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8183.jpg ... 151578 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8183.THM ... 10069 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8184.CRW ... 6871302 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8184.jpg ... 85452 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8184.THM ... 7789 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8185.CRW ... 7176994 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8185.jpg ... 125237 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8185.THM ... 9181 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8186.CRW ... 8875164 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8186.jpg ... 238386 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8186.THM ... 13311 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8187.CRW ... 7382040 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8187.jpg ... 180311 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8187.THM ... 11341 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8188.CRW ... 7535602 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8188.jpg ... 222185 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8188.THM ... 11964 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8189.CRW ... 7065884 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8189.jpg ... 166191 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8189.THM ... 10229 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8190.CRW ... 6787606 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8190.jpg ... 136645 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8190.THM ... 10299 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8191.CRW ... 8307548 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8191.jpg ... 204408 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8191.THM ... 11013 bytes
>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>> CRW_8192.CRW ...
>> -----
>>
>>
>>
>> Once the process hung, access to the zfs drive was blocked, any  
>> process including finder that attempted access would hang.  I could  
>> not kill/restart finder.  Kill -9 did not work.  System shutdown  
>> was blocked, etc.
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080912/921b3247/attachment-0001.html 

From ndellofano at apple.com  Fri Sep 12 18:25:55 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 12 Sep 2008 18:25:55 -0700
Subject: [zfs-discuss] ditto process hang from hfs to a zfs drive
In-Reply-To: <B1F54A30-522F-42C6-853A-F205AC900AE4@Sun.COM>
References: <BAC5EA10-D315-4A76-BC46-5B1603556AE8@sun.com>
	<C11883C7-DEDE-4436-AD3D-4BD6CC1A8E4D@apple.com>
	<B1F54A30-522F-42C6-853A-F205AC900AE4@Sun.COM>
Message-ID: <EC968068-F40D-4008-B7AB-37697F32AF5A@apple.com>

hmm... lemme dig a little.  When you say "old" HFS drive, how old is  
it?  Tiger? Jaguar? just curious, though I"m sure it's somehow my  
fault :)
I haven't seen a ditto hang a machine since we fixed the hang in 117....

Noel

On Sep 12, 2008, at 9:11 AM, Bill Winnett wrote:

>
> On Sep 12, 2008, at 12:03 PM, No?l Dellofano wrote:
>
>> hey Bill,
>> What Leopard and zfs build are you running?
> 10.5.4  build 9E17 of Leopard and build 119 of ZFS
>>
>> Noel
>>
>> On Sep 12, 2008, at 5:03 AM, Bill Winnett wrote:
>>
>>>
>>>
>>> I recently had a ditto process hang my system.    The process was  
>>> moving a photo collection from a old hfs drive to a new ZFS drive.
>>>
>>> Here is all the info I could collect before the inevitable end and  
>>> power-off.
>>>
>>> ---
>>> Sampling process 290 for 3 seconds with 1 millisecond of run time  
>>> between samples
>>> Sampling completed, processing symbols...
>>> Analysis of sampling ditto (pid 290) every 1 millisecond
>>> Call graph:
>>>     2712 Thread_2503
>>>       2712 0x1b52
>>>         2712 0x2ee4
>>>           2712 BOMCopierCopyWithOptions
>>>             2712 _copyFromDirToDir
>>>               2712 _copyDir
>>>                 2712 _copyFromDirToDir
>>>                   2712 _copyDir
>>>                     2712 _copyFromDirToDir
>>>                       2712 _copyDir
>>>                         2712 _copyFromDirToDir
>>>                           2712 _copyFile
>>>                             2712 _executeBomCopySpecification
>>>                               2712 _BOMFileDirectWrite
>>>                                 2712 write$UNIX2003
>>>                                   2712 write$UNIX2003
>>>
>>> Total number in stack (recursive counted multiple, when >=5):
>>>
>>> Sort by top of stack, same collapsed (when >= 5):
>>>         write$UNIX2003        2712
>>> Sample analysis of process 290 written to file /dev/stdout
>>>
>>>
>>>
>>> open files
>>>
>>> /Volumes/Photos_232G
>>> /usr/bin/ditto
>>> /usr/lib/dyld
>>> /private/var/db/dyld/dyld_shared_cache_i386
>>> /dev/ttys000
>>> /dev/ttys000
>>> /dev/ttys000
>>> /Volumes/Photos-e
>>> /Volumes/Photos-e/Home Photo's
>>> /Volumes/Photos-e/Home Photo's/2005
>>> /Volumes/Photos-e/Home Photo's/2005/2005-12-25 - Christmas
>>> /Volumes/Photos-e/Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8192.CRW
>>> /Volumes/Photos_232G/Home Photo's/2005/2005-12-25 -  
>>> Christmas/.BC.Si7KIz
>>>
>>> statistics
>>>
>>> threads:1
>>> ports:13
>>> CPU Time:2.24.90
>>> Context Switches:1196561
>>> faults:612928
>>> page ins:41
>>> Mach Messages In:41
>>> Mach Messages Out:64
>>> Mach System Calls:64
>>> Unix System Calls:1487774
>>>
>>>
>>>
>>> memory
>>>
>>> Real Memory Size:628.00 KB
>>> Virtual Memory Size:17.64 MB
>>> Shared memory Size:184.00 KB
>>> Private Memory Size:200.00 KB
>>> Virtual Private Memory:9.16 MB
>>>
>>>
>>> what shows in the ditto terminel
>>>
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8182.jpg ... 162865 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8182.THM ... 9549 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8183.CRW ... 7177486 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8183.jpg ... 151578 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8183.THM ... 10069 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8184.CRW ... 6871302 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8184.jpg ... 85452 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8184.THM ... 7789 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8185.CRW ... 7176994 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8185.jpg ... 125237 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8185.THM ... 9181 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8186.CRW ... 8875164 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8186.jpg ... 238386 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8186.THM ... 13311 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8187.CRW ... 7382040 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8187.jpg ... 180311 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8187.THM ... 11341 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8188.CRW ... 7535602 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8188.jpg ... 222185 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8188.THM ... 11964 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8189.CRW ... 7065884 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8189.jpg ... 166191 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8189.THM ... 10229 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8190.CRW ... 6787606 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8190.jpg ... 136645 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8190.THM ... 10299 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8191.CRW ... 8307548 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8191.jpg ... 204408 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8191.THM ... 11013 bytes
>>> copying file ./Home Photo's/2005/2005-12-25 - Christmas/ 
>>> CRW_8192.CRW ...
>>> -----
>>>
>>>
>>>
>>> Once the process hung, access to the zfs drive was blocked, any  
>>> process including finder that attempted access would hang.  I  
>>> could not kill/restart finder.  Kill -9 did not work.  System  
>>> shutdown was blocked, etc.
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080912/da24386c/attachment.html 

From ndellofano at apple.com  Fri Sep 12 18:34:23 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 12 Sep 2008 18:34:23 -0700
Subject: [zfs-discuss] copies=2 striped pool. one disk died. rescue
	possible?
In-Reply-To: <3D16C89B-9A1E-424A-A6C5-F2AF2316BD03@jasonrm.net>
References: <66f535320809110545l587a5119h577c59d12d25b039@mail.gmail.com>
	<636fd28e0809110557k5439ea10p30469d4aad6ccd1e@mail.gmail.com>
	<66f535320809110604l12020023ye0dff0343b551f64@mail.gmail.com>
	<3D16C89B-9A1E-424A-A6C5-F2AF2316BD03@jasonrm.net>
Message-ID: <5F05DA39-7F14-4BC4-A604-A939DF0075AE@apple.com>

Jason is correct below.  We added copies for people that were  
restricted to only being able to run one drive, so if you've got some  
really important data but only one drive (travelling with a laptop?),  
we'll space these copies out on the disk as much as we can since most  
drive failures are partial drive failures.  Then you're odds are  
greater then you are able to recover the file(s) either saving the  
disk yourself or using a salvage program such as Disk Warrior.


Noel

On Sep 12, 2008, at 2:42 AM, Jason Richard McNeil wrote:

> Just to follow up on this idea of using copies=2 instead of mirroring.
>
> I just did a small test using local files using the exact scenario you
> suggested and in my opinion, this is a very, very bad idea.
>
> If you have parts of one drive that fail, maybe, just maybe you will
> be lucky and recover what you need, but if you have a complete drive
> failure... at least in the test I did this evening, your data will be
> gone. ZFS will treat it as a severe failure and will halt the OS to
> prevent further damage. There really just isn't any guarantee that
> your data can survive a hard disk failure except when using mirroring
> or raidz. The copies option seems best suited to handle cases where a
> single drive might have read/write failures, but if the drive is
> pulled out of the system or dies completely, I don't expect the copies
> option to help you out at all. You might be able to get this to work,
> but as far as I've ever seen, this is not a good idea.
>
> I could be wrong in what i've said so far, but here is the basic
> point... it's a very bad idea if this is any sort of data you care
> about. What I've defined as non-critical data in the past always turns
> out to be more critical later on down the road. Maybe that is just  
> me...
>
> Here is my recommendation, not knowing much more about your situation.
> Use two drives of about equal size, mirror them and start your pool
> with those two drives. Now if you need more storage you can either add
> more drives, two at a time, mirroring each and adding it to the pool.
> When you can't add more sets of drives, replace the single smallest
> drive with a larger one and resilver. Once the resilver is complete,
> replace the second drive in that same mirror set with a larger drive
> and resilver again. Upon completion your pool will resize as it now
> realizes there is more space available in that mirror set. Usually you
> want to add more mirror sets before replacing smaller drives, but that
> is of course your call based upon your circumstances.
>
> This is what I am running now, I used to run raidz, but have found a
> set of three two-way mirrors provide me with better data security.
> If luck is on my side I could even have up to three drives fail (one
> drive out of each mirror) and not suffer any data loss.
>
> mypool
>   mirror
>     disk6s2
>     disk8s2
>   mirror
>     disk5s2
>     disk7s2
>   mirror
>     disk0s2
>     disk3s2
>
> Jason R. McNeil
>
> On Sep 11, 2008, at 6:04 AM, Oliver Oli wrote:
>
>> On Thu, Sep 11, 2008 at 2:57 PM, Alex Blewitt
>> <alex.blewitt at gmail.com> wrote:
>>> If you want mixed (i.e. some striped, some mirrored) you can always
>>> partition your disks and have one partition mirrored and the other
>>> striped.
>>>
>>> Alex
>>
>> Which I would do, if ZFS had the option to resize. I just cannot
>> predict how many GB I have to mirror and how much of the data is
>> non-critical. I'm looking for some flexible schema.
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Fri Sep 12 18:50:17 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 12 Sep 2008 18:50:17 -0700
Subject: [zfs-discuss] Finder eject vs. multiple pools
In-Reply-To: <8A9BC02F-4014-4518-9147-29689BFA7249@thinkpink.com>
References: <8A9BC02F-4014-4518-9147-29689BFA7249@thinkpink.com>
Message-ID: <405AE051-61D4-4355-BA6A-C7821AFB9B94@apple.com>

you sir made the right call :)
So in general I strongly encourage using command line for right now.   
I"m still working with the diskarb people here at Apple as well as the  
Finder kids to work ZFS into the rest of the system.  Right now, most  
of the system is dumbfounded when not presented with a simple /dev  
node.  Everyone has the assumption that you have one filesystem per  
device, which isn't the case for ZFS so it doesn't really mesh well yet.
Working on it...

Noel :)

On Sep 11, 2008, at 10:54 AM, Brian Pinkerton wrote:

> After doing some backups today from a couple of ZFS file systems on a
> "permanent" pool, I tried using the Finder to eject the backup disks
> (a couple of 1TB drives with a single striped ZFS pool) and got the
> Finder message:
>
> 	The device containing "Backup2" also contains 5 other volumes that
> will not be ejected.  Are you sure...
>
> Looks like the Finder somehow considers all ZFS filesystems to be on
> the same device, even if they're on completely separate devices/pools.
>
> I wasn't brave enough to just say "yes", but "zfs export -f Backup2"
> worked just fine. :)
>
> bri
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From zorg at sogeeky.net  Fri Sep 12 19:07:34 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Fri, 12 Sep 2008 19:07:34 -0700
Subject: [zfs-discuss] Finder eject vs. multiple pools
In-Reply-To: <405AE051-61D4-4355-BA6A-C7821AFB9B94@apple.com>
References: <8A9BC02F-4014-4518-9147-29689BFA7249@thinkpink.com>
	<405AE051-61D4-4355-BA6A-C7821AFB9B94@apple.com>
Message-ID: <A6B0535B-CA9C-4352-9E16-2828957B0A02@sogeeky.net>

Hey, you may have just hinted at a possible workaround...  Ever given  
any thought to emulating the necessary /dev nodes needed to make  
everything happy?

P.S.  If I'm speaking nievely about things I don't understand, feel  
free to tell me so. :)

On Sep 12, 2008, at 6:50 PM, No?l Dellofano <ndellofano at apple.com>  
wrote:

> you sir made the right call :)
> So in general I strongly encourage using command line for right now.
> I"m still working with the diskarb people here at Apple as well as the
> Finder kids to work ZFS into the rest of the system.  Right now, most
> of the system is dumbfounded when not presented with a simple /dev
> node.  Everyone has the assumption that you have one filesystem per
> device, which isn't the case for ZFS so it doesn't really mesh well  
> yet.
> Working on it...
>
> Noel :)
>
> On Sep 11, 2008, at 10:54 AM, Brian Pinkerton wrote:
>
>> After doing some backups today from a couple of ZFS file systems on a
>> "permanent" pool, I tried using the Finder to eject the backup disks
>> (a couple of 1TB drives with a single striped ZFS pool) and got the
>> Finder message:
>>
>>    The device containing "Backup2" also contains 5 other volumes that
>> will not be ejected.  Are you sure...
>>
>> Looks like the Finder somehow considers all ZFS filesystems to be on
>> the same device, even if they're on completely separate devices/ 
>> pools.
>>
>> I wasn't brave enough to just say "yes", but "zfs export -f Backup2"
>> worked just fine. :)
>>
>> bri
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From tnn at t86.dk  Sat Sep 13 10:36:56 2008
From: tnn at t86.dk (=?UTF-8?Q?Troels_N=C3=B8rgaard_Nielsen?=)
Date: Sat, 13 Sep 2008 19:36:56 +0200
Subject: [zfs-discuss] zfs-crypto implementation
Message-ID: <c43d90580809131036g6ea9be87i17bd8ffb26b70fe9@mail.gmail.com>

Hey everyone, No?l,

In some time the OpenSolaris zfs-crypto have been feature complete for
first phase of the implementation to bring on-disk encryption support
for our beloved filesystem, ZFS. As of 5th September 2008 Darren
Moffat has annouced the project going into QA/Code review.

With this in mind, what are Apple's intentions regarding a OS X port
of these features? I personally see it as one of the greatest features
to mobile business users, to have support for full disk-encryption. I
know that FileVault's history goes back some time, but it did take on
the problem at the wrong end, as in a userland application workaround
to provide users with user directory encryption.

So, what is your and Apple's plans for zfs disk encryption support, is
it something we should be looking forward to?
As far as I can see in the source code, it seems as Darren's code is
pretty portable, with the exception for the ties to the Sun
Cryptographic framework, but I'm far from no expert.

Thanks for the openness around the project so far.

----
Best Regards

Troels N?rgaard Nielsen
t86 Consultancy

From oliver.oli+0815 at gmail.com  Sun Sep 14 06:35:32 2008
From: oliver.oli+0815 at gmail.com (Oliver Oli)
Date: Sun, 14 Sep 2008 15:35:32 +0200
Subject: [zfs-discuss] copies=2 striped pool. one disk died. rescue
	possible?
In-Reply-To: <5F05DA39-7F14-4BC4-A604-A939DF0075AE@apple.com>
References: <66f535320809110545l587a5119h577c59d12d25b039@mail.gmail.com>
	<636fd28e0809110557k5439ea10p30469d4aad6ccd1e@mail.gmail.com>
	<66f535320809110604l12020023ye0dff0343b551f64@mail.gmail.com>
	<3D16C89B-9A1E-424A-A6C5-F2AF2316BD03@jasonrm.net>
	<5F05DA39-7F14-4BC4-A604-A939DF0075AE@apple.com>
Message-ID: <66f535320809140635m662c7d60ye75686dc54721978@mail.gmail.com>

thank you for the clarification and advise. i think i will buy another
drive and use raidz on 3 drives.

On Sat, Sep 13, 2008 at 3:34 AM, No?l Dellofano <ndellofano at apple.com> wrote:
> Jason is correct below.  We added copies for people that were
> restricted to only being able to run one drive, so if you've got some
> really important data but only one drive (travelling with a laptop?),
> we'll space these copies out on the disk as much as we can since most
> drive failures are partial drive failures.  Then you're odds are
> greater then you are able to recover the file(s) either saving the
> disk yourself or using a salvage program such as Disk Warrior.
>
>
> Noel
>
> On Sep 12, 2008, at 2:42 AM, Jason Richard McNeil wrote:
>
>> Just to follow up on this idea of using copies=2 instead of mirroring.
>>
>> I just did a small test using local files using the exact scenario you
>> suggested and in my opinion, this is a very, very bad idea.
>>
>> If you have parts of one drive that fail, maybe, just maybe you will
>> be lucky and recover what you need, but if you have a complete drive
>> failure... at least in the test I did this evening, your data will be
>> gone. ZFS will treat it as a severe failure and will halt the OS to
>> prevent further damage. There really just isn't any guarantee that
>> your data can survive a hard disk failure except when using mirroring
>> or raidz. The copies option seems best suited to handle cases where a
>> single drive might have read/write failures, but if the drive is
>> pulled out of the system or dies completely, I don't expect the copies
>> option to help you out at all. You might be able to get this to work,
>> but as far as I've ever seen, this is not a good idea.
>>
>> I could be wrong in what i've said so far, but here is the basic
>> point... it's a very bad idea if this is any sort of data you care
>> about. What I've defined as non-critical data in the past always turns
>> out to be more critical later on down the road. Maybe that is just
>> me...
>>
>> Here is my recommendation, not knowing much more about your situation.
>> Use two drives of about equal size, mirror them and start your pool
>> with those two drives. Now if you need more storage you can either add
>> more drives, two at a time, mirroring each and adding it to the pool.
>> When you can't add more sets of drives, replace the single smallest
>> drive with a larger one and resilver. Once the resilver is complete,
>> replace the second drive in that same mirror set with a larger drive
>> and resilver again. Upon completion your pool will resize as it now
>> realizes there is more space available in that mirror set. Usually you
>> want to add more mirror sets before replacing smaller drives, but that
>> is of course your call based upon your circumstances.
>>
>> This is what I am running now, I used to run raidz, but have found a
>> set of three two-way mirrors provide me with better data security.
>> If luck is on my side I could even have up to three drives fail (one
>> drive out of each mirror) and not suffer any data loss.
>>
>> mypool
>>   mirror
>>     disk6s2
>>     disk8s2
>>   mirror
>>     disk5s2
>>     disk7s2
>>   mirror
>>     disk0s2
>>     disk3s2
>>
>> Jason R. McNeil
>>
>> On Sep 11, 2008, at 6:04 AM, Oliver Oli wrote:
>>
>>> On Thu, Sep 11, 2008 at 2:57 PM, Alex Blewitt
>>> <alex.blewitt at gmail.com> wrote:
>>>> If you want mixed (i.e. some striped, some mirrored) you can always
>>>> partition your disks and have one partition mirrored and the other
>>>> striped.
>>>>
>>>> Alex
>>>
>>> Which I would do, if ZFS had the option to resize. I just cannot
>>> predict how many GB I have to mirror and how much of the data is
>>> non-critical. I'm looking for some flexible schema.
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

From ndellofano at apple.com  Mon Sep 15 19:28:24 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon, 15 Sep 2008 19:28:24 -0700
Subject: [zfs-discuss] zfs-crypto implementation
In-Reply-To: <c43d90580809131036g6ea9be87i17bd8ffb26b70fe9@mail.gmail.com>
References: <c43d90580809131036g6ea9be87i17bd8ffb26b70fe9@mail.gmail.com>
Message-ID: <BC51F9A3-ACE2-4F45-8810-6224C3BB0BCE@apple.com>

So in all likelihood, yes, we'll port it over.  But as always, no  
promises ;)
Depends largely on the final implementation of the code.  Since it is  
in code review and talking to the Sun ZFS kids, it seems it's quite a  
length of code, it still may be quite a while until the crypto code  
goes into Solaris.  But when it does go in, I'll definitely be taking  
a look at it to see if it will work out for porting as well.

thanks for your interest in ZFS!
Noel

On Sep 13, 2008, at 10:36 AM, Troels N?rgaard Nielsen wrote:

> Hey everyone, No?l,
>
> In some time the OpenSolaris zfs-crypto have been feature complete for
> first phase of the implementation to bring on-disk encryption support
> for our beloved filesystem, ZFS. As of 5th September 2008 Darren
> Moffat has annouced the project going into QA/Code review.
>
> With this in mind, what are Apple's intentions regarding a OS X port
> of these features? I personally see it as one of the greatest features
> to mobile business users, to have support for full disk-encryption. I
> know that FileVault's history goes back some time, but it did take on
> the problem at the wrong end, as in a userland application workaround
> to provide users with user directory encryption.
>
> So, what is your and Apple's plans for zfs disk encryption support, is
> it something we should be looking forward to?
> As far as I can see in the source code, it seems as Darren's code is
> pretty portable, with the exception for the ties to the Sun
> Cryptographic framework, but I'm far from no expert.
>
> Thanks for the openness around the project so far.
>
> ----
> Best Regards
>
> Troels N?rgaard Nielsen
> t86 Consultancy
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Mon Sep 15 19:33:50 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon, 15 Sep 2008 19:33:50 -0700
Subject: [zfs-discuss] Finder eject vs. multiple pools
In-Reply-To: <A6B0535B-CA9C-4352-9E16-2828957B0A02@sogeeky.net>
References: <8A9BC02F-4014-4518-9147-29689BFA7249@thinkpink.com>
	<405AE051-61D4-4355-BA6A-C7821AFB9B94@apple.com>
	<A6B0535B-CA9C-4352-9E16-2828957B0A02@sogeeky.net>
Message-ID: <63763E18-093C-46F6-B238-6416DDECBA3F@apple.com>

the workaround just isn't worth it right now, since disutility is  
being rewritten to better handle these issues and we're also switching  
ZFS to be a first class IOkit citizen (right now we use a type of shim  
layer).  Since ZFS isn't fully supported for snowleopard  client, i  
just don't have the bandwidth to deal with it :(  Hence why I  
recommend the command line for right now.

Noel :)

On Sep 12, 2008, at 7:07 PM, Mr. Zorg wrote:

> Hey, you may have just hinted at a possible workaround...  Ever  
> given any thought to emulating the necessary /dev nodes needed to  
> make everything happy?
>
> P.S.  If I'm speaking nievely about things I don't understand, feel  
> free to tell me so. :)
>
> On Sep 12, 2008, at 6:50 PM, No?l Dellofano <ndellofano at apple.com>  
> wrote:
>
>> you sir made the right call :)
>> So in general I strongly encourage using command line for right now.
>> I"m still working with the diskarb people here at Apple as well as  
>> the
>> Finder kids to work ZFS into the rest of the system.  Right now, most
>> of the system is dumbfounded when not presented with a simple /dev
>> node.  Everyone has the assumption that you have one filesystem per
>> device, which isn't the case for ZFS so it doesn't really mesh well  
>> yet.
>> Working on it...
>>
>> Noel :)
>>
>> On Sep 11, 2008, at 10:54 AM, Brian Pinkerton wrote:
>>
>>> After doing some backups today from a couple of ZFS file systems  
>>> on a
>>> "permanent" pool, I tried using the Finder to eject the backup disks
>>> (a couple of 1TB drives with a single striped ZFS pool) and got the
>>> Finder message:
>>>
>>>   The device containing "Backup2" also contains 5 other volumes that
>>> will not be ejected.  Are you sure...
>>>
>>> Looks like the Finder somehow considers all ZFS filesystems to be on
>>> the same device, even if they're on completely separate devices/ 
>>> pools.
>>>
>>> I wasn't brave enough to just say "yes", but "zfs export -f Backup2"
>>> worked just fine. :)
>>>
>>> bri
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Mon Sep 15 19:38:14 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon, 15 Sep 2008 19:38:14 -0700
Subject: [zfs-discuss] Initialization of a pool during boot
In-Reply-To: <52204649-1CCD-4FD5-8D5C-863DD09032BE@re.be>
References: <52204649-1CCD-4FD5-8D5C-863DD09032BE@re.be>
Message-ID: <C681E1A8-F432-47D8-8370-EC5F95FADF56@apple.com>

this sounds like the exact symptom of an unformatted disk.

Did you use "diskutil partitionDisk disk0 ...." to format the disks  
for ZFS before you created your ZFS pool with them?

Noel

On Sep 9, 2008, at 12:07 AM, Werner Donn? wrote:

> Hi,
>
> There is something strange about the initialization of a raidz pool on
> a set of three USB disks. I use ZFS 119 on a Mac Mini with Mac OS X
> 10.5.4.
> During the boot sequence the disks go on and off several times. I
> usually
> have to boot two or three times before the pool works properly. In  
> most
> cases the pool is in a degraded state with one disk that is not
> available.
> Strangely enough it is not always the same disk.
>
> In other cases the pool is completely online, but very slow. Normally
> there is always activity on all disks at the same time, but in this
> case when two of them are very active the third is not or much less  
> and
> vice versa.
>
> Once the pool is online in a normal way it is very fast and shows no
> problems for the rest of the day.
>
> Best regards,
>
> Werner.
> --
> Werner Donn?  --  Re                                     http://www.pincette.biz
> Engelbeekstraat 8                                               http://www.re.be
> BE-3300 Tienen
> tel: (+32) 486 425803	e-mail: werner.donne at re.be
>
>
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From sah.list at gmail.com  Mon Sep 15 19:57:33 2008
From: sah.list at gmail.com (Sean Hafeez)
Date: Mon, 15 Sep 2008 19:57:33 -0700
Subject: [zfs-discuss] Hard Lockup on zpool status
Message-ID: <497BEB72-D07C-4D17-9300-E2DEF4C7B848@gmail.com>

rioja:~ me$ zpool list
NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
data2                  2.53T    993G   1.56T    38%  ONLINE     -
rioja:~ me$ zpool status
   pool: data2
  state: ONLINE
status: One or more devices has experienced an error resulting in data
	corruption.  Applications may be affected.
action: Restore the file in question if possible.  Otherwise restore the
	entire pool from backup.
    see: http://www.sun.com/msg/ZFS-8000-8A
  scrub: none requested
config:

	NAME         STATE     READ WRITE CKSUM
	data2        ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk2s2  ONLINE       0     0     0
	    disk1s2  ONLINE       0     0     0
	    disk3s2  ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk5s2  ONLINE       0     0     0
	    disk4s2  ONLINE       0     0     0
	    disk6s2  ONLINE       0     0     0
	    disk7s2  ONLINE       0     0     0

...at this point the system locks up.

There was another zpool that fail and was removed (external USB disk).

I have to reboot the system. Hard lockup. Any ideas.

Thanks!
Sean



From werner.donne at re.be  Tue Sep 16 00:07:11 2008
From: werner.donne at re.be (=?ISO-8859-1?Q?Werner_Donn=E9?=)
Date: Tue, 16 Sep 2008 09:07:11 +0200
Subject: [zfs-discuss] Initialization of a pool during boot
In-Reply-To: <C681E1A8-F432-47D8-8370-EC5F95FADF56@apple.com>
References: <52204649-1CCD-4FD5-8D5C-863DD09032BE@re.be>
	<C681E1A8-F432-47D8-8370-EC5F95FADF56@apple.com>
Message-ID: <62451018-5A7C-42E5-9B61-772382FBC5C3@re.be>

Hi No?l,

I did indeed. This is the output of diskutil:

werner at re-mac-4> diskutil list
/dev/disk0
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *74.5 Gi     
disk0
    1:                        EFI                         200.0 Mi    
disk0s1
    2:                  Apple_HFS Macintosh HD            74.1 Gi     
disk0s2
/dev/disk1
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *465.8 Gi    
disk1
    1:                        EFI                         200.0 Mi    
disk1s1
    2:                        ZFS re                      465.4 Gi    
disk1s2
/dev/disk2
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *465.8 Gi    
disk2
    1:                        EFI                         200.0 Mi    
disk2s1
    2:                        ZFS re                      465.4 Gi    
disk2s2
/dev/disk3
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *465.8 Gi    
disk3
    1:                        EFI                         200.0 Mi    
disk3s1
    2:                        ZFS re                      465.4 Gi    
disk3s2

The situation is getting worse. Now after the first boot I get a message
box saying there is an uninitialized disk, with the proposal to  
initialize,
ignore or eject it. After that I got a few panics.

After six or seven reboots, some of which with a power cycle, I get a  
perfectly
working zpool. The stages in between give me a degraded or a slow  
pool. It is
as if the pool is not mounted in time for some other initialization  
parts.

Best regards,

Werner.

On 16 Sep 2008, at 04:38, No?l Dellofano wrote:

> this sounds like the exact symptom of an unformatted disk.
>
> Did you use "diskutil partitionDisk disk0 ...." to format the disks  
> for ZFS before you created your ZFS pool with them?
>
> Noel
>
> On Sep 9, 2008, at 12:07 AM, Werner Donn? wrote:
>
>> Hi,
>>
>> There is something strange about the initialization of a raidz pool  
>> on
>> a set of three USB disks. I use ZFS 119 on a Mac Mini with Mac OS X
>> 10.5.4.
>> During the boot sequence the disks go on and off several times. I
>> usually
>> have to boot two or three times before the pool works properly. In  
>> most
>> cases the pool is in a degraded state with one disk that is not
>> available.
>> Strangely enough it is not always the same disk.
>>
>> In other cases the pool is completely online, but very slow. Normally
>> there is always activity on all disks at the same time, but in this
>> case when two of them are very active the third is not or much less  
>> and
>> vice versa.
>>
>> Once the pool is online in a normal way it is very fast and shows no
>> problems for the rest of the day.
>>
>> Best regards,
>>
>> Werner.
>> --
>> Werner Donn?  --  Re                                     http://www.pincette.biz
>> Engelbeekstraat 8                                               http://www.re.be
>> BE-3300 Tienen
>> tel: (+32) 486 425803	e-mail: werner.donne at re.be
>>
>>
>>
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

--
Werner Donn?  --  Re                                     http://www.pincette.biz
Engelbeekstraat 8                                               http://www.re.be
BE-3300 Tienen
tel: (+32) 486 425803	e-mail: werner.donne at re.be






From apple.mail.list at oryx.cc  Tue Sep 16 00:41:36 2008
From: apple.mail.list at oryx.cc (Jerry K)
Date: Tue, 16 Sep 2008 02:41:36 -0500
Subject: [zfs-discuss] zfs-crypto implementation
In-Reply-To: <BC51F9A3-ACE2-4F45-8810-6224C3BB0BCE@apple.com>
References: <c43d90580809131036g6ea9be87i17bd8ffb26b70fe9@mail.gmail.com>
	<BC51F9A3-ACE2-4F45-8810-6224C3BB0BCE@apple.com>
Message-ID: <48CF6330.3010403@oryx.cc>

If you read the project notes, it is planned for integration into 
OpenSolaris/Nevada around the build 105 time line.  The current build is 
97, unless I have missed build 98.

Jerry K



No?l Dellofano wrote:
> So in all likelihood, yes, we'll port it over.  But as always, no  
> promises ;)
> Depends largely on the final implementation of the code.  Since it is  
> in code review and talking to the Sun ZFS kids, it seems it's quite a  
> length of code, it still may be quite a while until the crypto code  
> goes into Solaris.  But when it does go in, I'll definitely be taking  
> a look at it to see if it will work out for porting as well.
> 
> thanks for your interest in ZFS!
> Noel
> 

From macrbg at mac.com  Tue Sep 16 05:49:06 2008
From: macrbg at mac.com (Robert Gordon)
Date: Tue, 16 Sep 2008 07:49:06 -0500
Subject: [zfs-discuss] zfs-crypto implementation
In-Reply-To: <48CF6330.3010403@oryx.cc>
References: <c43d90580809131036g6ea9be87i17bd8ffb26b70fe9@mail.gmail.com>
	<BC51F9A3-ACE2-4F45-8810-6224C3BB0BCE@apple.com>
	<48CF6330.3010403@oryx.cc>
Message-ID: <663EE74E-4B89-43B2-BAF7-189669FB417E@mac.com>


On Sep 16, 2008, at 2:41 AM, Jerry K wrote:

> If you read the project notes, it is planned for integration into
> OpenSolaris/Nevada around the build 105 time line.  The current  
> build is
> 97, unless I have missed build 98.
>
> Jerry K
>

you did, build 98 closed yesterday, so the current build is 99; (http://opensolaris.org/os/community/on/schedule/ 
) -- regardless of which build Nevada is at and the *target* for the  
integration of zfs-crypto... Noel's email set the expectation of a  
maybe.. :) .. I'd rather see a solid / fully integrated ZFS  
implementation before anything else..

Robert.

From al at runlevel7.org  Wed Sep 17 08:20:26 2008
From: al at runlevel7.org (Al Gordon)
Date: Wed, 17 Sep 2008 11:20:26 -0400
Subject: [zfs-discuss] Hard Lockup on zpool status
In-Reply-To: <497BEB72-D07C-4D17-9300-E2DEF4C7B848@gmail.com>
References: <497BEB72-D07C-4D17-9300-E2DEF4C7B848@gmail.com>
Message-ID: <23F857DF-01DE-489F-85A7-DEBD6AC77A4F@runlevel7.org>

I've encountered this as well.  No solutions, just a +1 for the  
problem reporting.

--

   -- AL --

On Sep 15, 2008, at 10:57 PM, Sean Hafeez wrote:

> rioja:~ me$ zpool list
> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
> ALTROOT
> data2                  2.53T    993G   1.56T    38%  ONLINE     -
> rioja:~ me$ zpool status
>   pool: data2
>  state: ONLINE
> status: One or more devices has experienced an error resulting in data
> 	corruption.  Applications may be affected.
> action: Restore the file in question if possible.  Otherwise restore  
> the
> 	entire pool from backup.
>    see: http://www.sun.com/msg/ZFS-8000-8A
>  scrub: none requested
> config:
>
> 	NAME         STATE     READ WRITE CKSUM
> 	data2        ONLINE       0     0     0
> 	  raidz1     ONLINE       0     0     0
> 	    disk2s2  ONLINE       0     0     0
> 	    disk1s2  ONLINE       0     0     0
> 	    disk3s2  ONLINE       0     0     0
> 	  raidz1     ONLINE       0     0     0
> 	    disk5s2  ONLINE       0     0     0
> 	    disk4s2  ONLINE       0     0     0
> 	    disk6s2  ONLINE       0     0     0
> 	    disk7s2  ONLINE       0     0     0
>
> ...at this point the system locks up.
>
> There was another zpool that fail and was removed (external USB disk).
>
> I have to reboot the system. Hard lockup. Any ideas.
>
> Thanks!
> Sean
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From sah.list at gmail.com  Wed Sep 17 08:30:32 2008
From: sah.list at gmail.com (Sean Hafeez)
Date: Wed, 17 Sep 2008 08:30:32 -0700
Subject: [zfs-discuss] Hard Lockup on zpool status
In-Reply-To: <23F857DF-01DE-489F-85A7-DEBD6AC77A4F@runlevel7.org>
References: <497BEB72-D07C-4D17-9300-E2DEF4C7B848@gmail.com>
	<23F857DF-01DE-489F-85A7-DEBD6AC77A4F@runlevel7.org>
Message-ID: <ECF0970B-A03D-458D-9B39-4398202AAF98@gmail.com>

Happy to see your email just to know it is not just me.

I am at a port where just typing:

zpool status

and hitting enter just locks the system hard. Nothing gets printed.  
ENTER = locked hard :)

-Sean

On Sep 17, 2008, at 8:20 AM, Al Gordon wrote:

> I've encountered this as well.  No solutions, just a +1 for the
> problem reporting.
>
> --
>
>   -- AL --
>
> On Sep 15, 2008, at 10:57 PM, Sean Hafeez wrote:
>
>> rioja:~ me$ zpool list
>> NAME                    SIZE    USED   AVAIL    CAP  HEALTH
>> ALTROOT
>> data2                  2.53T    993G   1.56T    38%  ONLINE     -
>> rioja:~ me$ zpool status
>>  pool: data2
>> state: ONLINE
>> status: One or more devices has experienced an error resulting in  
>> data
>> 	corruption.  Applications may be affected.
>> action: Restore the file in question if possible.  Otherwise restore
>> the
>> 	entire pool from backup.
>>   see: http://www.sun.com/msg/ZFS-8000-8A
>> scrub: none requested
>> config:
>>
>> 	NAME         STATE     READ WRITE CKSUM
>> 	data2        ONLINE       0     0     0
>> 	  raidz1     ONLINE       0     0     0
>> 	    disk2s2  ONLINE       0     0     0
>> 	    disk1s2  ONLINE       0     0     0
>> 	    disk3s2  ONLINE       0     0     0
>> 	  raidz1     ONLINE       0     0     0
>> 	    disk5s2  ONLINE       0     0     0
>> 	    disk4s2  ONLINE       0     0     0
>> 	    disk6s2  ONLINE       0     0     0
>> 	    disk7s2  ONLINE       0     0     0
>>
>> ...at this point the system locks up.
>>
>> There was another zpool that fail and was removed (external USB  
>> disk).
>>
>> I have to reboot the system. Hard lockup. Any ideas.
>>
>> Thanks!
>> Sean
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From franzschmalzl at spamfreemail.de  Wed Sep 17 09:56:22 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Wed, 17 Sep 2008 18:56:22 +0200
Subject: [zfs-discuss] Rsync over afp with zfs backend
Message-ID: <CC70C229-4E98-4DF5-8A81-0C1D3A433C75@spamfreemail.de>

hey there beloved list :)

I'm facing some issues here, it's yours to decide if this is a bug or  
just a thing that isn't possible.

When backing up the FileFault encrypted homefolder of my macbook air  
to an afp shared zfs filesystem I get all kinds of " operation not  
supported on socket" messages.
rsync fails to set timestamps ( not supported on socket ) neither is  
it able to modify extended attributes or delete ( no permission )

permissions are rwxrwxrwx however...

sooo, what's wrong ?

is rsync over afp generally a bad idea ?
is it zfs' fault ?
what am i going to do?
panic?



greetings

ruebezahl



p.s.
ill drop you my rsync line

rsync -ruvaE --progress --inplace --delete




From William.Winnett at Sun.COM  Thu Sep 18 07:32:15 2008
From: William.Winnett at Sun.COM (Bill Winnett)
Date: Thu, 18 Sep 2008 10:32:15 -0400
Subject: [zfs-discuss] ditto process hang from hfs to a zfs drive
In-Reply-To: <EC968068-F40D-4008-B7AB-37697F32AF5A@apple.com>
References: <BAC5EA10-D315-4A76-BC46-5B1603556AE8@sun.com>
	<C11883C7-DEDE-4436-AD3D-4BD6CC1A8E4D@apple.com>
	<B1F54A30-522F-42C6-853A-F205AC900AE4@Sun.COM>
	<EC968068-F40D-4008-B7AB-37697F32AF5A@apple.com>
Message-ID: <BA86DC13-7E1D-4BB3-9B6C-D8CF6E6E3DC9@Sun.COM>


On Sep 12, 2008, at 9:25 PM, No?l Dellofano wrote:

> hmm... lemme dig a little.  When you say "old" HFS drive, how old is  
> it?  Tiger? Jaguar? just curious, though I"m sure it's somehow my  
> fault :)

Sorry to reply so late.  When I used the term old, it was a euphemism  
for the non ZFS drive.  The "old" drive is a 300G Western Digital  
Mybook formated with HFS+ on Leapoard.  The drive is maybe physically  
a year old.


> I haven't seen a ditto hang a machine since we fixed the hang in  
> 117....
>
> Noel
>
> On Sep 12, 2008, at 9:11 AM, Bill Winnett wrote:
>
>>
>> On Sep 12, 2008, at 12:03 PM, No?l Dellofano wrote:
>>
>>> hey Bill,
>>> What Leopard and zfs build are you running?
>> 10.5.4  build 9E17 of Leopard and build 119 of ZFS
>>>
>>> Noel
>>>
>>> On Sep 12, 2008, at 5:03 AM, Bill Winnett wrote:
>>>
>>>>

stuff deleted!!!!

>>>>
>>>>
>>>>
>>>> Once the process hung, access to the zfs drive was blocked, any  
>>>> process including finder that attempted access would hang.  I  
>>>> could not kill/restart finder.  Kill -9 did not work.  System  
>>>> shutdown was blocked, etc.
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080918/7f386094/attachment.html 

From darwinskernel at gmail.com  Sun Sep 21 09:26:33 2008
From: darwinskernel at gmail.com (Charles Darwin)
Date: Sun, 21 Sep 2008 12:26:33 -0400
Subject: [zfs-discuss] git clone <zfs-source-code>
Message-ID: <D9294569-D92A-4672-86BE-9F30B364C2E0@gmail.com>

Hi all,

Is there a Git repository of zfs' source code?


Charles 

From aorchid at mac.com  Sun Sep 21 10:10:18 2008
From: aorchid at mac.com (Aric Gregson)
Date: Sun, 21 Sep 2008 10:10:18 -0700
Subject: [zfs-discuss] Unable to Import Zpool after Clean Install of Leopard
Message-ID: <6982AA9EA6B57FA4AEB08EF5@ag-pb.local>

Mac ZFS Community,

I am desperately seeking advice on how to import an established zpool 
after clean re-install of Leopard on a MacPro 4-core intel machine. The 
zpool was last running on the 119 build in the same version of Leopard 
that I reinstalled and I reinstalled the zfs binaries as per the 
instructions and then rebooted. The usual warning that the disks were 
unrecognized popped up and I hit ignorel. However, when I go to run 
'sudo zpool import -f peipool' I get 'bus error' and nothing happens. 
Any command with zpool returns the same error.

I have checked and all binaries are installed with proper permissions. 
The pool was formatted exactly as per the directions at Mac ZFS at 
macosforge, but I have always had to do an import -f upon rebooting the 
computer (prior to re-install). The disks are a two disk, internal 
mirror.

What log can I look at that may be helpful here? I am going to attempt 
running a live solaris CD to see if that will import the disks and then 
I can copy over to an external mac formatted disk and see what happens.

thanks!!

aric

From zorg at sogeeky.net  Sun Sep 21 11:31:18 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Sun, 21 Sep 2008 11:31:18 -0700
Subject: [zfs-discuss] Unable to Import Zpool after Clean Install of
	Leopard
In-Reply-To: <6982AA9EA6B57FA4AEB08EF5@ag-pb.local>
References: <6982AA9EA6B57FA4AEB08EF5@ag-pb.local>
Message-ID: <4CA3093E-BE69-413C-ABAE-152D5B3F9550@sogeeky.net>

In my limited experience, a bus error generally indicates some type of  
binary incompatability. Either with the executable itself or one of  
the libraries it uses. Are you SURE everything is the same?  All  
patched up, etc?

On Sep 21, 2008, at 10:10 AM, Aric Gregson <aorchid at mac.com> wrote:

> Mac ZFS Community,
>
> I am desperately seeking advice on how to import an established zpool
> after clean re-install of Leopard on a MacPro 4-core intel machine.  
> The
> zpool was last running on the 119 build in the same version of Leopard
> that I reinstalled and I reinstalled the zfs binaries as per the
> instructions and then rebooted. The usual warning that the disks were
> unrecognized popped up and I hit ignorel. However, when I go to run
> 'sudo zpool import -f peipool' I get 'bus error' and nothing happens.
> Any command with zpool returns the same error.
>
> I have checked and all binaries are installed with proper permissions.
> The pool was formatted exactly as per the directions at Mac ZFS at
> macosforge, but I have always had to do an import -f upon rebooting  
> the
> computer (prior to re-install). The disks are a two disk, internal
> mirror.
>
> What log can I look at that may be helpful here? I am going to attempt
> running a live solaris CD to see if that will import the disks and  
> then
> I can copy over to an external mac formatted disk and see what  
> happens.
>
> thanks!!
>
> aric
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From aorchid at mac.com  Sun Sep 21 11:46:33 2008
From: aorchid at mac.com (Aric Gregson)
Date: Sun, 21 Sep 2008 11:46:33 -0700
Subject: [zfs-discuss] Unable to Import Zpool after Clean Install of
	Leopard
In-Reply-To: <4CA3093E-BE69-413C-ABAE-152D5B3F9550@sogeeky.net>
References: <6982AA9EA6B57FA4AEB08EF5@ag-pb.local>
	<4CA3093E-BE69-413C-ABAE-152D5B3F9550@sogeeky.net>
Message-ID: <70EE5F540698046AAAE96844@ag-pb.local>

--On September 21, 2008 11:31:18 AM -0700 "Mr. Zorg" <zorg at sogeeky.net> 
sent:

> Are you SURE everything is the same?  All patched up, etc?

Much to my embarrassment everything is not the same. Many apologies to 
you all and thank you very much for your prompt replies. I have just 
confirmed that ZFS will not run on Tiger...my wife installed 10.4 
instead of 10.5...that should explain everything. My apologies.

;(

aric

From richmc at gmail.com  Sun Sep 21 12:44:03 2008
From: richmc at gmail.com (Richard McClellan)
Date: Sun, 21 Sep 2008 12:44:03 -0700
Subject: [zfs-discuss] first impression: wow!
Message-ID: <6CABA161-B4C1-4E55-A558-9E5BC9FB9503@gmail.com>

Hi All,

I'm using Mac OS Forge's ZFS on several disks over iSCSI and I am  
quite impressed so far.  The performance is fantastic. rsync of very  
large files/directories claims transfer rates in the >80MB/s range.  
Xbench numbers are so good they are suspicious.  Performance is much,  
much better than when Open Solaris 2008.05 "owned" the disks and made  
them an iSCSI target, albeit that was running in VMware Server 2.0 RC2  
which may have poor networking, etc.  The Getting Started docs are  
very good and enough to quickly get up and running.

I have a few questions:
1. Is there a "mission statement" or something that indicates what is  
planned for this implementation of ZFS?
2. How stable is this ZFS implementation?  Can I trust it with  
important stuff?
3. Will support/development be ongoing?  Is it a goal that the  
transition to an official Apple release of ZFS (Snow Leopard?) be  
possible?
4. Do Apple OS updates/patches present a problem?

For the curious, the server is a PC running CentOS 5.2.  iscsitarget v. 
0.4.16 is in charge of the disks which are presented as a single  
target (multiple LUNs) in blockio mode.  The client is a Mac Mini  
(Intel) runing Leopard v10.5.5.  The iSCSI initiator is Studio Network  
Solution's free globalSan v3.3.0.43.  The network is running at 1  
gigabit.

		Thanks for ZFS on the Mac!!
		Rich

From zorg at sogeeky.net  Sun Sep 21 14:24:04 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Sun, 21 Sep 2008 14:24:04 -0700
Subject: [zfs-discuss] first impression: wow!
In-Reply-To: <6CABA161-B4C1-4E55-A558-9E5BC9FB9503@gmail.com>
References: <6CABA161-B4C1-4E55-A558-9E5BC9FB9503@gmail.com>
Message-ID: <165D0553-BB60-4767-B55C-495E9239985E@sogeeky.net>

1.  Not sure!

2.  As long as you follow the getting started docs, yes. The  
occasional glitch may hang or crash the system, but i've yet to hear  
of anyone losing data.

3.  Absolutely. This *is* the official Apple effort, so it certainly  
should work. :)

4.  Nope. Apple updates work fine, though you have to do zfs updates  
manually for now.



On Sep 21, 2008, at 12:44 PM, Richard McClellan <richmc at gmail.com>  
wrote:

> Hi All,
>
> I'm using Mac OS Forge's ZFS on several disks over iSCSI and I am
> quite impressed so far.  The performance is fantastic. rsync of very
> large files/directories claims transfer rates in the >80MB/s range.
> Xbench numbers are so good they are suspicious.  Performance is much,
> much better than when Open Solaris 2008.05 "owned" the disks and made
> them an iSCSI target, albeit that was running in VMware Server 2.0 RC2
> which may have poor networking, etc.  The Getting Started docs are
> very good and enough to quickly get up and running.
>
> I have a few questions:
> 1. Is there a "mission statement" or something that indicates what is
> planned for this implementation of ZFS?
> 2. How stable is this ZFS implementation?  Can I trust it with
> important stuff?
> 3. Will support/development be ongoing?  Is it a goal that the
> transition to an official Apple release of ZFS (Snow Leopard?) be
> possible?
> 4. Do Apple OS updates/patches present a problem?
>
> For the curious, the server is a PC running CentOS 5.2.  iscsitarget  
> v.
> 0.4.16 is in charge of the disks which are presented as a single
> target (multiple LUNs) in blockio mode.  The client is a Mac Mini
> (Intel) runing Leopard v10.5.5.  The iSCSI initiator is Studio Network
> Solution's free globalSan v3.3.0.43.  The network is running at 1
> gigabit.
>
>        Thanks for ZFS on the Mac!!
>        Rich
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From lopez.on.the.lists at yellowspace.net  Mon Sep 22 10:55:52 2008
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Mon, 22 Sep 2008 19:55:52 +0200
Subject: [zfs-discuss] first impression: wow!
In-Reply-To: <6CABA161-B4C1-4E55-A558-9E5BC9FB9503@gmail.com>
References: <6CABA161-B4C1-4E55-A558-9E5BC9FB9503@gmail.com>
Message-ID: <F22D395D-CAF1-44FE-AF43-A81ACD4DFBA7@yellowspace.net>


> 2. How stable is this ZFS implementation?  Can I trust it with
> important stuff?

haven't experienced data loss till now, however, the
missing features, optimization and integration with
finder/afp server/spotlight are still there and still
annoying in my experience (having all filesystems named
after the pool, for example, is not so nice..).

if you follow(ed) the list, you notice that development
has been quite constant over this year, so although
the 'apples' on this list can't give us a  timeline,
i wouldn't give up hope on a relatively bugfree
implementation by the xmas timeline... :)

... so my feeling right now is:  zfs on osx will
rock when it's done, but it's not done yet..

(== read: i'm keeping it on my macbook, cause i love
to see it evolve, but i'm not using it for business
critical services yet... although i'm tempted to do
so each time new bits are released...)

:)

regards and kudos,

Lorenzo



From aorchid at mac.com  Mon Sep 22 13:42:45 2008
From: aorchid at mac.com (Aric Gregson)
Date: Mon, 22 Sep 2008 13:42:45 -0700
Subject: [zfs-discuss] first impression: wow!
In-Reply-To: <F22D395D-CAF1-44FE-AF43-A81ACD4DFBA7@yellowspace.net>
References: <6CABA161-B4C1-4E55-A558-9E5BC9FB9503@gmail.com>
	<F22D395D-CAF1-44FE-AF43-A81ACD4DFBA7@yellowspace.net>
Message-ID: <20080922134245.d3d1af30.aorchid@mac.com>

On Mon, 22 Sep 2008 19:55:52 +0200
Lorenzo Perone <lopez.on.the.lists at yellowspace.net> wrote:

> > 2. How stable is this ZFS implementation?  Can I trust it with
> > important stuff?
> 
> haven't experienced data loss till now, however, the
> missing features, optimization and integration with
> finder/afp server/spotlight are still there and still
> annoying in my experience (having all filesystems named
> after the pool, for example, is not so nice..).

We have been using it for video editing with Final Cut for a few months
now and haven't experienced difficulty with it. We have just moved the
user partitions to ZFS and will see how that goes. The problems are as
above, mainly that the Finder cannot come up with a name other than the
pool name. This results in the poolname being displayed in the Finder
sidebar rather than the username and can be confusing. To get around
this, we mounted the zfs datasets that are shared (Music, Pictures,
FC work) under /Users/Shared. We then placed a 'Shared' icon in the
Finder sidebar and clicking on this allows one to find the ZFS datasets
under Shared with their proper names. Additionally, we have a problem
where the zpool is not mounted upon reboot and requires a manual 'import
-f' to be done, so one user has to be on the boot disk without zfs and
automatic login must be turned off. I find this to be the most annoying
problem, the others can be worked around even by those who are
unfamiliar with CL.

aric

From ndellofano at apple.com  Mon Sep 22 15:39:52 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon, 22 Sep 2008 15:39:52 -0700
Subject: [zfs-discuss] git clone <zfs-source-code>
In-Reply-To: <D9294569-D92A-4672-86BE-9F30B364C2E0@gmail.com>
References: <D9294569-D92A-4672-86BE-9F30B364C2E0@gmail.com>
Message-ID: <4DEA3701-C971-4CED-BA21-A920BB17E58C@apple.com>

nope, you can download all the source code off of the zfs.macosforge  
site, but we have no public repository set up.

Noel

On Sep 21, 2008, at 9:26 AM, Charles Darwin wrote:

> Hi all,
>
> Is there a Git repository of zfs' source code?
>
>
> Charles
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From Brad.Allred at ForEveryBody.com  Tue Sep 23 12:26:59 2008
From: Brad.Allred at ForEveryBody.com (Brad Allred)
Date: Tue, 23 Sep 2008 13:26:59 -0600
Subject: [zfs-discuss] 10.5.5 update broke ZFS group ACLs
Message-ID: <46703FFA-C1B7-4540-B1BE-08C6C25CFBE7@ForEveryBody.com>

after updating to 10.5.5 server our Active directory group ACLs  
stopped working (ZFS 119); the entries are still there but the group  
name shows up as a long and cryptic hex string. I tried recursively  
removing ACLs on the ZFS share and reapplying fresh ones, but the same  
thing happened. I also noticed that chmod wasn't traversing down the  
entire way, it would go a few folders down then stop. I am still able  
to apply Active Directory user ACLs, but this is a huge pain.
Could somebody kindly post any necessary steps to get my group ACLs  
working again? Is there an impending update to the Mac ZFS binaries  
that will improve ZFS ACLs?

I posted a while back about ZFS ACL inheritance not working at all,  
that is still the case and would like to know if this is known and if  
there is anyway to enable it.
I tried changing some settings using the ZFS command; the settings  
stuck, but there is no change in behavior regarding ACL inheritance.

here is the out put of a zfs get all:

NAME      PROPERTY       VALUE                  SOURCE
Graphics  type           filesystem             -
Graphics  creation       Tue May 20 10:50 2008  -
Graphics  used           1.07T                  -
Graphics  available      956G                   -
Graphics  referenced     1.07T                  -
Graphics  compressratio  1.00x                  -
Graphics  mounted        yes                    -
Graphics  quota          none                   default
Graphics  reservation    none                   default
Graphics  recordsize     128K                   default
Graphics  mountpoint     /Volumes/Graphics      default
Graphics  sharenfs       off                    default
Graphics  checksum       on                     default
Graphics  compression    off                    default
Graphics  atime          on                     default
Graphics  devices        on                     default
Graphics  exec           on                     default
Graphics  setuid         on                     default
Graphics  readonly       off                    default
Graphics  zoned          off                    default
Graphics  snapdir        hidden                 default
Graphics  aclmode        passthrough            local
Graphics  aclinherit     passthrough            local
Graphics  canmount       on                     default
Graphics  shareiscsi     off                    default
Graphics  xattr          on                     default
Graphics  copies         1                      default
Graphics  version        1                      -

as you can see the aclmode and inherit are set to "passthrough",  
according to the Apple zfs man page (http://developer.apple.com/documentation/Darwin/Reference/Manpages/man8/zfs.8.html 
) this should do what I desire. Am I missing something?

I would really love this to work like it should
what settings, if any, do i need to set/unset with the zfs command?
should I be applying the ACLs using something other than server admin?  
If so how do I go about it?

thank you for you hard work on what I'm sure will be a killer file  
system come july of next year.

From zfs at hessmann.de  Tue Sep 23 15:21:34 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Wed, 24 Sep 2008 00:21:34 +0200
Subject: [zfs-discuss] Panic during diff (Assertion failure: zap_hash(zap,
	name) ==   hash failed, 0 == 0)
Message-ID: <3EC8DF33-4705-4A34-999D-C7BA5FB250D8@hessmann.de>

Morning everyone,


I'm looking at some snapshots of a (backup) filesystem right now and  
delete them if they do not contain anything important.
To do this, I clone the snapshots and compare them via diff:

diff -qr clone115 clone116 | grep -v -e 'DS_Store' | sort >  
20080430121525.20080430151525.txt

To speed things up, I compare up to 15 clones in parallel. Takes about  
5 hours to finish, but that's fine.
While this worked out fine the last ~120 clones, I finally got a very  
interesting Kernel Panic a few minutes ago, which seems to be quite  
similar to what Bill Winnett mentioned around March and is tracked in <rdar://problem/5694099 
 >:

======
Tue Sep 23 23:52:09 2008
panic(cpu 0 caller 0x00BD7962): "zap_hash(zap, name) == hash failed, 0  
== 0"@/Volumes/pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/ 
zfs/zap_micro.c:117
Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x561b36b8 : 0x12b0fa (0x4592a4 0x561b36ec 0x133243 0x0)
0x561b3708 : 0xbd7962 (0xc3edd8 0xc3edbc 0x0 0xc17d7c)
0x561b37b8 : 0xbd8499 (0x0 0x0 0x0 0x0)
0x561b3818 : 0xc013f1 (0x1c88c4c0 0x435b 0x0 0x6b32aa0)
0x561b3898 : 0xc01654 (0x561b38cc 0x39e09db8 0x561b3958 0x561b38c8)
0x561b38e8 : 0xb9f3bc (0x39e09db8 0x561b3958 0x561b3df4 0x6)
0x561b3998 : 0x1f2ea2 (0x561b39b8 0x2 0x561b39e8 0x44fe8e)
0x561b39e8 : 0x1d49b4 (0x5a77d00 0x561b3df4 0x561b3f08 0x1c990e84)
0x561b3a78 : 0x1d5739 (0x561b3ddc 0x100 0x561b3dfc 0x0)
0x561b3b38 : 0x1e434d (0x561b3ddc 0x202 0x561b3ba8 0x14343c)
0x561b3d88 : 0x1e4771 (0xbfffe614 0x0 0x0 0x0)
0x561b3f48 : 0x1e480a (0xbfffe614 0x0 0x0 0x0)
0x561b3f78 : 0x3ddde2 (0x4faf2b0 0x1c990d80 0x1c990dc4 0x0)
0x561b3fc8 : 0x19f2c3 (0x430db80 0x0 0x1a20b5 0x430db80)
No mapping exists for frame pointer
Backtrace terminated-invalid frame pointer 0xbfffe788
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0xb9b000->0xc66fff

BSD process name corresponding to current thread: diff

Mac OS version:
9E17

Kernel version:
Darwin Kernel Version 9.4.0: Mon Jun  9 19:30:53 PDT 2008;  
root:xnu-1228.5.20~1/RELEASE_I386
System model name: Macmini2,1 (Mac-F4208EAA)
======

The Filesystem is a RAID.Z which lives on 3 external FW drives (750GB  
each). It is 99-100% full (< 2GB left).

Can I be of any further assistance to narrow down the problem?


Best regards,

Christian

From richmc at gmail.com  Tue Sep 23 21:11:46 2008
From: richmc at gmail.com (Richard McClellan)
Date: Tue, 23 Sep 2008 21:11:46 -0700
Subject: [zfs-discuss] changing iSCSI target info w/o reboot locks up Mac OS
Message-ID: <4E73F191-E630-4F71-A51F-4B0B29EF0E04@gmail.com>

Hi,

I've discovered that changing iSCSI target settings cause a kernel  
panic in ZFS code.

To do this I:
1. mount (initiate) an iSCSI target
2. unmount the target
3. change the LUNs on the iSCSI target
4. restart iSCSI
5. remount (initiate) the iSCSI target

Within 10 seconds the screen ghosts out and a message in several  
languages informs me to press and hold the power button for several  
seconds to restart the computer.  Here's the message that was sent to  
Apple:

Tue Sep 23 21:02:29 2008
panic(cpu 0 caller 0x00BCDD21): "ZFS: I/O failure (write on <unknown>  
off 0: zio 0xb739ee8 [L0 object directory] 400L/200P  
DVA[0]=<0:640027cc00:400> DVA[1]=<0:d200163800:400>  
DVA[2]=<0:1b8004a5c00:400> fletcher4 lzjb LE contiguous birth=52073  
fill=1 cksum=cd77a5e82:563ac882499:12708c86bf606:2ada8ee44bf317):  
error " "6"@/Volumes/pixie_dust/home/ndellofano/zfs-work/zfs-119/ 
zfs_kext/zfs/zio.c:918
Backtrace (CPU 0), Frame : Return Address (4 potential args on stack)
0x3f857e48 : 0x12b0fa (0x459234 0x3f857e7c 0x133243 0x0)
0x3f857e98 : 0xbcdd21 (0xc3d6b0 0xc3d6a4 0xc39400 0xc6a670)
0x3f857f18 : 0xbca3fb (0xb739ee8 0x34 0x3f857f38 0xc1118a)
0x3f857f58 : 0xc29283 (0xb739ee8 0x1 0xc73ef4 0xb4818f8)
0x3f857fc8 : 0x19eccc (0xb4818f8 0x0 0x1a20b5 0xb674410)
Backtrace terminated-invalid frame pointer 0
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0xbb6000->0xc81fff

BSD process name corresponding to current thread: kernel_task

Mac OS version:
9F33

Kernel version:
Darwin Kernel Version 9.5.0: Wed Sep  3 11:29:43 PDT 2008;  
root:xnu-1228.7.58~1/RELEASE_I386
System model name: Macmini2,1 (Mac-F4208EAA)

Is ZFS caching information about a particular target that causes it to  
s#!t the bed when the target has in fact changed?  The problem goes  
away after the crash, so this is purely informational (so far!).

		Rich

From ndellofano at apple.com  Tue Sep 23 23:40:04 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue, 23 Sep 2008 23:40:04 -0700
Subject: [zfs-discuss] changing iSCSI target info w/o reboot locks up
	Mac OS
In-Reply-To: <4E73F191-E630-4F71-A51F-4B0B29EF0E04@gmail.com>
References: <4E73F191-E630-4F71-A51F-4B0B29EF0E04@gmail.com>
Message-ID: <31E47CD2-AD3A-4ACC-8C02-50ADC5B66343@apple.com>

this is expected since in essence what you're doing below is pulling  
drives from a live ZFS pool.  ZFS keeps track of what devices belong  
in it's pool, and writes labels to those devices and then updates  
those devices periodically.  Even if a pool is unmounted, it is still  
in the ZFS namespace so it still exists.  If you want to change around  
device configs, you need to first delete the pool.

The panic below is forced since we see the drive we're trying to write  
to has disappeared, hence we can't write to it, and rather then lie  
about what got written, we force a panic rather then render the  
filesystem inconsistent.

Noel

On Sep 23, 2008, at 9:11 PM, Richard McClellan wrote:

> Hi,
>
> I've discovered that changing iSCSI target settings cause a kernel
> panic in ZFS code.
>
> To do this I:
> 1. mount (initiate) an iSCSI target
> 2. unmount the target
> 3. change the LUNs on the iSCSI target
> 4. restart iSCSI
> 5. remount (initiate) the iSCSI target
>
> Within 10 seconds the screen ghosts out and a message in several
> languages informs me to press and hold the power button for several
> seconds to restart the computer.  Here's the message that was sent to
> Apple:
>
> Tue Sep 23 21:02:29 2008
> panic(cpu 0 caller 0x00BCDD21): "ZFS: I/O failure (write on <unknown>
> off 0: zio 0xb739ee8 [L0 object directory] 400L/200P
> DVA[0]=<0:640027cc00:400> DVA[1]=<0:d200163800:400>
> DVA[2]=<0:1b8004a5c00:400> fletcher4 lzjb LE contiguous birth=52073
> fill=1 cksum=cd77a5e82:563ac882499:12708c86bf606:2ada8ee44bf317):
> error " "6"@/Volumes/pixie_dust/home/ndellofano/zfs-work/zfs-119/
> zfs_kext/zfs/zio.c:918
> Backtrace (CPU 0), Frame : Return Address (4 potential args on stack)
> 0x3f857e48 : 0x12b0fa (0x459234 0x3f857e7c 0x133243 0x0)
> 0x3f857e98 : 0xbcdd21 (0xc3d6b0 0xc3d6a4 0xc39400 0xc6a670)
> 0x3f857f18 : 0xbca3fb (0xb739ee8 0x34 0x3f857f38 0xc1118a)
> 0x3f857f58 : 0xc29283 (0xb739ee8 0x1 0xc73ef4 0xb4818f8)
> 0x3f857fc8 : 0x19eccc (0xb4818f8 0x0 0x1a20b5 0xb674410)
> Backtrace terminated-invalid frame pointer 0
>       Kernel loadable modules in backtrace (with dependencies):
>          com.apple.filesystems.zfs(8.0)@0xbb6000->0xc81fff
>
> BSD process name corresponding to current thread: kernel_task
>
> Mac OS version:
> 9F33
>
> Kernel version:
> Darwin Kernel Version 9.5.0: Wed Sep  3 11:29:43 PDT 2008;
> root:xnu-1228.7.58~1/RELEASE_I386
> System model name: Macmini2,1 (Mac-F4208EAA)
>
> Is ZFS caching information about a particular target that causes it to
> s#!t the bed when the target has in fact changed?  The problem goes
> away after the crash, so this is purely informational (so far!).
>
> 		Rich
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From zfs at hessmann.de  Wed Sep 24 10:03:23 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Wed, 24 Sep 2008 19:03:23 +0200
Subject: [zfs-discuss] export/import Pool based on files
Message-ID: <FA76474A-89B0-469E-8C2E-720539A4BDD7@hessmann.de>

Good evening,


for fun, I created a Pool "test" with 2 RAID.Z virtual devices of 3  
files each (mkfile 64m file0 ... file5).
After playing around for some time, replacing devices/files with  
bigger ones, I decided to export and import the Pool to make the  
additional space available:

=======
Galahad:ZFS hessi$ zpool export -f test
Galahad:ZFS hessi$ zpool import test
cannot import 'test': no such pool available
Galahad:ZFS hessi$ zpool import -f test
cannot import 'test': no such pool available
Galahad:ZFS hessi$ zpool import
no pools available to import
Galahad:ZFS hessi$ zpool import -D
no pools available to import
=======

I understand that I have to format disks with ZFS to prevent errors  
during import, but AFAIK I can't format files which I use as devices.  
So what did I do wrong?


Best regards,

Christian

From franzschmalzl at spamfreemail.de  Thu Sep 25 04:56:59 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Thu, 25 Sep 2008 13:56:59 +0200
Subject: [zfs-discuss] resize zfs partition
Message-ID: <07A80340-44E0-4E31-A28F-E249B2BCF07B@spamfreemail.de>

hey list!

i currently have 2 partitions on my main harddrive
1. HFS+ for osx
2. ZFS for home

would it be possible to resize the zfs partition ?

( i need some native windows here, shame on me )

greetings

franz




From aorchid at mac.com  Thu Sep 25 10:22:10 2008
From: aorchid at mac.com (Aric Gregson)
Date: Thu, 25 Sep 2008 10:22:10 -0700
Subject: [zfs-discuss] resize zfs partition
In-Reply-To: <07A80340-44E0-4E31-A28F-E249B2BCF07B@spamfreemail.de>
References: <07A80340-44E0-4E31-A28F-E249B2BCF07B@spamfreemail.de>
Message-ID: <20080925102210.a58fda68.aorchid@mac.com>

On Thu, 25 Sep 2008 13:56:59 +0200
ruebezahl <franzschmalzl at spamfreemail.de> wrote:

> would it be possible to resize the zfs partition ?

You cannot shrink a zpool. If the size of the zfs filesystems on
the zpool is smaller than the zpool itself, you should be able to
backup the zpool filesystems elsewhere, then create a new zpool to the
size you like and then send the filesystems back to it.

aric

From ndellofano at apple.com  Thu Sep 25 13:48:48 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu, 25 Sep 2008 13:48:48 -0700
Subject: [zfs-discuss] export/import Pool based on files
In-Reply-To: <FA76474A-89B0-469E-8C2E-720539A4BDD7@hessmann.de>
References: <FA76474A-89B0-469E-8C2E-720539A4BDD7@hessmann.de>
Message-ID: <DBA95F1C-3241-48F9-BF18-D759BF26EA9C@apple.com>

for file vdevs you need to tell import where to look, so if your files  
you were using as devices lived in /Users/Shared, you'd need to do:

zpool import -d /Users/Shared test

Noel

On Sep 24, 2008, at 10:03 AM, Christian He?mann wrote:

> Good evening,
>
>
> for fun, I created a Pool "test" with 2 RAID.Z virtual devices of 3
> files each (mkfile 64m file0 ... file5).
> After playing around for some time, replacing devices/files with
> bigger ones, I decided to export and import the Pool to make the
> additional space available:
>
> =======
> Galahad:ZFS hessi$ zpool export -f test
> Galahad:ZFS hessi$ zpool import test
> cannot import 'test': no such pool available
> Galahad:ZFS hessi$ zpool import -f test
> cannot import 'test': no such pool available
> Galahad:ZFS hessi$ zpool import
> no pools available to import
> Galahad:ZFS hessi$ zpool import -D
> no pools available to import
> =======
>
> I understand that I have to format disks with ZFS to prevent errors
> during import, but AFAIK I can't format files which I use as devices.
> So what did I do wrong?
>
>
> Best regards,
>
> Christian
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Thu Sep 25 14:00:28 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu, 25 Sep 2008 14:00:28 -0700
Subject: [zfs-discuss] resize zfs partition
In-Reply-To: <07A80340-44E0-4E31-A28F-E249B2BCF07B@spamfreemail.de>
References: <07A80340-44E0-4E31-A28F-E249B2BCF07B@spamfreemail.de>
Message-ID: <A49FC704-E7F8-4A99-B2AD-7E6612F7E3A9@apple.com>

are you trying to make the ZFS partition bigger or smaller?

If you want to make it bigger, since you only have one hard drive  
you're sharing, in order to give zfs more space you're going to have  
to either
1. repartition your hfs partition, split it into two, and zpool add  
the new partition (the one you're not keeping osx on) to your pool
2. back up your zfs pool, repartition your hard drive completely  
giving the ZFS partition more space

Noel

On Sep 25, 2008, at 4:56 AM, ruebezahl wrote:

> hey list!
>
> i currently have 2 partitions on my main harddrive
> 1. HFS+ for osx
> 2. ZFS for home
>
> would it be possible to resize the zfs partition ?
>
> ( i need some native windows here, shame on me )
>
> greetings
>
> franz
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From franzschmalzl at spamfreemail.de  Thu Sep 25 14:25:18 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Thu, 25 Sep 2008 23:25:18 +0200
Subject: [zfs-discuss] resize zfs partition
In-Reply-To: <A49FC704-E7F8-4A99-B2AD-7E6612F7E3A9@apple.com>
References: <07A80340-44E0-4E31-A28F-E249B2BCF07B@spamfreemail.de>
	<A49FC704-E7F8-4A99-B2AD-7E6612F7E3A9@apple.com>
Message-ID: <4848D4B5-80AB-47CC-B24D-8D7EBC5E5C90@spamfreemail.de>

jaaah, i wanted to shrink it down,

i basically ended up deleting everything and restore from my raidz  
array :)

but thanks for answers noel and aric


franz




On 25.09.2008, at 23:00, No?l Dellofano wrote:

> are you trying to make the ZFS partition bigger or smaller?
>
> If you want to make it bigger, since you only have one hard drive  
> you're sharing, in order to give zfs more space you're going to have  
> to either
> 1. repartition your hfs partition, split it into two, and zpool add  
> the new partition (the one you're not keeping osx on) to your pool
> 2. back up your zfs pool, repartition your hard drive completely  
> giving the ZFS partition more space
>
> Noel
>
> On Sep 25, 2008, at 4:56 AM, ruebezahl wrote:
>
>> hey list!
>>
>> i currently have 2 partitions on my main harddrive
>> 1. HFS+ for osx
>> 2. ZFS for home
>>
>> would it be possible to resize the zfs partition ?
>>
>> ( i need some native windows here, shame on me )
>>
>> greetings
>>
>> franz
>>
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From zfs at hessmann.de  Thu Sep 25 16:20:49 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Fri, 26 Sep 2008 01:20:49 +0200
Subject: [zfs-discuss] export/import Pool based on files
In-Reply-To: <DBA95F1C-3241-48F9-BF18-D759BF26EA9C@apple.com>
References: <FA76474A-89B0-469E-8C2E-720539A4BDD7@hessmann.de>
	<DBA95F1C-3241-48F9-BF18-D759BF26EA9C@apple.com>
Message-ID: <75FA3442-0FC4-4D2B-8DCE-FB4B89F09607@hessmann.de>

On 25.09.2008, at 22:48, No?l Dellofano wrote:

>> I understand that I have to format disks with ZFS to prevent errors
>> during import, but AFAIK I can't format files which I use as devices.
>> So what did I do wrong?
> for file vdevs you need to tell import where to look, so if your  
> files you were using as devices lived in /Users/Shared, you'd need  
> to do:
>
> zpool import -d /Users/Shared test


I was going to write whether you could document that, but fortunately  
I took a second look at the man page and there it is, right in the  
middle... stupid me. :-(

Thanks No?l.


Best regards,

Christian

From zfs at hessmann.de  Fri Sep 26 01:26:48 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Fri, 26 Sep 2008 10:26:48 +0200
Subject: [zfs-discuss] chksum error on RAID.Z - severity?
Message-ID: <DB9A2F12-AD43-41B9-913A-23DA8977D519@hessmann.de>

Me again...

After adding three more disks to my RAID.Z Pool (now 2x 3 disks) I  
issued a scrub just to be sure everything is fine.

Well, apparently it isn't, since zpool tells me I have chksum errors  
(85% done, 5 errors so far) on one of the additional disks (which  
isn't new and the smallest of all: old 3x 750GB, new 500/200/120GB).  
That same disk worked fine as a standalone ZFS device until yesterday,  
with scrubs without any error messages.

After searching the web and the opensolaris zfs-discuss mailinglist  
for quite some time, I discovered that there are/were a few bugs  
related to false chksum error messages during scrub, especially with  
high activity Pools and lots of unfinished scrubs (this is my... ~10th  
attempt on this Pool due to snapshots interrupting).

What is the status of "our" ZFS for OS X? Any known bugs regarding  
false chksum error messages? I would love to make additional tests,  
with zdb for example, but I think that's not available for OS X yet,  
is it?

It wouldn't be that big an issue to exchange this disk with another,  
possibly bigger one, but is it necessary?

Thank you.


Best regards,

Christian

From zfs at hessmann.de  Fri Sep 26 04:49:40 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Fri, 26 Sep 2008 13:49:40 +0200
Subject: [zfs-discuss] iTunes doesn't find specific files on ZFS
Message-ID: <006C58F7-15BD-4366-9080-F2167D48DBBB@hessmann.de>

and again me...

Known problem?

iTunes 8 (and before that, 7 as well) seems to have some problems with  
finding specific songs on my external ZFS volume (mounted on /Volumes/ 
music.hessi) with a filesystem iTunes on it:

galahad:~ hessi$ zfs list
NAME                                USED  AVAIL  REFER  MOUNTPOINT
music.hessi                         122G  23,7G  21,2G  /Volumes/ 
music.hessi
music.hessi/iTunes                  101G  23,7G  99,3G  /Volumes/ 
music.hessi/iTunes

Surprisingly, it only occurs with (apparently random) files in these  
three directories:

/Volumes/music.hessi/iTunes/iTunes.hessi/iTunes Music/My Chemical  
Romance/Three Cheers for Sweet Revenge/
/Volumes/music.hessi/iTunes/iTunes.hessi/iTunes Music/Wir sind Helden/ 
Die Reklamation/
/Volumes/music.hessi/iTunes/iTunes.hessi/iTunes Music/Tegan And Sara/ 
So Jealous/

All filenames have a space between track number and title, some of  
them have special characters:

08 Interlude.m4a
05 I'm Not Okay (I Promise).m4p
10 M?ssen nur wollen.m4a

There are 2829 Directories/Artists in /Volumes/music.hessi/iTunes/ 
iTunes.hessi/iTunes Music/, a lot of them with similar length (if not  
longer) and spaces as well as special characters like above in their  
paths and filenames. Still, during the last couple of months, it only  
occurred in these three directories.

After manually pointing iTunes to the right file, it finds and plays  
it, even between reboots, exports and imports of the pool. The error  
occurs about once a week, each time with a different file (in one of  
these directories).

Any hint on what's going on? Can I be of any more assistance?

Thank you.


Best regards,

Christian

From troels.n.nielsen at gmail.com  Sun Sep 14 12:33:59 2008
From: troels.n.nielsen at gmail.com (=?ISO-8859-1?Q?Troels_N=F8rgaard_Nielsen?=)
Date: Sun, 14 Sep 2008 21:33:59 +0200
Subject: [zfs-discuss] zfs-crypto for Mac OS X
Message-ID: <6024FDE6-9F8F-4654-8056-BA80CA525863@gmail.com>

Hey everyone, No?l,

In some time the OpenSolaris zfs-crypto have been feature complete for
first phase of the implementation to bring on-disk encryption support
for our beloved filesystem, ZFS. As of 5th September 2008 Darren
Moffat has annouced the project going into QA/Code review.

With this in mind, what are Apple's intentions regarding a OS X port
of these features? I personally see it as one of the greatest features
to mobile business users, to have support for full disk-encryption. I
know that FileVault's history goes back some time, but it did take on
the problem at the wrong end, as in a userland application workaround
to provide users with user directory encryption.

So, what is your and Apple's plans for zfs disk encryption support, is
it something we should be looking forward to?
As far as I can see in the source code, it seems as Darren's code is
pretty portable, with the exception for the ties to the Sun
Cryptographic framework, but I'm far from no expert.

Thanks for the openness around the project so far.

----
Best Regards

Troels N?rgaard Nielsen
t86 Consultancy




From mrezny at hexaneinc.com  Fri Sep 26 11:27:24 2008
From: mrezny at hexaneinc.com (Matthew Rezny)
Date: Fri, 26 Sep 2008 11:27:24 -0700
Subject: [zfs-discuss] chksum error on RAID.Z - severity?
Message-ID: <E0B498CA-4477-4C5F-9CC0-2545D42B04C3@hexaneinc.com>

> and again me...
>
> Known problem?
>
> iTunes 8 (and before that, 7 as well) seems to have some problems with
> finding specific songs on my external ZFS volume (mounted on /Volumes/
> music.hessi) with a filesystem iTunes on it:
>
> galahad:~ hessi$ zfs list
> NAME                                USED  AVAIL  REFER  MOUNTPOINT
> music.hessi                         122G  23,7G  21,2G  /Volumes/
> music.hessi
> music.hessi/iTunes                  101G  23,7G  99,3G  /Volumes/
> music.hessi/iTunes
>
> Surprisingly, it only occurs with (apparently random) files in these
> three directories:
>
> /Volumes/music.hessi/iTunes/iTunes.hessi/iTunes Music/My Chemical
> Romance/Three Cheers for Sweet Revenge/
> /Volumes/music.hessi/iTunes/iTunes.hessi/iTunes Music/Wir sind Helden/
> Die Reklamation/
> /Volumes/music.hessi/iTunes/iTunes.hessi/iTunes Music/Tegan And Sara/
> So Jealous/
>
> All filenames have a space between track number and title, some of
> them have special characters:
>
> 08 Interlude.m4a
> 05 I'm Not Okay (I Promise).m4p
> 10 M?ssen nur wollen.m4a
>
> There are 2829 Directories/Artists in /Volumes/music.hessi/iTunes/
> iTunes.hessi/iTunes Music/, a lot of them with similar length (if not
> longer) and spaces as well as special characters like above in their
> paths and filenames. Still, during the last couple of months, it only
> occurred in these three directories.
>
> After manually pointing iTunes to the right file, it finds and plays
> it, even between reboots, exports and imports of the pool. The error
> occurs about once a week, each time with a different file (in one of
> these directories).
>
> Any hint on what's going on? Can I be of any more assistance?
>
> Thank you.
>
>
> Best regards,
>
> Christian
>
I have seen this problem myself and I believe I know the source of the  
problem. The good news is that once you have all instances fixed, it  
stays fixed. The other good news is that if you start with ZFS it  
doesn't affect you. The bad news is that almost all of us started with  
HFS+ and migrated our iTunes Library to ZFS.

One of the (few) really nice things about iTunes is the library  
management. Editing a song title, album name, or artist name in the  
library not only updates the tags, but renames the file and/or folder  
it is in. iTunes stores the complete list of files in an XML document  
so it doesn't have to walk the folder tree every time its opened. As  
long as you always manage the library though iTunes, the XML library  
file should always be in sync with the music files in the library.  
However, the definition of in-sync depends on the underlying filesystem.

With HFS+, the file system is case preserving, but not case sensitive.  
So, if you rename a file such that only the case has changed, you can  
still open it with the old name. With ZFS, which is truly case  
sensitive, the attempt to open with the old name, differing only by  
case, fails. When you have your iTunes library on an HFS+ drive and  
you edit a song attribute that affects full file path (artist name,  
album title or song title) but only by changing the case, the XML  
library file reflects the change but the file is not actually renamed.  
That is fairly harmless, though not technically correct, as long as  
your files are still on the HFS+ volume. However, when you migrate  
your library to a ZFS volume, and the case does matter, these files  
are suddenly inaccessible. Worse, since iTunes doesn't actually try to  
open a file until you want to play it or edit the tags, you don't  
immediately know which are affected when the library is initially  
migrated.

The workaround, as you've found, is to re-point iTunes to the very  
same file, forcing it to update its filename in the XML library file.  
This then makes the filename consistent between library and on-disk.  
Whenever I bumped into one of these, I would simply reprocess the  
entire album knowing that chances are it affects the whole thing. The  
proper fix is to fix iTunes so that it actually renames the file  
rather than taking the shortcut and assuming it will still be  
accessible with the case difference.

This is actually an issue for the iTunes team rather than the ZFS  
team, but perhaps No?l can go kick the right people in the butt and  
get this resolved. There are a lot more bugs in iTunes which are more  
severe, but they remain unfixed across major versions despite feedback  
sent through the iTunes feedback mechanism. So, I wouldn't expect much  
action on this bug without pressure from within Apple. The iTunes team  
is much more focused on adding yet more flashy trash 'features' than  
making the basic operations function properly. For example, if you are  
in album shuffle, and two bands have albums by the same name, iTunes  
can't play either as an album because it gets confused as it considers  
only the album tag rather than a combination of artist and album tag.  
If you have a large library, this particular bug comes up very easily.  
The real fix would actually involve looking as track and disc tags as  
well since some bands have albums and singles of the same name, which  
also get mixed up. Though, the most bizarre omission with album  
shuffle is the fact that party shuffle, the only mode that allows you  
to see what's next in the random play, ignores the setting and still  
picks individual songs, even though every other aspect of iTunes  
respects the setting including smart playlists set to randomly select  
on certain criteria.



From aorchid at mac.com  Fri Sep 26 12:19:41 2008
From: aorchid at mac.com (Aric Gregson)
Date: Fri, 26 Sep 2008 12:19:41 -0700
Subject: [zfs-discuss] iDisk and ZFS Stability Problem? (or just an iDisk
	problem)
Message-ID: <20080926121941.281ee4c7.aorchid@mac.com>

Mac ZFS Community,

We recently (days ago) migrated two users' entire home directories to a
ZFS formatted internal drives (mirrored, entire slice both disks). This
was after using ZFS for Final Cut for many months without any problems. 

Currently, when attempting to mount our own iDisk to view contents or 
copy a file to or from the iDisk, the Finder freezes---sort of. The copy
to or from iDisk does not work, it says that it is copying, but actually
never completes the process, a file of the same name and 0 bytes shows
up on the iDisk. iDisk sync is off, because I thought this was the
problem two days ago.

Meanwhile, the Finder allows one to open applications, click through
windows and open files, but will not update any new files on the
computer. For example, if you save a new file to the Desktop, the
Finder will not show you the file, even though it exists in
Terminal (and in Finder upon reboot). 

Additionally, you cannot copy anything from a mounted disk (dmg, USB) to
anywhere on the computer, ZFS or HFS+ formatted disks, until the
Finder restarts (restarting the computer).

Logout does not work in these cases either, I have to do a manual
reboot from Terminal. I haven't tried to specifically kill Finder, as I
am not clear on how to do that in Mac.

Has anyone experienced anything vaguely similar? Is this an iDisk issue
or does the iDisk/Finder combination have problems with ZFS? That said,
cadaver does not have issues with ZFS and iDisk connections, as I can
copy files to and from our iDisk using Solaris (on ZFS). In my
experience, the Finder/iDisk combination has been unusable on Mac in
the past, perhaps this is just a continuation of the same.

Thanks for any ideas. 

> uname -a
Darwin PowerMac1.local 9.5.0 Darwin Kernel Version 9.5.0: Wed Sep  3
11:29:43 PDT 2008; root:xnu-1228.7.58~1/RELEASE_I386 i386

aric

From aorchid at mac.com  Fri Sep 26 12:27:57 2008
From: aorchid at mac.com (Aric Gregson)
Date: Fri, 26 Sep 2008 12:27:57 -0700
Subject: [zfs-discuss] iDisk and ZFS Stability Problem? (or just an
 iDisk problem)
In-Reply-To: <20080926121941.281ee4c7.aorchid@mac.com>
References: <20080926121941.281ee4c7.aorchid@mac.com>
Message-ID: <20080926122757.cc9a4b37.aorchid@mac.com>

Apologies. It seems that this remains a Mac Finder issue, as I doubt
that all of these persons on the discussion forum have formatted their
computers to ZFS.

<http://discussions.apple.com/thread.jspa?threadID=1703193&tstart=135>

On Fri, 26 Sep 2008 12:19:41 -0700
Aric Gregson <aorchid at mac.com> wrote:

> Mac ZFS Community,
> 
> We recently (days ago) migrated two users' entire home directories to
> a ZFS formatted internal drives (mirrored, entire slice both disks).
> This was after using ZFS for Final Cut for many months without any
> problems. 
> 
> Currently, when attempting to mount our own iDisk to view contents or 
> copy a file to or from the iDisk, the Finder freezes---sort of. The
> copy to or from iDisk does not work, it says that it is copying, but
> actually never completes the process, a file of the same name and 0
> bytes shows up on the iDisk. iDisk sync is off, because I thought
> this was the problem two days ago.
> 
> Meanwhile, the Finder allows one to open applications, click through
> windows and open files, but will not update any new files on the
> computer. For example, if you save a new file to the Desktop, the
> Finder will not show you the file, even though it exists in
> Terminal (and in Finder upon reboot). 
> 
> Additionally, you cannot copy anything from a mounted disk (dmg, USB)
> to anywhere on the computer, ZFS or HFS+ formatted disks, until the
> Finder restarts (restarting the computer).
> 
> Logout does not work in these cases either, I have to do a manual
> reboot from Terminal. I haven't tried to specifically kill Finder, as
> I am not clear on how to do that in Mac.
> 
> Has anyone experienced anything vaguely similar? Is this an iDisk
> issue or does the iDisk/Finder combination have problems with ZFS?
> That said, cadaver does not have issues with ZFS and iDisk
> connections, as I can copy files to and from our iDisk using Solaris
> (on ZFS). In my experience, the Finder/iDisk combination has been
> unusable on Mac in the past, perhaps this is just a continuation of
> the same.
> 
> Thanks for any ideas. 
> 
> > uname -a
> Darwin PowerMac1.local 9.5.0 Darwin Kernel Version 9.5.0: Wed Sep  3
> 11:29:43 PDT 2008; root:xnu-1228.7.58~1/RELEASE_I386 i386
> 
> aric
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


-- 
Aric Gregson

From ndellofano at apple.com  Fri Sep 26 18:43:12 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 26 Sep 2008 18:43:12 -0700
Subject: [zfs-discuss] iDisk and ZFS Stability Problem? (or just an
	iDisk problem)
In-Reply-To: <20080926122757.cc9a4b37.aorchid@mac.com>
References: <20080926121941.281ee4c7.aorchid@mac.com>
	<20080926122757.cc9a4b37.aorchid@mac.com>
Message-ID: <71A02FB2-FAAC-4F44-A735-0305963D31BA@apple.com>

hey Aric, thanks for doing the homework.  That's good info to know.   
Sorry your experience is so bad :(

I'll see if I can find internally whos working on the iDisk Freezing  
issue and if it's coming in an update hopefully soon...

thanks!
Noel

On Sep 26, 2008, at 12:27 PM, Aric Gregson wrote:

> Apologies. It seems that this remains a Mac Finder issue, as I doubt
> that all of these persons on the discussion forum have formatted their
> computers to ZFS.
>
> <http://discussions.apple.com/thread.jspa?threadID=1703193&tstart=135>
>
> On Fri, 26 Sep 2008 12:19:41 -0700
> Aric Gregson <aorchid at mac.com> wrote:
>
>> Mac ZFS Community,
>>
>> We recently (days ago) migrated two users' entire home directories to
>> a ZFS formatted internal drives (mirrored, entire slice both disks).
>> This was after using ZFS for Final Cut for many months without any
>> problems.
>>
>> Currently, when attempting to mount our own iDisk to view contents or
>> copy a file to or from the iDisk, the Finder freezes---sort of. The
>> copy to or from iDisk does not work, it says that it is copying, but
>> actually never completes the process, a file of the same name and 0
>> bytes shows up on the iDisk. iDisk sync is off, because I thought
>> this was the problem two days ago.
>>
>> Meanwhile, the Finder allows one to open applications, click through
>> windows and open files, but will not update any new files on the
>> computer. For example, if you save a new file to the Desktop, the
>> Finder will not show you the file, even though it exists in
>> Terminal (and in Finder upon reboot).
>>
>> Additionally, you cannot copy anything from a mounted disk (dmg, USB)
>> to anywhere on the computer, ZFS or HFS+ formatted disks, until the
>> Finder restarts (restarting the computer).
>>
>> Logout does not work in these cases either, I have to do a manual
>> reboot from Terminal. I haven't tried to specifically kill Finder, as
>> I am not clear on how to do that in Mac.
>>
>> Has anyone experienced anything vaguely similar? Is this an iDisk
>> issue or does the iDisk/Finder combination have problems with ZFS?
>> That said, cadaver does not have issues with ZFS and iDisk
>> connections, as I can copy files to and from our iDisk using Solaris
>> (on ZFS). In my experience, the Finder/iDisk combination has been
>> unusable on Mac in the past, perhaps this is just a continuation of
>> the same.
>>
>> Thanks for any ideas.
>>
>>> uname -a
>> Darwin PowerMac1.local 9.5.0 Darwin Kernel Version 9.5.0: Wed Sep  3
>> 11:29:43 PDT 2008; root:xnu-1228.7.58~1/RELEASE_I386 i386
>>
>> aric
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
> -- 
> Aric Gregson
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From zorg at sogeeky.net  Fri Sep 26 21:08:23 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Fri, 26 Sep 2008 21:08:23 -0700
Subject: [zfs-discuss] iDisk and ZFS Stability Problem? (or just an
	iDisk problem)
In-Reply-To: <71A02FB2-FAAC-4F44-A735-0305963D31BA@apple.com>
References: <20080926121941.281ee4c7.aorchid@mac.com>
	<20080926122757.cc9a4b37.aorchid@mac.com>
	<71A02FB2-FAAC-4F44-A735-0305963D31BA@apple.com>
Message-ID: <3448D3F1-3AC3-4FB2-AE79-95EFA005CE66@sogeeky.net>

Now if only apple were so forthcoming on other projects!  A little  
open dialog goes a long way. :)

Notice how none of us are pissed off at zfs being buggy?  Compare that  
with iPhone 2.0/2.1...  LOL

Thanks, No?l. Do other people know you're doing this?  :)

On Sep 26, 2008, at 6:43 PM, No?l Dellofano <ndellofano at apple.com>  
wrote:

> hey Aric, thanks for doing the homework.  That's good info to know.
> Sorry your experience is so bad :(
>
> I'll see if I can find internally whos working on the iDisk Freezing
> issue and if it's coming in an update hopefully soon...
>
> thanks!
> Noel
>
> On Sep 26, 2008, at 12:27 PM, Aric Gregson wrote:
>
>> Apologies. It seems that this remains a Mac Finder issue, as I doubt
>> that all of these persons on the discussion forum have formatted  
>> their
>> computers to ZFS.
>>
>> <http://discussions.apple.com/thread.jspa? 
>> threadID=1703193&tstart=135>
>>
>> On Fri, 26 Sep 2008 12:19:41 -0700
>> Aric Gregson <aorchid at mac.com> wrote:
>>
>>> Mac ZFS Community,
>>>
>>> We recently (days ago) migrated two users' entire home directories  
>>> to
>>> a ZFS formatted internal drives (mirrored, entire slice both disks).
>>> This was after using ZFS for Final Cut for many months without any
>>> problems.
>>>
>>> Currently, when attempting to mount our own iDisk to view contents  
>>> or
>>> copy a file to or from the iDisk, the Finder freezes---sort of. The
>>> copy to or from iDisk does not work, it says that it is copying, but
>>> actually never completes the process, a file of the same name and 0
>>> bytes shows up on the iDisk. iDisk sync is off, because I thought
>>> this was the problem two days ago.
>>>
>>> Meanwhile, the Finder allows one to open applications, click through
>>> windows and open files, but will not update any new files on the
>>> computer. For example, if you save a new file to the Desktop, the
>>> Finder will not show you the file, even though it exists in
>>> Terminal (and in Finder upon reboot).
>>>
>>> Additionally, you cannot copy anything from a mounted disk (dmg,  
>>> USB)
>>> to anywhere on the computer, ZFS or HFS+ formatted disks, until the
>>> Finder restarts (restarting the computer).
>>>
>>> Logout does not work in these cases either, I have to do a manual
>>> reboot from Terminal. I haven't tried to specifically kill Finder,  
>>> as
>>> I am not clear on how to do that in Mac.
>>>
>>> Has anyone experienced anything vaguely similar? Is this an iDisk
>>> issue or does the iDisk/Finder combination have problems with ZFS?
>>> That said, cadaver does not have issues with ZFS and iDisk
>>> connections, as I can copy files to and from our iDisk using Solaris
>>> (on ZFS). In my experience, the Finder/iDisk combination has been
>>> unusable on Mac in the past, perhaps this is just a continuation of
>>> the same.
>>>
>>> Thanks for any ideas.
>>>
>>>> uname -a
>>> Darwin PowerMac1.local 9.5.0 Darwin Kernel Version 9.5.0: Wed Sep  3
>>> 11:29:43 PDT 2008; root:xnu-1228.7.58~1/RELEASE_I386 i386
>>>
>>> aric
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>>
>> -- 
>> Aric Gregson
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From kona8lend at gmail.com  Sat Sep 27 07:34:23 2008
From: kona8lend at gmail.com (Kona Blend)
Date: Sat, 27 Sep 2008 10:34:23 -0400
Subject: [zfs-discuss] Handbrake 0.9.2 solved?
Message-ID: <0C436401-3CB8-4530-9597-68A6CA3B8939@gmail.com>

Hi, I did a search of archives in this mailing list and found that a  
few Handbrake users have experienced the issue where using a source  
VIDEO_TS/ stored on ZFS causes Handbrake to malfunction. When copying  
the same folder to HFS+, Handbrake operates as expected.

A patch addressing the issue has been submitted to Handbrake devs and  
hopefully they'll accept it on their trunk branch. The problem is in  
code assuming a device-name can be associated with any folder and in  
case of ZFS (as has been mentioned on this list) there is none. Short  
story short, the patch checks for a NULL vMDeviceID supplied by  
FSGetVolumeParms() and prevents the runtime exception:

[NSString stringWithCString:]: NULL cString

and encoding then proceeds as expected.

--Kona Blend

From franzschmalzl at spamfreemail.de  Sun Sep 28 08:46:36 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Sun, 28 Sep 2008 17:46:36 +0200
Subject: [zfs-discuss] late mount
Message-ID: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>

hey there!

guys: PANIC !

It seems my internal zpool which contains my homefolder gets mounted  
very late in the booting process, maybe not even until i trie to login.
which results in my library/wallpaper and so on not getting loaded  
properly

when logging in as another user first, and then logging in again it  
works ?

is there a way to force osx to mount my zpool at boot time ?

i tried via a launchd entry, but failed.


regards

franz




From ndellofano at apple.com  Mon Sep 29 19:32:46 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon, 29 Sep 2008 19:32:46 -0700
Subject: [zfs-discuss] Handbrake 0.9.2 solved?
In-Reply-To: <0C436401-3CB8-4530-9597-68A6CA3B8939@gmail.com>
References: <0C436401-3CB8-4530-9597-68A6CA3B8939@gmail.com>
Message-ID: <6F59ED50-21CC-407B-9C16-E00197476FCD@apple.com>

awesome! Thanks for the info and the update :)

Noel

On Sep 27, 2008, at 7:34 AM, Kona Blend wrote:

> Hi, I did a search of archives in this mailing list and found that a
> few Handbrake users have experienced the issue where using a source
> VIDEO_TS/ stored on ZFS causes Handbrake to malfunction. When copying
> the same folder to HFS+, Handbrake operates as expected.
>
> A patch addressing the issue has been submitted to Handbrake devs and
> hopefully they'll accept it on their trunk branch. The problem is in
> code assuming a device-name can be associated with any folder and in
> case of ZFS (as has been mentioned on this list) there is none. Short
> story short, the patch checks for a NULL vMDeviceID supplied by
> FSGetVolumeParms() and prevents the runtime exception:
>
> [NSString stringWithCString:]: NULL cString
>
> and encoding then proceeds as expected.
>
> --Kona Blend
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From werner.donne at re.be  Tue Sep 30 00:09:29 2008
From: werner.donne at re.be (=?ISO-8859-1?Q?Werner_Donn=E9?=)
Date: Tue, 30 Sep 2008 09:09:29 +0200
Subject: [zfs-discuss] late mount
In-Reply-To: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
Message-ID: <76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>

Hi,

This is how it started for me too. Since then it has deteriorated
gradually up to the point, today, where after twenty reboots I still
have no online pool. I wonder if ZFS is suitable for USB-disks on Mac
OS X. If it isn't I'm going to get a Solaris box and move the pool to  
it.

Regards,

Werner.

On 28 Sep 2008, at 17:46, ruebezahl wrote:

> hey there!
>
> guys: PANIC !
>
> It seems my internal zpool which contains my homefolder gets mounted
> very late in the booting process, maybe not even until i trie to  
> login.
> which results in my library/wallpaper and so on not getting loaded
> properly
>
> when logging in as another user first, and then logging in again it
> works ?
>
> is there a way to force osx to mount my zpool at boot time ?
>
> i tried via a launchd entry, but failed.
>
>
> regards
>
> franz
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

--
Werner Donn?  --  Re                                     http://www.pincette.biz
Engelbeekstraat 8                                               http://www.re.be
BE-3300 Tienen
tel: (+32) 486 425803	e-mail: werner.donne at re.be






From lists at loveturtle.net  Tue Sep 30 05:14:04 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Tue, 30 Sep 2008 08:14:04 -0400
Subject: [zfs-discuss] late mount
In-Reply-To: <76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
Message-ID: <48E2180C.4090102@loveturtle.net>

I just use the login window to add a few more seconds to the login time. 
Usually by the time the login window loads and i type my username and 
password + take a single breath my home dir has mounted and i can safely 
log in.

I also have cron snapshot my home dir by hourly so that if i do log in 
too quick and lose my settings i can just log out and rollback.


Werner Donn? wrote:
> Hi,
>
> This is how it started for me too. Since then it has deteriorated
> gradually up to the point, today, where after twenty reboots I still
> have no online pool. I wonder if ZFS is suitable for USB-disks on Mac
> OS X. If it isn't I'm going to get a Solaris box and move the pool to  
> it.
>
> Regards,
>
> Werner.
>
> On 28 Sep 2008, at 17:46, ruebezahl wrote:
>
>   
>> hey there!
>>
>> guys: PANIC !
>>
>> It seems my internal zpool which contains my homefolder gets mounted
>> very late in the booting process, maybe not even until i trie to  
>> login.
>> which results in my library/wallpaper and so on not getting loaded
>> properly
>>
>> when logging in as another user first, and then logging in again it
>> works ?
>>
>> is there a way to force osx to mount my zpool at boot time ?
>>
>> i tried via a launchd entry, but failed.
>>
>>
>> regards
>>
>> franz
>>
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>     
>
> --
> Werner Donn?  --  Re                                     http://www.pincette.biz
> Engelbeekstraat 8                                               http://www.re.be
> BE-3300 Tienen
> tel: (+32) 486 425803	e-mail: werner.donne at re.be
>
>
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>   


From ndellofano at apple.com  Tue Sep 30 13:31:24 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue, 30 Sep 2008 13:31:24 -0700
Subject: [zfs-discuss] late mount
In-Reply-To: <48E2180C.4090102@loveturtle.net>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
	<48E2180C.4090102@loveturtle.net>
Message-ID: <EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>

This is strange.  A group of us at Apple keep our home directories on  
ZFS on our laptops on the internal drive.  Are all of you guys using  
the internal drives, or USB drives for your home directories? or some  
other type of detachable (non internal bay) drive?  There has been  
some issues in the past noted with external drives since a lot of them  
have power saving features and such so the drive sometimes isn't  
actually spun up,  or takes a bit to get spun up.

Noel

On Sep 30, 2008, at 5:14 AM, Dillon Kass wrote:

> I just use the login window to add a few more seconds to the login  
> time.
> Usually by the time the login window loads and i type my username and
> password + take a single breath my home dir has mounted and i can  
> safely
> log in.
>
> I also have cron snapshot my home dir by hourly so that if i do log in
> too quick and lose my settings i can just log out and rollback.
>
>
> Werner Donn? wrote:
>> Hi,
>>
>> This is how it started for me too. Since then it has deteriorated
>> gradually up to the point, today, where after twenty reboots I still
>> have no online pool. I wonder if ZFS is suitable for USB-disks on Mac
>> OS X. If it isn't I'm going to get a Solaris box and move the pool to
>> it.
>>
>> Regards,
>>
>> Werner.
>>
>> On 28 Sep 2008, at 17:46, ruebezahl wrote:
>>
>>
>>> hey there!
>>>
>>> guys: PANIC !
>>>
>>> It seems my internal zpool which contains my homefolder gets mounted
>>> very late in the booting process, maybe not even until i trie to
>>> login.
>>> which results in my library/wallpaper and so on not getting loaded
>>> properly
>>>
>>> when logging in as another user first, and then logging in again it
>>> works ?
>>>
>>> is there a way to force osx to mount my zpool at boot time ?
>>>
>>> i tried via a launchd entry, but failed.
>>>
>>>
>>> regards
>>>
>>> franz
>>>
>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>
>> --
>> Werner Donn?  --  Re                                     http://www.pincette.biz
>> Engelbeekstraat 8                                               http://www.re.be
>> BE-3300 Tienen
>> tel: (+32) 486 425803	e-mail: werner.donne at re.be
>>
>>
>>
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From andy at aligature.com  Tue Sep 30 13:36:01 2008
From: andy at aligature.com (Andrew Webber)
Date: Tue, 30 Sep 2008 16:36:01 -0400
Subject: [zfs-discuss] late mount
In-Reply-To: <EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
	<48E2180C.4090102@loveturtle.net>
	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
Message-ID: <60b50dc10809301336l47415701jee2a6d5c6a75c7@mail.gmail.com>

This may be off topic, but...

Noel, how are you guys keeping your home directories on ZFS?  Did you
partition the internal drive into an HFS+ partition and a ZFS partition?  Or
is there some secret to booting from ZFS that I don't know?

Best,
Andy

On Tue, Sep 30, 2008 at 4:31 PM, No?l Dellofano <ndellofano at apple.com>wrote:

> This is strange.  A group of us at Apple keep our home directories on
> ZFS on our laptops on the internal drive.  Are all of you guys using
> the internal drives, or USB drives for your home directories? or some
> other type of detachable (non internal bay) drive?  There has been
> some issues in the past noted with external drives since a lot of them
> have power saving features and such so the drive sometimes isn't
> actually spun up,  or takes a bit to get spun up.
>
> Noel
>
> On Sep 30, 2008, at 5:14 AM, Dillon Kass wrote:
>
> > I just use the login window to add a few more seconds to the login
> > time.
> > Usually by the time the login window loads and i type my username and
> > password + take a single breath my home dir has mounted and i can
> > safely
> > log in.
> >
> > I also have cron snapshot my home dir by hourly so that if i do log in
> > too quick and lose my settings i can just log out and rollback.
> >
> >
> > Werner Donn? wrote:
> >> Hi,
> >>
> >> This is how it started for me too. Since then it has deteriorated
> >> gradually up to the point, today, where after twenty reboots I still
> >> have no online pool. I wonder if ZFS is suitable for USB-disks on Mac
> >> OS X. If it isn't I'm going to get a Solaris box and move the pool to
> >> it.
> >>
> >> Regards,
> >>
> >> Werner.
> >>
> >> On 28 Sep 2008, at 17:46, ruebezahl wrote:
> >>
> >>
> >>> hey there!
> >>>
> >>> guys: PANIC !
> >>>
> >>> It seems my internal zpool which contains my homefolder gets mounted
> >>> very late in the booting process, maybe not even until i trie to
> >>> login.
> >>> which results in my library/wallpaper and so on not getting loaded
> >>> properly
> >>>
> >>> when logging in as another user first, and then logging in again it
> >>> works ?
> >>>
> >>> is there a way to force osx to mount my zpool at boot time ?
> >>>
> >>> i tried via a launchd entry, but failed.
> >>>
> >>>
> >>> regards
> >>>
> >>> franz
> >>>
> >>>
> >>>
> >>> _______________________________________________
> >>> zfs-discuss mailing list
> >>> zfs-discuss at lists.macosforge.org
> >>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >>>
> >>
> >> --
> >> Werner Donn?  --  Re
> http://www.pincette.biz
> >> Engelbeekstraat 8
> http://www.re.be
> >> BE-3300 Tienen
> >> tel: (+32) 486 425803        e-mail: werner.donne at re.be
> >>
> >>
> >>
> >>
> >>
> >> _______________________________________________
> >> zfs-discuss mailing list
> >> zfs-discuss at lists.macosforge.org
> >> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >>
> >
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss at lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080930/8f888361/attachment.html 

From lists at durin42.com  Tue Sep 30 13:44:07 2008
From: lists at durin42.com (Augie Fackler)
Date: Tue, 30 Sep 2008 15:44:07 -0500
Subject: [zfs-discuss] late mount
In-Reply-To: <EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
	<48E2180C.4090102@loveturtle.net>
	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
Message-ID: <46CAEFC3-A3EA-4A7A-B696-3084D92F93FC@durin42.com>


On Sep 30, 2008, at 3:31 PM, No?l Dellofano wrote:

> This is strange.  A group of us at Apple keep our home directories on
> ZFS on our laptops on the internal drive.  Are all of you guys using
> the internal drives, or USB drives for your home directories? or some
> other type of detachable (non internal bay) drive?  There has been
> some issues in the past noted with external drives since a lot of them
> have power saving features and such so the drive sometimes isn't
> actually spun up,  or takes a bit to get spun up.

I've got a ZFS partition on the internal in my MBP, and I can  
generally count on having to manually use zpool to import it when I  
reboot. If it'd help, I can try to debug why that is, but I'm honestly  
not sure where I'd look.

Augie

> Noel
>
> On Sep 30, 2008, at 5:14 AM, Dillon Kass wrote:
>
>> I just use the login window to add a few more seconds to the login
>> time.
>> Usually by the time the login window loads and i type my username and
>> password + take a single breath my home dir has mounted and i can
>> safely
>> log in.
>>
>> I also have cron snapshot my home dir by hourly so that if i do log  
>> in
>> too quick and lose my settings i can just log out and rollback.
>>
>>
>> Werner Donn? wrote:
>>> Hi,
>>>
>>> This is how it started for me too. Since then it has deteriorated
>>> gradually up to the point, today, where after twenty reboots I still
>>> have no online pool. I wonder if ZFS is suitable for USB-disks on  
>>> Mac
>>> OS X. If it isn't I'm going to get a Solaris box and move the pool  
>>> to
>>> it.
>>>
>>> Regards,
>>>
>>> Werner.
>>>
>>> On 28 Sep 2008, at 17:46, ruebezahl wrote:
>>>
>>>
>>>> hey there!
>>>>
>>>> guys: PANIC !
>>>>
>>>> It seems my internal zpool which contains my homefolder gets  
>>>> mounted
>>>> very late in the booting process, maybe not even until i trie to
>>>> login.
>>>> which results in my library/wallpaper and so on not getting loaded
>>>> properly
>>>>
>>>> when logging in as another user first, and then logging in again it
>>>> works ?
>>>>
>>>> is there a way to force osx to mount my zpool at boot time ?
>>>>
>>>> i tried via a launchd entry, but failed.
>>>>
>>>>
>>>> regards
>>>>
>>>> franz
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>
>>> --
>>> Werner Donn?  --  Re                                     http://www.pincette.biz
>>> Engelbeekstraat 8                                               http://www.re.be
>>> BE-3300 Tienen
>>> tel: (+32) 486 425803	e-mail: werner.donne at re.be
>>>
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From lists at loveturtle.net  Tue Sep 30 14:11:10 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Tue, 30 Sep 2008 17:11:10 -0400
Subject: [zfs-discuss] late mount
In-Reply-To: <EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
	<48E2180C.4090102@loveturtle.net>
	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
Message-ID: <48E295EE.6070506@loveturtle.net>

I have a mac pro and I'm just using two 320gb sata drives internally 
(neither of these are my HFS+ /)

Without using the login window I would say about half the time I log in 
I'll be welcomed to a default dock & wallpaper but with all of my 
desktop icons.

It seems like there's a check done to see if my home directory exists 
and since it's not mounted yet it fails and attempts to generate a nice 
default. only somewhere between checking to see if my home dir exists & 
generating a default my home dir will mount and the real files in my 
home dir will be overwritten.

You guys aren't seeing this? Maybe that's because you're using zfs on 
slices on the same disk that your HFS+ / lives on and we're (or at least 
"I am") not.

most of the time if I quickly login as >console as soon as the 
loginwindow launches I can see debug messages from fseventsd about "log 
dir: /some/zfs/mountpoint/.fseventsd getting new uuid: blah blah" So 
loginwindow is definitely launching before these filesystems mount. (i 
just did this right now to be sure).

No?l Dellofano wrote:
> This is strange.  A group of us at Apple keep our home directories on 
> ZFS on our laptops on the internal drive.  Are all of you guys using 
> the internal drives, or USB drives for your home directories? or some 
> other type of detachable (non internal bay) drive?  There has been 
> some issues in the past noted with external drives since a lot of them 
> have power saving features and such so the drive sometimes isn't 
> actually spun up,  or takes a bit to get spun up.
>
> Noel
>
> On Sep 30, 2008, at 5:14 AM, Dillon Kass wrote:
>
>> I just use the login window to add a few more seconds to the login time.
>> Usually by the time the login window loads and i type my username and
>> password + take a single breath my home dir has mounted and i can safely
>> log in.
>>
>> I also have cron snapshot my home dir by hourly so that if i do log in
>> too quick and lose my settings i can just log out and rollback.
>>
>>
>> Werner Donn? wrote:
>>> Hi,
>>>
>>> This is how it started for me too. Since then it has deteriorated
>>> gradually up to the point, today, where after twenty reboots I still
>>> have no online pool. I wonder if ZFS is suitable for USB-disks on Mac
>>> OS X. If it isn't I'm going to get a Solaris box and move the pool to
>>> it.
>>>
>>> Regards,
>>>
>>> Werner.
>>>
>>> On 28 Sep 2008, at 17:46, ruebezahl wrote:
>>>
>>>
>>>> hey there!
>>>>
>>>> guys: PANIC !
>>>>
>>>> It seems my internal zpool which contains my homefolder gets mounted
>>>> very late in the booting process, maybe not even until i trie to
>>>> login.
>>>> which results in my library/wallpaper and so on not getting loaded
>>>> properly
>>>>
>>>> when logging in as another user first, and then logging in again it
>>>> works ?
>>>>
>>>> is there a way to force osx to mount my zpool at boot time ?
>>>>
>>>> i tried via a launchd entry, but failed.
>>>>
>>>>
>>>> regards
>>>>
>>>> franz
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>
>>> -- 
>>> Werner Donn?  --  Re                                     
>>> http://www.pincette.biz
>>> Engelbeekstraat 8                                               
>>> http://www.re.be
>>> BE-3300 Tienen
>>> tel: (+32) 486 425803    e-mail: werner.donne at re.be
>>>
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From aorchid at mac.com  Tue Sep 30 15:09:28 2008
From: aorchid at mac.com (Aric Gregson)
Date: Tue, 30 Sep 2008 15:09:28 -0700
Subject: [zfs-discuss] late mount
In-Reply-To: <46CAEFC3-A3EA-4A7A-B696-3084D92F93FC@durin42.com>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
	<48E2180C.4090102@loveturtle.net>
	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
	<46CAEFC3-A3EA-4A7A-B696-3084D92F93FC@durin42.com>
Message-ID: <20080930150928.f989f8f6.aorchid@mac.com>

On Tue, 30 Sep 2008 15:44:07 -0500
Augie Fackler <lists at durin42.com> wrote:

> I've got a ZFS partition on the internal in my MBP, and I can  
> generally count on having to manually use zpool to import it when I  
> reboot.

We are having a similar problem here on a MacPro. Our 'home' for the
main user is on a ZFS (only) formatted, mirrored internal drive. When
we reboot, we have to log in as another user on the HFS+ boot drive
(created just for this purpose) and manually force import the ZFS
pools. Every single time. 

I'd love to help debug this and figure out what the problem is. Not
sure where to look, but will if pointed in the right direction....

thanks, aric

From lists at loveturtle.net  Tue Sep 30 15:26:37 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Tue, 30 Sep 2008 18:26:37 -0400
Subject: [zfs-discuss] late mount
In-Reply-To: <20080930150928.f989f8f6.aorchid@mac.com>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>	<48E2180C.4090102@loveturtle.net>	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>	<46CAEFC3-A3EA-4A7A-B696-3084D92F93FC@durin42.com>
	<20080930150928.f989f8f6.aorchid@mac.com>
Message-ID: <48E2A79D.9020100@loveturtle.net>

 From what I've seen on this list the "not importing on boot" is 
generally because diskutil wasn't used first to prepare the disk before 
adding it to your zpool. Even if you did use diskutil to prepare the 
disk if you inserted the entire disk and not the s2 slice it would 
probably act this way as well.

note that you can also log in as the user >console from loginwindow and 
it will drop you to the command line. This is a little easier than 
creating a second user.

Aric Gregson wrote:
> On Tue, 30 Sep 2008 15:44:07 -0500
> Augie Fackler<lists at durin42.com>  wrote:
>
>    
>> I've got a ZFS partition on the internal in my MBP, and I can
>> generally count on having to manually use zpool to import it when I
>> reboot.
>>      
>
> We are having a similar problem here on a MacPro. Our 'home' for the
> main user is on a ZFS (only) formatted, mirrored internal drive. When
> we reboot, we have to log in as another user on the HFS+ boot drive
> (created just for this purpose) and manually force import the ZFS
> pools. Every single time.
>
> I'd love to help debug this and figure out what the problem is. Not
> sure where to look, but will if pointed in the right direction....
>
> thanks, aric
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>    


From aorchid at mac.com  Tue Sep 30 15:53:09 2008
From: aorchid at mac.com (Aric Gregson)
Date: Tue, 30 Sep 2008 15:53:09 -0700
Subject: [zfs-discuss] late mount
In-Reply-To: <48E2A79D.9020100@loveturtle.net>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
	<48E2180C.4090102@loveturtle.net>
	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
	<46CAEFC3-A3EA-4A7A-B696-3084D92F93FC@durin42.com>
	<20080930150928.f989f8f6.aorchid@mac.com>
	<48E2A79D.9020100@loveturtle.net>
Message-ID: <20080930155309.71cbf62e.aorchid@mac.com>

On Tue, 30 Sep 2008 18:26:37 -0400
Dillon Kass <lists at loveturtle.net> wrote:

>  From what I've seen on this list the "not importing on boot" is 
> generally because diskutil wasn't used first to prepare the disk
> before adding it to your zpool. Even if you did use diskutil to
> prepare the disk if you inserted the entire disk and not the s2 slice
> it would probably act this way as well.

That is what I keep reading and hearing, but I created them only a
couple of months ago following the recipe on the Mac ZFS site, so I
suspect that I created them properly. Although, I may have inserted the
entire disk, as this is what I do in solaris. Unfortunately, there is
no real way to figure that out now, or is there?

> note that you can also log in as the user >console from loginwindow
> and it will drop you to the command line. This is a little easier
> than creating a second user.

Thanks, I'll try that in the future. 

-- 
Aric Gregson

From mcamou at tecnoguru.com  Tue Sep 30 16:02:47 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Wed, 1 Oct 2008 01:02:47 +0200
Subject: [zfs-discuss] Advice for ZFS config
Message-ID: <3FDB5662-FC5E-4DA1-84F3-0215EF30BDF4@tecnoguru.com>

Hi all,

I am planning to turn a Mac Mini into a disk server / media center to  
replace a Linux machine I have running with MythTV. I am planning to  
use ZFS for the filesystem. The thing is, I'm not exactly sure which  
will be the best hard drive config.

I currently have about 1.7TB of data in several independent USB disks.  
I want to create a single ZFS volume. I have 2x1TB disks and 3x500GB  
disks.

My question is regarding how to best organize the disks into pools to  
get a bit of data security (of which I currently have none). I see two  
options:

1. Mirror the 2x1TB drives and RAIDZ the 3x500GB drives, giving 2TB of  
data space

2. Partition the 2x!TB drives into 2x500GB partitions each. RAIDZ one  
partition from each drive with 2x500GB hard drives, and also RAIDZ the  
other partition in each drive with 1x500GB drive. That would give me  
2.5TB of data.

I would prefer to use approach 2) since it gives me more data space  
but I'd like your thoughts on why this might be a bad idea, or on  
other ideas on how to lay out the pools.

Thanks in advance,
-Mario.

From kona8lend at gmail.com  Tue Sep 30 17:08:02 2008
From: kona8lend at gmail.com (Kona Blend)
Date: Tue, 30 Sep 2008 20:08:02 -0400
Subject: [zfs-discuss] Advice for ZFS config
In-Reply-To: <3FDB5662-FC5E-4DA1-84F3-0215EF30BDF4@tecnoguru.com>
References: <3FDB5662-FC5E-4DA1-84F3-0215EF30BDF4@tecnoguru.com>
Message-ID: <24082E9C-3D6E-4BD9-8FE6-F6807295A010@gmail.com>

I would recommend case #1 because the performance characteristic will  
be that writes will behave like striping across 2 spindles, and reads  
will behave at worse like striping across 2 spindles and at best like  
striping across 3 spindles. And up to 2 drives (1 drive from each set)  
may fail without losing data.

Case #2 sounds pretty bad; as though it will simply behave like 1  
spindle for any reads or writes. Drive failure survival is murky. You  
can survive at least 1 drive failure, or 2, depending on the  
combinations. As I understand it:

A = 1TB, A.1 = 500, A.2 = 500
B = 1TB, B.1 = 500, B.2 = 500
C = 500GB
D = 500GB
E = 500GB

RAIDZ { A.1 + B.1 + C + D } + RAIDZ { A.2 + B.2 + E }

Is there a case where the RAIDZs can service the requests  
concurrently? RAIDZ does full stripe-writes, thus drives A,B act as  
contention points between the concat'd RAIDZs.

--Kona Blend

On 30-Sep-08, at 7:02 PM, Mario Camou wrote:

> Hi all,
>
> I am planning to turn a Mac Mini into a disk server / media center to
> replace a Linux machine I have running with MythTV. I am planning to
> use ZFS for the filesystem. The thing is, I'm not exactly sure which
> will be the best hard drive config.
>
> I currently have about 1.7TB of data in several independent USB disks.
> I want to create a single ZFS volume. I have 2x1TB disks and 3x500GB
> disks.
>
> My question is regarding how to best organize the disks into pools to
> get a bit of data security (of which I currently have none). I see two
> options:
>
> 1. Mirror the 2x1TB drives and RAIDZ the 3x500GB drives, giving 2TB of
> data space
>
> 2. Partition the 2x!TB drives into 2x500GB partitions each. RAIDZ one
> partition from each drive with 2x500GB hard drives, and also RAIDZ the
> other partition in each drive with 1x500GB drive. That would give me
> 2.5TB of data.
>
> I would prefer to use approach 2) since it gives me more data space
> but I'd like your thoughts on why this might be a bad idea, or on
> other ideas on how to lay out the pools.
>
> Thanks in advance,
> -Mario.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From lists at loveturtle.net  Tue Sep 30 18:45:06 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Tue, 30 Sep 2008 21:45:06 -0400
Subject: [zfs-discuss] late mount
In-Reply-To: <20080930155309.71cbf62e.aorchid@mac.com>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>	<48E2180C.4090102@loveturtle.net>	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>	<46CAEFC3-A3EA-4A7A-B696-3084D92F93FC@durin42.com>	<20080930150928.f989f8f6.aorchid@mac.com>	<48E2A79D.9020100@loveturtle.net>
	<20080930155309.71cbf62e.aorchid@mac.com>
Message-ID: <48E2D622.4010504@loveturtle.net>

Aric Gregson wrote:
> On Tue, 30 Sep 2008 18:26:37 -0400
> Dillon Kass <lists at loveturtle.net> wrote:
>
>   
> Unfortunately, there is
> no real way to figure that out now, or is there?
>   
Sure, You should be able to just look at the disks in zpool status
They will either say like disk0s2 or disk0

From aorchid at mac.com  Tue Sep 30 21:51:31 2008
From: aorchid at mac.com (Aric Gregson)
Date: Tue, 30 Sep 2008 21:51:31 -0700
Subject: [zfs-discuss] MSWord 2004 V11.5.0 (080429) Problem Saving Files to
	ZFS Partition
Message-ID: <48E301D3.9010106@mac.com>

I'm wondering if anyone has seen a similar problem with this version of 
MSWord and saving files to ZFS partitions. I have not seen this error 
previously, I can guess as to what is happening, but I do not know of a 
work-around.

Word will save a new file the first time without problems. However, once 
you start editing the file and choose to save your changes or if 
autosave tries to save the file, this dialog pops up. You can hit OK, 
then choose to save the file manually. It will then ask if you want to 
overwrite the existing file. You choose yes and this same dialogue 
appears again. The changes to the file are actually saved at this point. 
It is a serious inconvenience for the user here, particularly when the 
Autosave feature makes you confront this dialogue every 15 minutes.

Any ideas? (other than using OpenOffice/NeoOffice which does not have 
this problem).

thanks, aric
-------------- next part --------------
A non-text attachment was scrubbed...
Name: wordDialogue.tiff
Type: image/tiff
Size: 49640 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080930/959053b3/attachment-0001.tiff 

