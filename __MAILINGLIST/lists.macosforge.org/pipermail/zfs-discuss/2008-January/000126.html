<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [zfs-discuss] Re: Development on ZFS (Andreas H?schler)
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=%5Bzfs-discuss%5D%20Re%3A%20Development%20on%20ZFS%20%28Andreas%20H%3Fschler%29&In-Reply-To=20080131211653.6921619D317%40lists.macosforge.org">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000125.html">
   
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[zfs-discuss] Re: Development on ZFS (Andreas H?schler)</H1>
    <B>Oliver Otway</B> 
    <A HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=%5Bzfs-discuss%5D%20Re%3A%20Development%20on%20ZFS%20%28Andreas%20H%3Fschler%29&In-Reply-To=20080131211653.6921619D317%40lists.macosforge.org"
       TITLE="[zfs-discuss] Re: Development on ZFS (Andreas H?schler)">oliver.otway at gmail.com
       </A><BR>
    <I>Thu Jan 31 22:34:12 PST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="000125.html">[zfs-discuss] How to activate/set ACL on ZFS?
</A></li>
        
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#126">[ date ]</a>
              <a href="thread.html#126">[ thread ]</a>
              <a href="subject.html#126">[ subject ]</a>
              <a href="author.html#126">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>&gt;<i> Andreas
</I>
When you copied over the files to ZFS, were you using GUI, or command  
line 'cp'
in either of those cases it's possible that an essential resource file  
is not being correctly copied across
Have you tried
dittto -rsrc src ... dst_directory

might help :)

man ditto for all usage

Ollie
&gt;<i>
</I>&gt;<i>
</I>&gt;<i> Hi No&#235;l,
</I>&gt;<i>
</I>&gt;&gt;<i> No idea, I'm not really familiar with ProjectBuilder and some of  
</I>&gt;&gt;<i> the other Apple dev tools.  I use vi :)
</I>&gt;&gt;<i> I don't see how this is ZFS related though.  Seems your project is  
</I>&gt;&gt;<i> likely missing some information it wants; I'd guess something isn't  
</I>&gt;&gt;<i> getting copied over correctly via NFS and hence it builds fine  
</I>&gt;&gt;<i> locally but is having issues via NFS? Have you tried using the  
</I>&gt;&gt;<i> command line Xcodebuild at all?  Maybe it will spit out more  
</I>&gt;&gt;<i> information than the gnumake does.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> sorry I can't be more help,
</I>&gt;<i>
</I>&gt;<i> Your're welcome! :-) Thanks for your response anyway!
</I>&gt;<i>
</I>&gt;<i> It works fine as long as I have the project dir on standard Solaris  
</I>&gt;<i> ufs exported via NFS to the Mac. So it must be zfs related somehow.
</I>&gt;<i>
</I>&gt;<i> No, we haven't tried Xcodebuild yet. For compatibility reasons  
</I>&gt;<i> (customer installations) we still ned to run and develop on MacOSX  
</I>&gt;<i> 10.2. :-)
</I>&gt;<i>
</I>&gt;<i> Regards,
</I>&gt;<i>
</I>&gt;<i> Andreas
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> From: No&#235;l Dellofano &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">ndellofano at apple.com</A>&gt;
</I>&gt;<i> Date: 30 January 2008 8:42:54 AM
</I>&gt;<i> To: Andreas H&#246;schler &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">ahoesch at smartsoft.de</A>&gt;
</I>&gt;<i> Cc: <A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">zfs-discuss at lists.macosforge.org</A>
</I>&gt;<i> Subject: Re: [zfs-discuss] Development on ZFS
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> No idea, I'm not really familiar with ProjectBuilder and some of the  
</I>&gt;<i> other Apple dev tools.  I use vi :)
</I>&gt;<i> I don't see how this is ZFS related though.  Seems your project is  
</I>&gt;<i> likely missing some information it wants; I'd guess something isn't  
</I>&gt;<i> getting copied over correctly via NFS and hence it builds fine  
</I>&gt;<i> locally but is having issues via NFS? Have you tried using the  
</I>&gt;<i> command line Xcodebuild at all?  Maybe it will spit out more  
</I>&gt;<i> information than the gnumake does.
</I>&gt;<i>
</I>&gt;<i> sorry I can't be more help,
</I>&gt;<i> Noel
</I>&gt;<i>
</I>&gt;<i> On Jan 26, 2008, at 9:24 AM, Andreas H&#246;schler wrote:
</I>&gt;<i>
</I>&gt;&gt;<i> Hi all,
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> we have successfully used Apples dev tools (actually  
</I>&gt;&gt;<i> ProjectBuilderWO.app but the same problem probably would occur with  
</I>&gt;&gt;<i> xCode) on an NFS mounted home dir from a Solaris box.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> We recently moved our home dirs to ZFS volume on this Solaris host  
</I>&gt;&gt;<i> and NFS exported /home with ZFS (it can do that directly). Whenever  
</I>&gt;&gt;<i> we try to build a project now we get
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> == Making ApplicationBuilder for ppc ==
</I>&gt;&gt;<i> Pre-build setup...
</I>&gt;&gt;<i> /bin/rm -f /home/ahoesch/Development/MacOSX/SmartObjectsPro/ 
</I>&gt;&gt;<i> ApplicationBuilder/ApplicationBuilder.build/derived_src/ 
</I>&gt;&gt;<i> TrustedPrecomps.txt
</I>&gt;&gt;<i> Building...
</I>&gt;&gt;<i> /usr/lib/mergeInfo PB.project CustomInfo.plist /home/ahoesch/ 
</I>&gt;&gt;<i> Development/MacOSX/SmartObjectsPro/ApplicationBuilder/ 
</I>&gt;&gt;<i> ApplicationBuilder.build/derived_src/Java.plist -o /home/ahoesch/ 
</I>&gt;&gt;<i> Development/MacOSX/SmartObjectsPro/ApplicationBuilder/ 
</I>&gt;&gt;<i> ApplicationBuilder.app/Resources/Info-macos.plist
</I>&gt;&gt;<i> Copying English resources...
</I>&gt;&gt;<i> Copying German resources...
</I>&gt;&gt;<i> gnumake: *** [copy-global-resources] Segmentation fault
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> If I copy the complete project directory to a a folder on the local  
</I>&gt;&gt;<i> disk (of the Mac) then the build works fine. What could be the  
</I>&gt;&gt;<i> problem? Any idea?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Thanks a lot!
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Regards,
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Andreas
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;<i> zfs-discuss mailing list
</I>&gt;&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">zfs-discuss at lists.macosforge.org</A>
</I>&gt;&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">http://lists.macosforge.org/mailman/listinfo/zfs-discuss</A>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> From: Erik Ableson &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">eableson at mac.com</A>&gt;
</I>&gt;<i> Date: 30 January 2008 8:37:08 PM
</I>&gt;<i> To: No&#235;l Dellofano &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">ndellofano at apple.com</A>&gt;
</I>&gt;<i> Cc: ZFS on OSX mailing list mailing list &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">zfs-discuss at lists.macosforge.org</A> 
</I>&gt;<i> &gt;
</I>&gt;<i> Subject: Re: [zfs-discuss] raidz
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> I'm not sure that I'm clear on this part of the discussion since  
</I>&gt;<i> this is a base feature of the Solaris ZFS that makes it attractive.
</I>&gt;<i>
</I>&gt;<i> To make sure that I'm clear on what the situation is :
</I>&gt;<i>
</I>&gt;<i> I have created a raidz pool of 3x500Gb disks.  Net result, I have a  
</I>&gt;<i> 1Tb volume that can tolerate the failure of a drive.  This is a  
</I>&gt;<i> perfect use case for both the SMB and home user market, what with  
</I>&gt;<i> exploding space requirements for video and hi-res photos etc.  But  
</I>&gt;<i> the day I need more storage, I plan to simple extend the raidz set  
</I>&gt;<i> using :
</I>&gt;<i>
</I>&gt;<i> zpool add [-fn] &lt;pool&gt; &lt;vdev&gt;
</I>&gt;<i>
</I>&gt;<i> with the addition of another drive of the same size. (tested on a  
</I>&gt;<i> Solaris x86 box)
</I>&gt;<i>
</I>&gt;<i> Caveat : adding a 750Gb disk to a raidz set composed of 500Gb disks  
</I>&gt;<i> will only result in 500Gb of the new drive being used.
</I>&gt;<i>
</I>&gt;<i> But the result should be that my existing pool goes from 3x500 (1Tb)  
</I>&gt;<i> to 4x500 (1.5Tb) and I don't have to migrate anything or move  
</I>&gt;<i> anything around.  Now if the point is that you're moving to another  
</I>&gt;<i> size disk it's more complicated.
</I>&gt;<i>
</I>&gt;<i> Replacing volumes with large disks requires that you export and  
</I>&gt;<i> import the volume for the new capacity to be recognized and it  
</I>&gt;<i> automatically resilvers the raidz volume to spread out the parity  
</I>&gt;<i> blocks.
</I>&gt;<i>
</I>&gt;<i> 3x500 -&gt; 3x750 would be a
</I>&gt;<i>
</I>&gt;<i> zpool replace [-f] &lt;pool&gt; &lt;device&gt; [new_device]
</I>&gt;<i>
</I>&gt;<i> and requires an export/import for the system to recognize the new  
</I>&gt;<i> capacity once all of the data has been copied to the new drives  
</I>&gt;<i> (tested on Solaris x86 box with firewire disks)
</I>&gt;<i>
</I>&gt;<i> Cheers,
</I>&gt;<i>
</I>&gt;<i> Erik
</I>&gt;<i>
</I>&gt;<i> Notes : my Solaris box is a crappy old Shuttle PC that's 5 years  
</I>&gt;<i> old. These tests are uniquely applicable to my tests on this box and  
</I>&gt;<i> I haven't even tried this on the current OS X ZFS port.
</I>&gt;<i>
</I>&gt;<i> On 30 janv. 08, at 00:03, No&#235;l Dellofano wrote:
</I>&gt;<i>
</I>&gt;&gt;<i> There's no current plans to add this functionality as of yet.   
</I>&gt;&gt;<i> Since this is a rather uninteresting problem for Solaris (a Thumper  
</I>&gt;&gt;<i> has 48 drives stock), it's not a problem they are concerned with,  
</I>&gt;&gt;<i> and we at Apple are working on stability and other more basic  
</I>&gt;&gt;<i> features like Spotlight bugs and Finder.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> We may consider doing this in the future at some point but even so  
</I>&gt;&gt;<i> it would be quite a while as it's a complicated problem since our  
</I>&gt;&gt;<i> raidz geometry is complex and as part of adding a drive we'd  
</I>&gt;&gt;<i> somehow have to 'rebalance' the raidz set when you add the new drive.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> So for now, I'm afraid Bob is going to have to start saving his  
</I>&gt;&gt;<i> pennies and have multiple raidz sets in the pool :)
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Noel
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> From: Boyd Waters &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">bwaters at nrao.edu</A>&gt;
</I>&gt;<i> Date: 31 January 2008 5:19:28 AM
</I>&gt;<i> To: Erik Ableson &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">eableson at mac.com</A>&gt;
</I>&gt;<i> Cc: ZFS on OSX mailing list mailing list &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">zfs-discuss at lists.macosforge.org</A> 
</I>&gt;<i> &gt;
</I>&gt;<i> Subject: Re: [zfs-discuss] raidz
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> On Jan 30, 2008, at 4:07 AM, Erik Ableson wrote:
</I>&gt;<i>
</I>&gt;&gt;<i> the day I need more storage, I plan to simple extend the raidz set  
</I>&gt;&gt;<i> using :
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> zpool add [-fn] &lt;pool&gt; &lt;vdev&gt;
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> the result should be that my existing pool goes from 3x500 (1Tb) to  
</I>&gt;&gt;<i> 4x500 (1.5Tb) and I don't have to migrate anything or move anything  
</I>&gt;&gt;<i> around
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> Yes, this is what people *want* o happen, but it doesn't happen that  
</I>&gt;<i> way. Not yet.
</I>&gt;<i>
</I>&gt;<i> As I understand it, you can't add devices to an existing raidz  
</I>&gt;<i> array. (On the ZFS that's currently shipping with Solaris, I think  
</I>&gt;<i> you can add on-line spares to an existing array, but that won't  
</I>&gt;<i> increase the storage capacity of the array.)
</I>&gt;<i>
</I>&gt;<i> You *can* add storage to an existing storage *pool*, as long as the  
</I>&gt;<i> new elements of the pool are at least as large as the existing  
</I>&gt;<i> elements.
</I>&gt;<i>
</I>&gt;<i> So in your example of a 3x500GB raidz, I *think* you could add a  
</I>&gt;<i> single 1TB drive to the pool, and it would stripe across the single  
</I>&gt;<i> drive and the raidz. Which is not what you want; the failure of the  
</I>&gt;<i> single 1TB drive would kill the whole pool.  So I'm not certain it  
</I>&gt;<i> will let you do that.
</I>&gt;<i>
</I>&gt;<i> I am pretty sure it would let you add another 3x500GB raidz to the  
</I>&gt;<i> pool, your I/O performance would go up because you're striping  
</I>&gt;<i> between two arrays, and each individual array could survive the  
</I>&gt;<i> failure of one of its devices. You would end up with a 2 TB pool.
</I>&gt;<i>
</I>&gt;<i> If you think that your Solaris box can do this, could you post the  
</I>&gt;<i> output of &quot;zpool status&quot; before and after adding your additional disk?
</I>&gt;<i>
</I>&gt;<i> Thanks!
</I>&gt;<i>
</I>&gt;<i> - boyd
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> From: No&#235;l Dellofano &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">ndellofano at apple.com</A>&gt;
</I>&gt;<i> Date: 31 January 2008 5:36:41 AM
</I>&gt;<i> To: Boyd Waters &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">bwaters at nrao.edu</A>&gt;
</I>&gt;<i> Cc: ZFS on OSX mailing list mailing list &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">zfs-discuss at lists.macosforge.org</A> 
</I>&gt;<i> &gt;
</I>&gt;<i> Subject: Re: [zfs-discuss] raidz
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;&gt;<i> Yes, this is what people *want* o happen, but it doesn't happen  
</I>&gt;&gt;<i> that way. Not yet.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> As I understand it, you can't add devices to an existing raidz  
</I>&gt;&gt;<i> array. (On the ZFS that's currently shipping with Solaris, I think  
</I>&gt;&gt;<i> you can add on-line spares to an existing array, but that won't  
</I>&gt;&gt;<i> increase the storage capacity of the array.)
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> You *can* add storage to an existing storage *pool*, as long as the  
</I>&gt;&gt;<i> new elements of the pool are at least as large as the existing  
</I>&gt;&gt;<i> elements.
</I>&gt;<i>
</I>&gt;<i> Yes exactly, all three of the above assertions Boyd makes are correct.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;&gt;<i> So in your example of a 3x500GB raidz, I *think* you could add a  
</I>&gt;&gt;<i> single 1TB drive to the pool, and it would stripe across the single  
</I>&gt;&gt;<i> drive and the raidz. Which is not what you want; the failure of the  
</I>&gt;&gt;<i> single 1TB drive would kill the whole pool.  So I'm not certain it  
</I>&gt;&gt;<i> will let you do that.
</I>&gt;<i>
</I>&gt;<i> So currently, ZFS will let you do this *however* when you issue the  
</I>&gt;<i> zpool add command to do this we will fail the request and warn you  
</I>&gt;<i> about the mismatched replication.  You then need to use '-f' if you  
</I>&gt;<i> wish to actually go ahead with it.  I used file vdevs below do demo  
</I>&gt;<i> this, but the same will happen with disks:
</I>&gt;<i>
</I>&gt;<i> sh-3.2# zpool status wombat
</I>&gt;<i>  pool: wombat
</I>&gt;<i> state: ONLINE
</I>&gt;<i> status: The pool is formatted using an older on-disk format.  The  
</I>&gt;<i> pool can
</I>&gt;<i> 	still be used, but some features are unavailable.
</I>&gt;<i> action: Upgrade the pool using 'zpool upgrade'.  Once this is done,  
</I>&gt;<i> the
</I>&gt;<i> 	pool will no longer be accessible on older software versions.
</I>&gt;<i> scrub: none requested
</I>&gt;<i> config:
</I>&gt;<i>
</I>&gt;<i> 	NAME                 STATE     READ WRITE CKSUM
</I>&gt;<i> 	wombat               ONLINE       0     0     0
</I>&gt;<i> 	  raidz1             ONLINE       0     0     0
</I>&gt;<i> 	    /var/root/vdev1  ONLINE       0     0     0
</I>&gt;<i> 	    /var/root/vdev2  ONLINE       0     0     0
</I>&gt;<i> 	    /var/root/vdev3  ONLINE       0     0     0
</I>&gt;<i>
</I>&gt;<i> errors: No known data errors
</I>&gt;<i> sh-3.2# zpool add wombat /var/root/vdev4
</I>&gt;<i> invalid vdev specification
</I>&gt;<i> use '-f' to override the following errors:
</I>&gt;<i> mismatched replication level: pool uses raidz and new vdev is file
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;&gt;<i> I am pretty sure it would let you add another 3x500GB raidz to the  
</I>&gt;&gt;<i> pool, your I/O performance would go up because you're striping  
</I>&gt;&gt;<i> between two arrays, and each individual array could survive the  
</I>&gt;&gt;<i> failure of one of its devices. You would end up with a 2 TB pool.
</I>&gt;<i>
</I>&gt;<i> You can do the above (add a new raidz set to your pool) and it will  
</I>&gt;<i> increase your performance as mentioned.  However I know as you said  
</I>&gt;<i> that this isn't exactly what you are looking for.
</I>&gt;<i>
</I>&gt;<i> Note that you can add as many disks as you like to mirrors and  
</I>&gt;<i> regular single stripe configs, but adding a single disk to a  
</I>&gt;<i> preexisting raidz set is not currently possible.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> Noel
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> On Jan 30, 2008, at 11:49 AM, Boyd Waters wrote:
</I>&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> On Jan 30, 2008, at 4:07 AM, Erik Ableson wrote:
</I>&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> the day I need more storage, I plan to simple extend the raidz set  
</I>&gt;&gt;&gt;<i> using :
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> zpool add [-fn] &lt;pool&gt; &lt;vdev&gt;
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> the result should be that my existing pool goes from 3x500 (1Tb)  
</I>&gt;&gt;&gt;<i> to 4x500 (1.5Tb) and I don't have to migrate anything or move  
</I>&gt;&gt;&gt;<i> anything around
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Yes, this is what people *want* o happen, but it doesn't happen  
</I>&gt;&gt;<i> that way. Not yet.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> As I understand it, you can't add devices to an existing raidz  
</I>&gt;&gt;<i> array. (On the ZFS that's currently shipping with Solaris, I think  
</I>&gt;&gt;<i> you can add on-line spares to an existing array, but that won't  
</I>&gt;&gt;<i> increase the storage capacity of the array.)
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> You *can* add storage to an existing storage *pool*, as long as the  
</I>&gt;&gt;<i> new elements of the pool are at least as large as the existing  
</I>&gt;&gt;<i> elements.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> So in your example of a 3x500GB raidz, I *think* you could add a  
</I>&gt;&gt;<i> single 1TB drive to the pool, and it would stripe across the single  
</I>&gt;&gt;<i> drive and the raidz. Which is not what you want; the failure of the  
</I>&gt;&gt;<i> single 1TB drive would kill the whole pool.  So I'm not certain it  
</I>&gt;&gt;<i> will let you do that.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> I am pretty sure it would let you add another 3x500GB raidz to the  
</I>&gt;&gt;<i> pool, your I/O performance would go up because you're striping  
</I>&gt;&gt;<i> between two arrays, and each individual array could survive the  
</I>&gt;&gt;<i> failure of one of its devices. You would end up with a 2 TB pool.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> If you think that your Solaris box can do this, could you post the  
</I>&gt;&gt;<i> output of &quot;zpool status&quot; before and after adding your additional  
</I>&gt;&gt;<i> disk?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Thanks!
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> - boyd
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;<i> zfs-discuss mailing list
</I>&gt;&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">zfs-discuss at lists.macosforge.org</A>
</I>&gt;&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">http://lists.macosforge.org/mailman/listinfo/zfs-discuss</A>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> From: Erik Ableson &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">eableson at mac.com</A>&gt;
</I>&gt;<i> Date: 31 January 2008 6:39:32 AM
</I>&gt;<i> To: No&#235;l Dellofano &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">ndellofano at apple.com</A>&gt;
</I>&gt;<i> Cc: ZFS on OSX mailing list mailing list &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">zfs-discuss at lists.macosforge.org</A> 
</I>&gt;<i> &gt;
</I>&gt;<i> Subject: Re: [zfs-discuss] raidz
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> OK - many thanks for the clarifications - I crossed a few wires  
</I>&gt;<i> reviewing a side discussion about the disk replacements under raidz  
</I>&gt;<i> and adding disks to a mirror.
</I>&gt;<i>
</I>&gt;<i> Going back to my test logs, the issue I was testing at the time was  
</I>&gt;<i> replacing existing disks with larger ones, which is a viable upgrade  
</I>&gt;<i> path and I hadn't actually tested the addition of a new disk to the  
</I>&gt;<i> raidz set.
</I>&gt;<i>
</I>&gt;<i> Cheers,
</I>&gt;<i>
</I>&gt;<i> Erik
</I>&gt;<i> On 30 janv. 08, at 21:06, No&#235;l Dellofano wrote:
</I>&gt;<i>
</I>&gt;&gt;&gt;<i> Yes, this is what people *want* o happen, but it doesn't happen  
</I>&gt;&gt;&gt;<i> that way. Not yet.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> As I understand it, you can't add devices to an existing raidz  
</I>&gt;&gt;&gt;<i> array. (On the ZFS that's currently shipping with Solaris, I think  
</I>&gt;&gt;&gt;<i> you can add on-line spares to an existing array, but that won't  
</I>&gt;&gt;&gt;<i> increase the storage capacity of the array.)
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> You *can* add storage to an existing storage *pool*, as long as  
</I>&gt;&gt;&gt;<i> the new elements of the pool are at least as large as the existing  
</I>&gt;&gt;&gt;<i> elements.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Yes exactly, all three of the above assertions Boyd makes are  
</I>&gt;&gt;<i> correct.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> From: Robert Rehnmark &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">robert at rehnmark.net</A>&gt;
</I>&gt;<i> Date: 1 February 2008 6:46:45 AM
</I>&gt;<i> To: ZFS on OSX mailing list mailing list &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">zfs-discuss at lists.macosforge.org</A> 
</I>&gt;<i> &gt;
</I>&gt;<i> Subject: [zfs-discuss] How to activate/set ACL on ZFS?
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> How do I set up and control ACL's?
</I>&gt;<i> In order to make the network-shared pool work with multiple users I  
</I>&gt;<i> need ACL that is inherited to all created folders and files.
</I>&gt;<i> So, can anybody please tell me how to make it happen?
</I>&gt;<i>
</I>&gt;<i> /Robert
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> _______________________________________________
</I>&gt;<i> zfs-discuss mailing list
</I>&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">zfs-discuss at lists.macosforge.org</A>
</I>&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">http://lists.macosforge.org/mailman/listinfo/zfs-discuss</A>
</I>
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000125.html">[zfs-discuss] How to activate/set ACL on ZFS?
</A></li>
	
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#126">[ date ]</a>
              <a href="thread.html#126">[ thread ]</a>
              <a href="subject.html#126">[ subject ]</a>
              <a href="author.html#126">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">More information about the zfs-discuss
mailing list</a><br>
</body></html>
