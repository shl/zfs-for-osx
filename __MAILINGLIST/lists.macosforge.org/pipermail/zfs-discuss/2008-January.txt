From wmertens at cisco.com  Mon Jan 14 06:31:47 2008
From: wmertens at cisco.com (Wout Mertens)
Date: Mon Jan 14 06:31:37 2008
Subject: [zfs-discuss] Case Sensitivity
Message-ID: <A73D730C-6E5F-410F-A98A-9A61DE1A358A@cisco.com>

Hi there,

so I'm wondering if the case-insensitiveness of HFS+ will be added as  
a feature to ZFS.

It's one of the things that made me say "Nice!" when I first started  
using OS X. I'd hate to lose it again :-(

Wout.
From dirkschelfhout at mac.com  Mon Jan 14 06:58:37 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Mon Jan 14 06:58:15 2008
Subject: [zfs-discuss] zpool status
Message-ID: <1508A358-5DCD-4B7D-8E08-709A45C1F1FF@mac.com>

Hi,

When I run this it nicely says at the bottom :

errors: No known data errors

I must say that is not a very confident statement, why not print : no  
data errors
As it  now is it seems to imply that there might be some data  
errors..........

Dirk
From mail at jameslegg.co.uk  Mon Jan 14 07:31:59 2008
From: mail at jameslegg.co.uk (James Legg)
Date: Mon Jan 14 07:31:35 2008
Subject: [zfs-discuss] zpool status
In-Reply-To: <1508A358-5DCD-4B7D-8E08-709A45C1F1FF@mac.com>
References: <1508A358-5DCD-4B7D-8E08-709A45C1F1FF@mac.com>
Message-ID: <97B9B0E2-A9D6-4DF0-991A-1693634538DB@jameslegg.co.uk>

Hi Dirk,

It might not be a confident statement but it is accurate. Data errors  
on a disk can't be detected until zfs actually reads the data on the  
disk and checksums it or an integrity check of the entire disk is done  
using a zfs scrub.

Hope that explains a bit.

James


On 14 Jan 2008, at 14:58, Dirk Schelfhout wrote:

> Hi,
>
> When I run this it nicely says at the bottom :
>
> errors: No known data errors
>
> I must say that is not a very confident statement, why not print :  
> no data errors
> As it  now is it seems to imply that there might be some data  
> errors..........
>
> Dirk
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From lists at loveturtle.net  Mon Jan 14 07:34:41 2008
From: lists at loveturtle.net (Dillon)
Date: Mon Jan 14 07:34:26 2008
Subject: [zfs-discuss] zpool status
In-Reply-To: <1508A358-5DCD-4B7D-8E08-709A45C1F1FF@mac.com>
References: <1508A358-5DCD-4B7D-8E08-709A45C1F1FF@mac.com>
Message-ID: <478B8111.6020308@loveturtle.net>

Sounds like a fine statement to me, It can only alert you to errors that 
are known!

You can always do a zpool scrub if you're concerned. If it still says no 
known errors after the scrub then you can rest assured that at the time 
of the scrub there were no errors, or else they would be known :-)

Dirk Schelfhout wrote:
> Hi,
>
> When I run this it nicely says at the bottom :
>
> errors: No known data errors
>
> I must say that is not a very confident statement, why not print : no 
> data errors
> As it  now is it seems to imply that there might be some data 
> errors..........
>
> Dirk
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From eableson at gmail.com  Mon Jan 14 04:49:10 2008
From: eableson at gmail.com (Erik ABLESON)
Date: Mon Jan 14 12:37:28 2008
Subject: [zfs-discuss] PPC ?
Message-ID: <304A33B1-F974-4EB7-88EB-ECB6B9D47B5F@gmail.com>

Hi there,

Just wondering if the latest files posted are universal binaries. I  
had installed 1.0.1 on both a MacBook Pro and a PowerBook which worked  
on the Intel but kernel panicked the G4.

Erik Ableson
+33 6 80 83 58 28
From ndellofano at apple.com  Mon Jan 14 15:02:07 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon Jan 14 15:02:43 2008
Subject: [zfs-discuss] RE: PPC ?
Message-ID: <F86F2284-AD74-48DB-ADD9-4C54A85D25AA@apple.com>

Yes,  all the latest binaries posted are universal FAT binaries.  The  
G4 panicked in ZFS as soon as you installed?  Latest version of  
Leopard on both? and which rev of the PowerBook and what hardware are  
you running?

Noel



> From: Erik ABLESON <eableson@gmail.com>
> Date: January 14, 2008 4:49:10 AM PST
> To: "zfs-discuss@lists.macosforge.org" <zfs-discuss@lists.macosforge.org 
> >
> Subject: PPC ?
>
>
> Hi there,
>
> Just wondering if the latest files posted are universal binaries. I  
> had installed 1.0.1 on both a MacBook Pro and a PowerBook which  
> worked on the Intel but kernel panicked the G4.
>
> Erik Ableson
> +33 6 80 83 58 28
>
>
>
> From: zfs-discuss-request@lists.macosforge.org
> Subject: confirm 17d2dd78e61302f4b78729f16146bd194303d810
>
>
> If you reply to this message, keeping the Subject: header intact,
> Mailman will discard the held message.  Do this if the message is
> spam.  If you reply to this message and include an Approved: header
> with the list password in it, the message will be approved for posting
> to the list.  The Approved: header can also appear in the first line
> of the body of the reply.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080114/21467598/attachment.html
From bhofmann at mac.com  Tue Jan 15 07:50:04 2008
From: bhofmann at mac.com (Bernd Hofmann)
Date: Tue Jan 15 07:49:49 2008
Subject: [zfs-discuss] Diggin into zfs...Questions ofa newbie
In-Reply-To: <F86F2284-AD74-48DB-ADD9-4C54A85D25AA@apple.com>
References: <F86F2284-AD74-48DB-ADD9-4C54A85D25AA@apple.com>
Message-ID: <51BC197C-6D10-4BA9-8E00-45FA4EFA4576@mac.com>

Hi List!

I downloaded and sucessfully installed the latest zfs-release on my  
macpro and have a few questions.

I devoted one testdisk (160 GB) to zfs, initialised it and named it  
"zfstank". I used the new zfs, not the backward-compatible ver. 6, but  
this may not be of a problem because i don't intend to use the (build- 
in) disk on other machines. So far, so good. The disc is visible in  
the finder only as a white removable drive-icon. i can copy to and  
from it, all seemed well for a beta. opening diskimages from the zfs- 
disk is ok, but closing the diskimage don't work (have to restart).

Trying to startup the same mac with another Startup-Volume (which has  
also the same zfs-build installed) didn't give me access to my zfstank- 
disc, instead it wants me to initialize it (wich i don't do). But   
going back to the other startupdisk with wich i have initialized my  
zfstank shows everything ok.

So i wonder what have i todo to make my zfstank-disk visible under  
other startup-disks?

I LOVE this new zfs-format and i want devote more discs to test it  
further. I intend do do some zraid with 3 or 4 discs (1 - 2 TB),  
mainly to use as a "warehouse", copying things to and from it, (maybe  
scrubing all week to see if things went well). As startupdisk i use a  
wd raptor in the normal hfsplus-format, so the zfs-discs will have not  
much interaction with the system/finder or specific programs. All  
disks will backed up constantly on other hfs-plus discs, so i have not  
much to loose (a panic here and there would be not so welcome, but if  
the zfs-format itself is "ready" then i would take this risk on...

What do you all think about this? I s this scenario too dangerous, is  
it too early or could i benefit from zfs now under this circumstances?  
So, maybe you can give us a bit more info (then on the website) on  
what is NOW possible with this release and what the differences are to  
the Release in production-use by SUN.

A already downloaded several whitepapers and will learn more, what i  
would need is a more practical approach to get deeper into it on the  
copmmandline for day-to-day work, so any links to tutorials are welcome.

Thank you all for this fantastic release, hope to see the technology  
mature soon for use as the main filesystem on os x.

B. Hofmann, Germany


-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080115/80e51b08/attachment-0001.html
From ndellofano at apple.com  Wed Jan 16 12:24:00 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed Jan 16 12:24:55 2008
Subject: [zfs-discuss] iTunes and ZFS announcement
Message-ID: <8C2FD22D-E025-4442-A389-F9785FD009C3@apple.com>

Hey everyone,

Just an FYI that iTunes 7.6 has a fix in it so now you can download  
content from iTunes onto your ZFS volumes.  If anyone still has any  
issues with this after upgrading, let me know.

thanks!
Noel 
From ragge at csc.kth.se  Wed Jan 16 12:49:52 2008
From: ragge at csc.kth.se (Ragnar Sundblad)
Date: Wed Jan 16 12:49:21 2008
Subject: [zfs-discuss] ZFS and cheap disks
Message-ID: <EE1FE08D-5C64-4732-82A7-35FAAB90F3A9@csc.kth.se>


The documentation says that ZFS loves cheap disks. ZFS also leaves  
guarantees regarding consistency and have mechanisms for transactions.

As I understand it, it isn't easy to control the write cache on all  
kinds of disks, especially many older PATA and SATA disks and disks  
behind different kind of bridges like raid controllers, USB bridges  
etc. Some don't have the mechanisms to control the cache implemented,  
some don't implement it correctly.

Can ZFS really leave those guarantees on such drives? If so, how is  
that implemented? If not, how bad can the result be if for example the  
power is lost half way through a cache flush and a arbitrary set if  
blocks make it to the disk and the rest don't?

/ragge

From danchr at daimi.au.dk  Wed Jan 16 19:20:28 2008
From: danchr at daimi.au.dk (Dan Villiom Podlaski Christiansen)
Date: Wed Jan 16 19:20:00 2008
Subject: [zfs-discuss] APM support
Message-ID: <4E6F82B4-40C9-445B-BADA-4EDC7CA1296A@daimi.au.dk>

Skipped content of type multipart/mixed-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 1945 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080117/9cf6aa23/smime.bin
From bplist at thinkpink.com  Thu Jan 17 00:10:53 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Thu Jan 17 00:10:15 2008
Subject: [zfs-discuss] reporting bugs
Message-ID: <19EFF445-9C33-4E07-9FEE-7088142ADF1A@thinkpink.com>

What's the best way to submitting bugs for the ZFS project?  I tried  
opening a ticket on macosforge.org, but it looks a bit rudimentary.

I'm getting a panic (below) on a 10.5.1 OSX reader and a 10.5.1 OSXS  
writer when running rsync.  If I --bwlimit the rsync to something slow  
like 4MB/s then it runs fine, but if run it unlimited one of the  
machines will panic.  The client just has a simple two-disk stripe,  
while the server has an 8-disk RAID 10 setup.

If this doesn't look familiar to anyone, I'll compile up a version  
that prints out a bit more debugging (kmem_alloc can fail in a bunch  
of ways!)

thanks,
bri


Wed Jan 16 20:24:11 2008
panic(cpu 3 caller 0x6D507974): "zfs: vmem_alloc couldn't alloc 65536  
bytes\n"@/Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/ 
zfs_kext/zfs/zfs_context.c:668
Latest stack backtrace for cpu 3:
      Backtrace:
         0x0009AD18 0x0009B6BC 0x00029DC4 0x6D507974 0x6D528A3C  
0x6D4ABC5C 0x6D4B8930 0x6D4B92A8
         0x6D4BC708 0x6D4C0B1C 0x6D50563C 0x6D4C87DC 0x6D498C38  
0x00108774 0x00100FF4 0x002AFC64
         0x002AFF60 0x003091FC 0x000B24C8 0x00000000
      Kernel loadable modules in backtrace (with dependencies):
         com.apple.filesystems.zfs(8.0)@0x6d495000->0x6d57efff
Proceeding back via exception chain:
   Exception state (sv=0x5856c000)
      PC=0x9496013C; MSR=0x0200F030; DAR=0xE0045000; DSISR=0x42000000;  
LR=0x00018CE0; R1=0xBFFFD910; XCP=0x00000030 (0xC00 - System call)

BSD process name corresponding to current thread: rsync

Mac OS version:
9B18

Kernel version:
Darwin Kernel Version 9.1.0: Wed Oct 31 17:48:21 PDT 2007;  
root:xnu-1228.0.2~1/RELEASE_PPC

From eric at hulteen.com  Thu Jan 17 00:36:07 2008
From: eric at hulteen.com (Eric A Hulteen)
Date: Thu Jan 17 00:35:34 2008
Subject: [zfs-discuss] system requirements?
Message-ID: <a06240805c3b4c14c359e@[10.0.1.2]>

I've inferred from the discussion that ZFS will run on both Intel and 
PowerPC Macs and I've also seen references only to Leopard while none 
to Tiger.  Could someone be explicit (here or in the FAQ) about the 
system requirements for running ZFS?  Are Developer tools required to 
be installed?

Eric
From mofo at fea.st  Thu Jan 17 06:36:54 2008
From: mofo at fea.st (Mike Coddington)
Date: Thu Jan 17 06:36:17 2008
Subject: [zfs-discuss] Slow write speeds?
Message-ID: <EA7D1644-E26A-4372-8BD6-AEE864166735@fea.st>

Last night I got ZFS set up on my secondary SATA drive in my G5. I was  
transferring about 60-70 GB of data from my primary drive to the new  
single-drive ZFS pool. What I was noticing is that the system would  
write ~200MB of data, pause, then write 200MB more, and so on. This is  
different than what I see with HFS+ where I get constant writing of  
data and the progress bar keeps on trucking away. Is this normal? What  
could be causing this?
From wmertens at cisco.com  Thu Jan 17 07:05:01 2008
From: wmertens at cisco.com (Wout Mertens)
Date: Thu Jan 17 07:04:55 2008
Subject: [zfs-discuss] ZFS and cheap disks
In-Reply-To: <EE1FE08D-5C64-4732-82A7-35FAAB90F3A9@csc.kth.se>
References: <EE1FE08D-5C64-4732-82A7-35FAAB90F3A9@csc.kth.se>
Message-ID: <BCA68E55-5EE1-45FD-AAC3-F7ED91FFFFC1@cisco.com>

On Jan 16, 2008, at 9:49 PM, Ragnar Sundblad wrote:

>
> The documentation says that ZFS loves cheap disks. ZFS also leaves  
> guarantees regarding consistency and have mechanisms for transactions.
>
> As I understand it, it isn't easy to control the write cache on all  
> kinds of disks, especially many older PATA and SATA disks and disks  
> behind different kind of bridges like raid controllers, USB bridges  
> etc. Some don't have the mechanisms to control the cache  
> implemented, some don't implement it correctly.
>
> Can ZFS really leave those guarantees on such drives? If so, how is  
> that implemented? If not, how bad can the result be if for example  
> the power is lost half way through a cache flush and a arbitrary set  
> if blocks make it to the disk and the rest don't?

I'm not an expert of the inner workings, but here goes:

ZFS uses a copy-on-write scheme combined with checksums and age  
counters. Using these features at the same time allows you to make  
changes of multiple blocks atomic.

Whenever you make a change to a block, that block gets copied and  
changed. Then its parent block gets copied and changed, all the way up  
to the superblock. Multiple copies of these metadata blocks are  
written btw, since they're so important.

So whenever a change was made but the super block wasn't updated  
because of a crash, that change simply doesn't exist.

If the superblock was changed but one of its child blocks didn't make  
it on disk due to write block reordering, ZFS will know that something  
happened because of the checksums. It can then react to this  
intelligently (although I don't know how it handles this particular  
situation).

Wout.
From bp at thinkpink.com  Thu Jan 17 00:09:28 2008
From: bp at thinkpink.com (Brian Pinkerton)
Date: Thu Jan 17 08:29:52 2008
Subject: [zfs-discuss] reporting bugs
Message-ID: <BC55E091-B78E-4BFE-81C9-0FA689C44AFD@thinkpink.com>

What's the best way to submitting bugs for the ZFS project?  I tried  
opening a ticket on macosforge.org, but it looks a bit rudimentary.

I'm getting a panic (below) on a 10.5.1 OSX reader and a 10.5.1 OSXS  
writer when running rsync.  If I --bwlimit the rsync to something slow  
like 4MB/s then it runs fine, but if run it unlimited one of the  
machines will panic.  The client just has a simple two-disk stripe,  
while the server has an 8-disk RAID 10 setup.

If this doesn't look familiar to anyone, I'll compile up a version  
that prints out a bit more debugging (kmem_alloc can fail in a bunch  
of ways!)

thanks,
bri


Wed Jan 16 20:24:11 2008
panic(cpu 3 caller 0x6D507974): "zfs: vmem_alloc couldn't alloc 65536  
bytes\n"@/Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/ 
zfs_kext/zfs/zfs_context.c:668
Latest stack backtrace for cpu 3:
       Backtrace:
          0x0009AD18 0x0009B6BC 0x00029DC4 0x6D507974 0x6D528A3C  
0x6D4ABC5C 0x6D4B8930 0x6D4B92A8
          0x6D4BC708 0x6D4C0B1C 0x6D50563C 0x6D4C87DC 0x6D498C38  
0x00108774 0x00100FF4 0x002AFC64
          0x002AFF60 0x003091FC 0x000B24C8 0x00000000
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0x6d495000->0x6d57efff
Proceeding back via exception chain:
    Exception state (sv=0x5856c000)
       PC=0x9496013C; MSR=0x0200F030; DAR=0xE0045000;  
DSISR=0x42000000; LR=0x00018CE0; R1=0xBFFFD910; XCP=0x00000030 (0xC00  
- System call)

BSD process name corresponding to current thread: rsync

Mac OS version:
9B18

Kernel version:
Darwin Kernel Version 9.1.0: Wed Oct 31 17:48:21 PDT 2007;  
root:xnu-1228.0.2~1/RELEASE_PPC

From bwaters at nrao.edu  Thu Jan 17 12:10:35 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Thu Jan 17 12:10:20 2008
Subject: [zfs-discuss] Diggin into zfs...Questions ofa newbie
In-Reply-To: <51BC197C-6D10-4BA9-8E00-45FA4EFA4576@mac.com>
References: <F86F2284-AD74-48DB-ADD9-4C54A85D25AA@apple.com>
	<51BC197C-6D10-4BA9-8E00-45FA4EFA4576@mac.com>
Message-ID: <7DA44ED0-5C1D-4214-9922-C397D05A0326@nrao.edu>


On Jan 15, 2008, at 8:50 AM, Bernd Hofmann wrote:

> I used the new zfs, not the backward-compatible ver. 6,

And that is the problem.

The other startup volume cannot read the ZFS version 8 on-disk format.

If the other startup volume is Leopard, then it could mount ZFS  
version 6 read-only; you should be able to install your ZFS release on  
that startup disk and it should work.

If the other startup volume is not Leopard, then you will get the  
prompt to initialize the disk, of course.

If you *must* see the disk under Tiger, you *might* be able to format  
the disk with ZFS version 6 and use FUSE-ZFS on Tiger under MacFUSE.  
Unless you are very confident with the technique involved, you should  
not try this, I think.


Good Luck!



   - boyd

Boyd Waters
http://www.aoc.nrao.edu/~bwaters





From bwaters at nrao.edu  Thu Jan 17 13:05:59 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Thu Jan 17 13:05:37 2008
Subject: [zfs-discuss] Diggin into zfs...Questions ofa newbie
In-Reply-To: <A69CBDDA-71E6-43B8-A61E-7D04208DFD71@mac.com>
References: <F86F2284-AD74-48DB-ADD9-4C54A85D25AA@apple.com>
	<51BC197C-6D10-4BA9-8E00-45FA4EFA4576@mac.com>
	<7DA44ED0-5C1D-4214-9922-C397D05A0326@nrao.edu>
	<A69CBDDA-71E6-43B8-A61E-7D04208DFD71@mac.com>
Message-ID: <7C06279B-3488-4234-AB14-3DDA7570A707@nrao.edu>


On Jan 17, 2008, at 1:54 PM, Bernd Hofmann wrote:

> I will make a backup of the data, reinitzialize it with the read- 
> compatible format of zfs and will see if it works then.

Before you do that, here's something else to try:

 From Leopard on starupdisk a, run the command

	zpool export tank

  (where tank is the name of your ZFS pool on that disk).


Then when you re-boot into startupdisk b, run the command

	zpool import

And see what you get.


Maybe reinitialization is not required.

It's always a Good Idea to export/import before sharing ZFS volumes  
between systems.


  - boyd

From bplist at thinkpink.com  Thu Jan 17 13:52:10 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Thu Jan 17 13:51:53 2008
Subject: [zfs-discuss] ZFS volumes not showing up in Server Admin?
Message-ID: <D9ADD229-014B-44BA-9467-9D28FF9B97ED@thinkpink.com>

My ZFS file systems aren't showing up in the File Sharing pane of  
Server Admin.  This makes administration a pain the neck when coupled  
with the fact that the sharenfs property is not yet supported.  Anyone  
else tried to get this to work?

thanks,
bri

From david299792 at googlemail.com  Thu Jan 17 14:07:38 2008
From: david299792 at googlemail.com (David Ritchie)
Date: Thu Jan 17 14:07:23 2008
Subject: [zfs-discuss] ZFS volumes not showing up in Server Admin?
In-Reply-To: <D9ADD229-014B-44BA-9467-9D28FF9B97ED@thinkpink.com>
References: <D9ADD229-014B-44BA-9467-9D28FF9B97ED@thinkpink.com>
Message-ID: <FB152DE3-BE5A-4BA6-9F95-041420117FDF@googlemail.com>

I tried sharing a zfs volume via AFP. I dropped the root zfs volume  
onto the sharing preferences pane and set it to read and write for  
everyone, which seemed to work. When I accessed it from another  
computer, though, I couldn't see any of the files on it or the other  
nested volumes or write anything to it. Maybe there are some issues  
there. I haven't investigated
Much further yet.

-- David 

On 17 Jan 2008, at 21:52, Brian Pinkerton <bplist@thinkpink.com> wrote:

> My ZFS file systems aren't showing up in the File Sharing pane of  
> Server Admin.  This makes administration a pain the neck when  
> coupled with the fact that the sharenfs property is not yet  
> supported.  Anyone else tried to get this to work?
>
> thanks,
> bri
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
From david299792 at googlemail.com  Thu Jan 17 14:25:46 2008
From: david299792 at googlemail.com (David Ritchie)
Date: Thu Jan 17 14:25:09 2008
Subject: [zfs-discuss] ZFS volumes not showing up in Server Admin?
In-Reply-To: <FB152DE3-BE5A-4BA6-9F95-041420117FDF@googlemail.com>
References: <D9ADD229-014B-44BA-9467-9D28FF9B97ED@thinkpink.com>
	<FB152DE3-BE5A-4BA6-9F95-041420117FDF@googlemail.com>
Message-ID: <7ECC6B83-6DD5-401F-A269-ED6EAF702B8D@googlemail.com>

Actually, scratch that. I just tried manually connecting to this  
shared ZFS volume (this is shared from the client version of Mac OS X)  
by selecting 'Connect to server' in the Finder and it now works fine.  
I think there may be something else funny (and unrelated to ZFS) going  
on.

On 17 Jan 2008, at 22:07, David Ritchie wrote:

> I tried sharing a zfs volume via AFP. I dropped the root zfs volume  
> onto the sharing preferences pane and set it to read and write for  
> everyone, which seemed to work. When I accessed it from another  
> computer, though, I couldn't see any of the files on it or the other  
> nested volumes or write anything to it. Maybe there are some issues  
> there. I haven't investigated
> Much further yet.
>
> -- David
> On 17 Jan 2008, at 21:52, Brian Pinkerton <bplist@thinkpink.com>  
> wrote:
>
>> My ZFS file systems aren't showing up in the File Sharing pane of  
>> Server Admin.  This makes administration a pain the neck when  
>> coupled with the fact that the sharenfs property is not yet  
>> supported.  Anyone else tried to get this to work?
>>
>> thanks,
>> bri
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From bplist at thinkpink.com  Thu Jan 17 15:01:47 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Thu Jan 17 15:01:35 2008
Subject: [zfs-discuss] ZFS volumes not showing up in Server Admin?
In-Reply-To: <7ECC6B83-6DD5-401F-A269-ED6EAF702B8D@googlemail.com>
References: <D9ADD229-014B-44BA-9467-9D28FF9B97ED@thinkpink.com>
	<FB152DE3-BE5A-4BA6-9F95-041420117FDF@googlemail.com>
	<7ECC6B83-6DD5-401F-A269-ED6EAF702B8D@googlemail.com>
Message-ID: <0E078822-7607-4861-AE0A-4515E7D48068@thinkpink.com>

Hi David,

I was specifically asking about the "Server Admin" application in OSXS  
(it has a whole different application for specifying sharing).

An update on that: I've found that if I specify the ZFS share points  
on the command line using "sharing -a /Volumes/a_pool/an_fs" then I  
can get them to show up in the UI.  The UI is buggy, but it is  
possible to work with the ZFS share points in the UI.

bri

On Jan 17, 2008, at 2:25 PM, David Ritchie wrote:

> Actually, scratch that. I just tried manually connecting to this  
> shared ZFS volume (this is shared from the client version of Mac OS  
> X) by selecting 'Connect to server' in the Finder and it now works  
> fine. I think there may be something else funny (and unrelated to  
> ZFS) going on.
>
> On 17 Jan 2008, at 22:07, David Ritchie wrote:
>
>> I tried sharing a zfs volume via AFP. I dropped the root zfs volume  
>> onto the sharing preferences pane and set it to read and write for  
>> everyone, which seemed to work. When I accessed it from another  
>> computer, though, I couldn't see any of the files on it or the  
>> other nested volumes or write anything to it. Maybe there are some  
>> issues there. I haven't investigated
>> Much further yet.
>>
>> -- David
>> On 17 Jan 2008, at 21:52, Brian Pinkerton <bplist@thinkpink.com>  
>> wrote:
>>
>>> My ZFS file systems aren't showing up in the File Sharing pane of  
>>> Server Admin.  This makes administration a pain the neck when  
>>> coupled with the fact that the sharenfs property is not yet  
>>> supported.  Anyone else tried to get this to work?
>>>
>>> thanks,
>>> bri
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

From david299792 at googlemail.com  Thu Jan 17 15:58:01 2008
From: david299792 at googlemail.com (David Ritchie)
Date: Thu Jan 17 15:57:25 2008
Subject: [zfs-discuss] ZFS volumes not showing up in Server Admin?
In-Reply-To: <0E078822-7607-4861-AE0A-4515E7D48068@thinkpink.com>
References: <D9ADD229-014B-44BA-9467-9D28FF9B97ED@thinkpink.com>
	<FB152DE3-BE5A-4BA6-9F95-041420117FDF@googlemail.com>
	<7ECC6B83-6DD5-401F-A269-ED6EAF702B8D@googlemail.com>
	<0E078822-7607-4861-AE0A-4515E7D48068@thinkpink.com>
Message-ID: <9CB8316B-362C-4011-9BF2-E4F87767F904@googlemail.com>

Hi Bri,

Yes, I realized that after I'd sent the email. At first I thought  
maybe there was a general AFP/ZFS problem, but it seems not.

Nested file systems seem to catch Mac OS X a bit unawares. I notice  
that the pool (hence 'root' zfs fs) is listed as a volume in the  
finder and in save dialogs, but the nested volumes aren't. When I  
dragged the root pool to the sharing prefs pane it worked fine, but  
dragging one of the nested FSs (which appear in the Finder as aliases)  
doesn't seem to work - it tries to create a share with the same name  
as the pool, but I can't connect to it. Sharing a folder contained  
within the nested fs works fine though. When I connect to the shared  
pool via AFP I don't see the nested FSs at all either. Although when I  
do that with NFS to a Solaris machine with ZFS it doesn't seem to show  
the contents of nested FSs either (I just checked). You've managed to  
export nested FSs from the command line now though, so that's something.




-- David


On 17 Jan 2008, at 23:01, Brian Pinkerton wrote:

> Hi David,
>
> I was specifically asking about the "Server Admin" application in  
> OSXS (it has a whole different application for specifying sharing).
>
> An update on that: I've found that if I specify the ZFS share points  
> on the command line using "sharing -a /Volumes/a_pool/an_fs" then I  
> can get them to show up in the UI.  The UI is buggy, but it is  
> possible to work with the ZFS share points in the UI.
>
> bri
>
> On Jan 17, 2008, at 2:25 PM, David Ritchie wrote:
>
>> Actually, scratch that. I just tried manually connecting to this  
>> shared ZFS volume (this is shared from the client version of Mac OS  
>> X) by selecting 'Connect to server' in the Finder and it now works  
>> fine. I think there may be something else funny (and unrelated to  
>> ZFS) going on.
>>
>> On 17 Jan 2008, at 22:07, David Ritchie wrote:
>>
>>> I tried sharing a zfs volume via AFP. I dropped the root zfs  
>>> volume onto the sharing preferences pane and set it to read and  
>>> write for everyone, which seemed to work. When I accessed it from  
>>> another computer, though, I couldn't see any of the files on it or  
>>> the other nested volumes or write anything to it. Maybe there are  
>>> some issues there. I haven't investigated
>>> Much further yet.
>>>
>>> -- David
>>> On 17 Jan 2008, at 21:52, Brian Pinkerton <bplist@thinkpink.com>  
>>> wrote:
>>>
>>>> My ZFS file systems aren't showing up in the File Sharing pane of  
>>>> Server Admin.  This makes administration a pain the neck when  
>>>> coupled with the fact that the sharenfs property is not yet  
>>>> supported.  Anyone else tried to get this to work?
>>>>
>>>> thanks,
>>>> bri
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>

From david299792 at googlemail.com  Thu Jan 17 16:23:26 2008
From: david299792 at googlemail.com (David Ritchie)
Date: Thu Jan 17 16:22:52 2008
Subject: [zfs-discuss] Home on a ZFS volume
Message-ID: <5B1B5EBA-821E-474B-9E0E-C5058607D296@googlemail.com>

If I were to stick another couple of drives in my Mac Pro for a ZFS  
pool and use them for my home folder (being where the most important  
data on my computer are) does anyone know:-

	1. What's the easiest way to set that up? Specify a different  
location for the home directory? Make a soft link form the /Users  
directory to a FS in the ZFS pool? Something else

	2. Are there any glaring problems with doing this? I guess ZFS on OS  
X hasn't been that thoroughly tested, but aside from that the 2 things  
which could be issues that I can see are the lack of properly working  
spotlight and the case sensitivity (which probably won't cause any  
problems).



-- David
From malexand at scilytics.com  Thu Jan 17 16:40:43 2008
From: malexand at scilytics.com (Michael Alexander)
Date: Thu Jan 17 16:40:11 2008
Subject: [zfs-discuss] Filename conventions
Message-ID: <42DBD20A-F51D-47AC-AB31-1B6FFC933FDE@scilytics.com>


What are the file name conventions under MacOSX and in general. While  
ZFS works very well in my testing so far, some double byte character  
file names seem to be a problem relative to HFS+, but the pattern is  
not obvious yet. -m.a.


From zfs-discuss at openhealth.org  Thu Jan 17 19:12:57 2008
From: zfs-discuss at openhealth.org (Jonathan Borden)
Date: Thu Jan 17 19:12:16 2008
Subject: [zfs-discuss] Kernel panic after SATA cable change
Message-ID: <7a0bc8420801171912v20dee3d7i435b25d7938857f2@mail.gmail.com>

I had created a zpool with 2 stripes of 3x250 Gb RAIDZ (total size 1 tb) and
then copied a bunch of files to the array.These discs are located in two 5
bay enclosures attached to a Sonnet X4P eSATA PM card.

Subsequently I shut down the computer (Powermac G5 dual 2.0 PCI-X version)
in order to add a Sonnet Tempo-X eSATA 8 card. Of course I had to move all
my cards around and moved the X4P into slot-4, tempo-X into slot-3 and a
dual intet gigabit ethernet card into slot-2.

Of course the cables were connected and disconnected.

When trying to reboot, the machine kernel panics during startup if I have
both of the enclosures connected with all the drives in the enclosure. If I
remove all the 250Gb drives from the enclosure the machine boots fine. The
panic was in the process: zpool.

Ideas?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080117/814c5d48/attachment.html
From dirkschelfhout at mac.com  Fri Jan 18 07:25:09 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Fri Jan 18 07:24:43 2008
Subject: [zfs-discuss] Home on a ZFS volume
In-Reply-To: <5B1B5EBA-821E-474B-9E0E-C5058607D296@googlemail.com>
References: <5B1B5EBA-821E-474B-9E0E-C5058607D296@googlemail.com>
Message-ID: <94701A87-819B-4086-89A3-BBDE243BD908@mac.com>

use dscl
http://www.macosxhints.com/article.php?story=20071025175202466
did this yesterday
On 18 Jan 2008, at 01:23, David Ritchie wrote:

> If I were to stick another couple of drives in my Mac Pro for a ZFS  
> pool and use them for my home folder (being where the most important  
> data on my computer are) does anyone know:-
>
> 	1. What's the easiest way to set that up? Specify a different  
> location for the home directory? Make a soft link form the /Users  
> directory to a FS in the ZFS pool? Something else
>
> 	2. Are there any glaring problems with doing this? I guess ZFS on  
> OS X hasn't been that thoroughly tested, but aside from that the 2  
> things which could be issues that I can see are the lack of properly  
> working spotlight and the case sensitivity (which probably won't  
> cause any problems).
>
>
>
> -- David
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080118/22a05926/attachment.html
From dirkschelfhout at mac.com  Fri Jan 18 11:07:54 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Fri Jan 18 11:07:21 2008
Subject: [zfs-discuss] Home on a ZFS volume
In-Reply-To: <94701A87-819B-4086-89A3-BBDE243BD908@mac.com>
References: <5B1B5EBA-821E-474B-9E0E-C5058607D296@googlemail.com>
	<94701A87-819B-4086-89A3-BBDE243BD908@mac.com>
Message-ID: <55C44A0D-1D23-4F63-AFEC-2D7C8E55BC1E@mac.com>

I just changed my home directory back. ( away from raidz )
all applications started to hang.
( macpro, 3disks in raidz )
Dirk
On 18 Jan 2008, at 16:25, Dirk Schelfhout wrote:

> use dscl
> http://www.macosxhints.com/article.php?story=20071025175202466
> did this yesterday
> On 18 Jan 2008, at 01:23, David Ritchie wrote:
>
>> If I were to stick another couple of drives in my Mac Pro for a ZFS  
>> pool and use them for my home folder (being where the most  
>> important data on my computer are) does anyone know:-
>>
>> 	1. What's the easiest way to set that up? Specify a different  
>> location for the home directory? Make a soft link form the /Users  
>> directory to a FS in the ZFS pool? Something else
>>
>> 	2. Are there any glaring problems with doing this? I guess ZFS on  
>> OS X hasn't been that thoroughly tested, but aside from that the 2  
>> things which could be issues that I can see are the lack of  
>> properly working spotlight and the case sensitivity (which probably  
>> won't cause any problems).
>>
>>
>>
>> -- David
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080118/27a7970d/attachment.html
From david299792 at googlemail.com  Fri Jan 18 12:53:03 2008
From: david299792 at googlemail.com (David Ritchie)
Date: Fri Jan 18 12:52:24 2008
Subject: [zfs-discuss] Home on a ZFS volume
In-Reply-To: <55C44A0D-1D23-4F63-AFEC-2D7C8E55BC1E@mac.com>
References: <5B1B5EBA-821E-474B-9E0E-C5058607D296@googlemail.com>
	<94701A87-819B-4086-89A3-BBDE243BD908@mac.com>
	<55C44A0D-1D23-4F63-AFEC-2D7C8E55BC1E@mac.com>
Message-ID: <65E40B1C-EF70-4CDC-BCB9-71611F00AC48@googlemail.com>

Hmm, that doesn't sound good. I wonder what happened there. I might  
give it a go this weekend anyway and see what happens. I wonder if  
anyone else has had any luck with this.


(thanks for the links to macosxhints on how to do this. I remembered  
seeing the advanced options for user accounts before, but I forgot how  
to get to it.)

-- David


On 18 Jan 2008, at 19:07, Dirk Schelfhout wrote:

> I just changed my home directory back. ( away from raidz )
> all applications started to hang.
> ( macpro, 3disks in raidz )
> Dirk
> On 18 Jan 2008, at 16:25, Dirk Schelfhout wrote:
>
>> use dscl
>> http://www.macosxhints.com/article.php?story=20071025175202466
>> did this yesterday
>> On 18 Jan 2008, at 01:23, David Ritchie wrote:
>>
>>> If I were to stick another couple of drives in my Mac Pro for a  
>>> ZFS pool and use them for my home folder (being where the most  
>>> important data on my computer are) does anyone know:-
>>>
>>> 	1. What's the easiest way to set that up? Specify a different  
>>> location for the home directory? Make a soft link form the /Users  
>>> directory to a FS in the ZFS pool? Something else
>>>
>>> 	2. Are there any glaring problems with doing this? I guess ZFS on  
>>> OS X hasn't been that thoroughly tested, but aside from that the 2  
>>> things which could be issues that I can see are the lack of  
>>> properly working spotlight and the case sensitivity (which  
>>> probably won't cause any problems).
>>>
>>>
>>>
>>> -- David
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080118/231994f1/attachment-0001.html
From lists at loveturtle.net  Fri Jan 18 14:20:22 2008
From: lists at loveturtle.net (Dillon)
Date: Fri Jan 18 14:19:43 2008
Subject: [zfs-discuss] Home on a ZFS volume
In-Reply-To: <65E40B1C-EF70-4CDC-BCB9-71611F00AC48@googlemail.com>
References: <5B1B5EBA-821E-474B-9E0E-C5058607D296@googlemail.com>	<94701A87-819B-4086-89A3-BBDE243BD908@mac.com>	<55C44A0D-1D23-4F63-AFEC-2D7C8E55BC1E@mac.com>
	<65E40B1C-EF70-4CDC-BCB9-71611F00AC48@googlemail.com>
Message-ID: <47912626.9080107@loveturtle.net>

I've been running my home dir on zfs since the night of oct 26

I have no idea how correct my method is but it works fine for me.

I have a zpool called genepool (get it? 'cause it's on Darwin! huh huh!) 
then I created genepool/Users (with zfs create)
Log out, at the loginwindow log in as >console, log in as root, move my 
home dir (turtle) from /Users to /Volumes/genepool/Users

then
rm -r /Users (or you could cp -Rp your home dir and then mv /Users to 
/old.Users if you want to be safe)
zfs set mountpoint=/Users genepool/Users
exit (which starts the gui back up)

log in as my user.
poof all done.

turtle@vier ~ $ zfs list genepool/Users
NAME             USED  AVAIL  REFER  MOUNTPOINT
genepool/Users  61.5G  88.1G  85.1M  /Users

The only thing I noticed is that I need to set it up to open the login 
prompt on boot and NOT to automatically log in as my user. If I 
automatically log in as my user sometimes the zpool doesn't get mounted 
in time and it tries to log in as my user and thinks I have no home 
directory (as if it were my first log in) which triggers it to generate 
me a new ~/Library and shit (which happens after the zpool mounts, and 
overwites my current settings)

If i have it boot to the login window that gives the zpool just enough 
time to mount. Seems to work fine.

Lately I created genepool/Users/turtle & genepool/Users/turtle/Library 
so that I could make hourly snapshots of my ~/Library just incase.

Cheers,
Dillon


David Ritchie wrote:
> Hmm, that doesn't sound good. I wonder what happened there. I might 
> give it a go this weekend anyway and see what happens. I wonder if 
> anyone else has had any luck with this.
>
>
> (thanks for the links to macosxhints on how to do this. I remembered 
> seeing the advanced options for user accounts before, but I forgot how 
> to get to it.)
>
> -- David
>
>
> On 18 Jan 2008, at 19:07, Dirk Schelfhout wrote:
>
>> I just changed my home directory back. ( away from raidz )
>> all applications started to hang.
>> ( macpro, 3disks in raidz )
>> Dirk
>> On 18 Jan 2008, at 16:25, Dirk Schelfhout wrote:
>>
>>> use dscl
>>> http://www.macosxhints.com/article.php?story=20071025175202466
>>> did this yesterday
>>> On 18 Jan 2008, at 01:23, David Ritchie wrote:
>>>
>>>> If I were to stick another couple of drives in my Mac Pro for a ZFS 
>>>> pool and use them for my home folder (being where the most 
>>>> important data on my computer are) does anyone know:-
>>>>
>>>> 1. What's the easiest way to set that up? Specify a different 
>>>> location for the home directory? Make a soft link form the /Users 
>>>> directory to a FS in the ZFS pool? Something else
>>>>
>>>> 2. Are there any glaring problems with doing this? I guess ZFS on 
>>>> OS X hasn't been that thoroughly tested, but aside from that the 2 
>>>> things which could be issues that I can see are the lack of 
>>>> properly working spotlight and the case sensitivity (which probably 
>>>> won't cause any problems).
>>>>
>>>>
>>>>
>>>> -- David
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org 
>>>> <mailto:zfs-discuss@lists.macosforge.org>
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org 
>>> <mailto:zfs-discuss@lists.macosforge.org>
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org 
>> <mailto:zfs-discuss@lists.macosforge.org>
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> ------------------------------------------------------------------------
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>   

From bhofmann at mac.com  Fri Jan 18 15:25:05 2008
From: bhofmann at mac.com (Bernd Hofmann)
Date: Fri Jan 18 15:24:23 2008
Subject: [zfs-discuss] Diggin into zfs...Questions ofa newbie
In-Reply-To: <7C06279B-3488-4234-AB14-3DDA7570A707@nrao.edu>
References: <F86F2284-AD74-48DB-ADD9-4C54A85D25AA@apple.com>
	<51BC197C-6D10-4BA9-8E00-45FA4EFA4576@mac.com>
	<7DA44ED0-5C1D-4214-9922-C397D05A0326@nrao.edu>
	<A69CBDDA-71E6-43B8-A61E-7D04208DFD71@mac.com>
	<7C06279B-3488-4234-AB14-3DDA7570A707@nrao.edu>
Message-ID: <28D012A8-C1A2-4AE9-A450-7404E077269F@mac.com>

And you are right!

I first used zfs unmount -f to circumvent the "device busy" message,  
then i was able to do a zpool export.

Back on the new startupdisk i used zpool import -f zfstank to import  
it and boom there it is!

Nice! Next step will be to test 3 big disks (1,75 TB) as zraid.

Thanks alot!

- Bernd

----------------------------------------------------

Am 17.01.2008 um 22:05 schrieb Boyd Waters:

> Before you do that, here's something else to try:
>
> From Leopard on starupdisk a, run the command
>
> 	zpool export tank
>
> (where tank is the name of your ZFS pool on that disk).
>
>
> Then when you re-boot into startupdisk b, run the command
>
> 	zpool import
>
> And see what you get.
>
>
> Maybe reinitialization is not required.
>
> It's always a Good Idea to export/import before sharing ZFS volumes  
> between systems.
>
>
> - boyd

From bwaters at nrao.edu  Fri Jan 18 17:53:10 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Fri Jan 18 17:52:37 2008
Subject: [zfs-discuss] Home on a ZFS volume
In-Reply-To: <47912626.9080107@loveturtle.net>
References: <5B1B5EBA-821E-474B-9E0E-C5058607D296@googlemail.com>	<94701A87-819B-4086-89A3-BBDE243BD908@mac.com>	<55C44A0D-1D23-4F63-AFEC-2D7C8E55BC1E@mac.com>
	<65E40B1C-EF70-4CDC-BCB9-71611F00AC48@googlemail.com>
	<47912626.9080107@loveturtle.net>
Message-ID: <2694A684-EF64-4574-9771-B81FB24BF7E9@nrao.edu>


On Jan 18, 2008, at 3:20 PM, Dillon wrote:

> zfs set mountpoint=/Users genepool/Users

That's brilliant!

Thanks very much for these notes!

Note that you can set the HOME directory on a per-user basis:

$ dscl . -read /Users/bwaters NFSHomeDirectory
NFSHomeDirectory: /Users/bwaters

$ dscl . -write /Users/bwaters NFSHomeDirectory /Volumes/genepool/ 
bwaters


I have not tried this. But I think it would work...

  - boyd

Boyd Waters
National Radio Astronomy Observatory
http://www.aoc.nrao.edu/~bwaters


From lists at loveturtle.net  Sat Jan 19 07:54:52 2008
From: lists at loveturtle.net (Dillon)
Date: Sat Jan 19 07:54:12 2008
Subject: [zfs-discuss] Home on a ZFS volume
In-Reply-To: <2694A684-EF64-4574-9771-B81FB24BF7E9@nrao.edu>
References: <5B1B5EBA-821E-474B-9E0E-C5058607D296@googlemail.com>	<94701A87-819B-4086-89A3-BBDE243BD908@mac.com>	<55C44A0D-1D23-4F63-AFEC-2D7C8E55BC1E@mac.com>
	<65E40B1C-EF70-4CDC-BCB9-71611F00AC48@googlemail.com>
	<47912626.9080107@loveturtle.net>
	<2694A684-EF64-4574-9771-B81FB24BF7E9@nrao.edu>
Message-ID: <47921D4C.7060001@loveturtle.net>

Cool, I didn't know about dscl.

Yeah, Everyone should always keep in mind that each fs created in your 
zpool can be mounted anywhere in the system. So for instance I have a 
zpool for /opt which is where my macports install lives.

The biggest problem I have with running my home dir on zfs is finders 
confusion over the path names sometimes. If I click on the hard drive 
and then click on "Users" it tries to take me to /Volumes/genepool even 
though that's not right.

I don't really use finder often though since I'm usually faster with 
mv,cp,so on. You can always get finder to open the correct directory by 
just saying typing open . (open working directory in finder) or open 
/Users or whatever. Even though it will show up as the zpool volume you 
will see the correct contents for whatever directory you opened.

Some applications get a little confused too, For instance I use EyeTV 
with my usb tv card on my workstation and it will crash non-stop unless 
I change the recording destination path off the zpool. There's probably 
other applications that suffer problems too so everyone will have to 
make their own decision at this stage of the game.

It works for me :-)

Cheers,
Dillon

Boyd Waters wrote:
>
> On Jan 18, 2008, at 3:20 PM, Dillon wrote:
>
>> zfs set mountpoint=/Users genepool/Users
>
> That's brilliant!
>
> Thanks very much for these notes!
>
> Note that you can set the HOME directory on a per-user basis:
>
> $ dscl . -read /Users/bwaters NFSHomeDirectory
> NFSHomeDirectory: /Users/bwaters
>
> $ dscl . -write /Users/bwaters NFSHomeDirectory /Volumes/genepool/bwaters
>
>
> I have not tried this. But I think it would work...
>
>  - boyd
>
> Boyd Waters
> National Radio Astronomy Observatory
> http://www.aoc.nrao.edu/~bwaters
>
>

From barry-dated-1201127716.e84bc4 at lustig.com  Fri Jan 18 14:35:15 2008
From: barry-dated-1201127716.e84bc4 at lustig.com (Barry Lustig)
Date: Sat Jan 19 20:20:25 2008
Subject: [zfs-discuss] Home on a ZFS volume
In-Reply-To: <47912626.9080107@loveturtle.net>
References: <5B1B5EBA-821E-474B-9E0E-C5058607D296@googlemail.com>
	<94701A87-819B-4086-89A3-BBDE243BD908@mac.com>
	<55C44A0D-1D23-4F63-AFEC-2D7C8E55BC1E@mac.com>
	<65E40B1C-EF70-4CDC-BCB9-71611F00AC48@googlemail.com>
	<47912626.9080107@loveturtle.net>
Message-ID: <9008CD95-A990-40C4-B907-E39331E24316@lustig.com>


On Jan 18, 2008, at 2:20 PM, Dillon wrote:

> I've been running my home dir on zfs since the night of oct 26
>
> I have no idea how correct my method is but it works fine for me.
>
> I have a zpool called genepool (get it? 'cause it's on Darwin! huh  
> huh!) then I created genepool/Users (with zfs create)
> Log out, at the loginwindow log in as >console, log in as root, move  
> my home dir (turtle) from /Users to /Volumes/genepool/Users

A much easier way of changing your home directory is via the Accounts  
pref pane.  Go to accounts, authenticate via the padlock, and then  
right click on your username and choose Advanced Options.  You can  
change the home dir path there to anywhere you want to.



barry

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 2415 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080118/d95a56bd/smime.bin
From dillon at terranova.net  Sat Jan 19 20:30:35 2008
From: dillon at terranova.net (Dillon)
Date: Sat Jan 19 20:31:20 2008
Subject: [zfs-discuss] Home on a ZFS volume
In-Reply-To: <9008CD95-A990-40C4-B907-E39331E24316@lustig.com>
References: <5B1B5EBA-821E-474B-9E0E-C5058607D296@googlemail.com>	<94701A87-819B-4086-89A3-BBDE243BD908@mac.com>	<55C44A0D-1D23-4F63-AFEC-2D7C8E55BC1E@mac.com>	<65E40B1C-EF70-4CDC-BCB9-71611F00AC48@googlemail.com>	<47912626.9080107@loveturtle.net>
	<9008CD95-A990-40C4-B907-E39331E24316@lustig.com>
Message-ID: <4792CE6B.5040002@terranova.net>

The steps I described had nothing to do with changing my home dir path. 
My home dir still points to /Users/turtle
>
> A much easier way of changing your home directory is via the Accounts 
> pref pane.  Go to accounts, authenticate via the padlock, and then 
> right click on your username and choose Advanced Options.  You can 
> change the home dir path there to anywhere you want to.
>
>
>
> barry
>
From zorg at sogeeky.net  Sun Jan 20 17:33:28 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Sun Jan 20 17:32:49 2008
Subject: [zfs-discuss] Possible to use TimeMachine on a zfs pool?
Message-ID: <02E9D834-4BBB-4DD1-8E60-C99C56BB5695@sogeeky.net>

Is it possible to make TimeMachine use a zfs pool?  If not yet, do any  
developers have an eta on the functionality?

Thanks!
-Mr. Zorg
From bwaters at nrao.edu  Sun Jan 20 18:14:53 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Sun Jan 20 18:14:13 2008
Subject: [zfs-discuss] Kernel Panic - can't allocate memory in zfs_context.c
Message-ID: <C8CE3861-25AC-4C57-9087-2A0F34188D5D@nrao.edu>

I was trying my crazy rsync things a la rdar://5386887 but it locked  
up after a bit:

Sun Jan 20 19:03:57 2008
panic(cpu 0 caller 0x8B2D9539): "zfs: vmem_alloc couldn't alloc 131072  
bytes\n"@/Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/ 
zfs_kext/zfs/zfs_context.c:668
Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x8243ba18 : 0x12b0e1 (0x456e30 0x8243ba4c 0x13321a 0x0)
0x8243ba68 : 0x8b2d9539 (0x8b333720 0x20000 0x20000 0x8b2d81ed)
0x8243ba98 : 0x8b2f3117 (0x8b347264 0x20000 0x0 0x8b2d8267)
0x8243baf8 : 0x8b28ff09 (0xbad9600 0x0 0xb525270 0x0)
0x8243bb18 : 0x8b29a136 (0x20000 0x0 0x8b340310 0xbadaf68)
0x8243bb98 : 0x8b29a989 (0xbadae00 0x0 0x8243bbc8 0x8b2aa96f)
0x8243bbd8 : 0x8b2a0a89 (0xae07000 0x20000 0xd01d61f8 0x0)
0x8243bc28 : 0x8b2a6fec (0xd01d61f8 0x20620900 0x20000 0x0)
0x8243bca8 : 0x8b280451 (0xd790650 0xd7be2 0x0 0x8243bebc)
0x8243bd78 : 0x1f5131 (0x8243bdac 0x246 0x8243bdd8 0x1d9207)
0x8243bdd8 : 0x1eb98f (0xc7c1c20 0x8243bebc 0x1 0x8243bf54)
0x8243be68 : 0x38a19f (0xb7d8800 0x8243bebc 0x0 0x8243bf54)
0x8243bf18 : 0x38a497 (0x8243bf54 0xb7d8800 0x3b80000 0x0)
0x8243bf78 : 0x3dbcab (0xb1b9bc0 0xb5de1e0 0xb5de224 0x37e4df)
0x8243bfc8 : 0x19f084 (0xb47c9e0 0x0 0x1a20b5 0xae28d60)
0xbfffd7a8 : 0x1920014 (0xfeadff0d 0xfef3fe7e 0xff6efe25 0xfd06fe4e)
Backtrace terminated-invalid frame pointer 0x18d00c0
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0x8b27d000->0x8b348fff

BSD process name corresponding to current thread: rsync

Mac OS version:
9C16

Kernel version:
Darwin Kernel Version 9.2.0: Tue Jan  8 10:59:40 PST 2008;  
root:xnu-1228.3.7~1/RELEASE_I386
System model name: MacPro1,1 (Mac-F4208DC8)

From ndellofano at apple.com  Mon Jan 21 12:55:39 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon Jan 21 12:55:12 2008
Subject: [zfs-discuss] Possible to use TimeMachine on a zfs pool?
In-Reply-To: <02E9D834-4BBB-4DD1-8E60-C99C56BB5695@sogeeky.net>
References: <02E9D834-4BBB-4DD1-8E60-C99C56BB5695@sogeeky.net>
Message-ID: <A55B9A74-141A-4225-B68A-D3FD5D474FC8@apple.com>

Currently no, it's not possible to make Time Machine use a zfs pool to  
either backup, or backup to.  Getting this functionality to work, and  
work properly, involves also getting .zfs up and running which I'm  
currently working on and also will involve some changes to Time  
Machine.  Getting .zfs will give everyone the ability to "see" and  
browse your zfs snapshots individually and hence give Time Machine the  
ability to display your ZFS snapshots for you in lovely GUI form :)

And no, in case anyone out there is wondering...  Time Machine will  
not be doing it's current 'hardlink backups' on a ZFS volume.  That's  
why we have snapshots in the first place instead :)

Snapshots, and all their related functionality currently work, with  
the exception of the above mentioned browsing snapshots for individual  
files.  A quick workaround for this that requires no additional space  
is to do a 'zfs clone' of your snapshot.  I put more details up on the  
ZFS Known Issues page.  Let me know if something doesn't work for your  
or anyone has any questions :)

Noel

On Jan 20, 2008, at 5:33 PM, Mr. Zorg wrote:

> Is it possible to make TimeMachine use a zfs pool?  If not yet, do  
> any developers have an eta on the functionality?
>
> Thanks!
> -Mr. Zorg
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From david299792 at googlemail.com  Mon Jan 21 13:20:59 2008
From: david299792 at googlemail.com (David Ritchie)
Date: Mon Jan 21 13:20:07 2008
Subject: [zfs-discuss] Possible to use TimeMachine on a zfs pool? (and
	Home on ZFS/Spotlight)
In-Reply-To: <A55B9A74-141A-4225-B68A-D3FD5D474FC8@apple.com>
References: <02E9D834-4BBB-4DD1-8E60-C99C56BB5695@sogeeky.net>
	<A55B9A74-141A-4225-B68A-D3FD5D474FC8@apple.com>
Message-ID: <74D0D087-30FF-43D1-971F-1441492D4020@googlemail.com>

Wow - does that mean that Time Machine may soon be able to use ZFS  
snapshots instead of HFS+ hard links? That would be awesome!

When ZFS is working well enough (see below) I''m definitely planning  
on using it for most of the information on my computer. It would have  
been a shame not to have the Time Machine UI though.

I wonder how this would work if you're storing things on ZFS directly  
rather than working on an HFS+ volume and having Time Machine backup  
to ZFS.

---

On a different note, do you have any idea when Spotlight might be  
working on nested ZFS file systems? It seems that spotlight will index  
the root FS/pool (albeit with some bugs apparently) but not any FSs  
contained within that. Even if they're set to mount elsewhere.

I tried moving my Home to a ZFS fs, and it mostly worked as far as I  
can tell (only tried it last night), but I would miss having spotlight  
for my home folder. I could search for Mail messages based on subject/ 
from/to, but not the whole message :(. It was very useful to be able  
to clone my User account in a matter of seconds to experiment.



-- David

On 21 Jan 2008, at 20:55, No?l Dellofano wrote:

> Currently no, it's not possible to make Time Machine use a zfs pool  
> to either backup, or backup to.  Getting this functionality to work,  
> and work properly, involves also getting .zfs up and running which  
> I'm currently working on and also will involve some changes to Time  
> Machine.  Getting .zfs will give everyone the ability to "see" and  
> browse your zfs snapshots individually and hence give Time Machine  
> the ability to display your ZFS snapshots for you in lovely GUI  
> form :)
>
> And no, in case anyone out there is wondering...  Time Machine will  
> not be doing it's current 'hardlink backups' on a ZFS volume.   
> That's why we have snapshots in the first place instead :)
>
> Snapshots, and all their related functionality currently work, with  
> the exception of the above mentioned browsing snapshots for  
> individual files.  A quick workaround for this that requires no  
> additional space is to do a 'zfs clone' of your snapshot.  I put  
> more details up on the ZFS Known Issues page.  Let me know if  
> something doesn't work for your or anyone has any questions :)
>
> Noel
>
> On Jan 20, 2008, at 5:33 PM, Mr. Zorg wrote:
>
>> Is it possible to make TimeMachine use a zfs pool?  If not yet, do  
>> any developers have an eta on the functionality?
>>
>> Thanks!
>> -Mr. Zorg
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From jamesblackburn at gmail.com  Mon Jan 21 13:50:05 2008
From: jamesblackburn at gmail.com (James Blackburn)
Date: Mon Jan 21 13:49:09 2008
Subject: [zfs-discuss] Possible to use TimeMachine on a zfs pool?
In-Reply-To: <A55B9A74-141A-4225-B68A-D3FD5D474FC8@apple.com>
References: <02E9D834-4BBB-4DD1-8E60-C99C56BB5695@sogeeky.net>
	<A55B9A74-141A-4225-B68A-D3FD5D474FC8@apple.com>
Message-ID: <5f0d46fb0801211350s677c12d4y4d81afbcd5e8c2f8@mail.gmail.com>

> And no, in case anyone out there is wondering...  Time Machine will
> not be doing it's current 'hardlink backups' on a ZFS volume.  That's
> why we have snapshots in the first place instead :)

Unfortunately, ZFS doesn't yet support block deduplication. Time
Machine from an HFS+ disk to a ZFS disk would presumably still require
the entire file to be replaced -- rather than a block level delta --
which would result in the same space overhead as in the current
implementation (unless you guys are more intelligent with diffing
files, or implement deduplication :) ).

James
From ndellofano at apple.com  Mon Jan 21 14:27:10 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon Jan 21 14:28:06 2008
Subject: [zfs-discuss] Possible to use TimeMachine on a zfs pool? (and
	Home on ZFS/Spotlight)
In-Reply-To: <74D0D087-30FF-43D1-971F-1441492D4020@googlemail.com>
References: <02E9D834-4BBB-4DD1-8E60-C99C56BB5695@sogeeky.net>
	<A55B9A74-141A-4225-B68A-D3FD5D474FC8@apple.com>
	<74D0D087-30FF-43D1-971F-1441492D4020@googlemail.com>
Message-ID: <77D804E1-AF34-4919-BEEF-360EE58F9177@apple.com>


> Wow - does that mean that Time Machine may soon be able to use ZFS  
> snapshots instead of HFS+ hard links? That would be awesome!

Awesome for sure, I'd be stoked;  I wouldn't hold my breath on "soon"  
though :)

> I wonder how this would work if you're storing things on ZFS  
> directly rather than working on an HFS+ volume and having Time  
> Machine backup to ZFS.

If you're working off a ZFS filesystem then life "would be" easy.  ZFS  
snapshots can be taken regularly(via the command line or some future  
TM button) and then you can use Time Machine to browse those snapshots  
(or use .zfs if you are old fashioned).   Also among other things we  
could then hook up Time Machine to use 'zfs send' and 'zfs send -i'   
and 'zfs recv' to backup your snapshots to your backup volume for  
posterity.  Note that you can 'zfs send' your filesystem to a file on  
any supported filesystem, so you can send your zfs snapshot to a file  
on an HFS+ backup drive if you like.  However you can only 'zfs recv'  
that backup into a zfs filesystem.
** Keep in mind this is all hypothetical as none of this has been  
discussed yet, but you get the idea :)
Is that what you were asking or did I misinterpret your question?

If you are interested in details in how our snapshots work(it's pretty  
cool stuff), Matt Ahrens has a great blog about it:
http://blogs.sun.com/ahrens/entry/is_it_magic


> On a different note, do you have any idea when Spotlight might be  
> working on nested ZFS file systems? It seems that spotlight will  
> index the root FS/pool (albeit with some bugs apparently) but not  
> any FSs contained within that. Even if they're set to mount elsewhere.
>
> I tried moving my Home to a ZFS fs, and it mostly worked as far as I  
> can tell (only tried it last night), but I would miss having  
> spotlight for my home folder. I could search for Mail messages based  
> on subject/from/to, but not the whole message :(. It was very useful  
> to be able to clone my User account in a matter of seconds to  
> experiment.

Sadly this is a known problem.  As I mentioned on the zfs page,  
there's still a bunch of unhappiness between ZFS and Spotlight which  
is causing issues which is why I currently recommend disabling it.  We  
are trying very hard to get it up and running though since I know a  
lot of people really depend on Spotlight, like you mentioned, for  
searching home directories.  We're fixing bugs one by one, hopefully  
we'll achieve a happy union between the two soon.

As far as nested filesystems, Spotlight won't search them because we  
flag all nested filesystems(an fs that is not the pool fs) as  
MNT_DONTBROWSE.  This is for a number of reasons that basically entail  
us needing a tighter integration with Finder.  For example,  if we  
don't use this flag then every filesystem you create Finder will show  
on your desktop, which if you have many, is really annoying.  Finder  
also wants to show anything filesystems it can browse in the sidebar  
in the Finder window.  Needless to say this kind of representation is  
fine for current use but doesn't scale with how ZFS is generally used  
(we encourage a filesystem ==  directory).  The Finder window goes  
nuts, and well, it's just not pretty :(

Please know that we are working on it and I'll continue to keep the  
list updated.  Thanks for using ZFS and continue sending the feedback  
and suggestions and questions :)

Noel


On Jan 21, 2008, at 1:20 PM, David Ritchie wrote:

> Wow - does that mean that Time Machine may soon be able to use ZFS  
> snapshots instead of HFS+ hard links? That would be awesome!
>
> When ZFS is working well enough (see below) I''m definitely planning  
> on using it for most of the information on my computer. It would  
> have been a shame not to have the Time Machine UI though.
>
> I wonder how this would work if you're storing things on ZFS  
> directly rather than working on an HFS+ volume and having Time  
> Machine backup to ZFS.
>
> ---
>
> On a different note, do you have any idea when Spotlight might be  
> working on nested ZFS file systems? It seems that spotlight will  
> index the root FS/pool (albeit with some bugs apparently) but not  
> any FSs contained within that. Even if they're set to mount elsewhere.
>
> I tried moving my Home to a ZFS fs, and it mostly worked as far as I  
> can tell (only tried it last night), but I would miss having  
> spotlight for my home folder. I could search for Mail messages based  
> on subject/from/to, but not the whole message :(. It was very useful  
> to be able to clone my User account in a matter of seconds to  
> experiment.
>
>
>
> -- David
>
> On 21 Jan 2008, at 20:55, No?l Dellofano wrote:
>
>> Currently no, it's not possible to make Time Machine use a zfs pool  
>> to either backup, or backup to.  Getting this functionality to  
>> work, and work properly, involves also getting .zfs up and running  
>> which I'm currently working on and also will involve some changes  
>> to Time Machine.  Getting .zfs will give everyone the ability to  
>> "see" and browse your zfs snapshots individually and hence give  
>> Time Machine the ability to display your ZFS snapshots for you in  
>> lovely GUI form :)
>>
>> And no, in case anyone out there is wondering...  Time Machine will  
>> not be doing it's current 'hardlink backups' on a ZFS volume.   
>> That's why we have snapshots in the first place instead :)
>>
>> Snapshots, and all their related functionality currently work, with  
>> the exception of the above mentioned browsing snapshots for  
>> individual files.  A quick workaround for this that requires no  
>> additional space is to do a 'zfs clone' of your snapshot.  I put  
>> more details up on the ZFS Known Issues page.  Let me know if  
>> something doesn't work for your or anyone has any questions :)
>>
>> Noel
>>
>> On Jan 20, 2008, at 5:33 PM, Mr. Zorg wrote:
>>
>>> Is it possible to make TimeMachine use a zfs pool?  If not yet, do  
>>> any developers have an eta on the functionality?
>>>
>>> Thanks!
>>> -Mr. Zorg
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

From zfs-discuss at openhealth.org  Mon Jan 21 14:30:32 2008
From: zfs-discuss at openhealth.org (Jonathan Borden)
Date: Mon Jan 21 14:30:31 2008
Subject: [zfs-discuss] Share filesystem as...
Message-ID: <7a0bc8420801211430i6445e735h8b971e783725e838@mail.gmail.com>

I've done:

#zpool create gene ...
#zfs create gene/Video
#zfs create gene/Photos

when browsing the computer on the network using Finder I see a shares named:

gene
gene1
gene2

but when looking into "gene" in the Finder I get a blank directory...
gene1 and gene2 correspond to gene/Video and gene/Photos

What is the proper way to expose the mountpoints/filesystems to the
remote Finder so that I can give them meaningful names?

(perhaps this is easy and I am missing something?)

Jonathan
From zorg at sogeeky.net  Mon Jan 21 14:59:11 2008
From: zorg at sogeeky.net (Mr. Zorg ...)
Date: Mon Jan 21 14:58:14 2008
Subject: [zfs-discuss] Possible to use TimeMachine on a zfs pool?
In-Reply-To: <A55B9A74-141A-4225-B68A-D3FD5D474FC8@apple.com>
References: <02E9D834-4BBB-4DD1-8E60-C99C56BB5695@sogeeky.net>
	<A55B9A74-141A-4225-B68A-D3FD5D474FC8@apple.com>
Message-ID: <fbaadd320801211459s4a27ba28g88b0e89a1effed72@mail.gmail.com>

On 1/21/08, No?l Dellofano <ndellofano@apple.com> wrote:
> Currently no, it's not possible to make Time Machine use a zfs pool to
> either backup, or backup to.  Getting this functionality to work, and
> work properly, involves also getting .zfs up and running which I'm
> currently working on and also will involve some changes to Time
> Machine.  ...

I understand that the current hardlink functionality is better handled
with snapshots, but for the time being I'd be perfectly happy with the
current implementation, but just on a zfs pool.  You can do hardlinks
on zfs, right?  The only problem i see is how to get Time Machine to
see the pool...  Barring that, I'd settle for backing up to an HFS+
formatted sparsebundle which sits on my zfs pool.  :)  But I can't get
it to see that volume either.  Any "hacks" I could employ to make it
work until proper zfs support is implemented in Time Machine?

Thanks to all you developers working on this, I'm so stoked to have
raidz -- I just want it *all* now.  :P

- Mr. Zorg
From ndellofano at apple.com  Mon Jan 21 15:10:56 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon Jan 21 15:11:30 2008
Subject: [zfs-discuss] Possible to use TimeMachine on a zfs pool?
In-Reply-To: <5f0d46fb0801211350s677c12d4y4d81afbcd5e8c2f8@mail.gmail.com>
References: <02E9D834-4BBB-4DD1-8E60-C99C56BB5695@sogeeky.net>
	<A55B9A74-141A-4225-B68A-D3FD5D474FC8@apple.com>
	<5f0d46fb0801211350s677c12d4y4d81afbcd5e8c2f8@mail.gmail.com>
Message-ID: <C4BAD80E-97FD-46A0-84C5-6EFF0C84AB01@apple.com>

As you guessed,  we do have a more intelligent implementation :)

Basically, we have block birth times associated with each block and  
also "dead" lists (blocks that were live in a previous snapshot/fs,  
but aren't in this current snapshot/fs).
These two things help us know which blocks "have changed" since the  
last snapshot or event.  We will actually only bother "updating" the  
snapshot with blocks that have a birth time *later* than the  
snapshot.  Hence the amount of space a snapshot takes up is  
proportional to the amount of data you have changed between it and the  
live filesystem you're snap-shotting.  (I say "updating" in quotes  
because of how our snapshots work.  Since ZFS is COW, snapshots are  
free, since essentially when a block changes in your live filesystem,  
instead of freeing that block(as we would done if you hadn't done any  
snapshots), we keep it around.)  Visually this is like two trees that  
start out the same, but gradually diverge the more changes you make to  
your live filesystem.  A good visual of this can be seen in the  
"Constant Time Snapshots" slide of the ZFS Filesystem presentation on  
open solaris:

http://opensolaris.org/os/community/zfs/docs/zfs_last.pdf

'zfs send' also is done at the block level.  So if you do a 'zfs send - 
i' for example, only the *blocks* that have changed between this  
snapshot and the last one will be sent over.
Incidentally, the block birth time is also what helps us know what to  
resilver in the case of a transient outage in a raidz or mirror such  
that we don't incur the overhead of having to blindly resilver the  
entire raidz or mirror.

If your're interested you can read up on how this works in more detail:
Matt's snapshot blog entry:
http://blogs.sun.com/ahrens/entry/is_it_magic

Jeffs resilver blog entry:
http://blogs.sun.com/bonwick/entry/smokin_mirrors

Noel

On Jan 21, 2008, at 1:50 PM, James Blackburn wrote:

>> And no, in case anyone out there is wondering...  Time Machine will
>> not be doing it's current 'hardlink backups' on a ZFS volume.  That's
>> why we have snapshots in the first place instead :)
>
> Unfortunately, ZFS doesn't yet support block deduplication. Time
> Machine from an HFS+ disk to a ZFS disk would presumably still require
> the entire file to be replaced -- rather than a block level delta --
> which would result in the same space overhead as in the current
> implementation (unless you guys are more intelligent with diffing
> files, or implement deduplication :) ).
>
> James

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080121/b3d1e7b1/attachment.html
From ndellofano at apple.com  Mon Jan 21 15:26:30 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon Jan 21 15:27:07 2008
Subject: [zfs-discuss] Home on a ZFS volume
In-Reply-To: <4792CE6B.5040002@terranova.net>
References: <5B1B5EBA-821E-474B-9E0E-C5058607D296@googlemail.com>
	<94701A87-819B-4086-89A3-BBDE243BD908@mac.com>
	<55C44A0D-1D23-4F63-AFEC-2D7C8E55BC1E@mac.com>
	<65E40B1C-EF70-4CDC-BCB9-71611F00AC48@googlemail.com>
	<47912626.9080107@loveturtle.net>
	<9008CD95-A990-40C4-B907-E39331E24316@lustig.com>
	<4792CE6B.5040002@terranova.net>
Message-ID: <33589337-01DB-43AB-AF3E-47854DB99682@apple.com>

It all should work, and sounds like it is, it just depends on how you  
want to do it.  The way Barry mentioned is actually what I did on my  
laptop just since it's how I happened to set it up, but what Dillon  
did will work fine too, it really just depends on what you want to put  
where and how :)

I installed, created a local admin user, logged in.  Created my ZFS  
pool on my other partition, created some filesystems within it, and  
then copied all my data over.   Then I just created a new user(I keep  
one local for test purposes, and one zfs), changed the new user's home  
directory to point to the mountpoint of my zfs pool home filesystem (/ 
Volumes/mytank/home/ndellofano) and I log in and voila my home  
directory is my ZFS filesystem.

Noel

On Jan 19, 2008, at 8:30 PM, Dillon wrote:

>
>>
>> A much easier way of changing your home directory is via the  
>> Accounts pref pane.  Go to accounts, authenticate via the padlock,  
>> and then right click on your username and choose Advanced Options.   
>> You can change the home dir path there to anywhere you want to.
>>
>>
>>
>> barry
>>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From ndellofano at apple.com  Mon Jan 21 15:33:46 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon Jan 21 15:34:19 2008
Subject: [zfs-discuss] Share filesystem as...
In-Reply-To: <7a0bc8420801211430i6445e735h8b971e783725e838@mail.gmail.com>
References: <7a0bc8420801211430i6445e735h8b971e783725e838@mail.gmail.com>
Message-ID: <92F44D48-52DF-42DF-AF2D-5D10F8B76007@apple.com>

Ick.  You're not missing anything, you shouldn't have to do anything  
special.  This should just work, or at least in my opinion it should.
I'm afraid this goes under the "play better with Finder" problems.   
I'll open a bug on this and investigate to see what's up.

thanks!
Noel

On Jan 21, 2008, at 2:30 PM, Jonathan Borden wrote:

> I've done:
>
> #zpool create gene ...
> #zfs create gene/Video
> #zfs create gene/Photos
>
> when browsing the computer on the network using Finder I see a  
> shares named:
>
> gene
> gene1
> gene2
>
> but when looking into "gene" in the Finder I get a blank directory...
> gene1 and gene2 correspond to gene/Video and gene/Photos
>
> What is the proper way to expose the mountpoints/filesystems to the
> remote Finder so that I can give them meaningful names?
>
> (perhaps this is easy and I am missing something?)
>
> Jonathan
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From ndellofano at apple.com  Mon Jan 21 15:37:57 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon Jan 21 15:38:29 2008
Subject: [zfs-discuss] Kernel Panic - can't allocate memory in
	zfs_context.c
In-Reply-To: <C8CE3861-25AC-4C57-9087-2A0F34188D5D@nrao.edu>
References: <C8CE3861-25AC-4C57-9087-2A0F34188D5D@nrao.edu>
Message-ID: <1B868817-2C2F-4D13-B1B4-E770958DB876@apple.com>

yikes.

9C16 isn't GM and um, well had some known "issues".  We should be  
rolling out the GM version update *very* soon.  Can you try your crazy  
rsync tests with the new update?? It has a lot of fixes.

Noel

p.s. Thanks for the steady updates and patience and I haven't  
forgotten about the rsync bug, it just keeps getting clobbered by  
other stuff :)

On Jan 20, 2008, at 6:14 PM, Boyd Waters wrote:

> I was trying my crazy rsync things a la rdar://5386887 but it locked  
> up after a bit:
>
> Sun Jan 20 19:03:57 2008
> panic(cpu 0 caller 0x8B2D9539): "zfs: vmem_alloc couldn't alloc  
> 131072 bytes\n"@/Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/ 
> zfs-102A/zfs_kext/zfs/zfs_context.c:668
> Backtrace, Format - Frame : Return Address (4 potential args on stack)
> 0x8243ba18 : 0x12b0e1 (0x456e30 0x8243ba4c 0x13321a 0x0)
> 0x8243ba68 : 0x8b2d9539 (0x8b333720 0x20000 0x20000 0x8b2d81ed)
> 0x8243ba98 : 0x8b2f3117 (0x8b347264 0x20000 0x0 0x8b2d8267)
> 0x8243baf8 : 0x8b28ff09 (0xbad9600 0x0 0xb525270 0x0)
> 0x8243bb18 : 0x8b29a136 (0x20000 0x0 0x8b340310 0xbadaf68)
> 0x8243bb98 : 0x8b29a989 (0xbadae00 0x0 0x8243bbc8 0x8b2aa96f)
> 0x8243bbd8 : 0x8b2a0a89 (0xae07000 0x20000 0xd01d61f8 0x0)
> 0x8243bc28 : 0x8b2a6fec (0xd01d61f8 0x20620900 0x20000 0x0)
> 0x8243bca8 : 0x8b280451 (0xd790650 0xd7be2 0x0 0x8243bebc)
> 0x8243bd78 : 0x1f5131 (0x8243bdac 0x246 0x8243bdd8 0x1d9207)
> 0x8243bdd8 : 0x1eb98f (0xc7c1c20 0x8243bebc 0x1 0x8243bf54)
> 0x8243be68 : 0x38a19f (0xb7d8800 0x8243bebc 0x0 0x8243bf54)
> 0x8243bf18 : 0x38a497 (0x8243bf54 0xb7d8800 0x3b80000 0x0)
> 0x8243bf78 : 0x3dbcab (0xb1b9bc0 0xb5de1e0 0xb5de224 0x37e4df)
> 0x8243bfc8 : 0x19f084 (0xb47c9e0 0x0 0x1a20b5 0xae28d60)
> 0xbfffd7a8 : 0x1920014 (0xfeadff0d 0xfef3fe7e 0xff6efe25 0xfd06fe4e)
> Backtrace terminated-invalid frame pointer 0x18d00c0
>      Kernel loadable modules in backtrace (with dependencies):
>         com.apple.filesystems.zfs(8.0)@0x8b27d000->0x8b348fff
>
> BSD process name corresponding to current thread: rsync
>
> Mac OS version:
> 9C16
>
> Kernel version:
> Darwin Kernel Version 9.2.0: Tue Jan  8 10:59:40 PST 2008;  
> root:xnu-1228.3.7~1/RELEASE_I386
> System model name: MacPro1,1 (Mac-F4208DC8)
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From david299792 at googlemail.com  Mon Jan 21 15:40:43 2008
From: david299792 at googlemail.com (David Ritchie)
Date: Mon Jan 21 15:39:51 2008
Subject: [zfs-discuss] Possible to use TimeMachine on a zfs pool? (and
	Home on ZFS/Spotlight)
In-Reply-To: <77D804E1-AF34-4919-BEEF-360EE58F9177@apple.com>
References: <02E9D834-4BBB-4DD1-8E60-C99C56BB5695@sogeeky.net>
	<A55B9A74-141A-4225-B68A-D3FD5D474FC8@apple.com>
	<74D0D087-30FF-43D1-971F-1441492D4020@googlemail.com>
	<77D804E1-AF34-4919-BEEF-360EE58F9177@apple.com>
Message-ID: <38957EBB-B52E-4BC8-9B72-FE50E7E25760@googlemail.com>


On 21 Jan 2008, at 22:27, No?l Dellofano wrote:

>
>> Wow - does that mean that Time Machine may soon be able to use ZFS  
>> snapshots instead of HFS+ hard links? That would be awesome!
>
> Awesome for sure, I'd be stoked;  I wouldn't hold my breath on  
> "soon" though :)
>
Ah well. I'm very impatient when it comes to these things :). I'd be  
pretty chuffed with just 'normal' ZFS snapshoting ability - even if it  
means I have to browse the snapshots by hand for the time being rather  
than using the fancy Time Machine UI.

>> I wonder how this would work if you're storing things on ZFS  
>> directly rather than working on an HFS+ volume and having Time  
>> Machine backup to ZFS.
>
> If you're working off a ZFS filesystem then life "would be" easy.   
> ZFS snapshots can be taken regularly(via the command line or some  
> future TM button) and then you can use Time Machine to browse those  
> snapshots (or use .zfs if you are old fashioned).

Yes, that would be ideal.

> Also among other things we could then hook up Time Machine to use  
> 'zfs send' and 'zfs send -i'  and 'zfs recv' to backup your  
> snapshots to your backup volume for posterity.

Fantastic. If this functionality were made part of Time Machine that'd  
be great. Otherwise I'll be setting up some cron jobs or something to  
do the same thing.

> Note that you can 'zfs send' your filesystem to a file on any  
> supported filesystem, so you can send your zfs snapshot to a file on  
> an HFS+ backup drive if you like.  However you can only 'zfs recv'  
> that backup into a zfs filesystem.

Quite so. If it's feasible, though, I figure I might as well zfs send  
the FS to another ZFS file system, because then I can reconstruct it  
at the other end (receive) and ZFS will check it's integrity etc. At  
the moment I have a cheap custom built PC running Solaris which I  
access via NFS. It has a ZFS RAID mirror on it. Sometimes I work  
directly from that and sometimes I rsync stuff to it.
>
> ** Keep in mind this is all hypothetical as none of this has been  
> discussed yet, but you get the idea :)
> Is that what you were asking or did I misinterpret your question?
>
Absolutely. The working directly on ZFS, periodic snapshoting and then  
using send/receive to backup to another ZFS volume somewhere over the  
network and having Time Machine provide a front end for restoring  
stuff from backup (in addition to the normal .zfs browsing option and  
along with all the usual ZFS features like clones etc) would be ideal.

I once set up an office Solaris server (cheap commodity job) with Macs  
connecting to it and using it for user home directories (via NFS) and  
set it up to snapshot all the user homes every 10 minutes (keeping the  
past 6) along with hourly and daily snapshots (the latter kept  
forever). It was great.

> If you are interested in details in how our snapshots work(it's  
> pretty cool stuff), Matt Ahrens has a great blog about it:
> http://blogs.sun.com/ahrens/entry/is_it_magic
>
>
>> On a different note, do you have any idea when Spotlight might be  
>> working on nested ZFS file systems? It seems that spotlight will  
>> index the root FS/pool (albeit with some bugs apparently) but not  
>> any FSs contained within that. Even if they're set to mount  
>> elsewhere.
>>
>> I tried moving my Home to a ZFS fs, and it mostly worked as far as  
>> I can tell (only tried it last night), but I would miss having  
>> spotlight for my home folder. I could search for Mail messages  
>> based on subject/from/to, but not the whole message :(. It was very  
>> useful to be able to clone my User account in a matter of seconds  
>> to experiment.
>
> Sadly this is a known problem.  As I mentioned on the zfs page,  
> there's still a bunch of unhappiness between ZFS and Spotlight which  
> is causing issues which is why I currently recommend disabling it.   
> We are trying very hard to get it up and running though since I know  
> a lot of people really depend on Spotlight, like you mentioned, for  
> searching home directories.  We're fixing bugs one by one, hopefully  
> we'll achieve a happy union between the two soon.
>
Good good.

> As far as nested filesystems, Spotlight won't search them because we  
> flag all nested filesystems(an fs that is not the pool fs) as  
> MNT_DONTBROWSE.  This is for a number of reasons that basically  
> entail us needing a tighter integration with Finder.  For example,   
> if we don't use this flag then every filesystem you create Finder  
> will show on your desktop, which if you have many, is really  
> annoying.  Finder also wants to show anything filesystems it can  
> browse in the sidebar in the Finder window.  Needless to say this  
> kind of representation is fine for current use but doesn't scale  
> with how ZFS is generally used (we encourage a filesystem ==   
> directory).  The Finder window goes nuts, and well, it's just not  
> pretty :(
>
Hmm, I can see the problem. Is there any solution to allowing  
Spotlight to index the nested volumes then? What I'm inclined, in  
general, to do here is create a zpool and then make: pool/Users, pool/ 
Users/david, pool/Users/david/Library, pool/Users/david/Library/Caches  
[to exclude them from snapshots] etc etc - in other words quite a lot  
of nested file systems. If Spotlight can't be made to index them the  
only solution I can find is to create a pool and use the whole pool  
for my home directory. Any other nested FS would then effectively live  
in my home (although I could change mountpoints I suppose). Whether  
this could be a workable setup I'm not sure. Or can peace be made  
between Spotlight and nested zfs FSs?

Would it, perhaps, be possible to set some specific nested zfs  
filesystems to not be flagged as MNT_DONTBROWSE? If there was an  
option to override it that might be a workaround (zfs set  
enablespotlight=on pool/Users/david ???). I guess, as you say:-

> Please know that we are working on it and I'll continue to keep the  
> list updated.  Thanks for using ZFS and continue sending the  
> feedback and suggestions and questions :)
>
No problem. I like ZFS very much indeed and I'll definitely be using  
it. Thanks for getting it working.

> Noel
>
>
> On Jan 21, 2008, at 1:20 PM, David Ritchie wrote:
>
>> Wow - does that mean that Time Machine may soon be able to use ZFS  
>> snapshots instead of HFS+ hard links? That would be awesome!
>>
>> When ZFS is working well enough (see below) I''m definitely  
>> planning on using it for most of the information on my computer. It  
>> would have been a shame not to have the Time Machine UI though.
>>
>> I wonder how this would work if you're storing things on ZFS  
>> directly rather than working on an HFS+ volume and having Time  
>> Machine backup to ZFS.
>>
>> ---
>>
>> On a different note, do you have any idea when Spotlight might be  
>> working on nested ZFS file systems? It seems that spotlight will  
>> index the root FS/pool (albeit with some bugs apparently) but not  
>> any FSs contained within that. Even if they're set to mount  
>> elsewhere.
>>
>> I tried moving my Home to a ZFS fs, and it mostly worked as far as  
>> I can tell (only tried it last night), but I would miss having  
>> spotlight for my home folder. I could search for Mail messages  
>> based on subject/from/to, but not the whole message :(. It was very  
>> useful to be able to clone my User account in a matter of seconds  
>> to experiment.
>>
>>
>>
>> -- David
>>
>> On 21 Jan 2008, at 20:55, No?l Dellofano wrote:
>>
>>> Currently no, it's not possible to make Time Machine use a zfs  
>>> pool to either backup, or backup to.  Getting this functionality  
>>> to work, and work properly, involves also getting .zfs up and  
>>> running which I'm currently working on and also will involve some  
>>> changes to Time Machine.  Getting .zfs will give everyone the  
>>> ability to "see" and browse your zfs snapshots individually and  
>>> hence give Time Machine the ability to display your ZFS snapshots  
>>> for you in lovely GUI form :)
>>>
>>> And no, in case anyone out there is wondering...  Time Machine  
>>> will not be doing it's current 'hardlink backups' on a ZFS  
>>> volume.  That's why we have snapshots in the first place instead :)
>>>
>>> Snapshots, and all their related functionality currently work,  
>>> with the exception of the above mentioned browsing snapshots for  
>>> individual files.  A quick workaround for this that requires no  
>>> additional space is to do a 'zfs clone' of your snapshot.  I put  
>>> more details up on the ZFS Known Issues page.  Let me know if  
>>> something doesn't work for your or anyone has any questions :)
>>>
>>> Noel
>>>
>>> On Jan 20, 2008, at 5:33 PM, Mr. Zorg wrote:
>>>
>>>> Is it possible to make TimeMachine use a zfs pool?  If not yet,  
>>>> do any developers have an eta on the functionality?
>>>>
>>>> Thanks!
>>>> -Mr. Zorg
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>

From ndellofano at apple.com  Mon Jan 21 15:56:05 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon Jan 21 15:56:38 2008
Subject: [zfs-discuss] system requirements?
In-Reply-To: <a06240805c3b4c14c359e@[10.0.1.2]>
References: <a06240805c3b4c14c359e@[10.0.1.2]>
Message-ID: <F368651D-17C8-4141-9A0A-00A71BB4CE3E@apple.com>

ZFS will only run on Leopard, not Tiger.  And yes, either Intel or PPC  
is fine.  You do *not* need developer tools in order to run ZFS,  
however if you wish to change or compile the source, you will require  
Xcode to do that.  I'll add this info to the ZFS FAQ so that it's more  
clear what you need and don't need.

Noel

On Jan 17, 2008, at 12:36 AM, Eric A Hulteen wrote:

> I've inferred from the discussion that ZFS will run on both Intel  
> and PowerPC Macs and I've also seen references only to Leopard while  
> none to Tiger.  Could someone be explicit (here or in the FAQ) about  
> the system requirements for running ZFS?  Are Developer tools  
> required to be installed?
>
> Eric
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From bhofmann at mac.com  Mon Jan 21 16:09:35 2008
From: bhofmann at mac.com (Bernd Hofmann)
Date: Mon Jan 21 16:08:46 2008
Subject: [zfs-discuss] Mounting zfs-fs in hfs homedir on another volume
	pushes hfs-homedir away
In-Reply-To: <33589337-01DB-43AB-AF3E-47854DB99682@apple.com>
References: <5B1B5EBA-821E-474B-9E0E-C5058607D296@googlemail.com>
	<94701A87-819B-4086-89A3-BBDE243BD908@mac.com>
	<55C44A0D-1D23-4F63-AFEC-2D7C8E55BC1E@mac.com>
	<65E40B1C-EF70-4CDC-BCB9-71611F00AC48@googlemail.com>
	<47912626.9080107@loveturtle.net>
	<9008CD95-A990-40C4-B907-E39331E24316@lustig.com>
	<4792CE6B.5040002@terranova.net>
	<33589337-01DB-43AB-AF3E-47854DB99682@apple.com>
Message-ID: <C0379E55-06C3-407A-B74F-AFB9421D18A1@mac.com>

It's very interesting and fun to play with zfs, i downloaded the zfs- 
admin guide and learn along.

I created a pool "spacelab" and within that pool i created a fs  
"mymusic" then i decided to look how it is working to mount "mymusic"  
elsewhere in my homedirectory so i did:

zfs mountpoint=/Volumes/RAID-MediaBay/bhofmann/mymusic (where bhofmann  
is my hfs-homedir which was previously moved from the Startupdisk to  
to another hfs-disc (RAID-MediaBay).

This worked, but sometimes in this process i must have done something  
wrong, zfs simply set my original userdirectory away and took over  
control.

after setting my zfs mountpoint my original (hfs+) homedir was moved  
over to /Volumes/RAID-MediaBay 1/bhofmann/

(zfs simply took place on the original homdir-spot and moved my hfs- 
homedir to a virtual volume with the same name and add "1" to the  
name, so after restarting my maschine i must set my homedir manually  
to the new destination zfs created for me :-(...).

Gladly enogh, nothing was deleted (Testmachine, no harm when something  
goes wrong)

Dunno if it was my fault, but maybe one should look if this behaviour  
is reproducable in case someone want mount his zfs-fs in a homedir  
which was previously moved to another hfs volume.

Bernd



Am 22.01.2008 um 00:26 schrieb No?l Dellofano:

> It all should work, and sounds like it is, it just depends on how  
> you want to do it.  The way Barry mentioned is actually what I did  
> on my laptop just since it's how I happened to set it up, but what  
> Dillon did will work fine too, it really just depends on what you  
> want to put where and how :)
>
> I installed, created a local admin user, logged in.  Created my ZFS  
> pool on my other partition, created some filesystems within it, and  
> then copied all my data over.   Then I just created a new user(I  
> keep one local for test purposes, and one zfs), changed the new  
> user's home directory to point to the mountpoint of my zfs pool home  
> filesystem (/Volumes/mytank/home/ndellofano) and I log in and voila  
> my home directory is my ZFS filesystem.
>
> Noel
>
> On Jan 19, 2008, at 8:30 PM, Dillon wrote:
>
>>
>>>
>>> A much easier way of changing your home directory is via the  
>>> Accounts pref pane.  Go to accounts, authenticate via the padlock,  
>>> and then right click on your username and choose Advanced  
>>> Options.  You can change the home dir path there to anywhere you  
>>> want to.
>>>
>>>
>>>
>>> barry
>>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From bwaters at nrao.edu  Mon Jan 21 22:05:17 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Mon Jan 21 22:05:43 2008
Subject: [zfs-discuss] Kernel Panic - can't allocate memory in
	zfs_context.c
In-Reply-To: <1B868817-2C2F-4D13-B1B4-E770958DB876@apple.com>
References: <C8CE3861-25AC-4C57-9087-2A0F34188D5D@nrao.edu>
	<1B868817-2C2F-4D13-B1B4-E770958DB876@apple.com>
Message-ID: <BBA1BA25-36F1-49F1-95FE-5BD8466F6FCF@nrao.edu>


On Jan 21, 2008, at 4:37 PM, No?l Dellofano wrote:

> yikes.
>
> Can you try your crazy rsync tests with the new update?? It has a  
> lot of fixes.


I've just completed a round of my "crazy rsync tests" :-) with rsync  
3.0pre8 compiled for 64-bit and it seems to work _great_.

Hmm!

http://samba.anu.edu.au/rsync/


./configure CC=/usr/bin/gcc-4.2 CFLAGS="-Os -ftree-vectorize - 
mtune=native -pipe -ffast-math -fPIC -arch x86_64 -Wl,-pie -fstack- 
protector"
make
sudo make install



So I don't know if my kernel panic during heavy rsync load are  
(primarily) ZFS problems. The problem I initially reported on this  
list was indeed a kernel panic in ZFS. Probably rsync needs some  
attention, too...


- boyd
http://www.aoc.nrao.edu/~bwaters

From bwaters at nrao.edu  Mon Jan 21 22:41:31 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Mon Jan 21 22:41:55 2008
Subject: [zfs-discuss] Share filesystem as...
In-Reply-To: <92F44D48-52DF-42DF-AF2D-5D10F8B76007@apple.com>
References: <7a0bc8420801211430i6445e735h8b971e783725e838@mail.gmail.com>
	<92F44D48-52DF-42DF-AF2D-5D10F8B76007@apple.com>
Message-ID: <5216E04C-7A9F-4F8E-ABE2-AB18883D5D40@nrao.edu>

FWIW...

I see a possibly-related Finder weirdness with my Home directory in  
the Finder if I open another volume with the same name.

I use FileVault at the moment; the volume name is the same as my user  
name. If I open a second 'username' volume (on a backup disk, for  
example), then the Finder sidebar will display the second volume as a  
removable disk (indicated by the icon), but if I select that icon, it  
navigates to my $HOME.

I can display the second volume just fine with command-G (Go), then  
typing /Volumes/username (the mount point of the second volume named  
'username').

Seems pretty clear to me that VFS is confusing the Finder. (Or maybe  
the Finder doesn't need any help to get confused.)

I'll file a bug on this (with some simple steps to reproduce).

But anyway I think that the Leopard Finder gets confused if there are  
multiple volumes mounted, with the same volume name... perhaps if the  
mount points aren't the expected /Volumes/volname, /Volumes/volname\  
1, /Volumes/volname\ 2...

I don't think it is (just) a ZFS bug.


  - boyd

http://www.aoc.nrao.edu/~bwaters



On Jan 21, 2008, at 4:33 PM, No?l Dellofano wrote:

> Ick.  You're not missing anything, you shouldn't have to do anything  
> special.  This should just work, or at least in my opinion it should.
> I'm afraid this goes under the "play better with Finder" problems.   
> I'll open a bug on this and investigate to see what's up.
>
> thanks!
> Noel
>
> On Jan 21, 2008, at 2:30 PM, Jonathan Borden wrote:
>
>> I've done:
>>
>> #zpool create gene ...
>> #zfs create gene/Video
>> #zfs create gene/Photos
>>
>> when browsing the computer on the network using Finder I see a  
>> shares named:
>>
>> gene
>> gene1
>> gene2
>>
>> but when looking into "gene" in the Finder I get a blank directory...
>> gene1 and gene2 correspond to gene/Video and gene/Photos
>>
>> What is the proper way to expose the mountpoints/filesystems to the
>> remote Finder so that I can give them meaningful names?
>>
>> (perhaps this is easy and I am missing something?)
>>
>> Jonathan
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From William.Winnett at Sun.COM  Mon Jan 21 23:19:55 2008
From: William.Winnett at Sun.COM (Bill Winnett)
Date: Mon Jan 21 23:14:47 2008
Subject: [zfs-discuss] Is there a now/will there be a web-based ZFS
	management tool
Message-ID: <8B9889A5-4971-4D85-8E8B-2DFCFFEF8204@sun.com>



Apparently Solaris has a ZFS web-based management tool as of 6/06  
Solaris 10.
See "http://docs.sun.com/app/docs/doc/819-5461/6n7ht6qr2?a=view#gbsbp"

Will OS X have similar functionality at some point?

Thanks,
-bill w.
From gabor at berczi.be  Tue Jan 22 01:07:31 2008
From: gabor at berczi.be (=?ISO-8859-1?Q?B=E9rczi_G=E1bor?=)
Date: Tue Jan 22 01:07:45 2008
Subject: [zfs-discuss] Two questions
Message-ID: <601576F2-52CD-4341-9C14-6CC14596A42D@berczi.be>

I use ZFS on an external firewire drive (two pools, for one will be  
mirrored to another hdd - this is also the backup pool, snapshotted  
from cron), and I have an 'Users' pool on a 12" iBook.

I've noticed a very strange behaviour: http://trac.macosforge.org/projects/zfs/ticket/2
Except for this, and some games which segfault on case-sensitive  
filesystems, I haven't really encountered any serious problems.

Also are there any specific hints on debugging/benchmarking ZFS  
performance, or is it just using Shark? Are there any known bugs in  
this area?

-- 
Gabor Berczi

From ndellofano at apple.com  Tue Jan 22 10:41:59 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue Jan 22 10:43:29 2008
Subject: [zfs-discuss] Is there a now/will there be a web-based
	ZFS	management tool
In-Reply-To: <8B9889A5-4971-4D85-8E8B-2DFCFFEF8204@sun.com>
References: <8B9889A5-4971-4D85-8E8B-2DFCFFEF8204@sun.com>
Message-ID: <E146A169-E4B9-412A-884A-4923E72A6EA6@apple.com>

We really haven't discussed any GUI plans yet for ZFS so I'm not sure  
what we'll end up with.  However, we will be much better integrated  
with Disk Utility in the future, so Disk Utility should eventually  
handle all things involving pool creation and formatting and such,  
we're currently working on that aspect.  The rest is still quite up in  
the air however.

Noel

On Jan 21, 2008, at 11:19 PM, Bill Winnett wrote:

>
>
> Apparently Solaris has a ZFS web-based management tool as of 6/06  
> Solaris 10.
> See "http://docs.sun.com/app/docs/doc/819-5461/6n7ht6qr2?a=view#gbsbp"
>
> Will OS X have similar functionality at some point?
>
> Thanks,
> -bill w.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From bplist at thinkpink.com  Tue Jan 22 10:58:14 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Tue Jan 22 10:58:28 2008
Subject: [zfs-discuss] Kernel Panic - can't allocate memory in
	zfs_context.c
In-Reply-To: <BBA1BA25-36F1-49F1-95FE-5BD8466F6FCF@nrao.edu>
References: <C8CE3861-25AC-4C57-9087-2A0F34188D5D@nrao.edu>
	<1B868817-2C2F-4D13-B1B4-E770958DB876@apple.com>
	<BBA1BA25-36F1-49F1-95FE-5BD8466F6FCF@nrao.edu>
Message-ID: <59537597-E328-4026-AC18-40F5F4D27562@thinkpink.com>

I've gotten this same ZFS panic a number of times with current  
software.  So far, I can say that it always coincides with doing  
something that involves a lot of files -- either rsync, du, or tar.

That said, the new rsync looks pretty great!  I'm looking forward to  
trying it out.

bri

On Jan 21, 2008, at 10:05 PM, Boyd Waters wrote:

>
> On Jan 21, 2008, at 4:37 PM, No?l Dellofano wrote:
>
>> yikes.
>>
>> Can you try your crazy rsync tests with the new update?? It has a  
>> lot of fixes.
>
>
> I've just completed a round of my "crazy rsync tests" :-) with rsync  
> 3.0pre8 compiled for 64-bit and it seems to work _great_.
>
> Hmm!
>
> http://samba.anu.edu.au/rsync/
>
>
> ./configure CC=/usr/bin/gcc-4.2 CFLAGS="-Os -ftree-vectorize - 
> mtune=native -pipe -ffast-math -fPIC -arch x86_64 -Wl,-pie -fstack- 
> protector"
> make
> sudo make install
>
>
>
> So I don't know if my kernel panic during heavy rsync load are  
> (primarily) ZFS problems. The problem I initially reported on this  
> list was indeed a kernel panic in ZFS. Probably rsync needs some  
> attention, too...
>
>
> - boyd
> http://www.aoc.nrao.edu/~bwaters
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

From ndellofano at apple.com  Tue Jan 22 10:59:51 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue Jan 22 11:01:24 2008
Subject: [zfs-discuss] Two questions
In-Reply-To: <601576F2-52CD-4341-9C14-6CC14596A42D@berczi.be>
References: <601576F2-52CD-4341-9C14-6CC14596A42D@berczi.be>
Message-ID: <A08D256B-92B6-40D8-858C-ED8B5D0E83BA@apple.com>

Currently we haven't done *any* performance work yet, other then  
general benchmarking along the way to make sure we haven't gone off  
the deep end or are performing horribly.  So be warned you're probably  
not going to see the best numbers as no work has been done in this area.

As far as benchmarking, just to keep track we've been using just some  
general benchmarks like postmark, dbench, and such in addition to  
timing certain commands (like a ls -lR of a large hierarchy for  
example). Are you looking for something or debugging something  
specific? Or just general?

Depending on what you're doing of course there are a variety of  
tools.   Eventually when Dtrace works on kexts, you can also use that  
to do perf work.  Two zfs specific tools you can use are:
- the 'zpool iostat' command to see io rates split up by pool.  There  
are a variety of options you use for it depending on what you want to  
look at
- the 'zoink' tool if you're interested in memory management and  
footprint info for ZFS


Noel

p.s. We will have the option of having case insensitive zfs  
filesystems soon (as a settable property via 'zfs set') in which case  
you can run your games out of that filesystem which should help the  
case sensitive grumbles.

On Jan 22, 2008, at 1:07 AM, B?rczi G?bor wrote:

> I use ZFS on an external firewire drive (two pools, for one will be  
> mirrored to another hdd - this is also the backup pool, snapshotted  
> from cron), and I have an 'Users' pool on a 12" iBook.
>
> I've noticed a very strange behaviour: http://trac.macosforge.org/projects/zfs/ticket/2
> Except for this, and some games which segfault on case-sensitive  
> filesystems, I haven't really encountered any serious problems.
>
> Also are there any specific hints on debugging/benchmarking ZFS  
> performance, or is it just using Shark? Are there any known bugs in  
> this area?
>
> -- 
> Gabor Berczi
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From bwaters at nrao.edu  Tue Jan 22 11:09:18 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Tue Jan 22 11:09:54 2008
Subject: [zfs-discuss] Two questions
In-Reply-To: <A08D256B-92B6-40D8-858C-ED8B5D0E83BA@apple.com>
References: <601576F2-52CD-4341-9C14-6CC14596A42D@berczi.be>
	<A08D256B-92B6-40D8-858C-ED8B5D0E83BA@apple.com>
Message-ID: <38DD785E-3F30-4406-A3AE-0226F184CA41@nrao.edu>


On Jan 22, 2008, at 11:59 AM, No?l Dellofano wrote:

>  timing certain commands (like a ls -lR of a large hierarchy for  
> example)

Using the timing info for the GNU implementation of ls would be a Bad  
Idea, because it tries to sort the output: it uses an awful^W simple  
sorting method to do so, polynomial in time with the number of  
elements.  If you try to ls a directory with 30,000 files in it,  
you'll wait a while, that has nothing to do with the filesystem.

I know that Darwin's implementation of ls isn't GNU's -- you've got  
the Solaris-like -@ option (is that POSIX 2003?) -- but I still see  
performance issues with ls.

Since most filesystems return the result of a dir read call as a  
sorted list, this is silly. There's a command-line option to disable  
the ls-internal sorting, I think, but beware..

  - boyd


From bwaters at nrao.edu  Tue Jan 22 11:18:05 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Tue Jan 22 11:22:28 2008
Subject: [zfs-discuss] Kernel Panic - can't allocate memory in
	zfs_context.c
In-Reply-To: <59537597-E328-4026-AC18-40F5F4D27562@thinkpink.com>
References: <C8CE3861-25AC-4C57-9087-2A0F34188D5D@nrao.edu>
	<1B868817-2C2F-4D13-B1B4-E770958DB876@apple.com>
	<BBA1BA25-36F1-49F1-95FE-5BD8466F6FCF@nrao.edu>
	<59537597-E328-4026-AC18-40F5F4D27562@thinkpink.com>
Message-ID: <D25C3DAA-C505-4EE5-AB26-479A8026222F@nrao.edu>

Ok, my crazy rysnc test is proceeding along with rsync 3.0pre8 (64- 
bit) and combined with ZFS and snapshots I've achieved nerd-vana. It  
just works.

Of course, by saying this I risk the wrath of the kernel-panic  
gremlins... nope, still cruising along! Great.

So for what it's worth my recommendation for the use of rsync 3.0  
rather than 2.{6,7,8} is hereby submitted.

The standard source tree handles Mac OS X extended attributes just  
fine, thank you -- the only files I have that still have resource  
forks are Finder aliases, and those still retain their forks. Colors,  
custom icons, ACLs, other extended attributes -- no problem so far.

http://samba.anu.edu.au/rsync/

Yay.

  - boyd



On Jan 22, 2008, at 11:58 AM, Brian Pinkerton wrote:

> I've gotten this same ZFS panic a number of times with current  
> software.  So far, I can say that it always coincides with doing  
> something that involves a lot of files -- either rsync, du, or tar.
>
> That said, the new rsync looks pretty great!  I'm looking forward to  
> trying it out.
>
> bri
>
> On Jan 21, 2008, at 10:05 PM, Boyd Waters wrote:
>
>>
>> On Jan 21, 2008, at 4:37 PM, No?l Dellofano wrote:
>>
>>> yikes.
>>>
>>> Can you try your crazy rsync tests with the new update?? It has a  
>>> lot of fixes.
>>
>>
>> I've just completed a round of my "crazy rsync tests" :-) with  
>> rsync 3.0pre8 compiled for 64-bit and it seems to work _great_.
>>
>> Hmm!
>>
>> http://samba.anu.edu.au/rsync/
>>
>>
>> ./configure CC=/usr/bin/gcc-4.2 CFLAGS="-Os -ftree-vectorize - 
>> mtune=native -pipe -ffast-math -fPIC -arch x86_64 -Wl,-pie -fstack- 
>> protector"
>> make
>> sudo make install
>>
>>
>>
>> So I don't know if my kernel panic during heavy rsync load are  
>> (primarily) ZFS problems. The problem I initially reported on this  
>> list was indeed a kernel panic in ZFS. Probably rsync needs some  
>> attention, too...
>>
>>
>> - boyd
>> http://www.aoc.nrao.edu/~bwaters
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>

From ndellofano at apple.com  Tue Jan 22 14:25:53 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue Jan 22 14:27:22 2008
Subject: [zfs-discuss] Two questions
In-Reply-To: <38DD785E-3F30-4406-A3AE-0226F184CA41@nrao.edu>
References: <601576F2-52CD-4341-9C14-6CC14596A42D@berczi.be>
	<A08D256B-92B6-40D8-858C-ED8B5D0E83BA@apple.com>
	<38DD785E-3F30-4406-A3AE-0226F184CA41@nrao.edu>
Message-ID: <77EFCBB0-A31D-4974-A421-53142F5658C6@apple.com>

Ah, you make a really good point.  Mostly right now I'm not running  
the 'ls' in a truly *large* hierarchy, I'm actually just running it on  
the xnu source tree, which branches out quite a bit but each  
individual directory isn't very large (at least by "large"  
standards).  At this point I was doing it to see more from a user's  
perspective "am I waiting longer for my results than I would on HFS 
+".  Since we were just using it as a rough guideline.
When we get into actual performance work though with real large  
directories and or heirarchies (thousands) you have a really good  
point which I'll definitely keep in mind, so thanks for the heads up.
Likely we'll drop it all together since it's not really a true  
benchmark in the first place.  Now that Filebench is open sourced, it  
would be ideal to start using that to supplement perf work when we get  
there.


Noel

On Jan 22, 2008, at 11:09 AM, Boyd Waters wrote:

>
> On Jan 22, 2008, at 11:59 AM, No?l Dellofano wrote:
>
>> timing certain commands (like a ls -lR of a large hierarchy for  
>> example)
>
> Using the timing info for the GNU implementation of ls would be a  
> Bad Idea, because it tries to sort the output: it uses an awful^W  
> simple sorting method to do so, polynomial in time with the number  
> of elements.  If you try to ls a directory with 30,000 files in it,  
> you'll wait a while, that has nothing to do with the filesystem.
>
> I know that Darwin's implementation of ls isn't GNU's -- you've got  
> the Solaris-like -@ option (is that POSIX 2003?) -- but I still see  
> performance issues with ls.
>
> Since most filesystems return the result of a dir read call as a  
> sorted list, this is silly. There's a command-line option to disable  
> the ls-internal sorting, I think, but beware..
>
> - boyd
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From kane at inius.com  Tue Jan 22 18:12:27 2008
From: kane at inius.com (Kane Dijkman)
Date: Tue Jan 22 18:12:28 2008
Subject: [zfs-discuss] Kernel Panic - can't allocate memory in
	zfs_context.c
In-Reply-To: <D25C3DAA-C505-4EE5-AB26-479A8026222F@nrao.edu>
References: <C8CE3861-25AC-4C57-9087-2A0F34188D5D@nrao.edu>
	<1B868817-2C2F-4D13-B1B4-E770958DB876@apple.com>
	<BBA1BA25-36F1-49F1-95FE-5BD8466F6FCF@nrao.edu>
	<59537597-E328-4026-AC18-40F5F4D27562@thinkpink.com>
	<D25C3DAA-C505-4EE5-AB26-479A8026222F@nrao.edu>
Message-ID: <25C7ED94-DF04-41CF-9731-64DD19D73431@inius.com>

Hi, I am new to the list. Looking to get ZFS running on my "mirrored  
nightly" backup raid. Right now I have 2 raid sets. One is used  
actively and the other contains a nightly mirroring of the first. I  
want to move this second raid to a ZFS pool.
I am using rsync to do the mirroring and per what is already known,  
the shipping version with Leopard dies when running on the ZFS drive.

So I saw Boyd's comments that the latest is working and am hoping to  
use that.

>>> I've just completed a round of my "crazy rsync tests" :-) with  
>>> rsync 3.0pre8 compiled for 64-bit and it seems to work _great_.
>>>
>>> Hmm!
>>>
>>> http://samba.anu.edu.au/rsync/
>>>
>>>
>>> ./configure CC=/usr/bin/gcc-4.2 CFLAGS="-Os -ftree-vectorize - 
>>> mtune=native -pipe -ffast-math -fPIC -arch x86_64 -Wl,-pie -fstack- 
>>> protector"
>>> make
>>> sudo make install

However, with the latest Dev tools install from Apple I only have  
gcc-4.0 so I get an error when building using the configure line  
above., and I don't know enough about all the params to tweak stuff  
myself.

Does anyone have a configure string that will lead to a good build on  
the latest release of Leopard?

Thanks,
Kane



On Jan 22, 2008, at 11:18 AM, Boyd Waters wrote:

> Ok, my crazy rysnc test is proceeding along with rsync 3.0pre8 (64- 
> bit) and combined with ZFS and snapshots I've achieved nerd-vana. It  
> just works.
>
> Of course, by saying this I risk the wrath of the kernel-panic  
> gremlins... nope, still cruising along! Great.
>
> So for what it's worth my recommendation for the use of rsync 3.0  
> rather than 2.{6,7,8} is hereby submitted.
>
> The standard source tree handles Mac OS X extended attributes just  
> fine, thank you -- the only files I have that still have resource  
> forks are Finder aliases, and those still retain their forks.  
> Colors, custom icons, ACLs, other extended attributes -- no problem  
> so far.
>
> http://samba.anu.edu.au/rsync/
>
> Yay.
>
> - boyd
>
>
>
> On Jan 22, 2008, at 11:58 AM, Brian Pinkerton wrote:
>
>> I've gotten this same ZFS panic a number of times with current  
>> software.  So far, I can say that it always coincides with doing  
>> something that involves a lot of files -- either rsync, du, or tar.
>>
>> That said, the new rsync looks pretty great!  I'm looking forward  
>> to trying it out.
>>
>> bri
>>
>> On Jan 21, 2008, at 10:05 PM, Boyd Waters wrote:
>>
>>>
>>> On Jan 21, 2008, at 4:37 PM, No?l Dellofano wrote:
>>>
>>>> yikes.
>>>>
>>>> Can you try your crazy rsync tests with the new update?? It has a  
>>>> lot of fixes.
>>>
>>>
>>> I've just completed a round of my "crazy rsync tests" :-) with  
>>> rsync 3.0pre8 compiled for 64-bit and it seems to work _great_.
>>>
>>> Hmm!
>>>
>>> http://samba.anu.edu.au/rsync/
>>>
>>>
>>> ./configure CC=/usr/bin/gcc-4.2 CFLAGS="-Os -ftree-vectorize - 
>>> mtune=native -pipe -ffast-math -fPIC -arch x86_64 -Wl,-pie -fstack- 
>>> protector"
>>> make
>>> sudo make install
>>>
>>>
>>>
>>> So I don't know if my kernel panic during heavy rsync load are  
>>> (primarily) ZFS problems. The problem I initially reported on this  
>>> list was indeed a kernel panic in ZFS. Probably rsync needs some  
>>> attention, too...
>>>
>>>
>>> - boyd
>>> http://www.aoc.nrao.edu/~bwaters
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss


------------------------------------------------------------------------------------------
"I prayed and prayed to God to give me a bicycle. Then I realized  
that's not the way he works. So I stole one and prayed for forgiveness."

From bwaters at nrao.edu  Tue Jan 22 21:57:46 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Tue Jan 22 21:58:03 2008
Subject: [zfs-discuss] Kernel Panic - can't allocate memory in
	zfs_context.c
In-Reply-To: <25C7ED94-DF04-41CF-9731-64DD19D73431@inius.com>
References: <C8CE3861-25AC-4C57-9087-2A0F34188D5D@nrao.edu>
	<1B868817-2C2F-4D13-B1B4-E770958DB876@apple.com>
	<BBA1BA25-36F1-49F1-95FE-5BD8466F6FCF@nrao.edu>
	<59537597-E328-4026-AC18-40F5F4D27562@thinkpink.com>
	<D25C3DAA-C505-4EE5-AB26-479A8026222F@nrao.edu>
	<25C7ED94-DF04-41CF-9731-64DD19D73431@inius.com>
Message-ID: <97EB893F-6CEC-42CF-AE8F-A4797C63C6A7@nrao.edu>


On Jan 22, 2008, at 7:12 PM, Kane Dijkman wrote:

> However, with the latest Dev tools install from Apple I only have  
> gcc-4.0 so I get an error when building using the configure line  
> above., and I don't know enough about all the params to tweak stuff  
> myself.


Sorry about that... the GCC 4.0 doesn't do "native" for mtune. It's  
just an optimization thing, it might not even be important.

In general, when faced with some lousy options that a wacko on a  
mailing list suggests, just try running "configure" without any  
options. So you might start with

./configure
make
sudo make install

  and that should work.


For Apple's GCC 4.0 on Leopard, running on a 64-bit Intel Mac, you  
could try this:

/configure CFLAGS="-Os -ftree-vectorize -pipe -fPIC -arch x86_64 -Wl,- 
pie"

I think the 64-bit option was worth mentioning for 2.x versions of  
rsync, becuase there were reports of 32-bit rsync actually *running  
out of memory* when traversing large hierarchies. Good grief. That  
means it grabbed more than two gigabytes of real RAM before the kernel  
killed it. Clearly that's pathological.  The 3.0 version of rsync  
seems to manage large hierarchies a bit at a time, and probably you  
don't need the 64-bit version so much. And the 64-bit version wouldn't  
run on my MacBook Pro (for instance, a not-Core-2 machine).

So if you are still reading this, you might try not setting the CFLAGS  
and probably it will work fine.

Sorry we're clogging up a ZFS list with rsync issues. But maybe I can  
find some overlap -- note that you can push snapshots from one ZFS  
array to another with zpool export/import. I wonder how my remote  
backup and mirroring strategies are going to change... I may not use  
rsync in the future, and just use ZFS!

Hope this helps!

   - boyd


From ndellofano at apple.com  Wed Jan 23 11:27:36 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed Jan 23 11:29:02 2008
Subject: [zfs-discuss] Filename conventions
In-Reply-To: <42DBD20A-F51D-47AC-AB31-1B6FFC933FDE@scilytics.com>
References: <42DBD20A-F51D-47AC-AB31-1B6FFC933FDE@scilytics.com>
Message-ID: <F7A6F611-7E0C-4C70-AFEB-85A934004073@apple.com>

Not quite sure exactly what kind of conventions you're referring to.   
In general, we export UTF-8 kNFD trough the API but store kNFC on disk.
There is a difference in the way names are looked up and dealt with in  
ZFS then HFS.  In ZFS everything is stored via hash in the ZAP which  
has a 256 byte limit.  HFS can store 256 Unicode chars which when  
converted to UTF-8 can exceed 256 bytes.  Hence perhaps you have some  
extremely long filenames that have a lot of accented or foreign chars  
in your data.


Noel

On Jan 17, 2008, at 4:40 PM, Michael Alexander wrote:

>
> What are the file name conventions under MacOSX and in general.  
> While ZFS works very well in my testing so far, some double byte  
> character file names seem to be a problem relative to HFS+, but the  
> pattern is not obvious yet. -m.a.
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From eric at hulteen.com  Wed Jan 23 13:40:21 2008
From: eric at hulteen.com (Eric A Hulteen)
Date: Wed Jan 23 13:40:21 2008
Subject: [zfs-discuss] "no pools available" after crash
Message-ID: <20080123134021.1bnu2nj7vos8w884@webmail.lmi.net>

   I have three drives configured as a raidz2 pool.  Everything was  
fine until I tried to use the Finder's duplicate folder menu item to  
increase the data stored in the pool to something just short of the  
total space available.  It's possible, given the space that parity  
information takes, that I asked to put more data into the pool than it  
could hold.  The Finder's copy dialog counted up the number of files  
to copy and then said "Estimating time remaining" (or something to  
that effect).  That message stayed in the copy dialog for 3 hours  
(while no copying took place).  Mac OS X appeared to be hung (no  
application would respond and command-option-escape did nothing).  I  
physically removed the three drives and then hard-rebooted the  
Macintosh (by holding down the power button for 8 seconds).

   Now "zpool status" reports "no pools available".  The three drives  
(that contained the pool) do show up in "diskutil list" (disk1, disk2,  
& disk3).

Mac OS X 10.5.1
PowerBook G4
PowerPC 1.33 GHz
768 MB RAM
build zfs-102A
ZFS pool version 8

   Don't do that?

   Now start over?

Regards,
Eric



From eric at hulteen.com  Wed Jan 23 14:06:02 2008
From: eric at hulteen.com (Eric A Hulteen)
Date: Wed Jan 23 14:06:01 2008
Subject: [zfs-discuss] "no pools available" after crash
In-Reply-To: <05AE870F-1C8C-4737-A944-EEDF91F4D6FD@ncsa.uiuc.edu>
References: <20080123134021.1bnu2nj7vos8w884@webmail.lmi.net>
	<05AE870F-1C8C-4737-A944-EEDF91F4D6FD@ncsa.uiuc.edu>
Message-ID: <20080123140602.5vldcaumy88s08s0@webmail.lmi.net>

"zpool import -a" didn't work, but "zpool import -af" did.
Now it's resilvering.


Quoting Matt Elliott <melliott@ncsa.uiuc.edu>:

>
> On Jan 23, 2008, at 3:40 PM, Eric A Hulteen wrote:
>
>> I have three drives configured as a raidz2 pool.  Everything was    
>> fine until I tried to use the Finder's duplicate folder menu item   
>> to  increase the data stored in the pool to something just short of  
>>  the  total space available.  It's possible, given the space that   
>> parity  information takes, that I asked to put more data into the   
>> pool than  it could hold.  The Finder's copy dialog counted up the   
>> number of  files to copy and then said "Estimating time remaining"   
>> (or  something to that effect).  That message stayed in the copy   
>> dialog  for 3 hours (while no copying took place).  Mac OS X   
>> appeared to be  hung (no application would respond and   
>> command-option-escape did  nothing).  I physically removed the   
>> three drives and then hard- rebooted the Macintosh (by holding down  
>>  the power button for 8  seconds).
>>
>> Now "zpool status" reports "no pools available".  The three drives   
>>  (that contained the pool) do show up in "diskutil list" (disk1,    
>> disk2, & disk3).
>
> try a "zpool import -a"




From bwaters at nrao.edu  Wed Jan 23 16:20:40 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Wed Jan 23 16:21:18 2008
Subject: [zfs-discuss] don't rename a clone or Bad Things Happen
Message-ID: <8D2AA27A-1783-40A5-9F09-7E36A3188D8F@nrao.edu>



zfs-102A read/write

zfs clone tank/myDisk@mySnapshot tank/myDisk__mySnapshot

(good)

/usr/local/bin/rsync-3.0 -avEx /Volumes/192.168.168.23/exported-nfs- 
disk/  /Volumes/tank/myDisk__mySnapshot

(ok)

zfs rename tank/myDisk__mySnapshot tank/myDisk__newStuff

POOF! -- all CPUs go to 100% utilization and just stay there. Mouse  
stops moving. Cats and dogs move in together. Machine locked up tight,  
no apparent disk activity. I go help someone with another computer,  
come back 15 minutes later, turn off the power.




Wed Jan 23 17:09:09 2008
panic(cpu 2 caller 0x00EAFC73): "[ZFS]: assertion failed in /Volumes/ 
pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/zfs_kext/zfs/ 
dnode_sync.c line 397: pass < 100"@/Volumes/pixie_dust/home/ndellofano/ 
zfs-work/wiki/zfs-102A/zfs_kext/zfs/dnode_sync.c:397
Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x8111fb28 : 0x12b0e1 (0x456e30 0x8111fb5c 0x13321a 0x0)
0x8111fb78 : 0xeafc73 (0xf1b400 0xf1abf4 0x18d 0xf1b3f4)
0x8111fc98 : 0xea872d (0x264f54b0 0x1 0x111fcf8 0x0)
0x8111fcd8 : 0xe85f0a (0x29dccda0 0x1 0x1 0x21a4ed86)
0x8111fd28 : 0x1f0448 (0xc95e000 0x0 0x15b6ece4 0x1d2c69)
0x8111fd58 : 0x1decb4 (0xc95e000 0x0 0x15b6ece4 0x1f4d49)
0x8111fda8 : 0x1def88 (0xc95e000 0x0 0x1 0x15b6ece4)
0x8111fdd8 : 0x1df12d (0xc95e000 0x0 0x15b6ece4 0x0)
0x8111ff78 : 0x3dbcab (0xbc03a00 0x15b6ebe0 0x15b6ec24 0x0)
0x8111ffc8 : 0x19f084 (0xbce2d40 0x0 0x1a20b5 0xbce2d40)
0xbfffdd58 : 0x9d2e8181 (0xe5b5d417 0xb7c37690 0xb610702b 0xca42c4d2)
Backtrace terminated-invalid frame pointer 0x15e5b8e6
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0xe84000->0xf4ffff

BSD process name corresponding to current thread: zfs

Mac OS version:
9C16

Kernel version:
Darwin Kernel Version 9.2.0: Tue Jan  8 10:59:40 PST 2008;  
root:xnu-1228.3.7~1/RELEASE_I386
System model name: MacPro1,1 (Mac-F4208DC8)

From ndellofano at apple.com  Wed Jan 23 17:14:55 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed Jan 23 17:16:21 2008
Subject: [zfs-discuss] don't rename a clone or Bad Things Happen
In-Reply-To: <8D2AA27A-1783-40A5-9F09-7E36A3188D8F@nrao.edu>
References: <8D2AA27A-1783-40A5-9F09-7E36A3188D8F@nrao.edu>
Message-ID: <BF666D94-B8BD-4AF0-91CD-03D45F4707D2@apple.com>

Oh yikes.  This is very not good.  Opening up a bug now going on the  
top of my queue.  Apologies to all cats, dogs and blown up clones in  
question.....

p.s.  Thanks for the continued bad ass testing...

Noel :)

On Jan 23, 2008, at 4:20 PM, Boyd Waters wrote:

>
>
> zfs-102A read/write
>
> zfs clone tank/myDisk@mySnapshot tank/myDisk__mySnapshot
>
> (good)
>
> /usr/local/bin/rsync-3.0 -avEx /Volumes/192.168.168.23/exported-nfs- 
> disk/  /Volumes/tank/myDisk__mySnapshot
>
> (ok)
>
> zfs rename tank/myDisk__mySnapshot tank/myDisk__newStuff
>
> POOF! -- all CPUs go to 100% utilization and just stay there. Mouse  
> stops moving. Cats and dogs move in together. Machine locked up  
> tight, no apparent disk activity. I go help someone with another  
> computer, come back 15 minutes later, turn off the power.
>
>
>
>
> Wed Jan 23 17:09:09 2008
> panic(cpu 2 caller 0x00EAFC73): "[ZFS]: assertion failed in /Volumes/ 
> pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/zfs_kext/zfs/ 
> dnode_sync.c line 397: pass < 100"@/Volumes/pixie_dust/home/ 
> ndellofano/zfs-work/wiki/zfs-102A/zfs_kext/zfs/dnode_sync.c:397
> Backtrace, Format - Frame : Return Address (4 potential args on stack)
> 0x8111fb28 : 0x12b0e1 (0x456e30 0x8111fb5c 0x13321a 0x0)
> 0x8111fb78 : 0xeafc73 (0xf1b400 0xf1abf4 0x18d 0xf1b3f4)
> 0x8111fc98 : 0xea872d (0x264f54b0 0x1 0x111fcf8 0x0)
> 0x8111fcd8 : 0xe85f0a (0x29dccda0 0x1 0x1 0x21a4ed86)
> 0x8111fd28 : 0x1f0448 (0xc95e000 0x0 0x15b6ece4 0x1d2c69)
> 0x8111fd58 : 0x1decb4 (0xc95e000 0x0 0x15b6ece4 0x1f4d49)
> 0x8111fda8 : 0x1def88 (0xc95e000 0x0 0x1 0x15b6ece4)
> 0x8111fdd8 : 0x1df12d (0xc95e000 0x0 0x15b6ece4 0x0)
> 0x8111ff78 : 0x3dbcab (0xbc03a00 0x15b6ebe0 0x15b6ec24 0x0)
> 0x8111ffc8 : 0x19f084 (0xbce2d40 0x0 0x1a20b5 0xbce2d40)
> 0xbfffdd58 : 0x9d2e8181 (0xe5b5d417 0xb7c37690 0xb610702b 0xca42c4d2)
> Backtrace terminated-invalid frame pointer 0x15e5b8e6
>      Kernel loadable modules in backtrace (with dependencies):
>         com.apple.filesystems.zfs(8.0)@0xe84000->0xf4ffff
>
> BSD process name corresponding to current thread: zfs
>
> Mac OS version:
> 9C16
>
> Kernel version:
> Darwin Kernel Version 9.2.0: Tue Jan  8 10:59:40 PST 2008;  
> root:xnu-1228.3.7~1/RELEASE_I386
> System model name: MacPro1,1 (Mac-F4208DC8)
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From ndellofano at apple.com  Wed Jan 23 17:52:46 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed Jan 23 17:54:12 2008
Subject: [zfs-discuss] "no pools available" after crash
In-Reply-To: <20080123140602.5vldcaumy88s08s0@webmail.lmi.net>
References: <20080123134021.1bnu2nj7vos8w884@webmail.lmi.net>
	<05AE870F-1C8C-4737-A944-EEDF91F4D6FD@ncsa.uiuc.edu>
	<20080123140602.5vldcaumy88s08s0@webmail.lmi.net>
Message-ID: <570CADEE-11F1-44D2-9A84-974C67C94E24@apple.com>

Yeah so in general, taking out the physical drives is the last thing  
ever to try, especially with ZFS since currently disconnecting *all*  
your live drives will a kernel panic.  The fact that it didn't makes  
me think the hang was actually above me somewhere and not in ZFS, but  
it's hard to say what was up without seeing it.  Also note that you'll  
also always have to use the '-f' argument to zpool import since the  
only time you don't need it is only if the pool was explicitly  
exported and as of now we don't explicitly export pools on reboot,  
hence you'll need the '-f'.

I'm curious about your Finder behavior though.  I've used it to copy  
files fine to raidz's and have had no issues.  If you don't have  
enough space Finder should tell you so.  I haven't ever used the  
Finder's duplicate folder menu but it shouldn't be doing anything much  
more interesting under the covers than copy and move calls.  Have you  
tried any of this via normal command line stuff like 'cp -r'?

Noel

On Jan 23, 2008, at 2:06 PM, Eric A Hulteen wrote:

> "zpool import -a" didn't work, but "zpool import -af" did.
> Now it's resilvering.
>
>
> Quoting Matt Elliott <melliott@ncsa.uiuc.edu>:
>
>>
>> On Jan 23, 2008, at 3:40 PM, Eric A Hulteen wrote:
>>
>>> I have three drives configured as a raidz2 pool.  Everything was    
>>> fine until I tried to use the Finder's duplicate folder menu item   
>>> to  increase the data stored in the pool to something just short  
>>> of  the  total space available.  It's possible, given the space  
>>> that  parity  information takes, that I asked to put more data  
>>> into the  pool than  it could hold.  The Finder's copy dialog  
>>> counted up the  number of  files to copy and then said "Estimating  
>>> time remaining"  (or  something to that effect).  That message  
>>> stayed in the copy  dialog  for 3 hours (while no copying took  
>>> place).  Mac OS X  appeared to be  hung (no application would  
>>> respond and  command-option-escape did  nothing).  I physically  
>>> removed the  three drives and then hard- rebooted the Macintosh  
>>> (by holding down  the power button for 8  seconds).
>>>
>>> Now "zpool status" reports "no pools available".  The three  
>>> drives   (that contained the pool) do show up in "diskutil  
>>> list" (disk1,   disk2, & disk3).
>>
>> try a "zpool import -a"
>
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From q0rban at gmail.com  Wed Jan 23 18:01:37 2008
From: q0rban at gmail.com (james sansbury)
Date: Wed Jan 23 18:01:49 2008
Subject: [zfs-discuss] don't rename a clone or Bad Things Happen
In-Reply-To: <AE61CCFF-7A0D-4CD0-8B77-1ECC2FD5D286@apple.com>
References: <8D2AA27A-1783-40A5-9F09-7E36A3188D8F@nrao.edu>
	<BF666D94-B8BD-4AF0-91CD-03D45F4707D2@apple.com>
	<eb3e47ad0801231725x29bf2d6w748e4aaccddf318c@mail.gmail.com>
	<AE61CCFF-7A0D-4CD0-8B77-1ECC2FD5D286@apple.com>
Message-ID: <eb3e47ad0801231801v4b0a4062v3a2dfec3352412b9@mail.gmail.com>

> Hey, welcome to the list :)
>

Thanks!

 Are you seeing something weird?
>

I am... And I couldn't find it on my quick glance through the archives.

First, some details as to what I'm working with.  I have a raid-z pool that
I created on Solaris 10, then upgraded to OpenSolaris Nevada (upgraded the
pool as well), and then just today took the plunge and exported it to try
and import into Leopard (ZFS 102a).

Everything seemed to go very smoothly, until I need to change some
permissions on some very large folders (300+GB).  A *chown -Rv user
folder*seems to work fine for about 2 minutes and then just hangs.  I
tried to
break out of the process.  Nothing.  I tried to open another terminal and
force unmount the pool.  Nothing.  I tried to eject the drives in Disk
Utility.  Nothing.  So then after waiting another 10 minutes or so I just
unplugged the drives.  Nothing.

I tried to shutdown and it wouldn't shut down so I just powered off, booted
up, tried everything again with all the same results, except that this time
I tried plugging back in the drives and letting it sit.  By doing this I was
at least able to get a kernel panic out of it! ;)

Wed Jan 23 20:20:05 2008
panic(cpu 1 caller 0x478C7DEC): "ZFS: I/O failure (write on <unknown> off 0:
zio 0x8dbc558 [L0 DMU dnode] 4000L/1400P DVA[0]=<0:4504f10400:1c00>
DVA[1]=<0:1402931400:1c00> fletcher4 lzjb LE contiguous birth=2435919
fill=32 cksum=1558cb8da00:3b800ab2fc2bc:63bff7300fb1ce0:bb30349fa21c1523):
error "
"6"@/Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/zfs_kext/zfs/zio.c:918
Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x47f2be48 : 0x12b0e1 (0x455670 0x47f2be7c 0x133238 0x0)
0x47f2be98 : 0x478c7dec (0x47935b90 0x47935b84 0x47931690 0x479646ec)
0x47f2bf18 : 0x478c44c0 (0x8dbc558 0x2b9 0x47f2bf38 0x4790b267)
0x47f2bf58 : 0x479230b0 (0x8dbc558 0x1 0x4796e110 0x5255a88)
0x47f2bfc8 : 0x19e2ec (0x5255a88 0x0 0x1a10b5 0x8bfde40)
Backtrace terminated-invalid frame pointer 0
      Kernel loadable modules in backtrace (with dependencies):
         com.apple.filesystems.zfs(8.0)@0x478b0000->0x4797bfff

BSD process name corresponding to current thread: kernel_task

Mac OS version:
9B18

Kernel version:
Darwin Kernel Version 9.1.0: Wed Oct 31 17:46:22 PDT 2007; root:
xnu-1228.0.2~1/RELEASE_I386
System model name: MacBookPro2,2 (Mac-F42187C8)



>
> Noel
>
> On Jan 23, 2008, at 5:25 PM, james sansbury wrote:
>
> Hi Noel,
>
> I just signed up for the zfs-discuss e-mail list.  Is there a current bug
> list to peruse, or should I just e-mail the bugs I've found to the list?
> I'd hate to report a bug that's already a well known issue.
>
> Thanks for all your work on this project,
>
> James
>
> On Jan 23, 2008 8:14 PM, No?l Dellofano <ndellofano@apple.com> wrote:
>
> > Oh yikes.  This is very not good.  Opening up a bug now going on the
> > top of my queue.  Apologies to all cats, dogs and blown up clones in
> > question.....
> >
> > p.s.  Thanks for the continued bad ass testing...
> >
> > Noel :)
> >
> > On Jan 23, 2008, at 4:20 PM, Boyd Waters wrote:
> >
> > >
> > >
> > > zfs-102A read/write
> > >
> > > zfs clone tank/myDisk@mySnapshot tank/myDisk__mySnapshot
> > >
> > > (good)
> > >
> > > /usr/local/bin/rsync-3.0 -avEx /Volumes/192.168.168.23/exported-nfs-
> > > disk/  /Volumes/tank/myDisk__mySnapshot
> > >
> > > (ok)
> > >
> > > zfs rename tank/myDisk__mySnapshot tank/myDisk__newStuff
> > >
> > > POOF! -- all CPUs go to 100% utilization and just stay there. Mouse
> > > stops moving. Cats and dogs move in together. Machine locked up
> > > tight, no apparent disk activity. I go help someone with another
> > > computer, come back 15 minutes later, turn off the power.
> > >
> > >
> > >
> > >
> > > Wed Jan 23 17:09:09 2008
> > > panic(cpu 2 caller 0x00EAFC73): "[ZFS]: assertion failed in /Volumes/
> > > pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/zfs_kext/zfs/
> > > dnode_sync.c line 397: pass < 100"@/Volumes/pixie_dust/home/
> > > ndellofano/zfs-work/wiki/zfs-102A/zfs_kext/zfs/dnode_sync.c:397
> > > Backtrace, Format - Frame : Return Address (4 potential args on stack)
> >
> > > 0x8111fb28 : 0x12b0e1 (0x456e30 0x8111fb5c 0x13321a 0x0)
> > > 0x8111fb78 : 0xeafc73 (0xf1b400 0xf1abf4 0x18d 0xf1b3f4)
> > > 0x8111fc98 : 0xea872d (0x264f54b0 0x1 0x111fcf8 0x0)
> > > 0x8111fcd8 : 0xe85f0a (0x29dccda0 0x1 0x1 0x21a4ed86)
> > > 0x8111fd28 : 0x1f0448 (0xc95e000 0x0 0x15b6ece4 0x1d2c69)
> > > 0x8111fd58 : 0x1decb4 (0xc95e000 0x0 0x15b6ece4 0x1f4d49)
> > > 0x8111fda8 : 0x1def88 (0xc95e000 0x0 0x1 0x15b6ece4)
> > > 0x8111fdd8 : 0x1df12d (0xc95e000 0x0 0x15b6ece4 0x0)
> > > 0x8111ff78 : 0x3dbcab (0xbc03a00 0x15b6ebe0 0x15b6ec24 0x0)
> > > 0x8111ffc8 : 0x19f084 (0xbce2d40 0x0 0x1a20b5 0xbce2d40)
> > > 0xbfffdd58 : 0x9d2e8181 (0xe5b5d417 0xb7c37690 0xb610702b 0xca42c4d2)
> > > Backtrace terminated-invalid frame pointer 0x15e5b8e6
> > >      Kernel loadable modules in backtrace (with dependencies):
> > >         com.apple.filesystems.zfs(8.0)@0xe84000->0xf4ffff
> > >
> > > BSD process name corresponding to current thread: zfs
> > >
> > > Mac OS version:
> > > 9C16
> > >
> > > Kernel version:
> > > Darwin Kernel Version 9.2.0: Tue Jan  8 10:59:40 PST 2008;
> > > root:xnu-1228.3.7~1/RELEASE_I386
> > > System model name: MacPro1,1 (Mac-F4208DC8)
> > >
> > > _______________________________________________
> > > zfs-discuss mailing list
> > > zfs-discuss@lists.macosforge.org
> > > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> >
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss@lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> >
>
>
>
> --
>
> "A society in which consumption has to be
> artificially stimulated in order to keep production
> going is a society founded on trash and waste,
> and such a society is a house built upon sand."
>
> -- Dorothy Sayers
>
>
>


-- 

"A society in which consumption has to be
artificially stimulated in order to keep production
going is a society founded on trash and waste,
and such a society is a house built upon sand."

-- Dorothy Sayers
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080123/2306cc3e/attachment-0001.html
From gabor at berczi.be  Thu Jan 24 04:42:16 2008
From: gabor at berczi.be (=?ISO-8859-1?Q?B=E9rczi_G=E1bor?=)
Date: Thu Jan 24 04:42:33 2008
Subject: [zfs-discuss] Two questions
In-Reply-To: <A08D256B-92B6-40D8-858C-ED8B5D0E83BA@apple.com>
References: <601576F2-52CD-4341-9C14-6CC14596A42D@berczi.be>
	<A08D256B-92B6-40D8-858C-ED8B5D0E83BA@apple.com>
Message-ID: <008120D5-0A26-449E-A432-A3775CEA4374@berczi.be>

Thanks. Is the info provided in ticket#2 sufficient?

On Jan 22, 2008, at 7:59 PM, No?l Dellofano wrote:

> Currently we haven't done *any* performance work yet, other then  
> general benchmarking along the way to make sure we haven't gone off  
> the deep end or are performing horribly.  So be warned you're  
> probably not going to see the best numbers as no work has been done  
> in this area.
>
> As far as benchmarking, just to keep track we've been using just  
> some general benchmarks like postmark, dbench, and such in addition  
> to timing certain commands (like a ls -lR of a large hierarchy for  
> example). Are you looking for something or debugging something  
> specific? Or just general?
>
> Depending on what you're doing of course there are a variety of  
> tools.   Eventually when Dtrace works on kexts, you can also use  
> that to do perf work.  Two zfs specific tools you can use are:
> - the 'zpool iostat' command to see io rates split up by pool.   
> There are a variety of options you use for it depending on what you  
> want to look at
> - the 'zoink' tool if you're interested in memory management and  
> footprint info for ZFS
>
>
> Noel
>
> p.s. We will have the option of having case insensitive zfs  
> filesystems soon (as a settable property via 'zfs set') in which  
> case you can run your games out of that filesystem which should help  
> the case sensitive grumbles.
>
> On Jan 22, 2008, at 1:07 AM, B?rczi G?bor wrote:
>
>>

-- 
Gabucino

From q0rban at gmail.com  Thu Jan 24 05:58:49 2008
From: q0rban at gmail.com (james sansbury)
Date: Thu Jan 24 05:59:16 2008
Subject: [zfs-discuss] don't rename a clone or Bad Things Happen
In-Reply-To: <eb3e47ad0801231801v4b0a4062v3a2dfec3352412b9@mail.gmail.com>
References: <8D2AA27A-1783-40A5-9F09-7E36A3188D8F@nrao.edu>
	<BF666D94-B8BD-4AF0-91CD-03D45F4707D2@apple.com>
	<eb3e47ad0801231725x29bf2d6w748e4aaccddf318c@mail.gmail.com>
	<AE61CCFF-7A0D-4CD0-8B77-1ECC2FD5D286@apple.com>
	<eb3e47ad0801231801v4b0a4062v3a2dfec3352412b9@mail.gmail.com>
Message-ID: <eb3e47ad0801240558j2f91e9doe28f575230b7f985@mail.gmail.com>

I'm sorry, please disregard this email, I forgot to change the subject.
Didn't mean to hijack this thread!  I will repost with a new subject.

thanks,
James


> Hey, welcome to the list :)
> >
>
> Thanks!
>
>  Are you seeing something weird?
> >
>
> I am... And I couldn't find it on my quick glance through the archives.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080124/d2e43a9c/attachment.html
From q0rban at gmail.com  Thu Jan 24 06:13:02 2008
From: q0rban at gmail.com (james sansbury)
Date: Thu Jan 24 06:13:09 2008
Subject: [zfs-discuss] Timing out
Message-ID: <eb3e47ad0801240613i4161ee03x8957a9e3812748b5@mail.gmail.com>

***This is a repost of a previous message posted in the wrong thread.
Please reply to this post.  Apologies.***

First, some details as to what I'm working with.  I have a raid-z pool that
I created on Solaris 10, then upgraded to OpenSolaris Nevada (upgraded the
pool as well), and then just today took the plunge and exported it to try
and import into Leopard (ZFS 102a).

Everything seemed to go very smoothly, until I needed to change ownership on
some very large folders (300+GB).  A *chown -Rv user folder* seemed to work
fine for about 2 minutes and then it hung.  I tried to break out of the
process.  Nothing.  I tried to open another terminal and force unmount the
pool.  Nothing.  I tried to eject the drives in Disk Utility.  Nothing.  So
then after waiting another 10 minutes or so I just unplugged the drives.
Nothing.

I tried to shutdown and it wouldn't shut down so I just powered off, booted
up, tried everything again with all the same results, except that this time
I tried plugging back in the drives and letting it sit.  By doing this I was
at least able to get a kernel panic out of it! ;) [I now realize from
another post that this kernel panic is most likely due to unplugging the
drives hot, so I won't be reposting the kernel panic info.]

I am also able to replicate all of this in finder with pretty much any
process (copy, move, etc.), provided the process takes long enough.

I may try this again today, leave the drives alone, and just let it sit to
see if I can get some sort of dump/panic; something to work with at least.

-James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080124/e85f5d43/attachment.html
From ndellofano at apple.com  Thu Jan 24 07:29:55 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu Jan 24 07:31:19 2008
Subject: [zfs-discuss] Two questions
In-Reply-To: <008120D5-0A26-449E-A432-A3775CEA4374@berczi.be>
References: <601576F2-52CD-4341-9C14-6CC14596A42D@berczi.be>
	<A08D256B-92B6-40D8-858C-ED8B5D0E83BA@apple.com>
	<008120D5-0A26-449E-A432-A3775CEA4374@berczi.be>
Message-ID: <12544BF1-BF58-45A3-9A57-96E715C74590@apple.com>

Yes looks to be.  I'll take the test program you have and give it a  
shot.

Thanks!
Noel

On Jan 24, 2008, at 4:42 AM, B?rczi G?bor wrote:

> Thanks. Is the info provided in ticket#2 sufficient?
>
> On Jan 22, 2008, at 7:59 PM, No?l Dellofano wrote:
>
>> Currently we haven't done *any* performance work yet, other then  
>> general benchmarking along the way to make sure we haven't gone off  
>> the deep end or are performing horribly.  So be warned you're  
>> probably not going to see the best numbers as no work has been done  
>> in this area.
>>
>> As far as benchmarking, just to keep track we've been using just  
>> some general benchmarks like postmark, dbench, and such in addition  
>> to timing certain commands (like a ls -lR of a large hierarchy for  
>> example). Are you looking for something or debugging something  
>> specific? Or just general?
>>
>> Depending on what you're doing of course there are a variety of  
>> tools.   Eventually when Dtrace works on kexts, you can also use  
>> that to do perf work.  Two zfs specific tools you can use are:
>> - the 'zpool iostat' command to see io rates split up by pool.   
>> There are a variety of options you use for it depending on what you  
>> want to look at
>> - the 'zoink' tool if you're interested in memory management and  
>> footprint info for ZFS
>>
>>
>> Noel
>>
>> p.s. We will have the option of having case insensitive zfs  
>> filesystems soon (as a settable property via 'zfs set') in which  
>> case you can run your games out of that filesystem which should  
>> help the case sensitive grumbles.
>>
>> On Jan 22, 2008, at 1:07 AM, B?rczi G?bor wrote:
>>
>>>
>
> -- 
> Gabucino
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From bwaters at nrao.edu  Thu Jan 24 10:10:07 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Thu Jan 24 10:10:34 2008
Subject: [zfs-discuss] Timing out
In-Reply-To: <eb3e47ad0801240613i4161ee03x8957a9e3812748b5@mail.gmail.com>
References: <eb3e47ad0801240613i4161ee03x8957a9e3812748b5@mail.gmail.com>
Message-ID: <8CF5E2CC-5487-4C5F-A5E7-AD16AE81398D@nrao.edu>


On Jan 24, 2008, at 7:13 AM, james sansbury wrote:

> seemed to work fine for about 2 minutes and then it hung.

James:

After the hang, do you have a mouse cursor? Does it move with your  
mouse? Does the GUI update?

Sorry if it sounds stupid, but I can't tell from your description.

Thanks!

  - boyd

From blustig at apple.com  Thu Jan 24 11:19:41 2008
From: blustig at apple.com (Barry Lustig)
Date: Thu Jan 24 11:21:03 2008
Subject: [zfs-discuss] Timing out
In-Reply-To: <8CF5E2CC-5487-4C5F-A5E7-AD16AE81398D@nrao.edu>
References: <eb3e47ad0801240613i4161ee03x8957a9e3812748b5@mail.gmail.com>
	<8CF5E2CC-5487-4C5F-A5E7-AD16AE81398D@nrao.edu>
Message-ID: <24303CEE-C1BA-48AC-8C8C-B8A5F17B108D@apple.com>


On Jan 24, 2008, at 10:10 AM, Boyd Waters wrote:

>
> On Jan 24, 2008, at 7:13 AM, james sansbury wrote:
>
>> seemed to work fine for about 2 minutes and then it hung.

In hanging cases like this it is always useful to try and get a  
stackshot(1).  You can attempt this via the Vulcan death grip (Shift- 
Ctrl-Cmd-Option-period).  If successful, it will leave the stackshot  
file in /Library/Logs.


barry



>>
>
> James:
>
> After the hang, do you have a mouse cursor? Does it move with your  
> mouse? Does the GUI update?
>
> Sorry if it sounds stupid, but I can't tell from your description.
>
> Thanks!
>
> - boyd
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4148 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080124/0e8f6519/smime-0001.bin
From ndellofano at apple.com  Thu Jan 24 12:38:25 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu Jan 24 12:39:47 2008
Subject: [zfs-discuss] ADC beta seed FYI
In-Reply-To: <E896F8AF-DAB6-413A-8D99-48BD8BADDE62@spamfreemail.de>
References: <E896F8AF-DAB6-413A-8D99-48BD8BADDE62@spamfreemail.de>
Message-ID: <18549537-C980-4AA5-9289-C78E8D3A6284@apple.com>

Hey everyone,

Just a quick announcement.  The seed put up on the ADC website is  
actually the *exact* same bits I've had posted on the zfs website  
(zfs-102A) for the past few weeks.  Hence they are actually not  
new :)  So if you're running zfs-102A you are running the latest bits  
released.  We just released them via ADC since it has a nice installer  
GUI party in case you fear or just hate the terminal window and  
tarballs.

thanks!
Noel

On Jan 24, 2008, at 12:22 PM, Franz Schmalzl wrote:

> seems to be a now beta seed was released...
> _______________________________________________
> Zfs-changes mailing list
> Zfs-changes@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-changes

From q0rban at gmail.com  Thu Jan 24 13:37:07 2008
From: q0rban at gmail.com (james sansbury)
Date: Thu Jan 24 13:37:21 2008
Subject: [zfs-discuss] Timing out
In-Reply-To: <8CF5E2CC-5487-4C5F-A5E7-AD16AE81398D@nrao.edu>
References: <eb3e47ad0801240613i4161ee03x8957a9e3812748b5@mail.gmail.com>
	<8CF5E2CC-5487-4C5F-A5E7-AD16AE81398D@nrao.edu>
Message-ID: <eb3e47ad0801241337s46d43938n41a54191b612dc24@mail.gmail.com>

On Jan 24, 2008 1:10 PM, Boyd wrote:

After the hang, do you have a mouse cursor? Does it move with your
> mouse? Does the GUI update?


Hi, sorry I didn't explain what I meant by "hang" very well.  What I mean is
that the chown process actually freezes, as in no more activity.  Everything
else works fine, unless I try to access the drive.  Also, If I try to quit
finder, it won't relaunch.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080124/c40539e0/attachment.html
From dsawyer at du.edu  Thu Jan 24 13:55:45 2008
From: dsawyer at du.edu (David Sawyer)
Date: Thu Jan 24 13:57:59 2008
Subject: [zfs-discuss] Abort Trap when attempting to create pool
Message-ID: <EF66C513-9598-4EF9-90F4-DB4829C8C398@du.edu>

Hi, All,

I'm attempting to get started with ZFS under Leopard (system a Mac  
Pro) and am running into a snag trying to create a basic one-disk  
pool. Following the instructions in the "Getting Started", I am able  
to successfully set the GPT label, but when I attempt to create the  
pool, it fails with an Abort Trap. This happens on both an internal  
SATA drive and an external Firewire drive. This is probably something  
elementary that I'm missing, but I can't spot what it would be. Any  
suggestions?

Thanks.

Dave Sawyer
From ndellofano at apple.com  Thu Jan 24 14:24:36 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu Jan 24 14:25:57 2008
Subject: [zfs-discuss] FYI: ADC packaging bug
Message-ID: <C9804266-92F4-43F0-A5B8-4490C17ACDC8@apple.com>

Sorry for the spam today.  Just a quick FYI, that there has been a  
packaging bug found in the package posted on the ZFS ADC site, so that  
site will be disabled for now until the bug is fixed.  So please use  
the old school install method with the tarballs I posted on  
zfs.macosforge site.  If you did try and install via the ADC site,  
please reainstall with the bits from zfs.macosforge, as other wise  
your zfs kext will not work correctly.

thanks!
Noel

From q0rban at gmail.com  Thu Jan 24 15:52:58 2008
From: q0rban at gmail.com (james sansbury)
Date: Thu Jan 24 15:52:50 2008
Subject: [zfs-discuss] Timing out
In-Reply-To: <24303CEE-C1BA-48AC-8C8C-B8A5F17B108D@apple.com>
References: <eb3e47ad0801240613i4161ee03x8957a9e3812748b5@mail.gmail.com>
	<8CF5E2CC-5487-4C5F-A5E7-AD16AE81398D@nrao.edu>
	<24303CEE-C1BA-48AC-8C8C-B8A5F17B108D@apple.com>
Message-ID: <eb3e47ad0801241552q3fe973c3n9ff33f7f7dffcc17@mail.gmail.com>

On Jan 24, 2008 2:19 PM, Barry Lustig <blustig@apple.com> wrote:

In hanging cases like this it is always useful to try and get a
> stackshot(1).
>
>
It must be in some sort of deadlock, because I saw no new log show up after
doing the stackshot hot-key combo.  I couldn't figure out how to run it
manually in terminal; is it only a daemon?

I've been sitting here for about an hour now with no progress on a Copy
process in Finder (and yet no errors either).  It's strange.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080124/f7ad26ef/attachment.html
From blustig at apple.com  Thu Jan 24 16:12:04 2008
From: blustig at apple.com (Barry Lustig)
Date: Thu Jan 24 16:13:25 2008
Subject: [zfs-discuss] Timing out
In-Reply-To: <eb3e47ad0801241552q3fe973c3n9ff33f7f7dffcc17@mail.gmail.com>
References: <eb3e47ad0801240613i4161ee03x8957a9e3812748b5@mail.gmail.com>
	<8CF5E2CC-5487-4C5F-A5E7-AD16AE81398D@nrao.edu>
	<24303CEE-C1BA-48AC-8C8C-B8A5F17B108D@apple.com>
	<eb3e47ad0801241552q3fe973c3n9ff33f7f7dffcc17@mail.gmail.com>
Message-ID: <BBF4A8DC-BBD0-4DC6-B3E5-A1CE21F321C3@apple.com>

Skipped content of type multipart/alternative-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4148 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080124/d80195de/smime.bin
From blustig at apple.com  Thu Jan 24 16:14:13 2008
From: blustig at apple.com (Barry Lustig)
Date: Thu Jan 24 16:19:48 2008
Subject: [zfs-discuss] Timing out
In-Reply-To: <eb3e47ad0801241337s46d43938n41a54191b612dc24@mail.gmail.com>
References: <eb3e47ad0801240613i4161ee03x8957a9e3812748b5@mail.gmail.com>
	<8CF5E2CC-5487-4C5F-A5E7-AD16AE81398D@nrao.edu>
	<eb3e47ad0801241337s46d43938n41a54191b612dc24@mail.gmail.com>
Message-ID: <595E513F-F0A5-4B2C-B997-EFAF38594AC6@apple.com>


On Jan 24, 2008, at 1:37 PM, james sansbury wrote:

> On Jan 24, 2008 1:10 PM, Boyd wrote:
>
> After the hang, do you have a mouse cursor? Does it move with your
> mouse? Does the GUI update?
>
> Hi, sorry I didn't explain what I meant by "hang" very well.  What I  
> mean is that the chown process actually freezes, as in no more  
> activity.  Everything else works fine, unless I try to access the  
> drive.  Also, If I try to quit finder, it won't relaunch.

Hmm.  If you happen to recreate the hang, can you run a "ps guax" and  
see if the chmod command is in the "E" state?  This might be related  
to a known bug.


barry

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4148 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080124/1bade488/smime-0001.bin
From q0rban at gmail.com  Thu Jan 24 17:52:33 2008
From: q0rban at gmail.com (james sansbury)
Date: Thu Jan 24 17:52:24 2008
Subject: [zfs-discuss] Timing out
In-Reply-To: <595E513F-F0A5-4B2C-B997-EFAF38594AC6@apple.com>
References: <eb3e47ad0801240613i4161ee03x8957a9e3812748b5@mail.gmail.com>
	<8CF5E2CC-5487-4C5F-A5E7-AD16AE81398D@nrao.edu>
	<eb3e47ad0801241337s46d43938n41a54191b612dc24@mail.gmail.com>
	<595E513F-F0A5-4B2C-B997-EFAF38594AC6@apple.com>
Message-ID: <eb3e47ad0801241752h57c80b42ma0b4ae504094d651@mail.gmail.com>

On Jan 24, 2008 7:14 PM, Barry Lustig <blustig@apple.com> wrote:

> Hmm.  If you happen to recreate the hang, can you run a "ps guax" and
> see if the chmod command is in the "E" state?  This might be related
>
to a known bug.


I'm not sure what any of this means, but after the hang, chown appears to be
in U+ state:

root      1079   0.0  0.0   599620    328 s000  U+    8:38PM   0:00.69 chown
-Rv q0rban q0rban

The only thing that is in the "E" state is "(Locum)":

root       633   0.0  0.0        0      0   ??   E    6:01PM   0:00.00(Locum)

You should be able to run it on the command line via:
>  "/usr/libexec/stackshot -t -i".
>

Well, after running stackshot, a stackshot.log file did open, and was
strangely empty.  But after perusing the console I noticed that about the
same time that I tried to kill Finder after the hang on a Copy, I started
receiving these errors:

24.Jan.08 6:56:30 PM com.apple.launchd[107]
([0x0-0x11011].com.apple.finder[123]) Exited: Terminated
24.Jan.08 6:56:50 PM com.apple.launchd[107] (0x10c4f0.Locum[633]) Exit
timeout elapsed (20 seconds). Killing.
24.Jan.08 6:56:55 PM com.apple.launchd[107] (0x10c4f0.Locum[633]) Did not
die after sending SIGKILL 5 seconds ago...
24.Jan.08 6:57:00 PM com.apple.launchd[107] (0x10c4f0.Locum[633]) Did not
die after sending SIGKILL 10 seconds ago...
24.Jan.08 6:57:05 PM com.apple.launchd[107] (0x10c4f0.Locum[633]) Did not
die after sending SIGKILL 15 seconds ago...
24.Jan.08 6:57:10 PM com.apple.launchd[107] (0x10c4f0.Locum[633]) Did not
die after sending SIGKILL 20 seconds ago...
[and on and on and on]

There's Locum again!

Not sure if that's helpful at all, but it looked suspicious.

-James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080124/65e31edc/attachment.html
From blustig at apple.com  Thu Jan 24 18:03:43 2008
From: blustig at apple.com (Barry Lustig)
Date: Thu Jan 24 18:05:06 2008
Subject: [zfs-discuss] Timing out
In-Reply-To: <eb3e47ad0801241752h57c80b42ma0b4ae504094d651@mail.gmail.com>
References: <eb3e47ad0801240613i4161ee03x8957a9e3812748b5@mail.gmail.com>
	<8CF5E2CC-5487-4C5F-A5E7-AD16AE81398D@nrao.edu>
	<eb3e47ad0801241337s46d43938n41a54191b612dc24@mail.gmail.com>
	<595E513F-F0A5-4B2C-B997-EFAF38594AC6@apple.com>
	<eb3e47ad0801241752h57c80b42ma0b4ae504094d651@mail.gmail.com>
Message-ID: <2D82E863-91ED-4701-8C05-432FF359937C@apple.com>

Skipped content of type multipart/alternative-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4148 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080124/7654947f/smime.bin
From gabor at berczi.be  Fri Jan 25 01:27:21 2008
From: gabor at berczi.be (=?ISO-8859-1?Q?B=E9rczi_G=E1bor?=)
Date: Fri Jan 25 01:27:24 2008
Subject: [zfs-discuss] memory limits
Message-ID: <6FCD7AEC-23E9-445E-895E-83EF4C55494C@berczi.be>

What are the possibilities of limiting ZFS memory usage on OS X?
(for example limiting the ARC size)

ZFS footprint:  346M used,  504M peak,  369M goal               299  
threads
ARC footprint:  232M used,  385M peak,  244M goal

-- 
Gabucino

From ndellofano at apple.com  Fri Jan 25 12:54:11 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri Jan 25 12:55:28 2008
Subject: [zfs-discuss] memory limits
In-Reply-To: <6FCD7AEC-23E9-445E-895E-83EF4C55494C@berczi.be>
References: <6FCD7AEC-23E9-445E-895E-83EF4C55494C@berczi.be>
Message-ID: <C4E37135-98A9-425B-B7CB-0B6014152D68@apple.com>

Depends on what kind of possibilities you are looking for.  Currently  
we set the high water mark for the ARC based on a percentage of the  
physical memory in the system.  If you'd like to tweak this number,  
you'll have to change the source and recompile the code because there  
is no 'mdb -k' on Mac in order to change the zfs_arc_max on a live  
kernel.  If you're interested in doing that you can find the code for  
the arc set up here in zfs_kext/zfs/arc.c  in arc_init.  There are a  
few iterations of things we check in order to gauge what to cap the  
arc at, so I'd recommend taking a look at the function, but you get  
the idea of what's up below:


void
arc_init(void)
{
#ifdef __APPLE__
         /*
          * Use more conservative limits in Mac OS X
          *
          * 2/3 of maximum zfs footprint
          */
         zfs_arc_max = (zfs_footprint.maximum / 3) * 2;
         zfs_arc_min = MAX((physmem * PAGESIZE) / 16, 64<<20);
#endif

....<<<< snip >>>>> .....

         /* set min cache to 1/32 of all memory, or 64MB, whichever is  
more */
         arc_c_min = MAX(arc_c / 4, 64<<20);
         /* set max to 3/4 of all memory, or all but 1GB, whichever is  
more */
         if (arc_c * 8 >= 1<<30)
                 arc_c_max = (arc_c * 8) - (1<<30);
         else
                 arc_c_max = arc_c_min;
         arc_c_max = MAX(arc_c * 6, arc_c_max);

	 /*
          * Allow the tunables to override our calculations if they are
          * reasonable (ie. over 64MB)
          */
         if (zfs_arc_max > 64<<20 && zfs_arc_max < physmem * PAGESIZE)
                 arc_c_max = zfs_arc_max;

Also in arc_init,  you'll see more rules where we have special  
settings for memory if you're physmem is greater then the 32-bit  
address space.  This case we haven't done too much experimenting yet  
and we'll likely tweek as we find what values work best for this.  For  
now, not many people have that much physmem anyway.

  Be warned that in general, the smaller you force the ARC, the worse  
your performance will get.  Obviously this differs based on your  
expected workload, but just a heads up :)


Noel

On Jan 25, 2008, at 1:27 AM, B?rczi G?bor wrote:

> What are the possibilities of limiting ZFS memory usage on OS X?
> (for example limiting the ARC size)
>
> ZFS footprint:  346M used,  504M peak,  369M goal               299  
> threads
> ARC footprint:  232M used,  385M peak,  244M goal
>
> -- 
> Gabucino
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From bwaters at nrao.edu  Fri Jan 25 15:12:13 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Fri Jan 25 15:12:42 2008
Subject: [zfs-discuss] memory limits
In-Reply-To: <C4E37135-98A9-425B-B7CB-0B6014152D68@apple.com>
References: <6FCD7AEC-23E9-445E-895E-83EF4C55494C@berczi.be>
	<C4E37135-98A9-425B-B7CB-0B6014152D68@apple.com>
Message-ID: <9800BDB6-31F7-4F2D-82FF-DBD9282F35E3@nrao.edu>


On Jan 25, 2008, at 1:54 PM, No?l Dellofano wrote:

> you'll have to change the source and recompile the code because  
> there is no 'mdb -k' on Mac in order to change the zfs_arc_max on a  
> live kernel.


That's good, but why not a sysctl parameter? So at least you could  
change it at kernel extension load time (er, boot time)

$ cat /etc/sysctl.conf

kern.sysv.shmmax=10485760
kern.sysv.shmmin=1
kern.sysv.shmmni=32
kern.sysv.shmseg=8
kern.sysv.shmall=4096
fs.zfs.arcmax=1073741824


=====
http://developer.apple.com/documentation/Darwin/Conceptual/KernelProgramming/boundaries/chapter_14_section_7.html


From ndellofano at apple.com  Fri Jan 25 16:18:13 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri Jan 25 16:19:30 2008
Subject: [zfs-discuss] memory limits
In-Reply-To: <9800BDB6-31F7-4F2D-82FF-DBD9282F35E3@nrao.edu>
References: <6FCD7AEC-23E9-445E-895E-83EF4C55494C@berczi.be>
	<C4E37135-98A9-425B-B7CB-0B6014152D68@apple.com>
	<9800BDB6-31F7-4F2D-82FF-DBD9282F35E3@nrao.edu>
Message-ID: <2AD6A5B9-10A6-467D-BE48-4E5A72B2C9B0@apple.com>

Yeah true, that's a good idea, we could add a sysctl like that to make  
life a bit easier if you wanted to tweek your values around before the  
kext gets loaded.  The only pain is the kext will get loaded  
automagically at boot time if it detects any zfs formatted drives.   
But it's easy just to move it and the readonly kext somewhere else  
during boot so it won't load at boot time.  I'll open a bug for it.

Noel :)


On Jan 25, 2008, at 3:12 PM, Boyd Waters wrote:

>
> On Jan 25, 2008, at 1:54 PM, No?l Dellofano wrote:
>
>> you'll have to change the source and recompile the code because  
>> there is no 'mdb -k' on Mac in order to change the zfs_arc_max on a  
>> live kernel.
>
>
> That's good, but why not a sysctl parameter? So at least you could  
> change it at kernel extension load time (er, boot time)
>
> $ cat /etc/sysctl.conf
>
> kern.sysv.shmmax=10485760
> kern.sysv.shmmin=1
> kern.sysv.shmmni=32
> kern.sysv.shmseg=8
> kern.sysv.shmall=4096
> fs.zfs.arcmax=1073741824
>
>
> =====
> http://developer.apple.com/documentation/Darwin/Conceptual/KernelProgramming/boundaries/chapter_14_section_7.html
>
>

From ndellofano at apple.com  Fri Jan 25 17:43:11 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri Jan 25 17:44:27 2008
Subject: [zfs-discuss] Timing out
In-Reply-To: <2D82E863-91ED-4701-8C05-432FF359937C@apple.com>
References: <eb3e47ad0801240613i4161ee03x8957a9e3812748b5@mail.gmail.com>
	<8CF5E2CC-5487-4C5F-A5E7-AD16AE81398D@nrao.edu>
	<eb3e47ad0801241337s46d43938n41a54191b612dc24@mail.gmail.com>
	<595E513F-F0A5-4B2C-B997-EFAF38594AC6@apple.com>
	<eb3e47ad0801241752h57c80b42ma0b4ae504094d651@mail.gmail.com>
	<2D82E863-91ED-4701-8C05-432FF359937C@apple.com>
Message-ID: <51A79088-7B44-4DDF-9717-8AB7F9B4DA63@apple.com>

Locum is Finder's "helper" and incidentally is also the process that's  
causing us heartache on getting the trash to empty.  There's something  
it doesn't seem to like about ZFS.  I'm still investigating it with  
the Finder guys....

this is really interesting to see it's being weird on you too... I'll  
add this to the bug...

Noel

On Jan 24, 2008, at 6:03 PM, Barry Lustig wrote:

>
> On Jan 24, 2008, at 5:52 PM, james sansbury wrote:
>
>> On Jan 24, 2008 7:14 PM, Barry Lustig <blustig@apple.com> wrote:
>> Hmm.  If you happen to recreate the hang, can you run a "ps guax" and
>> see if the chmod command is in the "E" state?  This might be related
>> to a known bug.
>>
>> I'm not sure what any of this means, but after the hang, chown  
>> appears to be in U+ state:
>>
>> root      1079   0.0  0.0   599620    328 s000  U+    8:38PM    
>> 0:00.69 chown -Rv q0rban q0rban
>>
>> The only thing that is in the "E" state is "(Locum)":
>>
>> root       633   0.0  0.0        0      0   ??   E    6:01PM    
>> 0:00.00 (Locum)
>
> I have no idea what it does but:
>
> /System/Library//PrivateFrameworks/DesktopServicesPriv.framework/ 
> Versions/A/Resources/Locum
>
>
> If you do manage to get a stackshot when it is hung up, it would be  
> helpful to file a bug with the info.
>
>
> barry
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080125/de641aae/attachment.html
From zorg at sogeeky.net  Fri Jan 25 18:14:08 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Fri Jan 25 18:14:08 2008
Subject: [zfs-discuss] Timing out
In-Reply-To: <51A79088-7B44-4DDF-9717-8AB7F9B4DA63@apple.com>
References: <eb3e47ad0801240613i4161ee03x8957a9e3812748b5@mail.gmail.com>
	<8CF5E2CC-5487-4C5F-A5E7-AD16AE81398D@nrao.edu>
	<eb3e47ad0801241337s46d43938n41a54191b612dc24@mail.gmail.com>
	<595E513F-F0A5-4B2C-B997-EFAF38594AC6@apple.com>
	<eb3e47ad0801241752h57c80b42ma0b4ae504094d651@mail.gmail.com>
	<2D82E863-91ED-4701-8C05-432FF359937C@apple.com>
	<51A79088-7B44-4DDF-9717-8AB7F9B4DA63@apple.com>
Message-ID: <070DB067-3F43-41F5-9200-CD5251F2C332@sogeeky.net>

FWIW, I had issues like this a while back while doing large mv or cp - 
r operations, also while doing chown on the whole drive. I just broke  
it up directory by directory and got past it. Didn't think enough of  
it to try and reproduce it at the time. Thought my "me too" might be  
of use to you somehow.

On Jan 25, 2008, at 5:43 PM, No?l Dellofano <ndellofano@apple.com>  
wrote:

> Locum is Finder's "helper" and incidentally is also the process  
> that's causing us heartache on getting the trash to empty.  There's  
> something it doesn't seem to like about ZFS.  I'm still  
> investigating it with the Finder guys....
>
> this is really interesting to see it's being weird on you too...  
> I'll add this to the bug...
>
> Noel
>
> On Jan 24, 2008, at 6:03 PM, Barry Lustig wrote:
>
>>
>> On Jan 24, 2008, at 5:52 PM, james sansbury wrote:
>>
>>> On Jan 24, 2008 7:14 PM, Barry Lustig <blustig@apple.com> wrote:
>>> Hmm.  If you happen to recreate the hang, can you run a "ps guax"  
>>> and
>>> see if the chmod command is in the "E" state?  This might be related
>>> to a known bug.
>>>
>>> I'm not sure what any of this means, but after the hang, chown  
>>> appears to be in U+ state:
>>>
>>> root      1079   0.0  0.0   599620    328 s000  U+    8:38PM    
>>> 0:00.69 chown -Rv q0rban q0rban
>>>
>>> The only thing that is in the "E" state is "(Locum)":
>>>
>>> root       633   0.0  0.0        0      0   ??   E    6:01PM    
>>> 0:00.00 (Locum)
>>
>> I have no idea what it does but:
>>
>> /System/Library//PrivateFrameworks/DesktopServicesPriv.framework/ 
>> Versions/A/Resources/Locum
>>
>>
>> If you do manage to get a stackshot when it is hung up, it would be  
>> helpful to file a bug with the info.
>>
>>
>> barry
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080125/d6c27f48/attachment.html
From jim at netgate.com  Fri Jan 25 18:24:45 2008
From: jim at netgate.com (Jim Thompson)
Date: Fri Jan 25 18:24:38 2008
Subject: [zfs-discuss] memory limits
In-Reply-To: <2AD6A5B9-10A6-467D-BE48-4E5A72B2C9B0@apple.com>
References: <6FCD7AEC-23E9-445E-895E-83EF4C55494C@berczi.be>
	<C4E37135-98A9-425B-B7CB-0B6014152D68@apple.com>
	<9800BDB6-31F7-4F2D-82FF-DBD9282F35E3@nrao.edu>
	<2AD6A5B9-10A6-467D-BE48-4E5A72B2C9B0@apple.com>
Message-ID: <C639A4F3-B20A-4C08-809B-C2168A34EABA@netgate.com>


I think freebsd did this...

see, for example: <http://lists.freebsd.org/pipermail/freebsd-fs/2007-November/003956.html 
 >

jim

On Jan 25, 2008, at 2:18 PM, No?l Dellofano wrote:

> Yeah true, that's a good idea, we could add a sysctl like that to  
> make life a bit easier if you wanted to tweek your values around  
> before the kext gets loaded.  The only pain is the kext will get  
> loaded automagically at boot time if it detects any zfs formatted  
> drives.  But it's easy just to move it and the readonly kext  
> somewhere else during boot so it won't load at boot time.  I'll open  
> a bug for it.
>
> Noel :)
>
>
> On Jan 25, 2008, at 3:12 PM, Boyd Waters wrote:
>
>>
>> On Jan 25, 2008, at 1:54 PM, No?l Dellofano wrote:
>>
>>> you'll have to change the source and recompile the code because  
>>> there is no 'mdb -k' on Mac in order to change the zfs_arc_max on  
>>> a live kernel.
>>
>>
>> That's good, but why not a sysctl parameter? So at least you could  
>> change it at kernel extension load time (er, boot time)
>>
>> $ cat /etc/sysctl.conf
>>
>> kern.sysv.shmmax=10485760
>> kern.sysv.shmmin=1
>> kern.sysv.shmmni=32
>> kern.sysv.shmseg=8
>> kern.sysv.shmall=4096
>> fs.zfs.arcmax=1073741824
>>
>>
>> =====
>> http://developer.apple.com/documentation/Darwin/Conceptual/KernelProgramming/boundaries/chapter_14_section_7.html
>>
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From lists at loveturtle.net  Fri Jan 25 18:40:36 2008
From: lists at loveturtle.net (Dillon)
Date: Fri Jan 25 18:40:28 2008
Subject: [zfs-discuss] memory limits
In-Reply-To: <C639A4F3-B20A-4C08-809B-C2168A34EABA@netgate.com>
References: <6FCD7AEC-23E9-445E-895E-83EF4C55494C@berczi.be>	<C4E37135-98A9-425B-B7CB-0B6014152D68@apple.com>	<9800BDB6-31F7-4F2D-82FF-DBD9282F35E3@nrao.edu>	<2AD6A5B9-10A6-467D-BE48-4E5A72B2C9B0@apple.com>
	<C639A4F3-B20A-4C08-809B-C2168A34EABA@netgate.com>
Message-ID: <479A9DA4.1000605@loveturtle.net>

Yes, on FreeBSD you can specify the arc_max size via a sysctl. This has 
to be put in /boot/loader.conf which is read by the loader and set 
before the kernel & modules are loaded. (This is also where you would 
set your vfs.root.mountfrom which allows you to specify a zpool to be 
mounted as /, you only need a small ufs partition for the freebsd kernel 
& loader config files, very acceptable workaround until they get the 
bsdloader to actually be able to boot zfs).

I don't suppose there is any way to set sysctl's at boot on Mac OSX and 
is probably beyond the scope of the ZFS team. The idea may be worth 
passing around at Apple though, maybe people can think of other handy 
uses for such functionality and if they dig it enough it could get 
implemented.

I was actually going to suggest a sysctl earlier but I got distracted 
and then Boyd beat me to it! :-)

Jim Thompson wrote:
>
> I think freebsd did this...
>
> see, for example: 
> <http://lists.freebsd.org/pipermail/freebsd-fs/2007-November/003956.html>
>
> jim
>
> On Jan 25, 2008, at 2:18 PM, No?l Dellofano wrote:
>
>> Yeah true, that's a good idea, we could add a sysctl like that to 
>> make life a bit easier if you wanted to tweek your values around 
>> before the kext gets loaded.  The only pain is the kext will get 
>> loaded automagically at boot time if it detects any zfs formatted 
>> drives.  But it's easy just to move it and the readonly kext 
>> somewhere else during boot so it won't load at boot time.  I'll open 
>> a bug for it.
>>
>> Noel :)
>>
>>
>> On Jan 25, 2008, at 3:12 PM, Boyd Waters wrote:
>>
>>>
>>> On Jan 25, 2008, at 1:54 PM, No?l Dellofano wrote:
>>>
>>>> you'll have to change the source and recompile the code because 
>>>> there is no 'mdb -k' on Mac in order to change the zfs_arc_max on a 
>>>> live kernel.
>>>
>>>
>>> That's good, but why not a sysctl parameter? So at least you could 
>>> change it at kernel extension load time (er, boot time)
>>>
>>> $ cat /etc/sysctl.conf
>>>
>>> kern.sysv.shmmax=10485760
>>> kern.sysv.shmmin=1
>>> kern.sysv.shmmni=32
>>> kern.sysv.shmseg=8
>>> kern.sysv.shmall=4096
>>> fs.zfs.arcmax=1073741824
>>>
>>>
>>> =====
>>> http://developer.apple.com/documentation/Darwin/Conceptual/KernelProgramming/boundaries/chapter_14_section_7.html 
>>>
>>>
>>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From franzschmalzl at spamfreemail.de  Sat Jan 26 06:17:49 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sat Jan 26 06:24:41 2008
Subject: [zfs-discuss] raidz
Message-ID: <78FC4CA0-A61D-4F9D-88F4-16E85EDFFEC2@spamfreemail.de>

i know it's not implemented yet, but will we see compatibility to add  
drives to an existing raidz group?


use case: bob has a raidz with 4 500g drives in his home office, and  
he recognises he needs more storage.
He goes to the next store and buys another 500gb drive and adds it to  
his raidz.
zfs adds it, resilvers and he's got 500gbs more of storage.


best regards
franz
From riscky at gmail.com  Sat Jan 26 11:26:04 2008
From: riscky at gmail.com (Riscky Abacus)
Date: Sat Jan 26 11:25:50 2008
Subject: [zfs-discuss] Newbie -- issues & questions
Message-ID: <51cfc2260801261126p406da825u8708d27ec9f5390b@mail.gmail.com>

Please forgive for asking such basic questions...

First question... on the download page we are instructed to make a
backup of of a few core system files, including
/System/Library/Extensions/zfs.kext This file does not exist on any of
my my three systems (10.5.1 build 9B18). I just have zfs.readonly.kext
I assume I should back up this file and later blow it away before I
copy over build/Release/zfs.kext --- yes I know the installer that was
pulled from ADC probably deals with this... but thats now gone.

Second.... I used the getting start set-up and created a puddle on an
older drive (I also updated to version 8). I found anytime I cared to
write to the drive I needed to supply and administrator password... so
I assume I did something wrong in the setup and tried again. Same
problem, okay no issue as I'm just "playing" around. So I started to
copy my iTunes Library over, and it took forever and developed an
error half way thru, dismissed the error and it kept on trying to copy
the rest of the files. (I think it has to do with me not turning of
spotlight first)

Now I can't do take the drive offline as it is always busy...  `zpool
export` always fails in one way or another. I turned off spotlight and
rebooted some issues.

I finally moved the drive to my laptop where I have installed the zfs
bits and was finally about to format the drive (note I wasn't able to
reformat the drive either on the system with the new zfs bits)... and
I'm going to try all over again... after some, hopeful, wise advise.
From bwaters at nrao.edu  Sat Jan 26 11:32:27 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Sat Jan 26 11:32:26 2008
Subject: [zfs-discuss] raidz
In-Reply-To: <78FC4CA0-A61D-4F9D-88F4-16E85EDFFEC2@spamfreemail.de>
References: <78FC4CA0-A61D-4F9D-88F4-16E85EDFFEC2@spamfreemail.de>
Message-ID: <2DBE5C35-6A5B-49D8-8A6F-A3194FB7D08D@nrao.edu>


On Jan 26, 2008, at 7:17 AM, Franz Schmalzl wrote:

> use case: bob has a raidz with 4 500g drives in his home office, and  
> he recognises he needs more storage.

In this case you can add to the *pool* by creating another 4-way raidz  
and then adding that to the existing pool.

So bob would have to purchase four drives,
  zpool add tank disk{6,7,8,9}s2

  where "tank" is the name of the existing pool.

And you'd actually have two raidz arrays, with striping between them.  
Overhead would be one disk from each raidz.

Note that tne four new disks do NOT have to be the same size as the  
original array's disks (but should be the same size themselves).

I have four 500-gig SATA disks in a really neat Addonics port- 
multiplier enclosure for my raidz. The total cost was $750. I've just  
purchased my fourth terabyte drive, so I'm going to add four 1-TB  
drives to my pool. One of the 500 GB drives and one of the 1-TB drives  
worth of storage will be parity overhead.

I *think* I'll see something like

$ zpool status
   pool: tank
  state: ONLINE
  scrub: none requested
config:

	NAME         STATE     READ WRITE CKSUM
	tank         ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk2s2  ONLINE       0     0     0
	    disk3s2  ONLINE       0     0     0
	    disk4s2  ONLINE       0     0     0
	    disk5s2  ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk6s2  ONLINE       0     0     0
	    disk7s2  ONLINE       0     0     0
	    disk8s2  ONLINE       0     0     0
	    disk9s2  ONLINE       0     0     0


This isn't the same thing as what you're asking for, of course. But  
there is a way to expand your existing pool, and the filesystems in  
your pool should be able to use the new storage.  As a bonus, I/O  
performance may go up somewhat.


From bwaters at nrao.edu  Sat Jan 26 11:35:20 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Sat Jan 26 11:35:27 2008
Subject: [zfs-discuss] Newbie -- issues & questions
In-Reply-To: <51cfc2260801261126p406da825u8708d27ec9f5390b@mail.gmail.com>
References: <51cfc2260801261126p406da825u8708d27ec9f5390b@mail.gmail.com>
Message-ID: <7E671437-245B-4318-B088-188F8E8B7EA8@nrao.edu>


On Jan 26, 2008, at 12:26 PM, Riscky Abacus wrote:

> on the download page we are instructed to make a
> backup of of a few core system files, including
> /System/Library/Extensions/zfs.kext This file does not exist on any of
> my my three systems (10.5.1 build 9B18). I just have zfs.readonly.kext
> I assume I should back up this file and later blow it away before I
> copy over build/Release/zfs.kext


Hmm, I didn't delete zfs.readonly.kext, I have both.

Is that bad?


From david299792 at googlemail.com  Sat Jan 26 11:43:29 2008
From: david299792 at googlemail.com (David Ritchie)
Date: Sat Jan 26 11:43:45 2008
Subject: [zfs-discuss] Newbie -- issues & questions
In-Reply-To: <7E671437-245B-4318-B088-188F8E8B7EA8@nrao.edu>
References: <51cfc2260801261126p406da825u8708d27ec9f5390b@mail.gmail.com>
	<7E671437-245B-4318-B088-188F8E8B7EA8@nrao.edu>
Message-ID: <E96C0EE5-E5D6-47DE-B949-B81815BB5D52@googlemail.com>



On 26 Jan 2008, at 19:35, Boyd Waters <bwaters@nrao.edu> wrote:

>
> On Jan 26, 2008, at 12:26 PM, Riscky Abacus wrote:
>
>> on the download page we are instructed to make a
>> backup of of a few core system files, including
>> /System/Library/Extensions/zfs.kext This file does not exist on any  
>> of
>> my my three systems (10.5.1 build 9B18). I just have  
>> zfs.readonly.kext
>> I assume I should back up this file and later blow it away before I
>> copy over build/Release/zfs.kext
>
>
> Hmm, I didn't delete zfs.readonly.kext, I have both.
>
> Is that bad?
>
I think not - I have both too (at least on one machine) and all seems  
ok. I think its a bit of a mistake in the docs.
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
From kane at inius.com  Sat Jan 26 11:51:18 2008
From: kane at inius.com (Kane Dijkman)
Date: Sat Jan 26 11:51:05 2008
Subject: [zfs-discuss] raidz
In-Reply-To: <2DBE5C35-6A5B-49D8-8A6F-A3194FB7D08D@nrao.edu>
References: <78FC4CA0-A61D-4F9D-88F4-16E85EDFFEC2@spamfreemail.de>
	<2DBE5C35-6A5B-49D8-8A6F-A3194FB7D08D@nrao.edu>
Message-ID: <D7939875-E734-41BC-9518-93E677023A06@inius.com>

Hmmm... any idea when the ability to add drives to an existing raidz  
group will be implemented? While the solution below certainly works it  
isnt nearly as convenient as just being able to add a single drive and  
grow as needed.



On Jan 26, 2008, at 11:32 AM, Boyd Waters wrote:

>
> On Jan 26, 2008, at 7:17 AM, Franz Schmalzl wrote:
>
>> use case: bob has a raidz with 4 500g drives in his home office,  
>> and he recognises he needs more storage.
>
> In this case you can add to the *pool* by creating another 4-way  
> raidz and then adding that to the existing pool.
>
> So bob would have to purchase four drives,
> zpool add tank disk{6,7,8,9}s2
>
> where "tank" is the name of the existing pool.
>
> And you'd actually have two raidz arrays, with striping between  
> them. Overhead would be one disk from each raidz.
>
> Note that tne four new disks do NOT have to be the same size as the  
> original array's disks (but should be the same size themselves).
>
> I have four 500-gig SATA disks in a really neat Addonics port- 
> multiplier enclosure for my raidz. The total cost was $750. I've  
> just purchased my fourth terabyte drive, so I'm going to add four 1- 
> TB drives to my pool. One of the 500 GB drives and one of the 1-TB  
> drives worth of storage will be parity overhead.
>
> I *think* I'll see something like
>
> $ zpool status
>  pool: tank
> state: ONLINE
> scrub: none requested
> config:
>
> 	NAME         STATE     READ WRITE CKSUM
> 	tank         ONLINE       0     0     0
> 	  raidz1     ONLINE       0     0     0
> 	    disk2s2  ONLINE       0     0     0
> 	    disk3s2  ONLINE       0     0     0
> 	    disk4s2  ONLINE       0     0     0
> 	    disk5s2  ONLINE       0     0     0
> 	  raidz1     ONLINE       0     0     0
> 	    disk6s2  ONLINE       0     0     0
> 	    disk7s2  ONLINE       0     0     0
> 	    disk8s2  ONLINE       0     0     0
> 	    disk9s2  ONLINE       0     0     0
>
>
> This isn't the same thing as what you're asking for, of course. But  
> there is a way to expand your existing pool, and the filesystems in  
> your pool should be able to use the new storage.  As a bonus, I/O  
> performance may go up somewhat.
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss


---------------------------------------------------------------------------------------------
A computer without a Microsoft operating system is like a dog without  
bricks tied to its head.

From danchr at daimi.au.dk  Sat Jan 26 14:13:58 2008
From: danchr at daimi.au.dk (Dan Villiom Podlaski Christiansen)
Date: Sat Jan 26 14:13:54 2008
Subject: [zfs-discuss] memory limits
In-Reply-To: <479A9DA4.1000605@loveturtle.net>
References: <6FCD7AEC-23E9-445E-895E-83EF4C55494C@berczi.be>	<C4E37135-98A9-425B-B7CB-0B6014152D68@apple.com>	<9800BDB6-31F7-4F2D-82FF-DBD9282F35E3@nrao.edu>	<2AD6A5B9-10A6-467D-BE48-4E5A72B2C9B0@apple.com>
	<C639A4F3-B20A-4C08-809B-C2168A34EABA@netgate.com>
	<479A9DA4.1000605@loveturtle.net>
Message-ID: <D6866A6A-FC94-4A75-8CA7-FAB17C80E9D8@daimi.au.dk>

On 26 Jan 2008, at 03:40, Dillon wrote:
> I don't suppose there is any way to set sysctl's at boot on Mac OSX  
> and is probably beyond the scope of the ZFS team. The idea may be  
> worth passing around at Apple though, maybe people can think of  
> other handy uses for such functionality and if they dig it enough it  
> could get implemented.

/etc/sysctl.conf works just fine in Leopard :-)

--

Dan Villiom Podlaski Christiansen
stud.scient., danchr@daimi.au.dk


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 1945 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080126/74563ac8/smime-0001.bin
From cemura at gmail.com  Sat Jan 26 14:26:02 2008
From: cemura at gmail.com (Chris Emura)
Date: Sat Jan 26 14:25:47 2008
Subject: [zfs-discuss] Newbie -- issues & questions
In-Reply-To: <7E671437-245B-4318-B088-188F8E8B7EA8@nrao.edu>
References: <51cfc2260801261126p406da825u8708d27ec9f5390b@mail.gmail.com>
	<7E671437-245B-4318-B088-188F8E8B7EA8@nrao.edu>
Message-ID: <6d4497ea0801261426x323034a3n5fa5c7331fc005cb@mail.gmail.com>

On Jan 26, 2008 11:35 AM, Boyd Waters <bwaters@nrao.edu> wrote:

>
> On Jan 26, 2008, at 12:26 PM, Riscky Abacus wrote:
>
> > on the download page we are instructed to make a
> > backup of of a few core system files, including
> > /System/Library/Extensions/zfs.kext This file does not exist on any of
> > my my three systems (10.5.1 build 9B18). I just have zfs.readonly.kext
> > I assume I should back up this file and later blow it away before I
> > copy over build/Release/zfs.kext
>

If you don't have a previous zfs.kext installed, don't worry about it.
There's also no need to backup the readonly kext if it isn't going to be
replaced.



>
>
>
> Hmm, I didn't delete zfs.readonly.kext, I have both.
>
> Is that bad?


Having both installed is not a problem.  We have logic to deal with which
kext to load.




Chris
( responding from my gmail.com account since I don't always have convenient
VPN access )



>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>



-- 
( Sorry if I sound more abrupt than usual; this email was sent from a mobile
device. )
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080126/8e94faa7/attachment.html
From ahoesch at smartsoft.de  Sat Jan 26 09:24:14 2008
From: ahoesch at smartsoft.de (=?ISO-8859-1?Q?Andreas_H=F6schler?=)
Date: Sat Jan 26 14:26:50 2008
Subject: [zfs-discuss] Development on ZFS
Message-ID: <8440A3C2-CC33-11DC-B2AA-000393CA0072@smartsoft.de>

Hi all,

we have successfully used Apples dev tools (actually  
ProjectBuilderWO.app but the same problem probably would occur with  
xCode) on an NFS mounted home dir from a Solaris box.

We recently moved our home dirs to ZFS volume on this Solaris host and  
NFS exported /home with ZFS (it can do that directly). Whenever we try  
to build a project now we get

== Making ApplicationBuilder for ppc ==
Pre-build setup...
/bin/rm -f  
/home/ahoesch/Development/MacOSX/SmartObjectsPro/ApplicationBuilder/ 
ApplicationBuilder.build/derived_src/TrustedPrecomps.txt
Building...
/usr/lib/mergeInfo PB.project CustomInfo.plist  
/home/ahoesch/Development/MacOSX/SmartObjectsPro/ApplicationBuilder/ 
ApplicationBuilder.build/derived_src/Java.plist -o  
/home/ahoesch/Development/MacOSX/SmartObjectsPro/ApplicationBuilder/ 
ApplicationBuilder.app/Resources/Info-macos.plist
Copying English resources...
Copying German resources...
gnumake: *** [copy-global-resources] Segmentation fault

If I copy the complete project directory to a a folder on the local  
disk (of the Mac) then the build works fine. What could be the problem?  
Any idea?

Thanks a lot!

Regards,

   Andreas

From franzschmalzl at spamfreemail.de  Sun Jan 27 03:28:58 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sun Jan 27 03:29:18 2008
Subject: [zfs-discuss] raidz
In-Reply-To: <2DBE5C35-6A5B-49D8-8A6F-A3194FB7D08D@nrao.edu>
References: <78FC4CA0-A61D-4F9D-88F4-16E85EDFFEC2@spamfreemail.de>
	<2DBE5C35-6A5B-49D8-8A6F-A3194FB7D08D@nrao.edu>
Message-ID: <34F1533F-D65A-4FBC-B593-6B40BE31E1BB@spamfreemail.de>

Thanks for the info, i already knew this was possible tough.
And it certainly would be a better solution for a storage company or  
really big offices.

But Bob doesn't have that much money :)

So adding just one drive would be way better for him....

>
> On Jan 26, 2008, at 7:17 AM, Franz Schmalzl wrote:
>
>> use case: bob has a raidz with 4 500g drives in his home office,  
>> and he recognises he needs more storage.
>
> In this case you can add to the *pool* by creating another 4-way  
> raidz and then adding that to the existing pool.
>
> So bob would have to purchase four drives,
> zpool add tank disk{6,7,8,9}s2
>
> where "tank" is the name of the existing pool.
>
> And you'd actually have two raidz arrays, with striping between  
> them. Overhead would be one disk from each raidz.
>
> Note that tne four new disks do NOT have to be the same size as the  
> original array's disks (but should be the same size themselves).
>
> I have four 500-gig SATA disks in a really neat Addonics port- 
> multiplier enclosure for my raidz. The total cost was $750. I've  
> just purchased my fourth terabyte drive, so I'm going to add four 1- 
> TB drives to my pool. One of the 500 GB drives and one of the 1-TB  
> drives worth of storage will be parity overhead.
>
> I *think* I'll see something like
>
> $ zpool status
>  pool: tank
> state: ONLINE
> scrub: none requested
> config:
>
> 	NAME         STATE     READ WRITE CKSUM
> 	tank         ONLINE       0     0     0
> 	  raidz1     ONLINE       0     0     0
> 	    disk2s2  ONLINE       0     0     0
> 	    disk3s2  ONLINE       0     0     0
> 	    disk4s2  ONLINE       0     0     0
> 	    disk5s2  ONLINE       0     0     0
> 	  raidz1     ONLINE       0     0     0
> 	    disk6s2  ONLINE       0     0     0
> 	    disk7s2  ONLINE       0     0     0
> 	    disk8s2  ONLINE       0     0     0
> 	    disk9s2  ONLINE       0     0     0
>
>
> This isn't the same thing as what you're asking for, of course. But  
> there is a way to expand your existing pool, and the filesystems in  
> your pool should be able to use the new storage.  As a bonus, I/O  
> performance may go up somewhat.
>
>

From franzschmalzl at spamfreemail.de  Sun Jan 27 14:07:49 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sun Jan 27 14:07:39 2008
Subject: [zfs-discuss] trash
Message-ID: <EDC20EB2-F525-4CA3-BC34-2AF6629F0B3A@spamfreemail.de>

i've created a raidz for testing purposes...

i just wanted to note that trash emptying via the finder doesn't work.

i can delete files via the finder, and they are added to the .Trashes  
folder on my zfs volume. But when i try to empty it, the finder  
windows just closes and the file is not deleted...

franz
From bhofmann at mac.com  Sun Jan 27 14:10:51 2008
From: bhofmann at mac.com (Bernd Hofmann)
Date: Sun Jan 27 14:10:34 2008
Subject: [zfs-discuss] trash
In-Reply-To: <EDC20EB2-F525-4CA3-BC34-2AF6629F0B3A@spamfreemail.de>
References: <EDC20EB2-F525-4CA3-BC34-2AF6629F0B3A@spamfreemail.de>
Message-ID: <D2627473-EA24-43B1-A11D-4BC71B227D07@mac.com>

Hi!

this is documented under known issues...:

http://trac.macosforge.org/projects/zfs/wiki/issues

bernd

Am 27.01.2008 um 23:07 schrieb Franz Schmalzl:

> i've created a raidz for testing purposes...
>
> i just wanted to note that trash emptying via the finder doesn't work.
>
> i can delete files via the finder, and they are added to  
> the .Trashes folder on my zfs volume. But when i try to empty it,  
> the finder windows just closes and the file is not deleted...
>
> franz
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From franzschmalzl at spamfreemail.de  Sun Jan 27 14:15:41 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sun Jan 27 14:15:28 2008
Subject: [zfs-discuss] trash
In-Reply-To: <D2627473-EA24-43B1-A11D-4BC71B227D07@mac.com>
References: <EDC20EB2-F525-4CA3-BC34-2AF6629F0B3A@spamfreemail.de>
	<D2627473-EA24-43B1-A11D-4BC71B227D07@mac.com>
Message-ID: <5C120A22-498B-482B-86CF-9EDD9A4A9372@spamfreemail.de>


On 27.01.2008, at 23:10, Bernd Hofmann wrote:

> Hi!
>
> this is documented under known issues...:
>
> http://trac.macosforge.org/projects/zfs/wiki/issues

Hi!

I'm sorry.. i've read the page but didn't notice...

franz


>
>
> bernd
>
> Am 27.01.2008 um 23:07 schrieb Franz Schmalzl:
>
>> i've created a raidz for testing purposes...
>>
>> i just wanted to note that trash emptying via the finder doesn't  
>> work.
>>
>> i can delete files via the finder, and they are added to  
>> the .Trashes folder on my zfs volume. But when i try to empty it,  
>> the finder windows just closes and the file is not deleted...
>>
>> franz
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From ndellofano at apple.com  Mon Jan 28 12:24:18 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon Jan 28 12:25:22 2008
Subject: [zfs-discuss] Newbie -- issues & questions
In-Reply-To: <51cfc2260801261126p406da825u8708d27ec9f5390b@mail.gmail.com>
References: <51cfc2260801261126p406da825u8708d27ec9f5390b@mail.gmail.com>
Message-ID: <04C4C3BD-3E5E-4AD8-A9ED-5EE8A97A9C2F@apple.com>

> Second.... I used the getting start set-up and created a puddle on an
> older drive (I also updated to version 8). I found anytime I cared to
> write to the drive I needed to supply and administrator password... so
> I assume I did something wrong in the setup and tried again. Same
> problem, okay no issue as I'm just "playing" around. So I started to
> copy my iTunes Library over, and it took forever and developed an
> error half way thru, dismissed the error and it kept on trying to copy
> the rest of the files. (I think it has to do with me not turning of
> spotlight first)

So right now, zfs requires admin privileges to do all pool  
operations.  Hence when you create the pool via:
# sudo zpool create whirl /dev/disk1s2
By default admin owns it hence is the only one who can write to it.   
You can change this after creation with a simple chown -R operation on  
the pool.  For example for my pool I did:
#sudo chown -R noel:staff /Volumes/whirl
which then allows me to write to the pool as myself with no admin  
privileges.


> Now I can't do take the drive offline as it is always busy...  `zpool
> export` always fails in one way or another. I turned off spotlight and
> rebooted some issues.


This is expected if you had spotlight running on the system.  Because  
of the type of references used, you'll always have to do a force  
unmount/force export of the pool.   A 'zpool export -f whirl' should  
work fine.  Did you try that and it still wouldn't work?

> I finally moved the drive to my laptop where I have installed the zfs
> bits and was finally about to format the drive (note I wasn't able to
> reformat the drive either on the system with the new zfs bits)... and
> I'm going to try all over again... after some, hopeful, wise advise.

Not sure about this, do you have more detail?  How were you trying to  
reformat the drive and what error were you getting? Note that you will  
have to 'zpool destroy' the pool that you had created previously on  
the drive *before* you try and reformat it.  Otherwise you'll get a  
'drive is in use' error.  Just unmounting or zpool exporting a zfs  
pool doesn't actually destroy it.  This functionality does differ from  
traditional unix FS so it's a little different to get used to at  
first....

Let me know if I missed any of your questions or you have any more,

Noel


On Jan 26, 2008, at 11:26 AM, Riscky Abacus wrote:

> Please forgive for asking such basic questions...
>
> First question... on the download page we are instructed to make a
> backup of of a few core system files, including
> /System/Library/Extensions/zfs.kext This file does not exist on any of
> my my three systems (10.5.1 build 9B18). I just have zfs.readonly.kext
> I assume I should back up this file and later blow it away before I
> copy over build/Release/zfs.kext --- yes I know the installer that was
> pulled from ADC probably deals with this... but thats now gone.
>
> Second.... I used the getting start set-up and created a puddle on an
> older drive (I also updated to version 8). I found anytime I cared to
> write to the drive I needed to supply and administrator password... so
> I assume I did something wrong in the setup and tried again. Same
> problem, okay no issue as I'm just "playing" around. So I started to
> copy my iTunes Library over, and it took forever and developed an
> error half way thru, dismissed the error and it kept on trying to copy
> the rest of the files. (I think it has to do with me not turning of
> spotlight first)
>
> Now I can't do take the drive offline as it is always busy...  `zpool
> export` always fails in one way or another. I turned off spotlight and
> rebooted some issues.
>
> I finally moved the drive to my laptop where I have installed the zfs
> bits and was finally about to format the drive (note I wasn't able to
> reformat the drive either on the system with the new zfs bits)... and
> I'm going to try all over again... after some, hopeful, wise advise.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From sbizna at tejat.net  Mon Jan 28 16:24:05 2008
From: sbizna at tejat.net (Solra Bizna)
Date: Mon Jan 28 16:23:45 2008
Subject: [zfs-discuss] ZFS Icon
Message-ID: <b2a3f2d80801281624n3e46426dm9b33eaac81522cf3@mail.gmail.com>

I've been following the ZFS-on-OSX situation for a while now. While I
haven't had the chance to seriously use it on a Mac OS X system, I
administer a ZFS-equipped server, and I love it. I can't wait until
the OSX port is "more stable" so I can deploy it confidently.
The most I've been able to do with ZFS on OSX is create a zpool and
some filesystems and do a few simple tests on them. Aside from the
weirdness in Finder (which is already known) I haven't spotted any
problems, but I never really gave it a stress test.
A while ago, I was on an icon kick and created an icon which IMO suits
ZFS much better than the current blank-disk-image icon. Here it is in
both PNG and ICNS format:
http://geordi.tejat.net/ZFSDisk/
In the end, ZFS on OSX will probably end up using an appropriate
generic icon, but until then I rather like the splash of
Solaris-purple this icon adds to my desktop.
(To use it as the icon for a volume or zpool, copy it to
".VolumeIcon.icns" at the root directory of the volume/zpool and
"/Developer/Tools/SetFile -a C /Volumes/<volumename>"; note that
Finder might take a while to "get" that there's a new icon.)
-:sigma.SB
From danchr at daimi.au.dk  Mon Jan 28 17:08:44 2008
From: danchr at daimi.au.dk (Dan Villiom Podlaski Christiansen)
Date: Mon Jan 28 17:08:28 2008
Subject: [zfs-discuss] ZFS Icon
In-Reply-To: <b2a3f2d80801281624n3e46426dm9b33eaac81522cf3@mail.gmail.com>
References: <b2a3f2d80801281624n3e46426dm9b33eaac81522cf3@mail.gmail.com>
Message-ID: <9B5B103E-9E16-4F41-B905-0AAE23237BEB@daimi.au.dk>

On 29 Jan 2008, at 01:24, Solra Bizna wrote:

> A while ago, I was on an icon kick and created an icon which IMO suits
> ZFS much better than the current blank-disk-image icon. Here it is in
> both PNG and ICNS format:
> http://geordi.tejat.net/ZFSDisk/

You should be aware that your use of the Sun logo is probably in  
breach of their trademark guidelines, specifically this section:
> Third parties may not use any Sun logo for any purpose without a  
> license to do so. This includes all Java logos, the Sun Microsystems  
> corporate logo, and all other product and program logos.


Personally, I'd suggest using a big ?Z? set in either Myriad Pro or  
Apple Garamond. I do like the color, though :-)

<http://www.sun.com/policies/trademarks/#logos>
<http://en.wikipedia.org/wiki/List_of_fonts_in_Mac_OS_X>
--

Dan Villiom Podlaski Christiansen
stud.scient., danchr@daimi.au.dk


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 1945 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080129/77bf5c86/smime.bin
From bwaters at nrao.edu  Mon Jan 28 18:40:12 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Mon Jan 28 18:40:08 2008
Subject: [zfs-discuss] ZFS Icon
In-Reply-To: <9B5B103E-9E16-4F41-B905-0AAE23237BEB@daimi.au.dk>
References: <b2a3f2d80801281624n3e46426dm9b33eaac81522cf3@mail.gmail.com>
	<9B5B103E-9E16-4F41-B905-0AAE23237BEB@daimi.au.dk>
Message-ID: <F8FF3FE3-0324-49C3-938D-2FE4F4B4FEEB@nrao.edu>


On Jan 28, 2008, at 6:08 PM, Dan Villiom Podlaski Christiansen wrote:

> On 29 Jan 2008, at 01:24, Solra Bizna wrote:
>
>> A while ago, I was on an icon kick and created an icon which IMO  
>> suits
>> ZFS much better than the current blank-disk-image icon. Here it is in
>> both PNG and ICNS format:
>> http://geordi.tejat.net/ZFSDisk/
>
> You should be aware that your use of the Sun logo is probably in  
> breach of their trademark guidelines, specifically this section:
>> Third parties may not use any Sun logo for any purpose without a  
>> license to do so. This includes all Java logos, the Sun  
>> Microsystems corporate logo, and all other product and program logos.
>
>
> Personally, I'd suggest using a big ?Z? set in either Myriad Pro or  
> Apple Garamond


Yeah, alas regarding the Sun logo.

But these icons are GREAT!

Thanks, Solra!

  - boyd

From lists at loveturtle.net  Mon Jan 28 19:02:56 2008
From: lists at loveturtle.net (Dillon)
Date: Mon Jan 28 19:02:52 2008
Subject: [zfs-discuss] ZFS Icon
In-Reply-To: <b2a3f2d80801281624n3e46426dm9b33eaac81522cf3@mail.gmail.com>
References: <b2a3f2d80801281624n3e46426dm9b33eaac81522cf3@mail.gmail.com>
Message-ID: <479E9760.8060109@loveturtle.net>

I likey, very nice. I will probably continue using this even after Apple 
picks an offical one. (unless it's really, really nice)
From sbizna at tejat.net  Mon Jan 28 23:37:29 2008
From: sbizna at tejat.net (Solra Bizna)
Date: Mon Jan 28 23:37:02 2008
Subject: [zfs-discuss] ZFS Icon
In-Reply-To: <9B5B103E-9E16-4F41-B905-0AAE23237BEB@daimi.au.dk>
References: <b2a3f2d80801281624n3e46426dm9b33eaac81522cf3@mail.gmail.com>
	<9B5B103E-9E16-4F41-B905-0AAE23237BEB@daimi.au.dk>
Message-ID: <b2a3f2d80801282337q78b264bbya084351400d762f0@mail.gmail.com>

On Jan 28, 2008 6:08 PM, Dan Villiom Podlaski Christiansen
<danchr@daimi.au.dk> wrote:
> You should be aware that your use of the Sun logo is probably in
> breach of their trademark guidelines ...
Aw, drat. Normally I'm so careful about that, too.
> Personally, I'd suggest using a big "Z" set in either Myriad Pro or
> Apple Garamond. ...
I don't have Myriad Pro on-hand, so I implemented your suggestion with
Apple Garamond. I replaced the old version on the server. It still
looks nice, but it looked a lot sexier with the Sun logo. :(
-:sigma.SB
From gabor at berczi.be  Tue Jan 29 00:57:38 2008
From: gabor at berczi.be (=?ISO-8859-1?Q?B=E9rczi_G=E1bor?=)
Date: Tue Jan 29 00:57:40 2008
Subject: [zfs-discuss] memory limits
In-Reply-To: <C4E37135-98A9-425B-B7CB-0B6014152D68@apple.com>
References: <6FCD7AEC-23E9-445E-895E-83EF4C55494C@berczi.be>
	<C4E37135-98A9-425B-B7CB-0B6014152D68@apple.com>
Message-ID: <8AD17058-90FB-47DA-AECE-068C08CE0AF5@berczi.be>


On Jan 25, 2008, at 9:54 PM, No?l Dellofano wrote:

> Be warned that in general, the smaller you force the ARC, the worse  
> your performance will get.  Obviously this differs based on your  
> expected workload, but just a heads up :)

I have swapping switched off (under OSX it was always a great  
performance degradation for me). The thing is, if XNU runs out of  
memory, it will become unresponsible and freeze. ZFS doesn't really  
help the situation :)

Currently:
PhysMem:  481M wired,   80M active,   38M inactive,  599M used,   30M  
free.
ZFS footprint:  314M used,  492M peak,  322M goal               299  
threads
ARC footprint:  224M used,  375M peak,  224M goal

So I have 38+30 = 68Mb memory actually free, yet ZFS doesn't free more  
memory. So maybe ZFS should set its memory limits also based on the  
amount of free (free+inactive) memory. Turning paging back on would  
evade data loss, but it would also end up in needless performance  
degradation.

>
>
>
> Noel
>
> On Jan 25, 2008, at 1:27 AM, B?rczi G?bor wrote:
>
>> What are the possibilities of limiting ZFS memory usage on OS X?
>> (for example limiting the ARC size)
>>
>> ZFS footprint:  346M used,  504M peak,  369M goal               299  
>> threads
>> ARC footprint:  232M used,  385M peak,  244M goal
>>
>> -- 
>> Gabucino
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

-- 
Gabucino

From danchr at daimi.au.dk  Tue Jan 29 01:40:58 2008
From: danchr at daimi.au.dk (Dan Villiom Podlaski Christiansen)
Date: Tue Jan 29 01:40:41 2008
Subject: [zfs-discuss] ZFS Icon
In-Reply-To: <b2a3f2d80801282337q78b264bbya084351400d762f0@mail.gmail.com>
References: <b2a3f2d80801281624n3e46426dm9b33eaac81522cf3@mail.gmail.com>
	<9B5B103E-9E16-4F41-B905-0AAE23237BEB@daimi.au.dk>
	<b2a3f2d80801282337q78b264bbya084351400d762f0@mail.gmail.com>
Message-ID: <6EB332D2-EB94-4990-A42E-8DBD26FCA0D8@daimi.au.dk>

On 29 Jan 2008, at 08:37, Solra Bizna wrote:
> On Jan 28, 2008 6:08 PM, Dan Villiom Podlaski Christiansen
> <danchr@daimi.au.dk> wrote:
>> You should be aware that your use of the Sun logo is probably in
>> breach of their trademark guidelines ...
> Aw, drat. Normally I'm so careful about that, too.

Yeah, I have a feeling Sun would be fairly protective about their logo?

>> Personally, I'd suggest using a big "Z" set in either Myriad Pro or
>> Apple Garamond. ...
> I don't have Myriad Pro on-hand, so I implemented your suggestion with
> Apple Garamond. I replaced the old version on the server. It still
> looks nice, but it looked a lot sexier with the Sun logo. :(

You can find Myriad Pro within the Adobe Reader application bundle,  
not sure if it'd look better, though. Still, the big ?Z? looks quite  
cool, and a bit nostalgic for those who remember the days when Apple  
Garamond was the corporate font of Apple ;-)

Might I suggest that you try to the Z make it a bit more obviously  
flat? Smaller at the top, larger at the bottom. To my untrained eye,  
it doesn't seem quite as pronounced as the FireWire icon.

Screenshot of the Z icon alongside a FireWire icon:
<http://villiom.dk/tmp/zfs-icon.png>
--

Dan Villiom Podlaski Christiansen
stud.scient., danchr@daimi.au.dk


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 1945 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080129/4f2833aa/smime-0001.bin
From sbizna at tejat.net  Tue Jan 29 11:46:06 2008
From: sbizna at tejat.net (Solra Bizna)
Date: Tue Jan 29 11:45:38 2008
Subject: [zfs-discuss] ZFS Icon
In-Reply-To: <6EB332D2-EB94-4990-A42E-8DBD26FCA0D8@daimi.au.dk>
References: <b2a3f2d80801281624n3e46426dm9b33eaac81522cf3@mail.gmail.com>
	<9B5B103E-9E16-4F41-B905-0AAE23237BEB@daimi.au.dk>
	<b2a3f2d80801282337q78b264bbya084351400d762f0@mail.gmail.com>
	<6EB332D2-EB94-4990-A42E-8DBD26FCA0D8@daimi.au.dk>
Message-ID: <b2a3f2d80801291146h4a902979jd83511f51e722eff@mail.gmail.com>

On 1/29/08, Dan Villiom Podlaski Christiansen <danchr@daimi.au.dk> wrote:
> Might I suggest that you try to the Z make it a bit more obviously
> flat? Smaller at the top, larger at the bottom. To my untrained eye,
> it doesn't seem quite as pronounced as the FireWire icon.
You're right, and normally I do that, but I had trouble getting it to
look good like that. I'll give it another go later.
-:sigma.SB
From ndellofano at apple.com  Tue Jan 29 11:57:19 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue Jan 29 11:58:18 2008
Subject: [zfs-discuss] memory limits
In-Reply-To: <8AD17058-90FB-47DA-AECE-068C08CE0AF5@berczi.be>
References: <6FCD7AEC-23E9-445E-895E-83EF4C55494C@berczi.be>
	<C4E37135-98A9-425B-B7CB-0B6014152D68@apple.com>
	<8AD17058-90FB-47DA-AECE-068C08CE0AF5@berczi.be>
Message-ID: <D4D40B07-AD14-4C77-84A5-189E03510516@apple.com>

> So I have 38+30 = 68Mb memory actually free, yet ZFS doesn't free  
> more memory. So maybe ZFS should set its memory limits also based on  
> the amount of free (free+inactive) memory. Turning paging back on  
> would evade data loss, but it would also end up in needless  
> performance degradation.

Yes, so currently this is a problem we are working to fix.  There's no  
hooks currently that ZFS can have into the VM and memory layer to get  
live feedback from the system when the system is feeling memory  
pressure.  We're working on getting some hooks into the lower layers  
so we have some clue of what is going on with the system and how to  
scale our resources accordingly.  This is a big consideration and  
issue.  We're working on a solution :)

Noel

On Jan 29, 2008, at 12:57 AM, B?rczi G?bor wrote:

>
> On Jan 25, 2008, at 9:54 PM, No?l Dellofano wrote:
>
>> Be warned that in general, the smaller you force the ARC, the worse  
>> your performance will get.  Obviously this differs based on your  
>> expected workload, but just a heads up :)
>
> I have swapping switched off (under OSX it was always a great  
> performance degradation for me). The thing is, if XNU runs out of  
> memory, it will become unresponsible and freeze. ZFS doesn't really  
> help the situation :)
>
> Currently:
> PhysMem:  481M wired,   80M active,   38M inactive,  599M used,    
> 30M free.
> ZFS footprint:  314M used,  492M peak,  322M goal               299  
> threads
> ARC footprint:  224M used,  375M peak,  224M goal
>
> So I have 38+30 = 68Mb memory actually free, yet ZFS doesn't free  
> more memory. So maybe ZFS should set its memory limits also based on  
> the amount of free (free+inactive) memory. Turning paging back on  
> would evade data loss, but it would also end up in needless  
> performance degradation.
>
>>
>>
>>
>> Noel
>>
>> On Jan 25, 2008, at 1:27 AM, B?rczi G?bor wrote:
>>
>>> What are the possibilities of limiting ZFS memory usage on OS X?
>>> (for example limiting the ARC size)
>>>
>>> ZFS footprint:  346M used,  504M peak,  369M goal                
>>> 299 threads
>>> ARC footprint:  232M used,  385M peak,  244M goal
>>>
>>> -- 
>>> Gabucino
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>
> -- 
> Gabucino
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From franzschmalzl at spamfreemail.de  Tue Jan 29 12:52:53 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Tue Jan 29 12:52:34 2008
Subject: [zfs-discuss] ztest
Message-ID: <555EE36E-5E40-4809-BADB-D0BBA5A51770@spamfreemail.de>

Hi!

I'm planning to build an external raidz for my music and movies...

before I place all my valuable data on the raid i wanted to to a bit  
of stress testing

I heard about a tool called ztest, which does extensive read/write  
tests, creates filesystems mounts/umounts them, destroys them, creates  
snapshots etc etc...

is there an OS X port?

regards

franz
From ndellofano at apple.com  Tue Jan 29 14:07:52 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue Jan 29 14:08:22 2008
Subject: [zfs-discuss] ztest
In-Reply-To: <555EE36E-5E40-4809-BADB-D0BBA5A51770@spamfreemail.de>
References: <555EE36E-5E40-4809-BADB-D0BBA5A51770@spamfreemail.de>
Message-ID: <A5A6A0E7-2EF8-4EA2-9F50-BCA34E51DBFC@apple.com>

Hey Franz,

Regretfully, we haven't ported the ztest framework yet, though it is  
also on the list of 'todo' items.  I opened a bug(RFE) on it a while  
ago:
<rdar://problem/5248062> ZFS: Port the ztest test suite

So hopefully we will have it ported soon, we just haven't had time yet  
since we were working on overall ZFS stability bugs.  As far as  
beating up on ZFS, you could try going old-school on it and using  
postmark, bonnie, or just copying, writing, and traversing any large  
heirarchy of files....

Noel

On Jan 29, 2008, at 12:52 PM, Franz Schmalzl wrote:

> Hi!
>
> I'm planning to build an external raidz for my music and movies...
>
> before I place all my valuable data on the raid i wanted to to a bit  
> of stress testing
>
> I heard about a tool called ztest, which does extensive read/write  
> tests, creates filesystems mounts/umounts them, destroys them,  
> creates snapshots etc etc...
>
> is there an OS X port?
>
> regards
>
> franz
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From zorg at sogeeky.net  Tue Jan 29 14:59:00 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Tue Jan 29 14:58:40 2008
Subject: [zfs-discuss] ztest
In-Reply-To: <555EE36E-5E40-4809-BADB-D0BBA5A51770@spamfreemail.de>
References: <555EE36E-5E40-4809-BADB-D0BBA5A51770@spamfreemail.de>
Message-ID: <6648528D-3B5A-4DBD-AE52-B0699FFC375E@sogeeky.net>

For what its worth, I've done the same thing (with external FireWire  
drives), except I was stupid enough to go trial by fire. As I played,  
I moved my data to and from its original volumes multiple times. I did  
bulk copies (many hundreds of GB), moves, chown, etc. While I did have  
it lockup on me several times forcing hard reboots (even had to  
reinstall leopard once or twice -- may or may not be related), not  
once did I loose anything.  Its been awesome.

On Jan 29, 2008, at 12:52 PM, Franz Schmalzl <franzschmalzl@spamfreemail.de 
 > wrote:

> Hi!
>
> I'm planning to build an external raidz for my music and movies...
>
> before I place all my valuable data on the raid i wanted to to a bit  
> of stress testing
>
> I heard about a tool called ztest, which does extensive read/write  
> tests, creates filesystems mounts/umounts them, destroys them,  
> creates snapshots etc etc...
>
> is there an OS X port?
>
> regards
>
> franz
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
From ndellofano at apple.com  Tue Jan 29 15:03:08 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue Jan 29 15:05:24 2008
Subject: [zfs-discuss] raidz
In-Reply-To: <34F1533F-D65A-4FBC-B593-6B40BE31E1BB@spamfreemail.de>
References: <78FC4CA0-A61D-4F9D-88F4-16E85EDFFEC2@spamfreemail.de>
	<2DBE5C35-6A5B-49D8-8A6F-A3194FB7D08D@nrao.edu>
	<34F1533F-D65A-4FBC-B593-6B40BE31E1BB@spamfreemail.de>
Message-ID: <500FD828-13AD-4505-943F-F06A72539701@apple.com>

There's no current plans to add this functionality as of yet.  Since  
this is a rather uninteresting problem for Solaris (a Thumper has 48  
drives stock), it's not a problem they are concerned with, and we at  
Apple are working on stability and other more basic features like  
Spotlight bugs and Finder.

We may consider doing this in the future at some point but even so it  
would be quite a while as it's a complicated problem since our raidz  
geometry is complex and as part of adding a drive we'd somehow have to  
'rebalance' the raidz set when you add the new drive.

So for now, I'm afraid Bob is going to have to start saving his  
pennies and have multiple raidz sets in the pool :)

Noel

On Jan 27, 2008, at 3:28 AM, Franz Schmalzl wrote:

> Thanks for the info, i already knew this was possible tough.
> And it certainly would be a better solution for a storage company or  
> really big offices.
>
> But Bob doesn't have that much money :)
>
> So adding just one drive would be way better for him....
>
>>
>> On Jan 26, 2008, at 7:17 AM, Franz Schmalzl wrote:
>>
>>> use case: bob has a raidz with 4 500g drives in his home office,  
>>> and he recognises he needs more storage.
>>
>> In this case you can add to the *pool* by creating another 4-way  
>> raidz and then adding that to the existing pool.
>>
>> So bob would have to purchase four drives,
>> zpool add tank disk{6,7,8,9}s2
>>
>> where "tank" is the name of the existing pool.
>>
>> And you'd actually have two raidz arrays, with striping between  
>> them. Overhead would be one disk from each raidz.
>>
>> Note that tne four new disks do NOT have to be the same size as the  
>> original array's disks (but should be the same size themselves).
>>
>> I have four 500-gig SATA disks in a really neat Addonics port- 
>> multiplier enclosure for my raidz. The total cost was $750. I've  
>> just purchased my fourth terabyte drive, so I'm going to add four 1- 
>> TB drives to my pool. One of the 500 GB drives and one of the 1-TB  
>> drives worth of storage will be parity overhead.
>>
>> I *think* I'll see something like
>>
>> $ zpool status
>> pool: tank
>> state: ONLINE
>> scrub: none requested
>> config:
>>
>> 	NAME         STATE     READ WRITE CKSUM
>> 	tank         ONLINE       0     0     0
>> 	  raidz1     ONLINE       0     0     0
>> 	    disk2s2  ONLINE       0     0     0
>> 	    disk3s2  ONLINE       0     0     0
>> 	    disk4s2  ONLINE       0     0     0
>> 	    disk5s2  ONLINE       0     0     0
>> 	  raidz1     ONLINE       0     0     0
>> 	    disk6s2  ONLINE       0     0     0
>> 	    disk7s2  ONLINE       0     0     0
>> 	    disk8s2  ONLINE       0     0     0
>> 	    disk9s2  ONLINE       0     0     0
>>
>>
>> This isn't the same thing as what you're asking for, of course. But  
>> there is a way to expand your existing pool, and the filesystems in  
>> your pool should be able to use the new storage.  As a bonus, I/O  
>> performance may go up somewhat.
>>
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From zorg at sogeeky.net  Tue Jan 29 17:05:02 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Tue Jan 29 17:05:22 2008
Subject: Fwd: [zfs-discuss] ztest
References: <20290CAE-206D-49EB-A992-13C1E3F33CCE@sogeeky.net>
Message-ID: <2CBBE98E-7EF6-4A7E-BB2E-3C6FC0AAA9C1@sogeeky.net>

I keep forgetting to reply to all. Others on the list might find this  
interesting, so I'm forwarding it.

Begin forwarded message:

> From: "Mr. Zorg" <zorg@sogeeky.net>
> Date: January 29, 2008 5:03:19 PM PST
> To: Eric A Hulteen <eric@hulteen.com>
> Subject: Re: [zfs-discuss] ztest
>

> The only real advice specific to zfs on OS X would be to copy small  
> chunks at a time. There seem to be some issues when copying large  
> folders (or applying chown or chmod on them), and although it hasn't  
> corrupted anything, it has locked up requiring a hard reboot.  A  
> little frustrating.
>
> Before zfs I was using Apple's software RAID, and I did learn  
> something from that. My array is on a mac mini which only has one  
> FireWire port. But the drives supported daisy chaining, so I just  
> did that. Don't do it. I had an enclosure fail, which had the effect  
> of taking everything daisy chained off of it offline too, rendering  
> the raid useless. Get yourself some firewire hubs and plug each  
> drive directly into those. Much less likely the hub will fail.
>
> Also, use good quality enclosures!  Cheap ones will fail under 24/7  
> usage.  I'm particularly fond of the AcomData E5 series. Built like  
> a tank and quite affordable. No fan, but if you space the drives  
> out, their solid aluminium case will stay plenty cool. I stacked  
> mine side by side, and put a couple of those internal hard drive  
> cooler fans ( I forget what they're called) on top using thermal  
> tape. I power them with an adjustable dc power brick I rigged with a  
> molex connector. Running the fans under voltage at 7.5v is nice and  
> quiet, and all 8 of my drives stay cool to the touch despite being  
> flush side by side with each other.
>
> Hope that helps!
>
> On Jan 29, 2008, at 3:06 PM, Eric A Hulteen <eric@hulteen.com> wrote:
>
>> Mr. Zorg,
>>
>> I'm planning to do the same thing -- 5 1TB Firewire drives in a  
>> Raidz2 configuration.  Any advice, particularly related to Firewire?
>>
>> Regards,
>> Eric
>> _________
>>
>> Quoting "Mr. Zorg" <zorg@sogeeky.net>:
>>
>>> For what its worth, I've done the same thing (with external FireWire
>>> drives), except I was stupid enough to go trial by fire. As I  
>>> played, I
>>> moved my data to and from its original volumes multiple times. I did
>>> bulk copies (many hundreds of GB), moves, chown, etc. While I did  
>>> have
>>> it lockup on me several times forcing hard reboots (even had to
>>> reinstall leopard once or twice -- may or may not be related), not  
>>> once
>>> did I loose anything.  Its been awesome.
>>>
>>> On Jan 29, 2008, at 12:52 PM, Franz Schmalzl
>>> <franzschmalzl@spamfreemail.de> wrote:
>>>
>>>> Hi!
>>>>
>>>> I'm planning to build an external raidz for my music and movies...
>>>>
>>>> before I place all my valuable data on the raid i wanted to to a   
>>>> bit of stress testing
>>>>
>>>> I heard about a tool called ztest, which does extensive read/ 
>>>> write  tests, creates filesystems mounts/umounts them, destroys  
>>>> them,  creates snapshots etc etc...
>>>>
>>>> is there an OS X port?
>>>>
>>>> regards
>>>>
>>>> franz
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>>
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080129/454cea1a/attachment-0001.html
From ahoesch at smartsoft.de  Tue Jan 29 15:22:04 2008
From: ahoesch at smartsoft.de (=?ISO-8859-1?Q?Andreas_H=F6schler?=)
Date: Tue Jan 29 17:23:48 2008
Subject: [zfs-discuss] Development on ZFS
In-Reply-To: <60DDE552-166D-4960-A4F6-052348871321@apple.com>
Message-ID: <00B2B7B7-CEC1-11DC-B229-000393CA0072@smartsoft.de>

Hi No?l,

> No idea, I'm not really familiar with ProjectBuilder and some of the 
> other Apple dev tools.  I use vi :)
> I don't see how this is ZFS related though.  Seems your project is 
> likely missing some information it wants; I'd guess something isn't 
> getting copied over correctly via NFS and hence it builds fine locally 
> but is having issues via NFS? Have you tried using the command line 
> Xcodebuild at all?  Maybe it will spit out more information than the 
> gnumake does.
>
> sorry I can't be more help,

Your're welcome! :-) Thanks for your response anyway!

It works fine as long as I have the project dir on standard Solaris ufs 
exported via NFS to the Mac. So it must be zfs related somehow.

No, we haven't tried Xcodebuild yet. For compatibility reasons 
(customer installations) we still ned to run and develop on MacOSX 
10.2. :-)

Regards,

  Andreas

From ndellofano at apple.com  Tue Jan 29 15:12:54 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue Jan 29 17:36:52 2008
Subject: [zfs-discuss] Development on ZFS
In-Reply-To: <8440A3C2-CC33-11DC-B2AA-000393CA0072@smartsoft.de>
References: <8440A3C2-CC33-11DC-B2AA-000393CA0072@smartsoft.de>
Message-ID: <60DDE552-166D-4960-A4F6-052348871321@apple.com>

No idea, I'm not really familiar with ProjectBuilder and some of the  
other Apple dev tools.  I use vi :)
I don't see how this is ZFS related though.  Seems your project is  
likely missing some information it wants; I'd guess something isn't  
getting copied over correctly via NFS and hence it builds fine locally  
but is having issues via NFS? Have you tried using the command line  
Xcodebuild at all?  Maybe it will spit out more information than the  
gnumake does.

sorry I can't be more help,
Noel

On Jan 26, 2008, at 9:24 AM, Andreas H?schler wrote:

> Hi all,
>
> we have successfully used Apples dev tools (actually  
> ProjectBuilderWO.app but the same problem probably would occur with  
> xCode) on an NFS mounted home dir from a Solaris box.
>
> We recently moved our home dirs to ZFS volume on this Solaris host  
> and NFS exported /home with ZFS (it can do that directly). Whenever  
> we try to build a project now we get
>
> == Making ApplicationBuilder for ppc ==
> Pre-build setup...
> /bin/rm -f /home/ahoesch/Development/MacOSX/SmartObjectsPro/ 
> ApplicationBuilder/ApplicationBuilder.build/derived_src/ 
> TrustedPrecomps.txt
> Building...
> /usr/lib/mergeInfo PB.project CustomInfo.plist /home/ahoesch/ 
> Development/MacOSX/SmartObjectsPro/ApplicationBuilder/ 
> ApplicationBuilder.build/derived_src/Java.plist -o /home/ahoesch/ 
> Development/MacOSX/SmartObjectsPro/ApplicationBuilder/ 
> ApplicationBuilder.app/Resources/Info-macos.plist
> Copying English resources...
> Copying German resources...
> gnumake: *** [copy-global-resources] Segmentation fault
>
> If I copy the complete project directory to a a folder on the local  
> disk (of the Mac) then the build works fine. What could be the  
> problem? Any idea?
>
> Thanks a lot!
>
> Regards,
>
>  Andreas
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From eableson at mac.com  Wed Jan 30 03:07:08 2008
From: eableson at mac.com (Erik Ableson)
Date: Wed Jan 30 03:07:14 2008
Subject: [zfs-discuss] raidz
In-Reply-To: <500FD828-13AD-4505-943F-F06A72539701@apple.com>
References: <78FC4CA0-A61D-4F9D-88F4-16E85EDFFEC2@spamfreemail.de>
	<2DBE5C35-6A5B-49D8-8A6F-A3194FB7D08D@nrao.edu>
	<34F1533F-D65A-4FBC-B593-6B40BE31E1BB@spamfreemail.de>
	<500FD828-13AD-4505-943F-F06A72539701@apple.com>
Message-ID: <8ED597AC-011C-43FC-867E-27D24A847455@mac.com>

I'm not sure that I'm clear on this part of the discussion since this  
is a base feature of the Solaris ZFS that makes it attractive.

To make sure that I'm clear on what the situation is :

I have created a raidz pool of 3x500Gb disks.  Net result, I have a  
1Tb volume that can tolerate the failure of a drive.  This is a  
perfect use case for both the SMB and home user market, what with  
exploding space requirements for video and hi-res photos etc.  But the  
day I need more storage, I plan to simple extend the raidz set using :

zpool add [-fn] <pool> <vdev>

with the addition of another drive of the same size. (tested on a  
Solaris x86 box)

Caveat : adding a 750Gb disk to a raidz set composed of 500Gb disks  
will only result in 500Gb of the new drive being used.

But the result should be that my existing pool goes from 3x500 (1Tb)  
to 4x500 (1.5Tb) and I don't have to migrate anything or move anything  
around.  Now if the point is that you're moving to another size disk  
it's more complicated.

Replacing volumes with large disks requires that you export and import  
the volume for the new capacity to be recognized and it automatically  
resilvers the raidz volume to spread out the parity blocks.

3x500 -> 3x750 would be a

zpool replace [-f] <pool> <device> [new_device]

and requires an export/import for the system to recognize the new  
capacity once all of the data has been copied to the new drives  
(tested on Solaris x86 box with firewire disks)

Cheers,

Erik

Notes : my Solaris box is a crappy old Shuttle PC that's 5 years old.  
These tests are uniquely applicable to my tests on this box and I  
haven't even tried this on the current OS X ZFS port.

On 30 janv. 08, at 00:03, No?l Dellofano wrote:

> There's no current plans to add this functionality as of yet.  Since  
> this is a rather uninteresting problem for Solaris (a Thumper has 48  
> drives stock), it's not a problem they are concerned with, and we at  
> Apple are working on stability and other more basic features like  
> Spotlight bugs and Finder.
>
> We may consider doing this in the future at some point but even so  
> it would be quite a while as it's a complicated problem since our  
> raidz geometry is complex and as part of adding a drive we'd somehow  
> have to 'rebalance' the raidz set when you add the new drive.
>
> So for now, I'm afraid Bob is going to have to start saving his  
> pennies and have multiple raidz sets in the pool :)
>
> Noel
From bwaters at nrao.edu  Wed Jan 30 11:49:28 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Wed Jan 30 11:50:04 2008
Subject: [zfs-discuss] raidz
In-Reply-To: <8ED597AC-011C-43FC-867E-27D24A847455@mac.com>
References: <78FC4CA0-A61D-4F9D-88F4-16E85EDFFEC2@spamfreemail.de>
	<2DBE5C35-6A5B-49D8-8A6F-A3194FB7D08D@nrao.edu>
	<34F1533F-D65A-4FBC-B593-6B40BE31E1BB@spamfreemail.de>
	<500FD828-13AD-4505-943F-F06A72539701@apple.com>
	<8ED597AC-011C-43FC-867E-27D24A847455@mac.com>
Message-ID: <D281763C-A193-4998-AD24-B796B94ACBF3@nrao.edu>


On Jan 30, 2008, at 4:07 AM, Erik Ableson wrote:

> the day I need more storage, I plan to simple extend the raidz set  
> using :
>
> zpool add [-fn] <pool> <vdev>
>
> the result should be that my existing pool goes from 3x500 (1Tb) to  
> 4x500 (1.5Tb) and I don't have to migrate anything or move anything  
> around


Yes, this is what people *want* o happen, but it doesn't happen that  
way. Not yet.

As I understand it, you can't add devices to an existing raidz array.  
(On the ZFS that's currently shipping with Solaris, I think you can  
add on-line spares to an existing array, but that won't increase the  
storage capacity of the array.)

You *can* add storage to an existing storage *pool*, as long as the  
new elements of the pool are at least as large as the existing elements.

So in your example of a 3x500GB raidz, I *think* you could add a  
single 1TB drive to the pool, and it would stripe across the single  
drive and the raidz. Which is not what you want; the failure of the  
single 1TB drive would kill the whole pool.  So I'm not certain it  
will let you do that.

I am pretty sure it would let you add another 3x500GB raidz to the  
pool, your I/O performance would go up because you're striping between  
two arrays, and each individual array could survive the failure of one  
of its devices. You would end up with a 2 TB pool.

If you think that your Solaris box can do this, could you post the  
output of "zpool status" before and after adding your additional disk?

Thanks!

  - boyd

From ndellofano at apple.com  Wed Jan 30 12:06:41 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed Jan 30 12:07:41 2008
Subject: [zfs-discuss] raidz
In-Reply-To: <D281763C-A193-4998-AD24-B796B94ACBF3@nrao.edu>
References: <78FC4CA0-A61D-4F9D-88F4-16E85EDFFEC2@spamfreemail.de>
	<2DBE5C35-6A5B-49D8-8A6F-A3194FB7D08D@nrao.edu>
	<34F1533F-D65A-4FBC-B593-6B40BE31E1BB@spamfreemail.de>
	<500FD828-13AD-4505-943F-F06A72539701@apple.com>
	<8ED597AC-011C-43FC-867E-27D24A847455@mac.com>
	<D281763C-A193-4998-AD24-B796B94ACBF3@nrao.edu>
Message-ID: <6C8FAFD9-68D6-4A52-94BE-D5FCDE9D7705@apple.com>

> Yes, this is what people *want* o happen, but it doesn't happen that  
> way. Not yet.
>
> As I understand it, you can't add devices to an existing raidz  
> array. (On the ZFS that's currently shipping with Solaris, I think  
> you can add on-line spares to an existing array, but that won't  
> increase the storage capacity of the array.)
>
> You *can* add storage to an existing storage *pool*, as long as the  
> new elements of the pool are at least as large as the existing  
> elements.

Yes exactly, all three of the above assertions Boyd makes are correct.



> So in your example of a 3x500GB raidz, I *think* you could add a  
> single 1TB drive to the pool, and it would stripe across the single  
> drive and the raidz. Which is not what you want; the failure of the  
> single 1TB drive would kill the whole pool.  So I'm not certain it  
> will let you do that.

So currently, ZFS will let you do this *however* when you issue the  
zpool add command to do this we will fail the request and warn you  
about the mismatched replication.  You then need to use '-f' if you  
wish to actually go ahead with it.  I used file vdevs below do demo  
this, but the same will happen with disks:

sh-3.2# zpool status wombat
   pool: wombat
  state: ONLINE
status: The pool is formatted using an older on-disk format.  The pool  
can
	still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
	pool will no longer be accessible on older software versions.
  scrub: none requested
config:

	NAME                 STATE     READ WRITE CKSUM
	wombat               ONLINE       0     0     0
	  raidz1             ONLINE       0     0     0
	    /var/root/vdev1  ONLINE       0     0     0
	    /var/root/vdev2  ONLINE       0     0     0
	    /var/root/vdev3  ONLINE       0     0     0

errors: No known data errors
sh-3.2# zpool add wombat /var/root/vdev4
invalid vdev specification
use '-f' to override the following errors:
mismatched replication level: pool uses raidz and new vdev is file



> I am pretty sure it would let you add another 3x500GB raidz to the  
> pool, your I/O performance would go up because you're striping  
> between two arrays, and each individual array could survive the  
> failure of one of its devices. You would end up with a 2 TB pool.

You can do the above (add a new raidz set to your pool) and it will  
increase your performance as mentioned.  However I know as you said  
that this isn't exactly what you are looking for.

Note that you can add as many disks as you like to mirrors and regular  
single stripe configs, but adding a single disk to a preexisting raidz  
set is not currently possible.


Noel




On Jan 30, 2008, at 11:49 AM, Boyd Waters wrote:

>
> On Jan 30, 2008, at 4:07 AM, Erik Ableson wrote:
>
>> the day I need more storage, I plan to simple extend the raidz set  
>> using :
>>
>> zpool add [-fn] <pool> <vdev>
>>
>> the result should be that my existing pool goes from 3x500 (1Tb) to  
>> 4x500 (1.5Tb) and I don't have to migrate anything or move anything  
>> around
>
>
> Yes, this is what people *want* o happen, but it doesn't happen that  
> way. Not yet.
>
> As I understand it, you can't add devices to an existing raidz  
> array. (On the ZFS that's currently shipping with Solaris, I think  
> you can add on-line spares to an existing array, but that won't  
> increase the storage capacity of the array.)
>
> You *can* add storage to an existing storage *pool*, as long as the  
> new elements of the pool are at least as large as the existing  
> elements.
>
> So in your example of a 3x500GB raidz, I *think* you could add a  
> single 1TB drive to the pool, and it would stripe across the single  
> drive and the raidz. Which is not what you want; the failure of the  
> single 1TB drive would kill the whole pool.  So I'm not certain it  
> will let you do that.
>
> I am pretty sure it would let you add another 3x500GB raidz to the  
> pool, your I/O performance would go up because you're striping  
> between two arrays, and each individual array could survive the  
> failure of one of its devices. You would end up with a 2 TB pool.
>
> If you think that your Solaris box can do this, could you post the  
> output of "zpool status" before and after adding your additional disk?
>
> Thanks!
>
> - boyd
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From eableson at mac.com  Wed Jan 30 13:09:32 2008
From: eableson at mac.com (Erik Ableson)
Date: Wed Jan 30 13:09:45 2008
Subject: [zfs-discuss] raidz
In-Reply-To: <6C8FAFD9-68D6-4A52-94BE-D5FCDE9D7705@apple.com>
References: <78FC4CA0-A61D-4F9D-88F4-16E85EDFFEC2@spamfreemail.de>
	<2DBE5C35-6A5B-49D8-8A6F-A3194FB7D08D@nrao.edu>
	<34F1533F-D65A-4FBC-B593-6B40BE31E1BB@spamfreemail.de>
	<500FD828-13AD-4505-943F-F06A72539701@apple.com>
	<8ED597AC-011C-43FC-867E-27D24A847455@mac.com>
	<D281763C-A193-4998-AD24-B796B94ACBF3@nrao.edu>
	<6C8FAFD9-68D6-4A52-94BE-D5FCDE9D7705@apple.com>
Message-ID: <AB88AC75-0BD5-4618-8225-0A3370D50CBF@mac.com>

OK - many thanks for the clarifications - I crossed a few wires  
reviewing a side discussion about the disk replacements under raidz  
and adding disks to a mirror.

Going back to my test logs, the issue I was testing at the time was  
replacing existing disks with larger ones, which is a viable upgrade  
path and I hadn't actually tested the addition of a new disk to the  
raidz set.

Cheers,

Erik
On 30 janv. 08, at 21:06, No?l Dellofano wrote:

>> Yes, this is what people *want* o happen, but it doesn't happen  
>> that way. Not yet.
>>
>> As I understand it, you can't add devices to an existing raidz  
>> array. (On the ZFS that's currently shipping with Solaris, I think  
>> you can add on-line spares to an existing array, but that won't  
>> increase the storage capacity of the array.)
>>
>> You *can* add storage to an existing storage *pool*, as long as the  
>> new elements of the pool are at least as large as the existing  
>> elements.
>
> Yes exactly, all three of the above assertions Boyd makes are correct.

From robert at rehnmark.net  Thu Jan 31 13:16:45 2008
From: robert at rehnmark.net (Robert Rehnmark)
Date: Thu Jan 31 13:16:50 2008
Subject: [zfs-discuss] How to activate/set ACL on ZFS?
Message-ID: <95F43E2E-3E2A-49A2-B0FC-235D05FC5553@rehnmark.net>

How do I set up and control ACL's?
In order to make the network-shared pool work with multiple users I  
need ACL that is inherited to all created folders and files.
So, can anybody please tell me how to make it happen?

/Robert
From oliver.otway at gmail.com  Thu Jan 31 22:34:12 2008
From: oliver.otway at gmail.com (Oliver Otway)
Date: Thu Jan 31 22:34:34 2008
Subject: [zfs-discuss] Re: Development on ZFS (Andreas H?schler)
In-Reply-To: <20080131211653.6921619D317@lists.macosforge.org>
References: <20080131211653.6921619D317@lists.macosforge.org>
Message-ID: <1C7C9931-DB88-469B-942A-45396F96FAE3@gmail.com>

> Andreas

When you copied over the files to ZFS, were you using GUI, or command  
line 'cp'
in either of those cases it's possible that an essential resource file  
is not being correctly copied across
Have you tried
dittto -rsrc src ... dst_directory

might help :)

man ditto for all usage

Ollie
>
>
> Hi No?l,
>
>> No idea, I'm not really familiar with ProjectBuilder and some of  
>> the other Apple dev tools.  I use vi :)
>> I don't see how this is ZFS related though.  Seems your project is  
>> likely missing some information it wants; I'd guess something isn't  
>> getting copied over correctly via NFS and hence it builds fine  
>> locally but is having issues via NFS? Have you tried using the  
>> command line Xcodebuild at all?  Maybe it will spit out more  
>> information than the gnumake does.
>>
>> sorry I can't be more help,
>
> Your're welcome! :-) Thanks for your response anyway!
>
> It works fine as long as I have the project dir on standard Solaris  
> ufs exported via NFS to the Mac. So it must be zfs related somehow.
>
> No, we haven't tried Xcodebuild yet. For compatibility reasons  
> (customer installations) we still ned to run and develop on MacOSX  
> 10.2. :-)
>
> Regards,
>
> Andreas
>
>
>
>
>
> From: No?l Dellofano <ndellofano@apple.com>
> Date: 30 January 2008 8:42:54 AM
> To: Andreas H?schler <ahoesch@smartsoft.de>
> Cc: zfs-discuss@lists.macosforge.org
> Subject: Re: [zfs-discuss] Development on ZFS
>
>
> No idea, I'm not really familiar with ProjectBuilder and some of the  
> other Apple dev tools.  I use vi :)
> I don't see how this is ZFS related though.  Seems your project is  
> likely missing some information it wants; I'd guess something isn't  
> getting copied over correctly via NFS and hence it builds fine  
> locally but is having issues via NFS? Have you tried using the  
> command line Xcodebuild at all?  Maybe it will spit out more  
> information than the gnumake does.
>
> sorry I can't be more help,
> Noel
>
> On Jan 26, 2008, at 9:24 AM, Andreas H?schler wrote:
>
>> Hi all,
>>
>> we have successfully used Apples dev tools (actually  
>> ProjectBuilderWO.app but the same problem probably would occur with  
>> xCode) on an NFS mounted home dir from a Solaris box.
>>
>> We recently moved our home dirs to ZFS volume on this Solaris host  
>> and NFS exported /home with ZFS (it can do that directly). Whenever  
>> we try to build a project now we get
>>
>> == Making ApplicationBuilder for ppc ==
>> Pre-build setup...
>> /bin/rm -f /home/ahoesch/Development/MacOSX/SmartObjectsPro/ 
>> ApplicationBuilder/ApplicationBuilder.build/derived_src/ 
>> TrustedPrecomps.txt
>> Building...
>> /usr/lib/mergeInfo PB.project CustomInfo.plist /home/ahoesch/ 
>> Development/MacOSX/SmartObjectsPro/ApplicationBuilder/ 
>> ApplicationBuilder.build/derived_src/Java.plist -o /home/ahoesch/ 
>> Development/MacOSX/SmartObjectsPro/ApplicationBuilder/ 
>> ApplicationBuilder.app/Resources/Info-macos.plist
>> Copying English resources...
>> Copying German resources...
>> gnumake: *** [copy-global-resources] Segmentation fault
>>
>> If I copy the complete project directory to a a folder on the local  
>> disk (of the Mac) then the build works fine. What could be the  
>> problem? Any idea?
>>
>> Thanks a lot!
>>
>> Regards,
>>
>> Andreas
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
>
>
>
>
> From: Erik Ableson <eableson@mac.com>
> Date: 30 January 2008 8:37:08 PM
> To: No?l Dellofano <ndellofano@apple.com>
> Cc: ZFS on OSX mailing list mailing list <zfs-discuss@lists.macosforge.org 
> >
> Subject: Re: [zfs-discuss] raidz
>
>
> I'm not sure that I'm clear on this part of the discussion since  
> this is a base feature of the Solaris ZFS that makes it attractive.
>
> To make sure that I'm clear on what the situation is :
>
> I have created a raidz pool of 3x500Gb disks.  Net result, I have a  
> 1Tb volume that can tolerate the failure of a drive.  This is a  
> perfect use case for both the SMB and home user market, what with  
> exploding space requirements for video and hi-res photos etc.  But  
> the day I need more storage, I plan to simple extend the raidz set  
> using :
>
> zpool add [-fn] <pool> <vdev>
>
> with the addition of another drive of the same size. (tested on a  
> Solaris x86 box)
>
> Caveat : adding a 750Gb disk to a raidz set composed of 500Gb disks  
> will only result in 500Gb of the new drive being used.
>
> But the result should be that my existing pool goes from 3x500 (1Tb)  
> to 4x500 (1.5Tb) and I don't have to migrate anything or move  
> anything around.  Now if the point is that you're moving to another  
> size disk it's more complicated.
>
> Replacing volumes with large disks requires that you export and  
> import the volume for the new capacity to be recognized and it  
> automatically resilvers the raidz volume to spread out the parity  
> blocks.
>
> 3x500 -> 3x750 would be a
>
> zpool replace [-f] <pool> <device> [new_device]
>
> and requires an export/import for the system to recognize the new  
> capacity once all of the data has been copied to the new drives  
> (tested on Solaris x86 box with firewire disks)
>
> Cheers,
>
> Erik
>
> Notes : my Solaris box is a crappy old Shuttle PC that's 5 years  
> old. These tests are uniquely applicable to my tests on this box and  
> I haven't even tried this on the current OS X ZFS port.
>
> On 30 janv. 08, at 00:03, No?l Dellofano wrote:
>
>> There's no current plans to add this functionality as of yet.   
>> Since this is a rather uninteresting problem for Solaris (a Thumper  
>> has 48 drives stock), it's not a problem they are concerned with,  
>> and we at Apple are working on stability and other more basic  
>> features like Spotlight bugs and Finder.
>>
>> We may consider doing this in the future at some point but even so  
>> it would be quite a while as it's a complicated problem since our  
>> raidz geometry is complex and as part of adding a drive we'd  
>> somehow have to 'rebalance' the raidz set when you add the new drive.
>>
>> So for now, I'm afraid Bob is going to have to start saving his  
>> pennies and have multiple raidz sets in the pool :)
>>
>> Noel
>
>
>
>
> From: Boyd Waters <bwaters@nrao.edu>
> Date: 31 January 2008 5:19:28 AM
> To: Erik Ableson <eableson@mac.com>
> Cc: ZFS on OSX mailing list mailing list <zfs-discuss@lists.macosforge.org 
> >
> Subject: Re: [zfs-discuss] raidz
>
>
>
> On Jan 30, 2008, at 4:07 AM, Erik Ableson wrote:
>
>> the day I need more storage, I plan to simple extend the raidz set  
>> using :
>>
>> zpool add [-fn] <pool> <vdev>
>>
>> the result should be that my existing pool goes from 3x500 (1Tb) to  
>> 4x500 (1.5Tb) and I don't have to migrate anything or move anything  
>> around
>
>
> Yes, this is what people *want* o happen, but it doesn't happen that  
> way. Not yet.
>
> As I understand it, you can't add devices to an existing raidz  
> array. (On the ZFS that's currently shipping with Solaris, I think  
> you can add on-line spares to an existing array, but that won't  
> increase the storage capacity of the array.)
>
> You *can* add storage to an existing storage *pool*, as long as the  
> new elements of the pool are at least as large as the existing  
> elements.
>
> So in your example of a 3x500GB raidz, I *think* you could add a  
> single 1TB drive to the pool, and it would stripe across the single  
> drive and the raidz. Which is not what you want; the failure of the  
> single 1TB drive would kill the whole pool.  So I'm not certain it  
> will let you do that.
>
> I am pretty sure it would let you add another 3x500GB raidz to the  
> pool, your I/O performance would go up because you're striping  
> between two arrays, and each individual array could survive the  
> failure of one of its devices. You would end up with a 2 TB pool.
>
> If you think that your Solaris box can do this, could you post the  
> output of "zpool status" before and after adding your additional disk?
>
> Thanks!
>
> - boyd
>
>
>
>
>
> From: No?l Dellofano <ndellofano@apple.com>
> Date: 31 January 2008 5:36:41 AM
> To: Boyd Waters <bwaters@nrao.edu>
> Cc: ZFS on OSX mailing list mailing list <zfs-discuss@lists.macosforge.org 
> >
> Subject: Re: [zfs-discuss] raidz
>
>
>> Yes, this is what people *want* o happen, but it doesn't happen  
>> that way. Not yet.
>>
>> As I understand it, you can't add devices to an existing raidz  
>> array. (On the ZFS that's currently shipping with Solaris, I think  
>> you can add on-line spares to an existing array, but that won't  
>> increase the storage capacity of the array.)
>>
>> You *can* add storage to an existing storage *pool*, as long as the  
>> new elements of the pool are at least as large as the existing  
>> elements.
>
> Yes exactly, all three of the above assertions Boyd makes are correct.
>
>
>
>> So in your example of a 3x500GB raidz, I *think* you could add a  
>> single 1TB drive to the pool, and it would stripe across the single  
>> drive and the raidz. Which is not what you want; the failure of the  
>> single 1TB drive would kill the whole pool.  So I'm not certain it  
>> will let you do that.
>
> So currently, ZFS will let you do this *however* when you issue the  
> zpool add command to do this we will fail the request and warn you  
> about the mismatched replication.  You then need to use '-f' if you  
> wish to actually go ahead with it.  I used file vdevs below do demo  
> this, but the same will happen with disks:
>
> sh-3.2# zpool status wombat
>  pool: wombat
> state: ONLINE
> status: The pool is formatted using an older on-disk format.  The  
> pool can
> 	still be used, but some features are unavailable.
> action: Upgrade the pool using 'zpool upgrade'.  Once this is done,  
> the
> 	pool will no longer be accessible on older software versions.
> scrub: none requested
> config:
>
> 	NAME                 STATE     READ WRITE CKSUM
> 	wombat               ONLINE       0     0     0
> 	  raidz1             ONLINE       0     0     0
> 	    /var/root/vdev1  ONLINE       0     0     0
> 	    /var/root/vdev2  ONLINE       0     0     0
> 	    /var/root/vdev3  ONLINE       0     0     0
>
> errors: No known data errors
> sh-3.2# zpool add wombat /var/root/vdev4
> invalid vdev specification
> use '-f' to override the following errors:
> mismatched replication level: pool uses raidz and new vdev is file
>
>
>
>> I am pretty sure it would let you add another 3x500GB raidz to the  
>> pool, your I/O performance would go up because you're striping  
>> between two arrays, and each individual array could survive the  
>> failure of one of its devices. You would end up with a 2 TB pool.
>
> You can do the above (add a new raidz set to your pool) and it will  
> increase your performance as mentioned.  However I know as you said  
> that this isn't exactly what you are looking for.
>
> Note that you can add as many disks as you like to mirrors and  
> regular single stripe configs, but adding a single disk to a  
> preexisting raidz set is not currently possible.
>
>
> Noel
>
>
>
>
> On Jan 30, 2008, at 11:49 AM, Boyd Waters wrote:
>
>>
>> On Jan 30, 2008, at 4:07 AM, Erik Ableson wrote:
>>
>>> the day I need more storage, I plan to simple extend the raidz set  
>>> using :
>>>
>>> zpool add [-fn] <pool> <vdev>
>>>
>>> the result should be that my existing pool goes from 3x500 (1Tb)  
>>> to 4x500 (1.5Tb) and I don't have to migrate anything or move  
>>> anything around
>>
>>
>> Yes, this is what people *want* o happen, but it doesn't happen  
>> that way. Not yet.
>>
>> As I understand it, you can't add devices to an existing raidz  
>> array. (On the ZFS that's currently shipping with Solaris, I think  
>> you can add on-line spares to an existing array, but that won't  
>> increase the storage capacity of the array.)
>>
>> You *can* add storage to an existing storage *pool*, as long as the  
>> new elements of the pool are at least as large as the existing  
>> elements.
>>
>> So in your example of a 3x500GB raidz, I *think* you could add a  
>> single 1TB drive to the pool, and it would stripe across the single  
>> drive and the raidz. Which is not what you want; the failure of the  
>> single 1TB drive would kill the whole pool.  So I'm not certain it  
>> will let you do that.
>>
>> I am pretty sure it would let you add another 3x500GB raidz to the  
>> pool, your I/O performance would go up because you're striping  
>> between two arrays, and each individual array could survive the  
>> failure of one of its devices. You would end up with a 2 TB pool.
>>
>> If you think that your Solaris box can do this, could you post the  
>> output of "zpool status" before and after adding your additional  
>> disk?
>>
>> Thanks!
>>
>> - boyd
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
>
>
>
>
> From: Erik Ableson <eableson@mac.com>
> Date: 31 January 2008 6:39:32 AM
> To: No?l Dellofano <ndellofano@apple.com>
> Cc: ZFS on OSX mailing list mailing list <zfs-discuss@lists.macosforge.org 
> >
> Subject: Re: [zfs-discuss] raidz
>
>
> OK - many thanks for the clarifications - I crossed a few wires  
> reviewing a side discussion about the disk replacements under raidz  
> and adding disks to a mirror.
>
> Going back to my test logs, the issue I was testing at the time was  
> replacing existing disks with larger ones, which is a viable upgrade  
> path and I hadn't actually tested the addition of a new disk to the  
> raidz set.
>
> Cheers,
>
> Erik
> On 30 janv. 08, at 21:06, No?l Dellofano wrote:
>
>>> Yes, this is what people *want* o happen, but it doesn't happen  
>>> that way. Not yet.
>>>
>>> As I understand it, you can't add devices to an existing raidz  
>>> array. (On the ZFS that's currently shipping with Solaris, I think  
>>> you can add on-line spares to an existing array, but that won't  
>>> increase the storage capacity of the array.)
>>>
>>> You *can* add storage to an existing storage *pool*, as long as  
>>> the new elements of the pool are at least as large as the existing  
>>> elements.
>>
>> Yes exactly, all three of the above assertions Boyd makes are  
>> correct.
>
>
>
>
>
> From: Robert Rehnmark <robert@rehnmark.net>
> Date: 1 February 2008 6:46:45 AM
> To: ZFS on OSX mailing list mailing list <zfs-discuss@lists.macosforge.org 
> >
> Subject: [zfs-discuss] How to activate/set ACL on ZFS?
>
>
> How do I set up and control ACL's?
> In order to make the network-shared pool work with multiple users I  
> need ACL that is inherited to all created folders and files.
> So, can anybody please tell me how to make it happen?
>
> /Robert
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

