From gagix at mac.com  Sat Mar  1 10:46:07 2008
From: gagix at mac.com (Gautam Godse)
Date: Sat, 1 Mar 2008 10:46:07 -0800
Subject: [zfs-discuss] creating a zpool changes disk partition label
Message-ID: <D94E6EEB-599F-483C-9EF2-11D313851A87@mac.com>

hi,
In continuation of my experiments with zfs, I destroyed the existing  
pool and created a new one.
But this time I made sure that I partitioned each disk in the array  
with the GPTFormat option to set the correct partition label.
However, after creating a zpool raidz array, i find that the partition  
label has been changed to FDisk_partition_scheme. Is this deliberate/ 
expected ? Or is this some anomaly in the system ?

This is the current output of 'diskutil list'

/dev/disk1
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:     FDisk_partition_scheme                        *465.8 Gi    
disk1
/dev/disk2
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:     FDisk_partition_scheme                        *465.8 Gi    
disk2
/dev/disk3
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:     FDisk_partition_scheme                        *465.8 Gi    
disk3
/dev/disk4
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:     FDisk_partition_scheme                        *465.8 Gi    
disk4


And here is the zpool status output:

sh-3.2# zpool status
   pool: Media
  state: ONLINE
  scrub: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	Media       ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    disk1   ONLINE       0     0     0
	    disk2   ONLINE       0     0     0
	    disk3   ONLINE       0     0     0
	    disk4   ONLINE       0     0     0

errors: No known data errors

--
Gautam

From gagix at mac.com  Sat Mar  1 10:55:25 2008
From: gagix at mac.com (Gautam Godse)
Date: Sat, 1 Mar 2008 10:55:25 -0800
Subject: [zfs-discuss] creating a zpool changes disk partition label
In-Reply-To: <D94E6EEB-599F-483C-9EF2-11D313851A87@mac.com>
References: <D94E6EEB-599F-483C-9EF2-11D313851A87@mac.com>
Message-ID: <229B8A39-1F39-4F7E-88AF-42B1E1225EB0@mac.com>

Looks like forcing a zpool export causes the partition label to change!

Here is the sequence of events i did to test:

------------------------------------------------------------------------------------------

sh-3.2# zpool create tank mirror /dev/disk5 /dev/disk6

#diskutil list

/dev/disk5
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *279.5 Gi    
disk5
/dev/disk6
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *279.5 Gi    
disk6

sh-3.2# zpool export tank
cannot unmount '/Volumes/tank': Resource busy

sh-3.2# zpool export -f tank

sh-3.2# diskutil list

/dev/disk5
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:     FDisk_partition_scheme                        *279.5 Gi    
disk5
    1:                                                    279.5 Gi    
disk5s1
/dev/disk6
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:     FDisk_partition_scheme                        *279.5 Gi    
disk6
    1:                                                    279.5 Gi    
disk6s1

------------------------------------------------------------------------------------------

What gives ???


On Mar 1, 2008, at 10:46 AM, Gautam Godse wrote:

> hi,
> In continuation of my experiments with zfs, I destroyed the existing
> pool and created a new one.
> But this time I made sure that I partitioned each disk in the array
> with the GPTFormat option to set the correct partition label.
> However, after creating a zpool raidz array, i find that the partition
> label has been changed to FDisk_partition_scheme. Is this deliberate/
> expected ? Or is this some anomaly in the system ?
>
> This is the current output of 'diskutil list'
>
> /dev/disk1
>    #:                       TYPE NAME                    SIZE
> IDENTIFIER
>    0:     FDisk_partition_scheme                        *465.8 Gi
> disk1
> /dev/disk2
>    #:                       TYPE NAME                    SIZE
> IDENTIFIER
>    0:     FDisk_partition_scheme                        *465.8 Gi
> disk2
> /dev/disk3
>    #:                       TYPE NAME                    SIZE
> IDENTIFIER
>    0:     FDisk_partition_scheme                        *465.8 Gi
> disk3
> /dev/disk4
>    #:                       TYPE NAME                    SIZE
> IDENTIFIER
>    0:     FDisk_partition_scheme                        *465.8 Gi
> disk4
>
>
> And here is the zpool status output:
>
> sh-3.2# zpool status
>   pool: Media
>  state: ONLINE
>  scrub: none requested
> config:
>
> 	NAME        STATE     READ WRITE CKSUM
> 	Media       ONLINE       0     0     0
> 	  raidz1    ONLINE       0     0     0
> 	    disk1   ONLINE       0     0     0
> 	    disk2   ONLINE       0     0     0
> 	    disk3   ONLINE       0     0     0
> 	    disk4   ONLINE       0     0     0
>
> errors: No known data errors
>
> --
> Gautam
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dirkschelfhout at mac.com  Sat Mar  1 11:05:01 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Sat, 1 Mar 2008 20:05:01 +0100
Subject: [zfs-discuss] creating a zpool changes disk partition label
In-Reply-To: <229B8A39-1F39-4F7E-88AF-42B1E1225EB0@mac.com>
References: <D94E6EEB-599F-483C-9EF2-11D313851A87@mac.com>
	<229B8A39-1F39-4F7E-88AF-42B1E1225EB0@mac.com>
Message-ID: <65407C55-581D-436A-A3D9-CDBDC6FCC72F@mac.com>

maybe it is this :
from man zpool :
under export :
		For  pools to be portable, you must give the zpool command whole  
disks, not just slices,
            	so that ZFS can label the disks with portable EFI  
labels.  Otherwise,  disk  drivers  on
            	platforms of different endianness will not recognize the  
disks.

On 01 Mar 2008, at 19:55, Gautam Godse wrote:

> Looks like forcing a zpool export causes the partition label to  
> change!
>
> Here is the sequence of events i did to test:
>
> ------------------------------------------------------------------------------------------
>
> sh-3.2# zpool create tank mirror /dev/disk5 /dev/disk6
>
> #diskutil list
>
> /dev/disk5
>    #:                       TYPE NAME                    SIZE
> IDENTIFIER
>    0:      GUID_partition_scheme                        *279.5 Gi
> disk5
> /dev/disk6
>    #:                       TYPE NAME                    SIZE
> IDENTIFIER
>    0:      GUID_partition_scheme                        *279.5 Gi
> disk6
>
> sh-3.2# zpool export tank
> cannot unmount '/Volumes/tank': Resource busy
>
> sh-3.2# zpool export -f tank
>
> sh-3.2# diskutil list
>
> /dev/disk5
>    #:                       TYPE NAME                    SIZE
> IDENTIFIER
>    0:     FDisk_partition_scheme                        *279.5 Gi
> disk5
>    1:                                                    279.5 Gi
> disk5s1
> /dev/disk6
>    #:                       TYPE NAME                    SIZE
> IDENTIFIER
>    0:     FDisk_partition_scheme                        *279.5 Gi
> disk6
>    1:                                                    279.5 Gi
> disk6s1
>
> ------------------------------------------------------------------------------------------
>
> What gives ???
>
>
> On Mar 1, 2008, at 10:46 AM, Gautam Godse wrote:
>
>> hi,
>> In continuation of my experiments with zfs, I destroyed the existing
>> pool and created a new one.
>> But this time I made sure that I partitioned each disk in the array
>> with the GPTFormat option to set the correct partition label.
>> However, after creating a zpool raidz array, i find that the  
>> partition
>> label has been changed to FDisk_partition_scheme. Is this deliberate/
>> expected ? Or is this some anomaly in the system ?
>>
>> This is the current output of 'diskutil list'
>>
>> /dev/disk1
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:     FDisk_partition_scheme                        *465.8 Gi
>> disk1
>> /dev/disk2
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:     FDisk_partition_scheme                        *465.8 Gi
>> disk2
>> /dev/disk3
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:     FDisk_partition_scheme                        *465.8 Gi
>> disk3
>> /dev/disk4
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:     FDisk_partition_scheme                        *465.8 Gi
>> disk4
>>
>>
>> And here is the zpool status output:
>>
>> sh-3.2# zpool status
>>  pool: Media
>> state: ONLINE
>> scrub: none requested
>> config:
>>
>> 	NAME        STATE     READ WRITE CKSUM
>> 	Media       ONLINE       0     0     0
>> 	  raidz1    ONLINE       0     0     0
>> 	    disk1   ONLINE       0     0     0
>> 	    disk2   ONLINE       0     0     0
>> 	    disk3   ONLINE       0     0     0
>> 	    disk4   ONLINE       0     0     0
>>
>> errors: No known data errors
>>
>> --
>> Gautam
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From gagix at mac.com  Sat Mar  1 11:27:10 2008
From: gagix at mac.com (Gautam Godse)
Date: Sat, 1 Mar 2008 11:27:10 -0800
Subject: [zfs-discuss] creating a zpool changes disk partition label
In-Reply-To: <65407C55-581D-436A-A3D9-CDBDC6FCC72F@mac.com>
References: <D94E6EEB-599F-483C-9EF2-11D313851A87@mac.com>
	<229B8A39-1F39-4F7E-88AF-42B1E1225EB0@mac.com>
	<65407C55-581D-436A-A3D9-CDBDC6FCC72F@mac.com>
Message-ID: <53B0A3EE-3858-498B-990B-5C5CC292692E@mac.com>

But these are whole disks not slices...
and zpool import works fine..
its just that every time I export the zpool I get the annoying 'Disk  
Insertion' dialog for every disk...
I don't know if these two events are related but it does seem like it..
i.e. export a zpool and the disk label changes to Fdisk and  
immediately MacOSX gives the 'Disk Insertion' dialog..

These disks did have the correct EFI and ZFS partitions/labels before  
zpool creation:
----------------
/dev/disk5
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *279.5 Gi    
disk5
    1:                        EFI                         200.0 Mi    
disk5s1
    2:                        ZFS                         279.1 Gi    
disk5s2
/dev/disk6
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *279.5 Gi    
disk6
    1:                        EFI                         200.0 Mi    
disk6s1
    2:                        ZFS                         279.1 Gi    
disk6s2
-------------------


On Mar 1, 2008, at 11:05 AM, Dirk Schelfhout wrote:

> maybe it is this :
> from man zpool :
> under export :
> 		For  pools to be portable, you must give the zpool command whole
> disks, not just slices,
>            	so that ZFS can label the disks with portable EFI
> labels.  Otherwise,  disk  drivers  on
>            	platforms of different endianness will not recognize the
> disks.
>
> On 01 Mar 2008, at 19:55, Gautam Godse wrote:
>
>> Looks like forcing a zpool export causes the partition label to
>> change!
>>
>> Here is the sequence of events i did to test:
>>
>> ------------------------------------------------------------------------------------------
>>
>> sh-3.2# zpool create tank mirror /dev/disk5 /dev/disk6
>>
>> #diskutil list
>>
>> /dev/disk5
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:      GUID_partition_scheme                        *279.5 Gi
>> disk5
>> /dev/disk6
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:      GUID_partition_scheme                        *279.5 Gi
>> disk6
>>
>> sh-3.2# zpool export tank
>> cannot unmount '/Volumes/tank': Resource busy
>>
>> sh-3.2# zpool export -f tank
>>
>> sh-3.2# diskutil list
>>
>> /dev/disk5
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:     FDisk_partition_scheme                        *279.5 Gi
>> disk5
>>   1:                                                    279.5 Gi
>> disk5s1
>> /dev/disk6
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:     FDisk_partition_scheme                        *279.5 Gi
>> disk6
>>   1:                                                    279.5 Gi
>> disk6s1
>>
>> ------------------------------------------------------------------------------------------
>>
>> What gives ???
>>
>>
>> On Mar 1, 2008, at 10:46 AM, Gautam Godse wrote:
>>
>>> hi,
>>> In continuation of my experiments with zfs, I destroyed the existing
>>> pool and created a new one.
>>> But this time I made sure that I partitioned each disk in the array
>>> with the GPTFormat option to set the correct partition label.
>>> However, after creating a zpool raidz array, i find that the
>>> partition
>>> label has been changed to FDisk_partition_scheme. Is this  
>>> deliberate/
>>> expected ? Or is this some anomaly in the system ?
>>>
>>> This is the current output of 'diskutil list'
>>>
>>> /dev/disk1
>>>  #:                       TYPE NAME                    SIZE
>>> IDENTIFIER
>>>  0:     FDisk_partition_scheme                        *465.8 Gi
>>> disk1
>>> /dev/disk2
>>>  #:                       TYPE NAME                    SIZE
>>> IDENTIFIER
>>>  0:     FDisk_partition_scheme                        *465.8 Gi
>>> disk2
>>> /dev/disk3
>>>  #:                       TYPE NAME                    SIZE
>>> IDENTIFIER
>>>  0:     FDisk_partition_scheme                        *465.8 Gi
>>> disk3
>>> /dev/disk4
>>>  #:                       TYPE NAME                    SIZE
>>> IDENTIFIER
>>>  0:     FDisk_partition_scheme                        *465.8 Gi
>>> disk4
>>>
>>>
>>> And here is the zpool status output:
>>>
>>> sh-3.2# zpool status
>>> pool: Media
>>> state: ONLINE
>>> scrub: none requested
>>> config:
>>>
>>> 	NAME        STATE     READ WRITE CKSUM
>>> 	Media       ONLINE       0     0     0
>>> 	  raidz1    ONLINE       0     0     0
>>> 	    disk1   ONLINE       0     0     0
>>> 	    disk2   ONLINE       0     0     0
>>> 	    disk3   ONLINE       0     0     0
>>> 	    disk4   ONLINE       0     0     0
>>>
>>> errors: No known data errors
>>>
>>> --
>>> Gautam
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From jbsnyder at gmail.com  Sat Mar  1 12:34:48 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Sat, 1 Mar 2008 14:34:48 -0600
Subject: [zfs-discuss] creating a zpool changes disk partition label
In-Reply-To: <53B0A3EE-3858-498B-990B-5C5CC292692E@mac.com>
References: <D94E6EEB-599F-483C-9EF2-11D313851A87@mac.com>
	<229B8A39-1F39-4F7E-88AF-42B1E1225EB0@mac.com>
	<65407C55-581D-436A-A3D9-CDBDC6FCC72F@mac.com>
	<53B0A3EE-3858-498B-990B-5C5CC292692E@mac.com>
Message-ID: <33644d3c0803011234g3da22b80tea90434823a8afc0@mail.gmail.com>

This is overall a point that I'm not entirely clear on.

I assume that by using diskutil to set up ZFS on a disk what one is
doing is actually allowing diskutil to make a guid partition table on
disk and then give a partition/slice to ZFS.  Does this mean the same
thing as "giving the whole disk to zfs"?  To me that would mean doing
a zpool create on the raw or root level of the device.  The language
in the man file is the same as on freebsd 7, with which I have made
raidz configurations using only zpool/zfs commands and haven't used
any other utility to wrap the disk in some sort of partition table.

One other thing of note is that I've tried using an old MBR table on a
disk and then replacing an HFS partition on said disk with a ZFS one.
The label on the partition does not change from the hfs label to a zfs
one.  Is this because there isn't an MBR partition type for ZFS or
because the label isn't being changed?  With a GPT, the label seems to
be adjusted appropriately on re-purposing an HFS partition.

Whether or not APM & MBR are officially recommended (not sure whether
they are or not), it is nice if they work.  It would be nice if
guidelines are laid out about what may or may not work on other ZFS
implementations (Solaris, BSD, FUSE).  Is the only recommended fashion
to get a disk to work between various implementations to use zpool
create on the whole disk as is mentioned in the man pages?

On Sat, Mar 1, 2008 at 1:27 PM, Gautam Godse <gagix at mac.com> wrote:
> But these are whole disks not slices...
>  and zpool import works fine..
>  its just that every time I export the zpool I get the annoying 'Disk
>  Insertion' dialog for every disk...
>  I don't know if these two events are related but it does seem like it..
>  i.e. export a zpool and the disk label changes to Fdisk and
>  immediately MacOSX gives the 'Disk Insertion' dialog..
>
>  These disks did have the correct EFI and ZFS partitions/labels before
>  zpool creation:
>  ----------------
>
> /dev/disk5
>     #:                       TYPE NAME                    SIZE
>  IDENTIFIER
>     0:      GUID_partition_scheme                        *279.5 Gi
>  disk5
>     1:                        EFI                         200.0 Mi
>  disk5s1
>     2:                        ZFS                         279.1 Gi
>  disk5s2
>
> /dev/disk6
>     #:                       TYPE NAME                    SIZE
>  IDENTIFIER
>     0:      GUID_partition_scheme                        *279.5 Gi
>  disk6
>     1:                        EFI                         200.0 Mi
>  disk6s1
>     2:                        ZFS                         279.1 Gi
>  disk6s2
>  -------------------
>
>
>
>
>  On Mar 1, 2008, at 11:05 AM, Dirk Schelfhout wrote:
>
>  > maybe it is this :
>  > from man zpool :
>  > under export :
>  >               For  pools to be portable, you must give the zpool command whole
>  > disks, not just slices,
>  >               so that ZFS can label the disks with portable EFI
>  > labels.  Otherwise,  disk  drivers  on
>  >               platforms of different endianness will not recognize the
>  > disks.
>  >
>  > On 01 Mar 2008, at 19:55, Gautam Godse wrote:
>  >
>  >> Looks like forcing a zpool export causes the partition label to
>  >> change!
>  >>
>  >> Here is the sequence of events i did to test:
>  >>
>  >> ------------------------------------------------------------------------------------------
>  >>
>  >> sh-3.2# zpool create tank mirror /dev/disk5 /dev/disk6
>  >>
>  >> #diskutil list
>  >>
>  >> /dev/disk5
>  >>   #:                       TYPE NAME                    SIZE
>  >> IDENTIFIER
>  >>   0:      GUID_partition_scheme                        *279.5 Gi
>  >> disk5
>  >> /dev/disk6
>  >>   #:                       TYPE NAME                    SIZE
>  >> IDENTIFIER
>  >>   0:      GUID_partition_scheme                        *279.5 Gi
>  >> disk6
>  >>
>  >> sh-3.2# zpool export tank
>  >> cannot unmount '/Volumes/tank': Resource busy
>  >>
>  >> sh-3.2# zpool export -f tank
>  >>
>  >> sh-3.2# diskutil list
>  >>
>  >> /dev/disk5
>  >>   #:                       TYPE NAME                    SIZE
>  >> IDENTIFIER
>  >>   0:     FDisk_partition_scheme                        *279.5 Gi
>  >> disk5
>  >>   1:                                                    279.5 Gi
>  >> disk5s1
>  >> /dev/disk6
>  >>   #:                       TYPE NAME                    SIZE
>  >> IDENTIFIER
>  >>   0:     FDisk_partition_scheme                        *279.5 Gi
>  >> disk6
>  >>   1:                                                    279.5 Gi
>  >> disk6s1
>  >>
>  >> ------------------------------------------------------------------------------------------
>  >>
>  >> What gives ???
>  >>
>  >>
>  >> On Mar 1, 2008, at 10:46 AM, Gautam Godse wrote:
>  >>
>  >>> hi,
>  >>> In continuation of my experiments with zfs, I destroyed the existing
>  >>> pool and created a new one.
>  >>> But this time I made sure that I partitioned each disk in the array
>  >>> with the GPTFormat option to set the correct partition label.
>  >>> However, after creating a zpool raidz array, i find that the
>  >>> partition
>  >>> label has been changed to FDisk_partition_scheme. Is this
>  >>> deliberate/
>  >>> expected ? Or is this some anomaly in the system ?
>  >>>
>  >>> This is the current output of 'diskutil list'
>  >>>
>  >>> /dev/disk1
>  >>>  #:                       TYPE NAME                    SIZE
>  >>> IDENTIFIER
>  >>>  0:     FDisk_partition_scheme                        *465.8 Gi
>  >>> disk1
>  >>> /dev/disk2
>  >>>  #:                       TYPE NAME                    SIZE
>  >>> IDENTIFIER
>  >>>  0:     FDisk_partition_scheme                        *465.8 Gi
>  >>> disk2
>  >>> /dev/disk3
>  >>>  #:                       TYPE NAME                    SIZE
>  >>> IDENTIFIER
>  >>>  0:     FDisk_partition_scheme                        *465.8 Gi
>  >>> disk3
>  >>> /dev/disk4
>  >>>  #:                       TYPE NAME                    SIZE
>  >>> IDENTIFIER
>  >>>  0:     FDisk_partition_scheme                        *465.8 Gi
>  >>> disk4
>  >>>
>  >>>
>  >>> And here is the zpool status output:
>  >>>
>  >>> sh-3.2# zpool status
>  >>> pool: Media
>  >>> state: ONLINE
>  >>> scrub: none requested
>  >>> config:
>  >>>
>  >>>     NAME        STATE     READ WRITE CKSUM
>  >>>     Media       ONLINE       0     0     0
>  >>>       raidz1    ONLINE       0     0     0
>  >>>         disk1   ONLINE       0     0     0
>  >>>         disk2   ONLINE       0     0     0
>  >>>         disk3   ONLINE       0     0     0
>  >>>         disk4   ONLINE       0     0     0
>  >>>
>  >>> errors: No known data errors
>  >>>
>  >>> --
>  >>> Gautam
>  >>> _______________________________________________
>  >>> zfs-discuss mailing list
>  >>> zfs-discuss at lists.macosforge.org
>  >>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>  >>
>  >> _______________________________________________
>  >> zfs-discuss mailing list
>  >> zfs-discuss at lists.macosforge.org
>  >> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>  >
>  > _______________________________________________
>  > zfs-discuss mailing list
>  > zfs-discuss at lists.macosforge.org
>  > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>  _______________________________________________
>  zfs-discuss mailing list
>  zfs-discuss at lists.macosforge.org
>  http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From i_see at macnews.de  Sun Mar  2 00:19:34 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Sun, 2 Mar 2008 09:19:34 +0100
Subject: [zfs-discuss] creating a zpool changes disk partition label
In-Reply-To: <mailman.415.1204403662.11791.zfs-discuss@lists.macosforge.org>
References: <mailman.415.1204403662.11791.zfs-discuss@lists.macosforge.org>
Message-ID: <5EEF45A4-64EF-4102-B97F-03F85C0B4780@macnews.de>

Hi,
this always seems to happen when you pass an entire diskto zfs instead  
of the main slice of a GUID-partitioned disk.
I also have some MBR-disks in my pool now and cannot easily revert it,  
since you cant shrink a pool easily and I don't have sufficient disk- 
space to redundantly moving all data from my pool and rebuilding it.
I just hope that i can avoid running into problems with just _some_  
disks in fact having a GUID EFI slice.
(I wasn't quite so lucky with my first pool, but that had been created  
with an earlier version of the code base, and I could get it working  
by purposely degrading it leaving the disk with the corrupted zfs- 
label out.)

ralf bertling

From i_see at macnews.de  Sun Mar  2 01:53:07 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Sun, 2 Mar 2008 10:53:07 +0100
Subject: [zfs-discuss] My 2 cents on zraid random read performance
In-Reply-To: <mailman.415.1204403662.11791.zfs-discuss@lists.macosforge.org>
References: <mailman.415.1204403662.11791.zfs-discuss@lists.macosforge.org>
Message-ID: <7D227A4B-9A01-49B0-B6FD-861AA4CA6AAB@macnews.de>

Hi,

there has been some discussion recently about the random-read- 
performance of zraid compared to mirrored disks and raid5.
Although I am a bit late to join this party, I want to add what i  
think are the theoretical implications of the COW-design on the issue.
I have not checked the code how much the things I discuss are  
fundamental design decisions that would force big changes to  
implement, I am just hoping to get the reasoning correct.

1. Full read
The fact that ZFS always writes data in full striped across all disks  
does NOT imply it always has to read data in full stripes.
If it is only interested in a particular byte, it would "know" on  
which disks that byte had been written and access just that disk.
The real question is HOW does zraid distribute the data and is it does  
this prevent accessing data that happens to be on individual drives.
If it can only access all disks at once then it is not reading data  
slower, it is just reading unnecessary data.
This is however not implied by the COW design or the wasted space/ 
redundancy ratio, but only by the way low levels of zfs process read  
requests.
 From benchmark it seems, zfs is indeed reading from all disks (except  
for the parity drive), so the situation could improve a lot.

2. Performance of Mirroring vs. RAID-Z
This is a question of read vs. write performance.
a) Mirrored disks generally have the write performance of the slowest  
disk in the array.
b) raid-z will do sequential writes most of the time thereby achieving  
the performance of the slowest disk * (raid width - redundancy level)
(Note that zfs cannot completely avoid random access for random  
writes, it just moves part of it to the time the data is read, which  
might hurt some benchmarks, as you get more random reads this way).
c) streaming read performance and random read performance (if  
saturated with requests) for mirrors is the sum of all the performance  
of all disks.
d) streaming read performance for raid-z is the performance of the  
slowest disk * (raid width - redundancy level)
e) random read performance for raid-z could be the performance of the  
slowest disk * (raid width - redundancy level)
This is because raid-z can not freely choose the disk to read from, so  
chances are requests will pile up for the slowest disk and all other  
requests have to wait for that. If, infact just one disk is much  
slower to fulfill a request, the data on that disk could as well be  
recomputed from the data on all other disks. This improvement would  
have some catches, as it is unlikely and would not detect silent data  
corruption on that slow disk. Having said that, silent data corruption  
will also be undetected on mirrored drives for all drives that are no  
actively used to fulfill the request. The only way to be sure in that  
area is regular scrubbing

3. Performance implications of zfs COPIES=n
Since for many scenarios random reads occur for large number of small  
files that occupy little disk-space, it might be worthwhile placing  
extra copies of small files onto the array. This improves the chances  
that random reads are on in fact sequential for one of the copies and  
minimizes necessary head movements while also allowing to choose the  
fastest drive that has holds the requested data.
I do not know is ZFS will always try the first copy first, and the  
other ones only in case of failure, but not doing so might be a  
worthwhile idea.
This would still not help in all situations, like database serves,  
that have big files in which random reads may occur frequently.
If zfs would try to group small files and/or additional copies  
together, it could also help statistical behaviour significantly.
This works best if you have more than one vdev in your pool.

To sum it up, situation is not as grim as it seems for raid-z
raid-z can offer much better write performance combined with better  
space utilization. If things are done well, read performance is only  
slightly lower, so for many use-cases raid-z would be preferable,  
especially if you keep in mind that starting with groups of four  
disks, raidz2 offers far better fault-tolerance at the same disk- 
utilization level. As of now, this is all quite theoretic as for  
implementation details. Maybe someone who knows more of the source  
code than me can shed some light on those ideas.

--
ralf bertling


From i_see at macnews.de  Sun Mar  2 05:51:44 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Sun, 2 Mar 2008 14:51:44 +0100
Subject: [zfs-discuss] Behaviour on bad sectors?
Message-ID: <ED779CDC-7757-443E-BF45-F653A40EEA34@macnews.de>

Hi,
during my adventures with zfs, I have also found that one of my old  
disk has approximately 1700 bad sectors that don't respond well to  
read requests. I know that other errors will probably follow, but for  
the time being and for the sake of the argument, I wil be using this  
setup.
I am not sure how my experiences are influenced by zfs, mac os x and/ 
or the firmware of the drive or the USB-enclosure i am using:

When a bad sector is encountered, additional read attempts follow in  
intervals of approx. 1 second for a while until the system finally  
decides the read attempt failed, an I/O error is logged in the Mac's  
system.log and the counter on read errors in increased.

While the sector is still probed, the data rate of the ZFS-Raidz drops  
to practically zero resulting in greatly increased scrub or data  
access times.
As far as I can tell, no attempt is made to:

a) map out the sector so it is not used again. (I am not sure if the  
repaired data is written by COW or by in-place updates)
b) remember the number or identity of bad sectors across reboots (or  
probably export/imports)
c) delay the initial request after a sub-second-timeout and try to  
compute the missing data from redundant information. (The request  
could be re-issued later or be treated as an error depending on setup.)

I don't know if zfs would automatically jump to a hot spare on the  
first read-error, but if not, the behaviour of my system would  
generally be recognised as "server not responding" by the normal and  
user. For a single error, the system takes approx. 20 seconds to  
notice. during which the pool does not respond at all and all  
applications trying to access data will be "beachballig" i.e. be  
stuck. In my situation, this results in minutes without throughput.

As a result, I would suggest that the behaviour is changed to:
- Try to avoid retries on I/O reads if they do not immediately succeed  
(only possible if the retries come from the OS and not from the USB  
enclosure)
- faster error detection by cancelling requests that are unlikely to  
return any meaningful data and flagging errors accordingly. (Of course  
one needs to be careful not to report thousands of errors if a disk  
has just chosen to spin down for energy saving or of the reasons (see  
P.S.))
- Add the ability to mark sectors that had problems as unavailable for  
future use (I know in a professional setup you would simply throw that  
disk away, but I'd rathe try to use it in an redundant configuration  
than not using redundancy at all. Right now I know, I might loose some  
data, if another disk breaks completely, but otherwise, I already have  
much higher reliability than I used to have.
- Store error information in the volume header of the Vdevs.
I do have long uptimes for a home configuration, but still think after  
a reboot I should see, if there have been disk errors.

--
ralf bertling

From i_see at macnews.de  Sun Mar  2 06:48:10 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Sun, 2 Mar 2008 15:48:10 +0100
Subject: [zfs-discuss] Feature request: bette utilization of mixed size
	disks in ZRaid
Message-ID: <8F59C082-3AB6-4553-9DBE-418626D39C7E@macnews.de>

Hi,
When I started deploying zfs, I was thrilled by the fact that I would  
be able to use my disks, that have very different sizes to form a  
dynamic redundant array with raidz. Not to long after that i found  
out, that this only results in rathe poor space utilization if the  
disks do not have the same size.
The first consequence was to build the second pool from the smaller  
disks and partitions of the larger disks that match the size of the  
smaller disks. This way, I can use the remaining space for non- 
redundant data (e.g. software updates that can easily be replaced if a  
drive fails) and Mac OS Time-Machine backups of my HFS+ drive.). Still  
Id would be nice to throw JBODODS (Just a bunch of disks of different  
sizes) at ZFS and automagically have most of the space available for  
use.

The math:
Obviously you have to pay for redundancy and if the biggest disk has  
no other disks to match it, some more space is lost.
To be exact, for a single VDev with raidz1, the size of the biggest  
disk can not be available for data, for Raidz2 the sum of the sizes of  
the two biggest disks/partitions are not usable.
The DIY-Workaround:

1.Order your disks by size. Use the size of the smallest disk (create  
partitions of the same size on the larger disks) and form a raidz vdev.
2. Use the smallest non-zero disk size to form another set of  
partitions that will form another raidz vdev.
3. Repeat step 2 until the remaining disks are insufficient to achieve  
the desired replication level.
4. (optional): Use the remaining space for storage needs with lower  
replication needs (i.e. /tmp Updates...)

The catch is, with the current ZFS implementation you should not add  
the vdevs to the same pool as ZFS will use striping assuming your  
partitions are in fact different disks. The resulting pool will  
work,but not run (running implies speed).
If you have different filesystems that you can easily separate, you  
should rather use independent pools to avoid this problem.

The next catch is, that replacing a disk by a bigger one will now  
involve moving large amounts of data and probably creating a number of  
new pools. I am unsure if there is an easy way to recreate a zfs  
filesystem with all snapshots and clones in this situation, so this  
might be a lot of additional work.

Request:
Allow zraid configurations to logically segment the devices in a vdev  
as outlined above, without trying to stripe across an individual device.
Ideally, the unused space could be more or less automatically be  
available for other uses.
The implied reconfiguration problems should become obsolete as soon as  
there will be a restriping and/or evacuation routine for zfs vdevs and/ 
or pools.

Now I have probably written enough about zfs for a day...

ralf bertling
PS: An interessting story about MTTDL I have witnessed:
A company was deploying a redundant RAID configuration 24/7 with its  
own UPS-system for the file-servers. Some maintenance work required to  
take the UPS and the fileserve down; this was scheduled on a weekend  
to cause minimum trouble. Everything went well up to the point when  
the fileserver should be started again. As it turned out, the bearings  
in most disks had gone bad over years, but while the spindles were  
turning, the problem was never recognized. Once stopped, the drived  
were unable to start turning again.
So apart from your redundancy level and your backup strategy and  
SMART, maybe you should consider a maintenance schedule to spin down a  
drive once in a while. ZFS always allows you to replace intact drives  
and remove / re-attach them. These MTTDL numbers assume that a disk's  
failure is independent of the others. However this is not always true.

From William.Winnett at Sun.COM  Sun Mar  2 16:17:06 2008
From: William.Winnett at Sun.COM (Bill Winnett)
Date: Sun, 02 Mar 2008 19:17:06 -0500
Subject: [zfs-discuss] zfs system/kernel panics
Message-ID: <33BA4ACA-2C39-4E51-ADE9-5CE71330EE45@sun.com>

Hi,

    I have experienced a few of these panics over the last few  
weeks.    I
    don't know if this tells you much, but here is the info.
    ---
    Sun Mar  2 18:55:15 2008
    panic(cpu 1 caller 0x00CF0A41): "zap_hash(zap, name) == hash  
failed, 0
    == 0"@/Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/
    zfs_kext/zfs/zap_micro.c:117
    Backtrace, Format - Frame : Return Address (4 potential args on  
stack)
    0x666736f8 : 0x12b0e1 (0x457024 0x6667372c 0x13321a 0x0)
    0x66673748 : 0xcf0a41 (0xd57470 0xd57454 0x0 0xd2ec90)
    0x666737f8 : 0xcf1578 (0x0 0x25a872d0 0x0 0x0)
    0x66673858 : 0xd1a3d2 (0x80c2d70 0x4ced9 0x0 0xf8de700)
    0x666738d8 : 0xd1a60b (0x6667390c 0x65932498 0x66673950 0x66673908)
    0x66673928 : 0xcb7dc7 (0x65932498 0x66673950 0x66673df4 0x1f0d72)
    0x666739a8 : 0x1f1b5f (0x666739c8 0x2 0x666739f8 0x44dc07)
    0x666739f8 : 0x1d36f9 (0x963ad00 0x66673df4 0x66673f08 0x7aca104)
    0x66673a88 : 0x1d447e (0x66673ddc 0x100 0x66673dfc 0x0)
    0x66673b48 : 0x1e2ff0 (0x66673ddc 0x0 0x0 0x0)
    0x66673d88 : 0x1e35e4 (0xb0523e98 0x0 0x0 0x0)
    0x66673f48 : 0x1e367d (0xb0523e98 0x0 0x0 0x0)
    0x66673f78 : 0x3dbe77 (0x7423c50 0x7aca000 0x7aca044 0x7dab7d8)
    0x66673fc8 : 0x19f084 (0x7da3880 0x0 0x1a20b5 0x740f8b8)

- Ignored:
    No mapping exists for frame pointer
    Backtrace terminated-invalid frame pointer 0xb0523f78
           Kernel loadable modules in backtrace (with dependencies):
              com.apple.filesystems.zfs(8.0)@0xcb4000->0xd7ffff

    BSD process name corresponding to current thread: fseventsd

    Mac OS version:
    9C31

    Kernel version:
    Darwin Kernel Version 9.2.0: Tue Feb  5 16:13:22 PST 2008;
    root:xnu-1228.3.13~1/RELEASE_I386
    System model name: iMac7,1 (Mac-F4238CC8)


    --

    Model: iMac7,1, BootROM IM71.007A.B00, 2 processors, Intel Core 2  
Duo,
    2.4 GHz, 4 GB
    Graphics: kHW_ATIr600M76Item, ATI,RadeonHD2600,
    spdisplays_pcie_device, 256 MB
    Memory Module: BANK 0/DIMM0, 2 GB, DDR2 SDRAM, 667 MHz
    Memory Module: BANK 1/DIMM1, 2 GB, DDR2 SDRAM, 667 MHz
    AirPort: spairport_wireless_card_type_airport_extreme (0x14E4,  
0x88),
    Broadcom BCM43xx 1.0 (4.170.46.3)
    Bluetooth: Version 2.1.0f14, 2 service, 1 devices, 1 incoming serial
    ports
    Network Service: AirPort, AirPort, en1
    Network Service: Parallels Host-Guest, Ethernet, en2
    Network Service: Parallels NAT, Ethernet, en3
    Serial ATA Device: WDC WD3200AAJS-40RYA0, 298.09 GB
    Parallel ATA Device: PIONEER DVD-RW  DVR-K06A
    USB Device: Keyboard Hub, Apple, Inc., high_speed, 500 mA
    USB Device: USB-PS/2 Trackball, Logitech, low_speed, 100 mA
    USB Device: Apple Keyboard, Apple, Inc, low_speed, 100 mA
    USB Device: Built-in iSight, Apple Inc., high_speed, 500 mA
    USB Device: Bluetooth USB Host Controller, Apple, Inc., full_speed,
    500 mA
    USB Device: IR Receiver, Apple Computer, Inc., low_speed, 500 mA
    FireWire Device: 1394 Storage Front Panel*, Maxtor, 800mbit_speed
    FireWire Device: 1394 Storage Front Panel*, Maxtor, 800mbit_speed
    FireWire Device: 1394 Storage Front Panel*, Maxtor, 800mbit_speed
    FireWire Device: 1394 Storage Front Panel*, Maxtor, 800mbit_speed






    It seems to be when the zfs volume is in high use.  Writes from a
    networked xp system to a shared directory and writes from a  
Parallels
    process to a private directory.    Reads and writes from a Solaris  
system
    to a shared directory.   The zfs volume is composed of 4 1tb  
firewire
    disks.




From jbsnyder at gmail.com  Tue Mar  4 06:58:04 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Tue, 4 Mar 2008 08:58:04 -0600
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on ZFS?
Message-ID: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>

Hi -

I was just thinking today about rolling more of my system over to ZFS
since, with some exceptions, it seems to be fairly stable in usage.  I
might Assume that Applications, Developer, maybe usr could switch over
to ZFS, but I'm not exactly sure what portions of the filesystem are
required to bring up OS X.  /Library, /System likely need to be on HFS
at this point I would assume, but what are the limitations, what needs
to be on HFS to get things up to the login screen?

Another question is:
Is it preferred to file bugs by using the Trac system?  Should they go
on zfs-discuss here?  If one has a panic should that just go in
through the normal Apple reporting system?  I do not have a full ADC
membership so I cannot put anything on rdar as far as I know.

Thanks!


-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From gagix at mac.com  Tue Mar  4 07:04:33 2008
From: gagix at mac.com (Gautam Godse)
Date: Tue, 4 Mar 2008 07:04:33 -0800
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
	ZFS?
In-Reply-To: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
Message-ID: <4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>

Frankly I have yet to be convinced that ZFS should be used for your  
Boot / User file system.
The advantages of ZFS are only apparent (to me) when ZFS is deployed  
in a redundant configuration (mirror/raid) and for backup, media file  
storage, database storage or something similar.
The only advantage of ZFS on the Boot/User filesystem would be the  
snapshot functionality and I think for now TimeMachine suffices.
Does anyone agree/disagree ?
--
Gautam

On Mar 4, 2008, at 6:58 AM, James Snyder wrote:

> Hi -
>
> I was just thinking today about rolling more of my system over to ZFS
> since, with some exceptions, it seems to be fairly stable in usage.  I
> might Assume that Applications, Developer, maybe usr could switch over
> to ZFS, but I'm not exactly sure what portions of the filesystem are
> required to bring up OS X.  /Library, /System likely need to be on HFS
> at this point I would assume, but what are the limitations, what needs
> to be on HFS to get things up to the login screen?
>
> Another question is:
> Is it preferred to file bugs by using the Trac system?  Should they go
> on zfs-discuss here?  If one has a panic should that just go in
> through the normal Apple reporting system?  I do not have a full ADC
> membership so I cannot put anything on rdar as far as I know.
>
> Thanks!
>
>
> -- 
> James Snyder
> Biomedical Engineering
> Northwestern University
> jbsnyder at gmail.com
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dirkschelfhout at mac.com  Tue Mar  4 07:13:49 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Tue, 4 Mar 2008 16:13:49 +0100
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put
	on	ZFS?
In-Reply-To: <4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
Message-ID: <86FFE7DA-4015-4FE9-ACD3-6FFBD224F785@mac.com>

I don't agree
ZFS checks the data. A corrupted file backed up with time machine is  
not seen.
But how much does this happen ? ( I mean that data gets corrupted  
inside a file )
Dirk
On 04 Mar 2008, at 16:04, Gautam Godse wrote:

> The only advantage of ZFS on the Boot/User filesystem would be the
> snapshot functionality and I think for now TimeMachine suffices.
> Does anyone agree/disagree ?


From Jonathan.Edwards at Sun.COM  Tue Mar  4 07:20:59 2008
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Tue, 04 Mar 2008 10:20:59 -0500
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put
	on	ZFS?
In-Reply-To: <4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
Message-ID: <4873FDA4-3283-4D2F-A13C-3B7A7D5DA031@sun.com>

actually - can ZFS be "bless"ed as a bootable filesystem? and what  
else will break without res forks and EAs?

you've got some other advantages around performance with a dedicated  
ARC, block checksums (more verifiability), metadata compressibility,  
and CoW .. but there seems to be a lot of legacy tied up in HFS+ that  
might be hard to break for a root FS .. it's already painful to watch  
the apps that currently break just going to a case sensitive install.

---
.je

On Mar 4, 2008, at 10:04 AM, Gautam Godse wrote:
> Frankly I have yet to be convinced that ZFS should be used for your
> Boot / User file system.
> The advantages of ZFS are only apparent (to me) when ZFS is deployed
> in a redundant configuration (mirror/raid) and for backup, media file
> storage, database storage or something similar.
> The only advantage of ZFS on the Boot/User filesystem would be the
> snapshot functionality and I think for now TimeMachine suffices.
> Does anyone agree/disagree ?
> --
> Gautam
>
> On Mar 4, 2008, at 6:58 AM, James Snyder wrote:
>
>> Hi -
>>
>> I was just thinking today about rolling more of my system over to ZFS
>> since, with some exceptions, it seems to be fairly stable in  
>> usage.  I
>> might Assume that Applications, Developer, maybe usr could switch  
>> over
>> to ZFS, but I'm not exactly sure what portions of the filesystem are
>> required to bring up OS X.  /Library, /System likely need to be on  
>> HFS
>> at this point I would assume, but what are the limitations, what  
>> needs
>> to be on HFS to get things up to the login screen?
>>
>> Another question is:
>> Is it preferred to file bugs by using the Trac system?  Should they  
>> go
>> on zfs-discuss here?  If one has a panic should that just go in
>> through the normal Apple reporting system?  I do not have a full ADC
>> membership so I cannot put anything on rdar as far as I know.
>>
>> Thanks!
>>
>>
>> -- 
>> James Snyder
>> Biomedical Engineering
>> Northwestern University
>> jbsnyder at gmail.com
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From jbsnyder at gmail.com  Tue Mar  4 07:25:03 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Tue, 4 Mar 2008 09:25:03 -0600
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
	ZFS?
In-Reply-To: <86FFE7DA-4015-4FE9-ACD3-6FFBD224F785@mac.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
	<86FFE7DA-4015-4FE9-ACD3-6FFBD224F785@mac.com>
Message-ID: <33644d3c0803040725qc6bff45ra062141b2714e4d9@mail.gmail.com>

It remains to be seen how often this takes place within actual user
systems, but in the broad scheme of things data corruption DOES occur:
http://storagemojo.com/2007/09/19/cerns-data-corruption-research/

While, yeah, it matters for home directory stuff, it would also be
nice to move system stuff over onto a filesystem that actually will
keep track of whether files have become corrupted.  This would help
immensely in tracking down problems that are due to data corruption on
the disk end of things rather than corruption caused by software, or
just other system bugs.  I would also assume that if ZFS requests a
block of data from disk and doesn't get the checksum it requested that
it would re-request this data.  So that if either the drive's CRC
checks didn't catch something or if somehow data was not transmitted
correctly over the interconnects, you might have a chance of getting
things right the second time, even if you don't have redundancy
(correct me if I'm wrong here).

The other thing is that it would be quite nice to just have everything
run through lzjb or zlib or lzo (if that gets added down the road), or
to have selected portions run through that so things like
documentation can get compressed down and take up less disk space.
>From what I understand for lzjb and lzo, you can actually get
increased disk performance since those algorithms are relatively
light-weight and you needn't write as much data to disk.  I've got my
home dir compressed with the default compression style and performance
seems pretty good.

I'm not asking about doing ZFS booting at this point, since I know
that doesn't work.  My question is more related to what can, at this
point, be put on ZFS without breaking the boot process?

On Tue, Mar 4, 2008 at 9:13 AM, Dirk Schelfhout <dirkschelfhout at mac.com> wrote:
> I don't agree
>  ZFS checks the data. A corrupted file backed up with time machine is
>  not seen.
>  But how much does this happen ? ( I mean that data gets corrupted
>  inside a file )
>  Dirk
>
> On 04 Mar 2008, at 16:04, Gautam Godse wrote:
>
>  > The only advantage of ZFS on the Boot/User filesystem would be the
>  > snapshot functionality and I think for now TimeMachine suffices.
>  > Does anyone agree/disagree ?
>
>
>
> _______________________________________________
>  zfs-discuss mailing list
>  zfs-discuss at lists.macosforge.org
>  http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From karl.gyllstrom at gmail.com  Tue Mar  4 08:22:23 2008
From: karl.gyllstrom at gmail.com (Karl Gyllstrom)
Date: Tue, 4 Mar 2008 11:22:23 -0500
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
	ZFS?
In-Reply-To: <4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
Message-ID: <B39279F6-6D40-4943-8FF9-CF7DBC1E0543@gmail.com>

I think the zfs snapshotting system affords an unparalleled backup  
granularity.  Backing up every 5, 30, 60, 1000 minutes doesn't seem to  
make much of a different performance-wise.

The compression is also useful.

On Mar 4, 2008, at 10:04 AM, Gautam Godse wrote:

> Frankly I have yet to be convinced that ZFS should be used for your
> Boot / User file system.
> The advantages of ZFS are only apparent (to me) when ZFS is deployed
> in a redundant configuration (mirror/raid) and for backup, media file
> storage, database storage or something similar.
> The only advantage of ZFS on the Boot/User filesystem would be the
> snapshot functionality and I think for now TimeMachine suffices.
> Does anyone agree/disagree ?
> --
> Gautam
>
> On Mar 4, 2008, at 6:58 AM, James Snyder wrote:
>
>> Hi -
>>
>> I was just thinking today about rolling more of my system over to ZFS
>> since, with some exceptions, it seems to be fairly stable in  
>> usage.  I
>> might Assume that Applications, Developer, maybe usr could switch  
>> over
>> to ZFS, but I'm not exactly sure what portions of the filesystem are
>> required to bring up OS X.  /Library, /System likely need to be on  
>> HFS
>> at this point I would assume, but what are the limitations, what  
>> needs
>> to be on HFS to get things up to the login screen?
>>
>> Another question is:
>> Is it preferred to file bugs by using the Trac system?  Should they  
>> go
>> on zfs-discuss here?  If one has a panic should that just go in
>> through the normal Apple reporting system?  I do not have a full ADC
>> membership so I cannot put anything on rdar as far as I know.
>>
>> Thanks!
>>
>>
>> -- 
>> James Snyder
>> Biomedical Engineering
>> Northwestern University
>> jbsnyder at gmail.com
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From gagix at mac.com  Tue Mar  4 10:11:29 2008
From: gagix at mac.com (Gautam Godse)
Date: Tue, 04 Mar 2008 10:11:29 -0800
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
 ZFS?
In-Reply-To: <7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
	<7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
Message-ID: <47CD90D1.2080901@mac.com>

You said the relevant thing: "For the casual computer user". I think 
majority of us would fall into that category - even among Mac users.
Hence my indifference to using ZFS on Boot/User filesystems.

To illustrate: I have'nt had a single file of any kind that got 
corrupted on my Mac since I started using Mac OSX in  2001. I do have 
backups on DVD for all important files and now TM backups also.. So for 
a casual user ZFS seems overkill on the Boot/User filesystems.

Makes perfect sense on external redundant disk arrays used for Media 
storage/backup.

Brett Ault-McCoy wrote:
> TimeMachine != zfs snapshots.  A full scan of the filesystem has to be 
> done to find what's changed, and any file that's changed (even if only 
> a single byte has changed) has to be copied in full (which is a real 
> pain for bittorrent downloads).  Plus, it's not a single point in time 
> backup.  Files can be changed during the backup resulting 
> in inconsistencies if related files are modified at just the right 
> time.  The other thing ZFS guards against is bit rot.
>
> For the casual users' computer none of that probably matters much 
> (though my old G4 comes to a screeching halt every time TimeMachine 
> starts up), but ZFS has many advantages over HFS+ and TimeMachine.
>
> ++Brett;
>
>
> On Tue, Mar 4, 2008 at 3:04 PM, Gautam Godse <gagix at mac.com 
> <mailto:gagix at mac.com>> wrote:
>
>     Frankly I have yet to be convinced that ZFS should be used for your
>     Boot / User file system.
>     The advantages of ZFS are only apparent (to me) when ZFS is deployed
>     in a redundant configuration (mirror/raid) and for backup, media file
>     storage, database storage or something similar.
>     The only advantage of ZFS on the Boot/User filesystem would be the
>     snapshot functionality and I think for now TimeMachine suffices.
>     Does anyone agree/disagree ?
>     --
>     Gautam
>
>     On Mar 4, 2008, at 6:58 AM, James Snyder wrote:
>
>     > Hi -
>     >
>     > I was just thinking today about rolling more of my system over
>     to ZFS
>     > since, with some exceptions, it seems to be fairly stable in
>     usage.  I
>     > might Assume that Applications, Developer, maybe usr could
>     switch over
>     > to ZFS, but I'm not exactly sure what portions of the filesystem are
>     > required to bring up OS X.  /Library, /System likely need to be
>     on HFS
>     > at this point I would assume, but what are the limitations, what
>     needs
>     > to be on HFS to get things up to the login screen?
>     >
>     > Another question is:
>     > Is it preferred to file bugs by using the Trac system?  Should
>     they go
>     > on zfs-discuss here?  If one has a panic should that just go in
>     > through the normal Apple reporting system?  I do not have a full ADC
>     > membership so I cannot put anything on rdar as far as I know.
>     >
>     > Thanks!
>     >
>     >
>     > --
>     > James Snyder
>     > Biomedical Engineering
>     > Northwestern University
>     > jbsnyder at gmail.com <mailto:jbsnyder at gmail.com>
>     > _______________________________________________
>     > zfs-discuss mailing list
>     > zfs-discuss at lists.macosforge.org
>     <mailto:zfs-discuss at lists.macosforge.org>
>     > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>     _______________________________________________
>     zfs-discuss mailing list
>     zfs-discuss at lists.macosforge.org
>     <mailto:zfs-discuss at lists.macosforge.org>
>     http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080304/5317bc6d/attachment-0001.html 

From jim at netgate.com  Tue Mar  4 10:18:12 2008
From: jim at netgate.com (Jim Thompson)
Date: Tue, 4 Mar 2008 08:18:12 -1000
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
	ZFS?
In-Reply-To: <47CD90D1.2080901@mac.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
	<7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
	<47CD90D1.2080901@mac.com>
Message-ID: <C661213A-322D-42A0-94AE-CBA2F87D1662@netgate.com>


On Mar 4, 2008, at 8:11 AM, Gautam Godse wrote:

> To illustrate: I have'nt had a single file of any kind that got  
> corrupted on my Mac since I started using Mac OSX in  2001.

How do you know?  :-)



From btm at pobox.com  Tue Mar  4 07:25:12 2008
From: btm at pobox.com (Brett Ault-McCoy)
Date: Tue, 4 Mar 2008 15:25:12 +0000
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
	ZFS?
In-Reply-To: <4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
Message-ID: <7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>

TimeMachine != zfs snapshots.  A full scan of the filesystem has to be done
to find what's changed, and any file that's changed (even if only a single
byte has changed) has to be copied in full (which is a real pain for
bittorrent downloads).  Plus, it's not a single point in time backup.  Files
can be changed during the backup resulting in inconsistencies if related
files are modified at just the right time.  The other thing ZFS guards
against is bit rot.
For the casual users' computer none of that probably matters much (though my
old G4 comes to a screeching halt every time TimeMachine starts up), but ZFS
has many advantages over HFS+ and TimeMachine.

++Brett;


On Tue, Mar 4, 2008 at 3:04 PM, Gautam Godse <gagix at mac.com> wrote:

> Frankly I have yet to be convinced that ZFS should be used for your
> Boot / User file system.
> The advantages of ZFS are only apparent (to me) when ZFS is deployed
> in a redundant configuration (mirror/raid) and for backup, media file
> storage, database storage or something similar.
> The only advantage of ZFS on the Boot/User filesystem would be the
> snapshot functionality and I think for now TimeMachine suffices.
> Does anyone agree/disagree ?
> --
> Gautam
>
> On Mar 4, 2008, at 6:58 AM, James Snyder wrote:
>
> > Hi -
> >
> > I was just thinking today about rolling more of my system over to ZFS
> > since, with some exceptions, it seems to be fairly stable in usage.  I
> > might Assume that Applications, Developer, maybe usr could switch over
> > to ZFS, but I'm not exactly sure what portions of the filesystem are
> > required to bring up OS X.  /Library, /System likely need to be on HFS
> > at this point I would assume, but what are the limitations, what needs
> > to be on HFS to get things up to the login screen?
> >
> > Another question is:
> > Is it preferred to file bugs by using the Trac system?  Should they go
> > on zfs-discuss here?  If one has a panic should that just go in
> > through the normal Apple reporting system?  I do not have a full ADC
> > membership so I cannot put anything on rdar as far as I know.
> >
> > Thanks!
> >
> >
> > --
> > James Snyder
> > Biomedical Engineering
> > Northwestern University
> > jbsnyder at gmail.com
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss at lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080304/47cd6917/attachment.html 

From gagix at mac.com  Tue Mar  4 10:32:13 2008
From: gagix at mac.com (Gautam Godse)
Date: Tue, 04 Mar 2008 10:32:13 -0800
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
 ZFS?
In-Reply-To: <C661213A-322D-42A0-94AE-CBA2F87D1662@netgate.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
	<7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
	<47CD90D1.2080901@mac.com>
	<C661213A-322D-42A0-94AE-CBA2F87D1662@netgate.com>
Message-ID: <47CD95AD.70303@mac.com>

Good point :-).
I guess I will not know until I try and access every file to make sure 
it has not been corrupted. and even then I wouldn't. But so far it has 
not made material difference in my day to day operations. Though the 
moment is does, I will know :-)

Jim Thompson wrote:
>
> On Mar 4, 2008, at 8:11 AM, Gautam Godse wrote:
>
>> To illustrate: I have'nt had a single file of any kind that got 
>> corrupted on my Mac since I started using Mac OSX in  2001.
>
> How do you know?  :-)
>
>

From jbsnyder at gmail.com  Tue Mar  4 10:37:10 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Tue, 4 Mar 2008 12:37:10 -0600
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
	ZFS?
In-Reply-To: <47CD90D1.2080901@mac.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
	<7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
	<47CD90D1.2080901@mac.com>
Message-ID: <33644d3c0803041037o31826b2q77e18424d448be7d@mail.gmail.com>

All of this is true, but you can make that argument about any sort of
replacement or upgrade piece of software so long as there isn't
something broken that seriously inhibits daily work.  HFS+ (and
various adjustments) have worked mostly fine since Mac OS 8.1.

Everyone is motivated for differing reasons, however.  If the "us"
(referring to casual users) you refer to are those on this list, I'm
guessing that if one has interest enough to follow what's going on
with a beta/development release filesystem, and perhaps even play with
it, that we're probably not casual users.

The casual Mac user may never even hear about ZFS (how many actually
even know what HFS+ is?), but the end result is that it will, or
could, provide benefit to a wide variety of users on /Users or /.
Snapshots could be taken before software upgrades (and done in a way
that's more robust than Windows restore points), so that for those
rare events where something does blow up, you have something to go
back to that is known to work.  Time Machine backups should be
somewhat smaller when using snapshots that keep track of block level
changes, making these backups faster, including over slower, wireless
links.  You could also have snapshots kept locally so that there's a
local "Time Machine" for today's files or this weeks' files even
though you happen to be away from the drive you're backing up to.
Filesystem compression can reduce the need for even more gigantic
disks on laptops, etc..

On a related note, for a point not brought up, I think one reason why
boot is important over just using /Users is this: deriving storage on
one disk from multiple fixed-sized partitions is probably something
that would be an annoyance for a "casual user."  People can certainly
understand mount points and partitions and whatnot (in a sense, they
do for having external USB drives or added internal storage), but
imagine having to explain to an aunt Sally that she needs to resize
her root partition and shrink her /Users paritition so that she can
install some new piece of software.

But yeah, I do understand your point :-)  If it ain't broke, don't fix it.

On Tue, Mar 4, 2008 at 12:11 PM, Gautam Godse <gagix at mac.com> wrote:
>
>  You said the relevant thing: "For the casual computer user". I think
> majority of us would fall into that category - even among Mac users.
>  Hence my indifference to using ZFS on Boot/User filesystems.
>
>  To illustrate: I have'nt had a single file of any kind that got corrupted
> on my Mac since I started using Mac OSX in  2001. I do have backups on DVD
> for all important files and now TM backups also.. So for a casual user ZFS
> seems overkill on the Boot/User filesystems.
>
>  Makes perfect sense on external redundant disk arrays used for Media
> storage/backup.
>
>  Brett Ault-McCoy wrote:
> TimeMachine != zfs snapshots.  A full scan of the filesystem has to be done
> to find what's changed, and any file that's changed (even if only a single
> byte has changed) has to be copied in full (which is a real pain for
> bittorrent downloads).  Plus, it's not a single point in time backup.  Files
> can be changed during the backup resulting in inconsistencies if related
> files are modified at just the right time.  The other thing ZFS guards
> against is bit rot.
>
>
> For the casual users' computer none of that probably matters much (though my
> old G4 comes to a screeching halt every time TimeMachine starts up), but ZFS
> has many advantages over HFS+ and TimeMachine.
>
>
> ++Brett;
>
>
>
>
>
>
> On Tue, Mar 4, 2008 at 3:04 PM, Gautam Godse <gagix at mac.com> wrote:
>
> > Frankly I have yet to be convinced that ZFS should be used for your
> > Boot / User file system.
> > The advantages of ZFS are only apparent (to me) when ZFS is deployed
> > in a redundant configuration (mirror/raid) and for backup, media file
> > storage, database storage or something similar.
> > The only advantage of ZFS on the Boot/User filesystem would be the
> > snapshot functionality and I think for now TimeMachine suffices.
> > Does anyone agree/disagree ?
> > --
> > Gautam
> >
> >
> >
> > On Mar 4, 2008, at 6:58 AM, James Snyder wrote:
> >
> > > Hi -
> > >
> > > I was just thinking today about rolling more of my system over to ZFS
> > > since, with some exceptions, it seems to be fairly stable in usage.  I
> > > might Assume that Applications, Developer, maybe usr could switch over
> > > to ZFS, but I'm not exactly sure what portions of the filesystem are
> > > required to bring up OS X.  /Library, /System likely need to be on HFS
> > > at this point I would assume, but what are the limitations, what needs
> > > to be on HFS to get things up to the login screen?
> > >
> > > Another question is:
> > > Is it preferred to file bugs by using the Trac system?  Should they go
> > > on zfs-discuss here?  If one has a panic should that just go in
> > > through the normal Apple reporting system?  I do not have a full ADC
> > > membership so I cannot put anything on rdar as far as I know.
> > >
> > > Thanks!
> > >
> > >
> > > --
> > > James Snyder
> > > Biomedical Engineering
> > > Northwestern University
> > > jbsnyder at gmail.com
> > > _______________________________________________
> > > zfs-discuss mailing list
> > > zfs-discuss at lists.macosforge.org
> > > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss at lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >
>
>
> _______________________________________________
>  zfs-discuss mailing list
>  zfs-discuss at lists.macosforge.org
>  http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From ndellofano at apple.com  Tue Mar  4 10:37:01 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue, 4 Mar 2008 10:37:01 -0800
Subject: [zfs-discuss] zfs system/kernel panics
In-Reply-To: <33BA4ACA-2C39-4E51-ADE9-5CE71330EE45@sun.com>
References: <33BA4ACA-2C39-4E51-ADE9-5CE71330EE45@sun.com>
Message-ID: <3C443790-5161-4A87-B6D5-B60D4AD627B5@apple.com>

Thanks Bill!

Yeah so this has been seen twice before.  Currently tracking it via
<rdar://problem/5694099> Assertion failure: zap_hash(zap, name) ==  
hash failed, 0 == 0,

We haven't narrowed it to a specific use case yet.  You're details  
below are helpful though, we'll try and emulate a higher load.

thanks!
Noel

On Mar 2, 2008, at 4:17 PM, Bill Winnett wrote:

> Hi,
>
>    I have experienced a few of these panics over the last few
> weeks.    I
>    don't know if this tells you much, but here is the info.
>    ---
>    Sun Mar  2 18:55:15 2008
>    panic(cpu 1 caller 0x00CF0A41): "zap_hash(zap, name) == hash
> failed, 0
>    == 0"@/Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/
>    zfs_kext/zfs/zap_micro.c:117
>    Backtrace, Format - Frame : Return Address (4 potential args on
> stack)
>    0x666736f8 : 0x12b0e1 (0x457024 0x6667372c 0x13321a 0x0)
>    0x66673748 : 0xcf0a41 (0xd57470 0xd57454 0x0 0xd2ec90)
>    0x666737f8 : 0xcf1578 (0x0 0x25a872d0 0x0 0x0)
>    0x66673858 : 0xd1a3d2 (0x80c2d70 0x4ced9 0x0 0xf8de700)
>    0x666738d8 : 0xd1a60b (0x6667390c 0x65932498 0x66673950 0x66673908)
>    0x66673928 : 0xcb7dc7 (0x65932498 0x66673950 0x66673df4 0x1f0d72)
>    0x666739a8 : 0x1f1b5f (0x666739c8 0x2 0x666739f8 0x44dc07)
>    0x666739f8 : 0x1d36f9 (0x963ad00 0x66673df4 0x66673f08 0x7aca104)
>    0x66673a88 : 0x1d447e (0x66673ddc 0x100 0x66673dfc 0x0)
>    0x66673b48 : 0x1e2ff0 (0x66673ddc 0x0 0x0 0x0)
>    0x66673d88 : 0x1e35e4 (0xb0523e98 0x0 0x0 0x0)
>    0x66673f48 : 0x1e367d (0xb0523e98 0x0 0x0 0x0)
>    0x66673f78 : 0x3dbe77 (0x7423c50 0x7aca000 0x7aca044 0x7dab7d8)
>    0x66673fc8 : 0x19f084 (0x7da3880 0x0 0x1a20b5 0x740f8b8)
>
> - Ignored:
>    No mapping exists for frame pointer
>    Backtrace terminated-invalid frame pointer 0xb0523f78
>           Kernel loadable modules in backtrace (with dependencies):
>              com.apple.filesystems.zfs(8.0)@0xcb4000->0xd7ffff
>
>    BSD process name corresponding to current thread: fseventsd
>
>    Mac OS version:
>    9C31
>
>    Kernel version:
>    Darwin Kernel Version 9.2.0: Tue Feb  5 16:13:22 PST 2008;
>    root:xnu-1228.3.13~1/RELEASE_I386
>    System model name: iMac7,1 (Mac-F4238CC8)
>
>
>    --
>
>    Model: iMac7,1, BootROM IM71.007A.B00, 2 processors, Intel Core 2
> Duo,
>    2.4 GHz, 4 GB
>    Graphics: kHW_ATIr600M76Item, ATI,RadeonHD2600,
>    spdisplays_pcie_device, 256 MB
>    Memory Module: BANK 0/DIMM0, 2 GB, DDR2 SDRAM, 667 MHz
>    Memory Module: BANK 1/DIMM1, 2 GB, DDR2 SDRAM, 667 MHz
>    AirPort: spairport_wireless_card_type_airport_extreme (0x14E4,
> 0x88),
>    Broadcom BCM43xx 1.0 (4.170.46.3)
>    Bluetooth: Version 2.1.0f14, 2 service, 1 devices, 1 incoming  
> serial
>    ports
>    Network Service: AirPort, AirPort, en1
>    Network Service: Parallels Host-Guest, Ethernet, en2
>    Network Service: Parallels NAT, Ethernet, en3
>    Serial ATA Device: WDC WD3200AAJS-40RYA0, 298.09 GB
>    Parallel ATA Device: PIONEER DVD-RW  DVR-K06A
>    USB Device: Keyboard Hub, Apple, Inc., high_speed, 500 mA
>    USB Device: USB-PS/2 Trackball, Logitech, low_speed, 100 mA
>    USB Device: Apple Keyboard, Apple, Inc, low_speed, 100 mA
>    USB Device: Built-in iSight, Apple Inc., high_speed, 500 mA
>    USB Device: Bluetooth USB Host Controller, Apple, Inc., full_speed,
>    500 mA
>    USB Device: IR Receiver, Apple Computer, Inc., low_speed, 500 mA
>    FireWire Device: 1394 Storage Front Panel*, Maxtor, 800mbit_speed
>    FireWire Device: 1394 Storage Front Panel*, Maxtor, 800mbit_speed
>    FireWire Device: 1394 Storage Front Panel*, Maxtor, 800mbit_speed
>    FireWire Device: 1394 Storage Front Panel*, Maxtor, 800mbit_speed
>
>
>
>
>
>
>    It seems to be when the zfs volume is in high use.  Writes from a
>    networked xp system to a shared directory and writes from a
> Parallels
>    process to a private directory.    Reads and writes from a Solaris
> system
>    to a shared directory.   The zfs volume is composed of 4 1tb
> firewire
>    disks.
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From jim at netgate.com  Tue Mar  4 10:45:43 2008
From: jim at netgate.com (Jim Thompson)
Date: Tue, 4 Mar 2008 08:45:43 -1000
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
	ZFS?
In-Reply-To: <7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
	<7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
Message-ID: <811351A0-5021-455B-9EEC-021E9455F357@netgate.com>


On Mar 4, 2008, at 5:25 AM, Brett Ault-McCoy wrote:

> TimeMachine != zfs snapshots.  A full scan of the filesystem has to  
> be done to find what's changed,

Well,... no.   Time Machine leverages the FSEvents framework.    It  
does have to do a full scan within a directory that is reported to  
have changed, but this does not equate to a "full scan of the  
filesystem", except in the degenerate case.




From eric at hulteen.com  Tue Mar  4 11:05:51 2008
From: eric at hulteen.com (Eric A Hulteen)
Date: Tue, 04 Mar 2008 11:05:51 -0800
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put
	on	ZFS?
In-Reply-To: <47CD95AD.70303@mac.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
	<7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
	<47CD90D1.2080901@mac.com>
	<C661213A-322D-42A0-94AE-CBA2F87D1662@netgate.com>
	<47CD95AD.70303@mac.com>
Message-ID: <20080304110551.czmap08zvo80o4gk@webmail.lmi.net>

   I would argue that if you haven't seen data corruption then you're  
not looking for it.  I have 28 disk drives, most of which are used for  
backups (I do backups for myself, family, and friends).  Every time I  
copy a file from one drive to the other I cksum both the data and  
resource forks (comparing the source to the copy).  I find data  
corruption errors about once a month (once every half-million files?).  
  These drives are from multiple vendors and are multiple sizes (80 GB  
to 1 TB) and multiple interfaces (PATA and SATA) copied from multiple  
computers (iMac, PowerBook, MacBook Pro) using multiple OSs (10.3,  
10.4, and 10.5).

Regards,
Eric

Quoting Gautam Godse <gagix at mac.com>:

> Good point :-).
> I guess I will not know until I try and access every file to make sure
> it has not been corrupted. and even then I wouldn't. But so far it has
> not made material difference in my day to day operations. Though the
> moment is does, I will know :-)
>
> Jim Thompson wrote:
>>
>> On Mar 4, 2008, at 8:11 AM, Gautam Godse wrote:
>>
>>> To illustrate: I have'nt had a single file of any kind that got
>>> corrupted on my Mac since I started using Mac OSX in  2001.
>>
>> How do you know?  :-)
>>
>>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>





From Jonathan.Edwards at Sun.COM  Tue Mar  4 11:06:03 2008
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Tue, 04 Mar 2008 14:06:03 -0500
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put
	on	ZFS?
In-Reply-To: <811351A0-5021-455B-9EEC-021E9455F357@netgate.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
	<7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
	<811351A0-5021-455B-9EEC-021E9455F357@netgate.com>
Message-ID: <C5DA3700-9C08-4F42-95CD-34E5F92AC92C@sun.com>


On Mar 4, 2008, at 1:45 PM, Jim Thompson wrote:
>
> On Mar 4, 2008, at 5:25 AM, Brett Ault-McCoy wrote:
>
>> TimeMachine != zfs snapshots.  A full scan of the filesystem has to
>> be done to find what's changed,
>
> Well,... no.   Time Machine leverages the FSEvents framework.    It
> does have to do a full scan within a directory that is reported to
> have changed, but this does not equate to a "full scan of the
> filesystem", except in the degenerate case.
>

i think the broader point is that with the fseventd framework you're  
essentially getting incremental changes since the last full backup and  
the changes are being tracked and copied as work might be ongoing -  
this is typically fine for a simple backup strategy .. but what you're  
not getting is a point-in-time snapshot with copy-on-write  
functionality for checkpointing a filesystem like you'd get with ZFS -  
this would more advantageous for guaranteeing the atomicity of updates  
at a particular state (ie: lockfs and then snap, or making sure you've  
got database consistency)

(note: with ZFS you'll still have to clone or send after you snapshot  
to get it onto a separate pool or volume)

---
.je

From btm at pobox.com  Tue Mar  4 14:53:20 2008
From: btm at pobox.com (Brett Ault-McCoy)
Date: Tue, 4 Mar 2008 22:53:20 +0000
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
	ZFS?
In-Reply-To: <811351A0-5021-455B-9EEC-021E9455F357@netgate.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
	<7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
	<811351A0-5021-455B-9EEC-021E9455F357@netgate.com>
Message-ID: <7bccd8dc0803041453h60537b3bia50b840705d32a65@mail.gmail.com>

All I know is that a 'zfs snapshot -r ZFSPool' on my 733MHz G4 takes <1
second but when TimeMachine starts up cpu usage goes to 100% for several
minutes and the machine becomes completely unusable.
Definitely *not* the same.

++Brett;


On Tue, Mar 4, 2008 at 6:45 PM, Jim Thompson <jim at netgate.com> wrote:

>
> On Mar 4, 2008, at 5:25 AM, Brett Ault-McCoy wrote:
>
> > TimeMachine != zfs snapshots.  A full scan of the filesystem has to
> > be done to find what's changed,
>
> Well,... no.   Time Machine leverages the FSEvents framework.    It
> does have to do a full scan within a directory that is reported to
> have changed, but this does not equate to a "full scan of the
> filesystem", except in the degenerate case.
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080304/81b6dd67/attachment.html 

From btm at pobox.com  Tue Mar  4 14:57:16 2008
From: btm at pobox.com (Brett Ault-McCoy)
Date: Tue, 4 Mar 2008 22:57:16 +0000
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
	ZFS?
In-Reply-To: <47CD90D1.2080901@mac.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
	<7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
	<47CD90D1.2080901@mac.com>
Message-ID: <7bccd8dc0803041457j1d4d2aa7m89e21c6b963f8d76@mail.gmail.com>

On Tue, Mar 4, 2008 at 6:11 PM, Gautam Godse <gagix at mac.com> wrote:

> To illustrate: I have'nt had a single file of any kind that got corrupted
> on my Mac since I started using Mac OSX in  2001. I do have backups on DVD
> for all important files and now TM backups also.. So for a casual user ZFS
> seems overkill on the Boot/User filesystems.
>

I've experienced a lot of bit rot on image files over the years.  I don't
necessarily think they are more succeptable, but it shows up more.  Even
most executable files won't necessarily notice a single bit being flipped at
random, but most image decompressors will fail spectacularly with high
probability on a random bit flip.

If you have ay significant quantity of image files try browsing through
them.  I'll bet you find some that are broken.

++Brett;
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080304/ffded9fe/attachment.html 

From bplist at thinkpink.com  Tue Mar  4 15:18:08 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Tue, 4 Mar 2008 15:18:08 -0800
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
	ZFS?
In-Reply-To: <7bccd8dc0803041453h60537b3bia50b840705d32a65@mail.gmail.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
	<7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
	<811351A0-5021-455B-9EEC-021E9455F357@netgate.com>
	<7bccd8dc0803041453h60537b3bia50b840705d32a65@mail.gmail.com>
Message-ID: <156CAB3C-6C36-40A9-9B3E-ECCDAE4C8035@thinkpink.com>

Yeah, I second that.  I love the idea of TimeMachine, but in my  
experience it's got a ways to go before it's ready for prime-time, and  
even further before I'll trust it as my primary backup solution.  As I  
set here at 3:15, TimeMachine has been "Finishing Backup" on one of  
its hourlies for nearly 8 hours.  Heck, I could have done a leisurely  
full backup of the 100GB or so in half that time!

bri


On Mar 4, 2008, at 2:53 PM, Brett Ault-McCoy wrote:

> All I know is that a 'zfs snapshot -r ZFSPool' on my 733MHz G4 takes  
> <1 second but when TimeMachine starts up cpu usage goes to 100% for  
> several minutes and the machine becomes completely unusable.
>
> Definitely *not* the same.
>
> ++Brett;
>
>
> On Tue, Mar 4, 2008 at 6:45 PM, Jim Thompson <jim at netgate.com> wrote:
>
> On Mar 4, 2008, at 5:25 AM, Brett Ault-McCoy wrote:
>
> > TimeMachine != zfs snapshots.  A full scan of the filesystem has to
> > be done to find what's changed,
>
> Well,... no.   Time Machine leverages the FSEvents framework.    It
> does have to do a full scan within a directory that is reported to
> have changed, but this does not equate to a "full scan of the
> filesystem", except in the degenerate case.
>
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From jim at netgate.com  Tue Mar  4 18:08:52 2008
From: jim at netgate.com (Jim Thompson)
Date: Tue, 4 Mar 2008 16:08:52 -1000
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
	ZFS?
In-Reply-To: <156CAB3C-6C36-40A9-9B3E-ECCDAE4C8035@thinkpink.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
	<7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
	<811351A0-5021-455B-9EEC-021E9455F357@netgate.com>
	<7bccd8dc0803041453h60537b3bia50b840705d32a65@mail.gmail.com>
	<156CAB3C-6C36-40A9-9B3E-ECCDAE4C8035@thinkpink.com>
Message-ID: <C6EF145C-CC95-462E-8BB0-AA9EE9FF4B99@netgate.com>

Sounds like you have a ton of small files. link(2) is syncronous.

All ZFS has to do for a snapshot is Inc the generation count.



On Mar 4, 2008, at 1:18 PM, Brian Pinkerton <bplist at thinkpink.com>  
wrote:

> Yeah, I second that.  I love the idea of TimeMachine, but in my
> experience it's got a ways to go before it's ready for prime-time, and
> even further before I'll trust it as my primary backup solution.  As I
> set here at 3:15, TimeMachine has been "Finishing Backup" on one of
> its hourlies for nearly 8 hours.  Heck, I could have done a leisurely
> full backup of the 100GB or so in half that time!
>
> bri
>
>
> On Mar 4, 2008, at 2:53 PM, Brett Ault-McCoy wrote:
>
>> All I know is that a 'zfs snapshot -r ZFSPool' on my 733MHz G4 takes
>> <1 second but when TimeMachine starts up cpu usage goes to 100% for
>> several minutes and the machine becomes completely unusable.
>>
>> Definitely *not* the same.
>>
>> ++Brett;
>>
>>
>> On Tue, Mar 4, 2008 at 6:45 PM, Jim Thompson <jim at netgate.com> wrote:
>>
>> On Mar 4, 2008, at 5:25 AM, Brett Ault-McCoy wrote:
>>
>>> TimeMachine != zfs snapshots.  A full scan of the filesystem has to
>>> be done to find what's changed,
>>
>> Well,... no.   Time Machine leverages the FSEvents framework.    It
>> does have to do a full scan within a directory that is reported to
>> have changed, but this does not equate to a "full scan of the
>> filesystem", except in the degenerate case.
>>
>>
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From btm at pobox.com  Wed Mar  5 02:06:36 2008
From: btm at pobox.com (Brett Ault-McCoy)
Date: Wed, 5 Mar 2008 10:06:36 +0000
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
	ZFS?
In-Reply-To: <C6EF145C-CC95-462E-8BB0-AA9EE9FF4B99@netgate.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
	<7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
	<811351A0-5021-455B-9EEC-021E9455F357@netgate.com>
	<7bccd8dc0803041453h60537b3bia50b840705d32a65@mail.gmail.com>
	<156CAB3C-6C36-40A9-9B3E-ECCDAE4C8035@thinkpink.com>
	<C6EF145C-CC95-462E-8BB0-AA9EE9FF4B99@netgate.com>
Message-ID: <7bccd8dc0803050206o2cd31b5cg60afbcba3707cf33@mail.gmail.com>

/Library + /System on my machine have 200K files.  A good argument for ZFS
snapshots and against TimeMachine.
++Brett;


On Wed, Mar 5, 2008 at 2:08 AM, Jim Thompson <jim at netgate.com> wrote:

> Sounds like you have a ton of small files. link(2) is syncronous.
>
> All ZFS has to do for a snapshot is Inc the generation count.
>
>
>
> On Mar 4, 2008, at 1:18 PM, Brian Pinkerton <bplist at thinkpink.com>
> wrote:
>
> > Yeah, I second that.  I love the idea of TimeMachine, but in my
> > experience it's got a ways to go before it's ready for prime-time, and
> > even further before I'll trust it as my primary backup solution.  As I
> > set here at 3:15, TimeMachine has been "Finishing Backup" on one of
> > its hourlies for nearly 8 hours.  Heck, I could have done a leisurely
> > full backup of the 100GB or so in half that time!
> >
> > bri
> >
> >
> > On Mar 4, 2008, at 2:53 PM, Brett Ault-McCoy wrote:
> >
> >> All I know is that a 'zfs snapshot -r ZFSPool' on my 733MHz G4 takes
> >> <1 second but when TimeMachine starts up cpu usage goes to 100% for
> >> several minutes and the machine becomes completely unusable.
> >>
> >> Definitely *not* the same.
> >>
> >> ++Brett;
> >>
> >>
> >> On Tue, Mar 4, 2008 at 6:45 PM, Jim Thompson <jim at netgate.com> wrote:
> >>
> >> On Mar 4, 2008, at 5:25 AM, Brett Ault-McCoy wrote:
> >>
> >>> TimeMachine != zfs snapshots.  A full scan of the filesystem has to
> >>> be done to find what's changed,
> >>
> >> Well,... no.   Time Machine leverages the FSEvents framework.    It
> >> does have to do a full scan within a directory that is reported to
> >> have changed, but this does not equate to a "full scan of the
> >> filesystem", except in the degenerate case.
> >>
> >>
> >>
> >>
> >> _______________________________________________
> >> zfs-discuss mailing list
> >> zfs-discuss at lists.macosforge.org
> >> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss at lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080305/bfdcb199/attachment-0001.html 

From lists at loveturtle.net  Wed Mar  5 09:15:13 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Wed, 05 Mar 2008 12:15:13 -0500
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
 ZFS?
In-Reply-To: <4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
Message-ID: <47CED521.8070508@loveturtle.net>

Gautam Godse wrote:
> Frankly I have yet to be convinced that ZFS should be used for your  
> Boot / User file system.
> The advantages of ZFS are only apparent (to me) when ZFS is deployed  
> in a redundant configuration (mirror/raid) 
I agree, mirror/raid is where ZFS shines.

What does that have to do with not being convinced that / should be ZFS? 
What's wrong with a mirror/raid /?

# uname -sr
FreeBSD 7.0-RC1

# df -h /
Filesystem    Size    Used   Avail Capacity  Mounted on
zroot         984M    235M    749M    24%    /

# zpool list
NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
universe               2.25T    389G   1.87T    16%  ONLINE     -
zroot                  1016M    235M    781M    23%  ONLINE     -

# zpool status zroot
  pool: zroot
 state: ONLINE
 scrub: none requested
config:

        NAME         STATE     READ WRITE CKSUM
        zroot        ONLINE       0     0     0
          mirror     ONLINE       0     0     0
            ad20s1d  ONLINE       0     0     0
            ad22s1d  ONLINE       0     0     0

errors: No known data errors


Sounds like a plan to me :-) I'd love to do this on OSX :-)

As for the question that started this conversation..

I wouldn't put too much on a zpool for now. You have to keep in mind 
that everything required for the system to start & for the zpool to 
import needs to be on the HFS+ partition/drive.

The zpool doesn't seem to import until rather late in the boot process 
so I wouldn't try to put too much of the system on your zpool because it 
will probably just cause more of a headache than it's worth. I just 
stick to keeping my home dir on ZFS.

From bplist at thinkpink.com  Wed Mar  5 09:17:49 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Wed, 5 Mar 2008 09:17:49 -0800
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
	ZFS?
In-Reply-To: <C6EF145C-CC95-462E-8BB0-AA9EE9FF4B99@netgate.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
	<7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
	<811351A0-5021-455B-9EEC-021E9455F357@netgate.com>
	<7bccd8dc0803041453h60537b3bia50b840705d32a65@mail.gmail.com>
	<156CAB3C-6C36-40A9-9B3E-ECCDAE4C8035@thinkpink.com>
	<C6EF145C-CC95-462E-8BB0-AA9EE9FF4B99@netgate.com>
Message-ID: <3684E310-97D9-47A0-ABCD-C3AD3BB5F8E7@thinkpink.com>

Disclaimer: I have no idea how Time Machine works.  I'm backing up to  
an AFP volume (which is actually a ZFS volume, as it happens), so how  
could TM be doing anything involving link(2)?  I do have a lot of  
small files on my disk (around 2 million), though the average file  
size is a respectable 204K.

bri


On Mar 4, 2008, at 6:08 PM, Jim Thompson wrote:

> Sounds like you have a ton of small files. link(2) is syncronous.
>
> All ZFS has to do for a snapshot is Inc the generation count.
>
>
>
> On Mar 4, 2008, at 1:18 PM, Brian Pinkerton <bplist at thinkpink.com>  
> wrote:
>
>> Yeah, I second that.  I love the idea of TimeMachine, but in my
>> experience it's got a ways to go before it's ready for prime-time,  
>> and
>> even further before I'll trust it as my primary backup solution.   
>> As I
>> set here at 3:15, TimeMachine has been "Finishing Backup" on one of
>> its hourlies for nearly 8 hours.  Heck, I could have done a leisurely
>> full backup of the 100GB or so in half that time!
>>
>> bri
>>
>>
>> On Mar 4, 2008, at 2:53 PM, Brett Ault-McCoy wrote:
>>
>>> All I know is that a 'zfs snapshot -r ZFSPool' on my 733MHz G4 takes
>>> <1 second but when TimeMachine starts up cpu usage goes to 100% for
>>> several minutes and the machine becomes completely unusable.
>>>
>>> Definitely *not* the same.
>>>
>>> ++Brett;
>>>
>>>
>>> On Tue, Mar 4, 2008 at 6:45 PM, Jim Thompson <jim at netgate.com>  
>>> wrote:
>>>
>>> On Mar 4, 2008, at 5:25 AM, Brett Ault-McCoy wrote:
>>>
>>>> TimeMachine != zfs snapshots.  A full scan of the filesystem has to
>>>> be done to find what's changed,
>>>
>>> Well,... no.   Time Machine leverages the FSEvents framework.    It
>>> does have to do a full scan within a directory that is reported to
>>> have changed, but this does not equate to a "full scan of the
>>> filesystem", except in the degenerate case.
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From jbsnyder at gmail.com  Wed Mar  5 11:17:23 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Wed, 5 Mar 2008 13:17:23 -0600
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
	ZFS?
In-Reply-To: <3684E310-97D9-47A0-ABCD-C3AD3BB5F8E7@thinkpink.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
	<7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
	<811351A0-5021-455B-9EEC-021E9455F357@netgate.com>
	<7bccd8dc0803041453h60537b3bia50b840705d32a65@mail.gmail.com>
	<156CAB3C-6C36-40A9-9B3E-ECCDAE4C8035@thinkpink.com>
	<C6EF145C-CC95-462E-8BB0-AA9EE9FF4B99@netgate.com>
	<3684E310-97D9-47A0-ABCD-C3AD3BB5F8E7@thinkpink.com>
Message-ID: <33644d3c0803051117l14bf6215t26b436ad00c735ed@mail.gmail.com>

This is my understanding of the matter, and so take with the caveat.
I'm sure someone will correct me if this is not the case.

It involves link(2) because your remote afp backups on time machine
are in sparse bundles on that remote host.  The link(2) isn't taking
place over AFP, but rather within that image. If the drive is local,
it doesn't need this sparse bundle container and just does link(2)
directly on the HFS+ volume.  I also am under the understanding
(correct me if wrong), that the link(2) implementation under HFS+ was
specifically added for TimeMachine.  This makes it so that when a
backup is updated you can have multiple simultaneous trees of files
that all look like a full filesystem, but data doesn't have to be
duplicated in each "snapshot" because of the hard links. (I've also
heard that the hard linking on HFS+ is a little of a hack, and that
Apple has limited access to it through the normal system commands in
accordance with this).  This is nice and elegant in some ways, but
it's not nearly as nice as zfs snapshots because you still have to
maintain a huge ball of hard links sitting on top of the filesystem.
Since it has to deal with hundreds of thousands or millions of files
in this manner, it can take some time for Time Machine to work with
all that :-)

This approach also has the downside that it isn't efficient for
dealing with changes in gigantic files (like vmware images).  I've not
checked into time machine to see if it handles things any more
efficiently than I would assume, but if 2MB change in a VMware image,
you have to retransfer the whole file.  Even if you use something like
rsync on both ends, you still have to let it traverse the whole file
and compute checksums to see which portions need to be transferred.

On Wed, Mar 5, 2008 at 11:17 AM, Brian Pinkerton <bplist at thinkpink.com> wrote:
> Disclaimer: I have no idea how Time Machine works.  I'm backing up to
>  an AFP volume (which is actually a ZFS volume, as it happens), so how
>  could TM be doing anything involving link(2)?  I do have a lot of
>  small files on my disk (around 2 million), though the average file
>  size is a respectable 204K.
>
>  bri
>
>
>
>
>  On Mar 4, 2008, at 6:08 PM, Jim Thompson wrote:
>
>  > Sounds like you have a ton of small files. link(2) is syncronous.
>  >
>  > All ZFS has to do for a snapshot is Inc the generation count.
>  >
>  >
>  >
>  > On Mar 4, 2008, at 1:18 PM, Brian Pinkerton <bplist at thinkpink.com>
>  > wrote:
>  >
>  >> Yeah, I second that.  I love the idea of TimeMachine, but in my
>  >> experience it's got a ways to go before it's ready for prime-time,
>  >> and
>  >> even further before I'll trust it as my primary backup solution.
>  >> As I
>  >> set here at 3:15, TimeMachine has been "Finishing Backup" on one of
>  >> its hourlies for nearly 8 hours.  Heck, I could have done a leisurely
>  >> full backup of the 100GB or so in half that time!
>  >>
>  >> bri
>  >>
>  >>
>  >> On Mar 4, 2008, at 2:53 PM, Brett Ault-McCoy wrote:
>  >>
>  >>> All I know is that a 'zfs snapshot -r ZFSPool' on my 733MHz G4 takes
>  >>> <1 second but when TimeMachine starts up cpu usage goes to 100% for
>  >>> several minutes and the machine becomes completely unusable.
>  >>>
>  >>> Definitely *not* the same.
>  >>>
>  >>> ++Brett;
>  >>>
>  >>>
>  >>> On Tue, Mar 4, 2008 at 6:45 PM, Jim Thompson <jim at netgate.com>
>  >>> wrote:
>  >>>
>  >>> On Mar 4, 2008, at 5:25 AM, Brett Ault-McCoy wrote:
>  >>>
>  >>>> TimeMachine != zfs snapshots.  A full scan of the filesystem has to
>  >>>> be done to find what's changed,
>  >>>
>  >>> Well,... no.   Time Machine leverages the FSEvents framework.    It
>  >>> does have to do a full scan within a directory that is reported to
>  >>> have changed, but this does not equate to a "full scan of the
>  >>> filesystem", except in the degenerate case.
>  >>>
>  >>>
>  >>>
>  >>>
>  >>> _______________________________________________
>  >>> zfs-discuss mailing list
>  >>> zfs-discuss at lists.macosforge.org
>  >>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>  >>
>  >> _______________________________________________
>  >> zfs-discuss mailing list
>  >> zfs-discuss at lists.macosforge.org
>  >> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>  _______________________________________________
>  zfs-discuss mailing list
>  zfs-discuss at lists.macosforge.org
>  http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From jbsnyder at gmail.com  Wed Mar  5 14:24:08 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Wed, 5 Mar 2008 16:24:08 -0600
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
	ZFS?
In-Reply-To: <33644d3c0803051117l14bf6215t26b436ad00c735ed@mail.gmail.com>
References: <33644d3c0803040658v126bbf4h54eff5b503cf4aa6@mail.gmail.com>
	<4F1312F9-E498-4A7E-B1FD-89AD0A6AB210@mac.com>
	<7bccd8dc0803040725i7a3e2061o8942cd9bfc4a1960@mail.gmail.com>
	<811351A0-5021-455B-9EEC-021E9455F357@netgate.com>
	<7bccd8dc0803041453h60537b3bia50b840705d32a65@mail.gmail.com>
	<156CAB3C-6C36-40A9-9B3E-ECCDAE4C8035@thinkpink.com>
	<C6EF145C-CC95-462E-8BB0-AA9EE9FF4B99@netgate.com>
	<3684E310-97D9-47A0-ABCD-C3AD3BB5F8E7@thinkpink.com>
	<33644d3c0803051117l14bf6215t26b436ad00c735ed@mail.gmail.com>
Message-ID: <33644d3c0803051424w31aa21dcl618596e8622e7dd6@mail.gmail.com>

Here's some discussion on Time Machine and hard links at Ars Technica:

http://arstechnica.com/reviews/os/mac-os-x-10-5.ars/14

On Wed, Mar 5, 2008 at 1:17 PM, James Snyder <jbsnyder at gmail.com> wrote:
> This is my understanding of the matter, and so take with the caveat.
>  I'm sure someone will correct me if this is not the case.
>
>  It involves link(2) because your remote afp backups on time machine
>  are in sparse bundles on that remote host.  The link(2) isn't taking
>  place over AFP, but rather within that image. If the drive is local,
>  it doesn't need this sparse bundle container and just does link(2)
>  directly on the HFS+ volume.  I also am under the understanding
>  (correct me if wrong), that the link(2) implementation under HFS+ was
>  specifically added for TimeMachine.  This makes it so that when a
>  backup is updated you can have multiple simultaneous trees of files
>  that all look like a full filesystem, but data doesn't have to be
>  duplicated in each "snapshot" because of the hard links. (I've also
>  heard that the hard linking on HFS+ is a little of a hack, and that
>  Apple has limited access to it through the normal system commands in
>  accordance with this).  This is nice and elegant in some ways, but
>  it's not nearly as nice as zfs snapshots because you still have to
>  maintain a huge ball of hard links sitting on top of the filesystem.
>  Since it has to deal with hundreds of thousands or millions of files
>  in this manner, it can take some time for Time Machine to work with
>  all that :-)
>
>  This approach also has the downside that it isn't efficient for
>  dealing with changes in gigantic files (like vmware images).  I've not
>  checked into time machine to see if it handles things any more
>  efficiently than I would assume, but if 2MB change in a VMware image,
>  you have to retransfer the whole file.  Even if you use something like
>  rsync on both ends, you still have to let it traverse the whole file
>  and compute checksums to see which portions need to be transferred.
>
>
>
>  On Wed, Mar 5, 2008 at 11:17 AM, Brian Pinkerton <bplist at thinkpink.com> wrote:
>  > Disclaimer: I have no idea how Time Machine works.  I'm backing up to
>  >  an AFP volume (which is actually a ZFS volume, as it happens), so how
>  >  could TM be doing anything involving link(2)?  I do have a lot of
>  >  small files on my disk (around 2 million), though the average file
>  >  size is a respectable 204K.
>  >
>  >  bri
>  >
>  >
>  >
>  >
>  >  On Mar 4, 2008, at 6:08 PM, Jim Thompson wrote:
>  >
>  >  > Sounds like you have a ton of small files. link(2) is syncronous.
>  >  >
>  >  > All ZFS has to do for a snapshot is Inc the generation count.
>  >  >
>  >  >
>  >  >
>  >  > On Mar 4, 2008, at 1:18 PM, Brian Pinkerton <bplist at thinkpink.com>
>  >  > wrote:
>  >  >
>  >  >> Yeah, I second that.  I love the idea of TimeMachine, but in my
>  >  >> experience it's got a ways to go before it's ready for prime-time,
>  >  >> and
>  >  >> even further before I'll trust it as my primary backup solution.
>  >  >> As I
>  >  >> set here at 3:15, TimeMachine has been "Finishing Backup" on one of
>  >  >> its hourlies for nearly 8 hours.  Heck, I could have done a leisurely
>  >  >> full backup of the 100GB or so in half that time!
>  >  >>
>  >  >> bri
>  >  >>
>  >  >>
>  >  >> On Mar 4, 2008, at 2:53 PM, Brett Ault-McCoy wrote:
>  >  >>
>  >  >>> All I know is that a 'zfs snapshot -r ZFSPool' on my 733MHz G4 takes
>  >  >>> <1 second but when TimeMachine starts up cpu usage goes to 100% for
>  >  >>> several minutes and the machine becomes completely unusable.
>  >  >>>
>  >  >>> Definitely *not* the same.
>  >  >>>
>  >  >>> ++Brett;
>  >  >>>
>  >  >>>
>  >  >>> On Tue, Mar 4, 2008 at 6:45 PM, Jim Thompson <jim at netgate.com>
>  >  >>> wrote:
>  >  >>>
>  >  >>> On Mar 4, 2008, at 5:25 AM, Brett Ault-McCoy wrote:
>  >  >>>
>  >  >>>> TimeMachine != zfs snapshots.  A full scan of the filesystem has to
>  >  >>>> be done to find what's changed,
>  >  >>>
>  >  >>> Well,... no.   Time Machine leverages the FSEvents framework.    It
>  >  >>> does have to do a full scan within a directory that is reported to
>  >  >>> have changed, but this does not equate to a "full scan of the
>  >  >>> filesystem", except in the degenerate case.
>  >  >>>
>  >  >>>
>  >  >>>
>  >  >>>
>  >  >>> _______________________________________________
>  >  >>> zfs-discuss mailing list
>  >  >>> zfs-discuss at lists.macosforge.org
>  >  >>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>  >  >>
>  >  >> _______________________________________________
>  >  >> zfs-discuss mailing list
>  >  >> zfs-discuss at lists.macosforge.org
>  >  >> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>  >
>  >  _______________________________________________
>  >  zfs-discuss mailing list
>  >  zfs-discuss at lists.macosforge.org
>  >  http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>  >
>
>
>
>  --
>
>
> James Snyder
>  Biomedical Engineering
>  Northwestern University
>  jbsnyder at gmail.com
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From btm at pobox.com  Thu Mar  6 04:39:12 2008
From: btm at pobox.com (Brett Ault-McCoy)
Date: Thu, 6 Mar 2008 12:39:12 +0000
Subject: [zfs-discuss] how to emulate zfs send | zfs recv
Message-ID: <7bccd8dc0803060439j5ed8bd36rb960a9ec9337e18a@mail.gmail.com>

One thing I would really, really like to have in zfs for Mac OS X is the
ability to pipe the output of zfs send to zfs recv.  Having to write it to a
file and then read from that file is painful.  It's okay for small
incremental snapshots, but is painful for full filesystem copies.  So, a
workaround until the support is there is to use a fifo.  If you do something
like:
  mkfifo /tmp/zfsfifo
  zfs recv -v -d NewPool < /tmp/zfsfifo
  zfs send OldPool/Filesystem at snapshot > /tmp/zfsfifo

the output from send will be piped into the input to recv.  You'll need to
run the two commands in different windows, or run the recv in the
background.  I'm thinking of writing a wrapper script of some sort to handle
creating the fifo and running the send and recv commands to make this easy.
 Something like:

   zsr [-vi] OldPool/Filesystem at snapshot NewPool[/Filesystem])

If anyone already knows of something similar I'd love to hear about it.
 Bonus points for making it work via an ssh pipe (ie. connecting the read
side of the fifo to an ssh process that sends the data to a fifo on a remote
machine and runs a zfs recv command on it to read from the fifo to make it
easy to copy snapshots to a remote host.

++brett;
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080306/cded06cf/attachment-0001.html 

From ilyxascrat at gmail.com  Thu Mar  6 04:51:35 2008
From: ilyxascrat at gmail.com (Ilya Tyshchenko)
Date: Thu, 6 Mar 2008 15:51:35 +0300
Subject: [zfs-discuss] how to emulate zfs send | zfs recv
In-Reply-To: <7bccd8dc0803060439j5ed8bd36rb960a9ec9337e18a@mail.gmail.com>
References: <7bccd8dc0803060439j5ed8bd36rb960a9ec9337e18a@mail.gmail.com>
Message-ID: <6BCCBDB5-3907-4CCB-8001-0AD78F101DB2@gmail.com>

Thanks ! Never think about !

It could be an interesting solution .

06.03.2008, ? 15:39, Brett Ault-McCoy ???????(?):

> One thing I would really, really like to have in zfs for Mac OS X is  
> the ability to pipe the output of zfs send to zfs recv.  Having to  
> write it to a file and then read from that file is painful.  It's  
> okay for small incremental snapshots, but is painful for full  
> filesystem copies.  So, a workaround until the support is there is  
> to use a fifo.  If you do something like:
>
>   mkfifo /tmp/zfsfifo
>   zfs recv -v -d NewPool < /tmp/zfsfifo
>   zfs send OldPool/Filesystem at snapshot > /tmp/zfsfifo
>
> the output from send will be piped into the input to recv.  You'll  
> need to run the two commands in different windows, or run the recv  
> in the background.  I'm thinking of writing a wrapper script of some  
> sort to handle creating the fifo and running the send and recv  
> commands to make this easy.  Something like:
>
>    zsr [-vi] OldPool/Filesystem at snapshot NewPool[/Filesystem])
>
> If anyone already knows of something similar I'd love to hear about  
> it.  Bonus points for making it work via an ssh pipe (ie. connecting  
> the read side of the fifo to an ssh process that sends the data to a  
> fifo on a remote machine and runs a zfs recv command on it to read  
> from the fifo to make it easy to copy snapshots to a remote host.
>
> ++brett;
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From jbsnyder at gmail.com  Thu Mar  6 06:08:16 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Thu, 6 Mar 2008 08:08:16 -0600
Subject: [zfs-discuss] how to emulate zfs send | zfs recv
In-Reply-To: <7bccd8dc0803060439j5ed8bd36rb960a9ec9337e18a@mail.gmail.com>
References: <7bccd8dc0803060439j5ed8bd36rb960a9ec9337e18a@mail.gmail.com>
Message-ID: <33644d3c0803060608j3b331005iebd7985505bb1e76@mail.gmail.com>

Zetaback has incorporated support for using FIFOs on Mac OS X systems.
 It's not just a wrapper though it's set up as something you would run
from a cron or launchd process on any type of server with perl
installed, you can set full and incremental backup intervals, and
specify patterns for which pools and filesystems to include.  It is
server-side driven for the scheduling, though, with an agent script
that runs on whatever machines you are backing up with ZFS
filesystems.  I'm thinking about adding a few features I'd like, but
haven't gotten around to it yet.

One thing of note that the author didn't have issues with, but I did,
was that the suggested way of running it is to enable root on your OS
X hosts and have your server do passwordless (or you could always do
passworded if you wanted to do things manually) logins.  For some
reason I couldn't get to to run appropriately as root and had to use
my user account on my OS X client machine to run the zetaback agent.
To make this work, since my home is on ZFS and it makes a fifo
wherever the home directory is I modified the zetaback_agent script to
make the fifo in /tmp.  If you make the fifo on the ZFS filesystem
you're backing up for some reason the fifo just accumulates the send
buffer and things don't work properly.

You can find zetaback here:
https://labs.omniti.com/trac/zetaback

For the fifo path change, look for this in zetaback_agent:
my $fifo = "zetaback_${$}_${FULL}${BASE}.fifo";

change it to:
my $fifo = "/tmp/zetaback_${$}_${FULL}${BASE}.fifo";

Ultimately though, I really hope that piping works in the near future
:-) I'd like to drive backups from the client end so that firewalls
and varying network locations wouldn't be as much of a problem save
for the link speed.

On Thu, Mar 6, 2008 at 6:39 AM, Brett Ault-McCoy <btm at pobox.com> wrote:
> One thing I would really, really like to have in zfs for Mac OS X is the
> ability to pipe the output of zfs send to zfs recv.  Having to write it to a
> file and then read from that file is painful.  It's okay for small
> incremental snapshots, but is painful for full filesystem copies.  So, a
> workaround until the support is there is to use a fifo.  If you do something
> like:
>
>   mkfifo /tmp/zfsfifo
>   zfs recv -v -d NewPool < /tmp/zfsfifo
>   zfs send OldPool/Filesystem at snapshot > /tmp/zfsfifo
>
>
> the output from send will be piped into the input to recv.  You'll need to
> run the two commands in different windows, or run the recv in the
> background.  I'm thinking of writing a wrapper script of some sort to handle
> creating the fifo and running the send and recv commands to make this easy.
> Something like:
>
>    zsr [-vi] OldPool/Filesystem at snapshot NewPool[/Filesystem])
>
> If anyone already knows of something similar I'd love to hear about it.
> Bonus points for making it work via an ssh pipe (ie. connecting the read
> side of the fifo to an ssh process that sends the data to a fifo on a remote
> machine and runs a zfs recv command on it to read from the fifo to make it
> easy to copy snapshots to a remote host.
>
> ++brett;
>
>
> _______________________________________________
>  zfs-discuss mailing list
>  zfs-discuss at lists.macosforge.org
>  http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From jbsnyder at gmail.com  Thu Mar  6 10:59:53 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Thu, 6 Mar 2008 12:59:53 -0600
Subject: [zfs-discuss] DTrace w/ OS X ZFS
Message-ID: <33644d3c0803061059g3fb24355i81ab4ae44267094d@mail.gmail.com>

I'm curious about making use of the dtrace probes in ZFS on OS X.  I
can see in the source release on macosforge that some of the probes
are commented out and others, eat least in the code there, look as if
they should be compiled in.  I don't however see them when I do a
dtrace -l.  I've poked around at some documentation for adding probes
to my own code, but I've still not 100% gotten a handle on how to
enable them on ZFS.  It sounds to me from documents like this one
(http://blogs.sun.com/tpenta/entry/dtrace_using_placing_sdt_probes)
that dtrace needs to be run on the generated binaries to embed the
probes?

I'm not sure that I'll actually do much debugging with it, but I
thought I would play around with it to get a sense for what ZFS is
doing while I'm working on top of it as well as getting a sense for
how to use dtrace on other applications.

Most of what I'm working with lately in terms of coding is in MATLAB,
so I don't have much of an opportunity there to use probes :-)

-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From i_see at macnews.de  Thu Mar  6 14:22:28 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Thu, 6 Mar 2008 23:22:28 +0100
Subject: [zfs-discuss] What Portions of the OS X Tree Can't Be Put on
	ZFS?
In-Reply-To: <mailman.870.1204807109.11791.zfs-discuss@lists.macosforge.org>
References: <mailman.870.1204807109.11791.zfs-discuss@lists.macosforge.org>
Message-ID: <431CDB6F-4851-4F0F-9110-69038A61F03D@macnews.de>

Hi,
If you want to effectively want to run your system of a ZFS-Volume/ 
root-pool you could try this:
1. Install a barebones-Mac OS on a HFS partition.
2. Configure your root pool
3. create a Disk-image of sufficient size to hold all relevant data
4. perform a union mount of the image via hdiutil late in the boot  
process (You could do so by polling when your pool is available and  
then performing the mount)

This way you achieve:
- None of the problems related to emptying the trash, copying files/ 
folders with non-ascii characters, using special attributes etc.
- All data changes are reflected in your zfs-pool
- Everything that is required to have zfs available is taken from zfs
-You can even mount the HFS+-System as read-only

Potential issues:
-When installing new software that requires changes to the boot  
process, you might have to perform multiple reboots to modify your  
base Filesystem.
-If you are using sparse images or sparsebundles freed space is not  
automatically recovered. (this could also be done during the boot  
process, before mounting the image.
-Extra care is required in the shutdown process, that the image is  
unmounted and the pool gets exported right before the machine is  
powered down. Otherwise you might hang on shutdown, as you sometimes  
do when you forget to export your pools. (Has someone already written  
a script that will export or forcibly export pools during shutdown?)

The main reason I don't want to use ZFS for that now, is that I want  
to have a usable system with just the internal HD of my mac-mini  
present. I do try to move most reasonably important data into my pool  
soon after it is created, but the system can temporarily work without  
that.

If anyone tries above method, I'd be interested to know how well it  
works.
--
ralf bertling

From jeroenvandeven at gmail.com  Thu Mar  6 19:49:35 2008
From: jeroenvandeven at gmail.com (Jeroen van de Ven)
Date: Fri, 7 Mar 2008 11:49:35 +0800
Subject: [zfs-discuss] ZFS as read/write
Message-ID: <b99b378d0803061949t56862717g2ca437311a11d775@mail.gmail.com>

Hello all,

I installed ZFS on my clean 10.5.2 installation and created a pool out
of one ZFS partition. It now automatically mounts at boot, but there
seems to be something wrong. I can drag folders onto the volume, but I
must authenticate to write to it. In Finder I can't make a new folder,
it seems to be mounted as read-only. Is there a way to fix this? I
created the pool with sudo, it might have to do with that.

regards,
Jeroen

P.S. I've never used mailing lists, am I doing it right?

From zorg at sogeeky.net  Thu Mar  6 20:09:51 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Thu, 6 Mar 2008 20:09:51 -0800
Subject: [zfs-discuss] ZFS as read/write
In-Reply-To: <b99b378d0803061949t56862717g2ca437311a11d775@mail.gmail.com>
References: <b99b378d0803061949t56862717g2ca437311a11d775@mail.gmail.com>
Message-ID: <354275CA-8734-4EBC-A679-DF78F7069C49@sogeeky.net>

Yes, zfs pools are created as root and unlike other external volumes,  
the zfs pool honors uid bits. As sudo, cd to /Volumes then apply a  
chown to the user(s)/group(s) you want to have access. You could  
probably also use acls, but I haven't quite mastered those. :)

On Mar 6, 2008, at 7:49 PM, "Jeroen van de Ven" <jeroenvandeven at gmail.com 
 > wrote:

> Hello all,
>
> I installed ZFS on my clean 10.5.2 installation and created a pool out
> of one ZFS partition. It now automatically mounts at boot, but there
> seems to be something wrong. I can drag folders onto the volume, but I
> must authenticate to write to it. In Finder I can't make a new folder,
> it seems to be mounted as read-only. Is there a way to fix this? I
> created the pool with sudo, it might have to do with that.
>
> regards,
> Jeroen
>
> P.S. I've never used mailing lists, am I doing it right?
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From danchr at daimi.au.dk  Fri Mar  7 09:19:34 2008
From: danchr at daimi.au.dk (Dan Villiom Podlaski Christiansen)
Date: Fri, 7 Mar 2008 18:19:34 +0100
Subject: [zfs-discuss] ZFS as read/write
In-Reply-To: <354275CA-8734-4EBC-A679-DF78F7069C49@sogeeky.net>
References: <b99b378d0803061949t56862717g2ca437311a11d775@mail.gmail.com>
	<354275CA-8734-4EBC-A679-DF78F7069C49@sogeeky.net>
Message-ID: <BB96F3FB-2571-4D75-A2E9-87F4A0BE32CF@daimi.au.dk>

On 7 Mar 2008, at 05:09, Mr. Zorg wrote:
> Yes, zfs pools are created as root and unlike other external volumes,
> the zfs pool honors uid bits. As sudo, cd to /Volumes then apply a
> chown to the user(s)/group(s) you want to have access. You could
> probably also use acls, but I haven't quite mastered those. :)

I suspect this qualifies as a bug in ZFS 102A. After a bit of testing  
with creating a new file system in an existing pool, I believe ZFS  
will set the ownership of the file system root directory to the user  
that issues the ?zfs create? command. If you create a new file system  
in an existing file system ? or pool ? but don't have write  
permissions to the parent root directory, the create will succeed but  
the mount fail. You should be able to fix that by changing the mount  
point.

After fiddling a bit around with creating, destroying and mounting ZFS  
file systems on an existing pool, I can't help but wonder how ZFS  
determines and handles ownership of pools; it doesn't seem to be  
mentioned in the man pages.

--

Dan Villiom Podlaski Christiansen
stud.scient., danchr at daimi.au.dk


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 1945 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080307/6a7f4fa7/attachment-0001.bin 

From bwaters at nrao.edu  Fri Mar  7 09:50:56 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Fri, 7 Mar 2008 10:50:56 -0700
Subject: [zfs-discuss] ZFS as read/write
In-Reply-To: <BB96F3FB-2571-4D75-A2E9-87F4A0BE32CF@daimi.au.dk>
References: <b99b378d0803061949t56862717g2ca437311a11d775@mail.gmail.com>
	<354275CA-8734-4EBC-A679-DF78F7069C49@sogeeky.net>
	<BB96F3FB-2571-4D75-A2E9-87F4A0BE32CF@daimi.au.dk>
Message-ID: <0B56F12E-CA5A-44F2-9FE2-86F84D38A416@nrao.edu>


On Mar 7, 2008, at 10:19 AM, Dan Villiom Podlaski Christiansen wrote:
> I believe ZFS will set the ownership of the file system root  
> directory to the user that issues the ?zfs create? command. If you  
> create a new file system in an existing file system ? or pool ? but  
> don't have write permissions to the parent root directory, the  
> create will succeed but the mount fail. You should be able to fix  
> that by changing the mount point.


I don't consider that a bug. Under ZFS, what's the real difference  
between creating a filesystem and creating a file?

Oh, I see.. if a user doesn't have write permission on the pool, they  
shouldn't be able to create a filesystem in that pool. Is that it?



From danchr at daimi.au.dk  Fri Mar  7 10:55:45 2008
From: danchr at daimi.au.dk (Dan Villiom Podlaski Christiansen)
Date: Fri, 7 Mar 2008 19:55:45 +0100
Subject: [zfs-discuss] ZFS as read/write
In-Reply-To: <0B56F12E-CA5A-44F2-9FE2-86F84D38A416@nrao.edu>
References: <b99b378d0803061949t56862717g2ca437311a11d775@mail.gmail.com>
	<354275CA-8734-4EBC-A679-DF78F7069C49@sogeeky.net>
	<BB96F3FB-2571-4D75-A2E9-87F4A0BE32CF@daimi.au.dk>
	<0B56F12E-CA5A-44F2-9FE2-86F84D38A416@nrao.edu>
Message-ID: <FB20D56E-8A59-454C-A507-F51607B2E445@daimi.au.dk>

/On 7 Mar 2008, at 18:50, Boyd Waters wrote:
>
> I don't consider that a bug. Under ZFS, what's the real difference
> between creating a filesystem and creating a file?

The difference is that a file has full permission and ownership  
information, and always has a parent folder with proper permissions  
set up for it. It appears a ZFS file system and pool has neither, yet  
is able to contain files with full permission and ownership information.

> Oh, I see.. if a user doesn't have write permission on the pool, they
> shouldn't be able to create a filesystem in that pool. Is that it?

Precisely.

I have a pool named Tank, and the root directory is only writeable for  
my account. For that pool, the following command succeeds, but fails  
to mount.
> env -i /usr/bin/sudo /usr/bin/sudo -u nobody /usr/sbin/zfs create  
> Tank/Test

After changing the mountpoint with similar commands, I get:
> % ls -ld /tmp/test
> drwxr-xr-x  2 nobody  nobody     2B  7 Mar 19:47 /tmp/test


This strikes me as odd.

--

Dan Villiom Podlaski Christiansen
stud.scient., danchr at daimi.au.dk


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 1945 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080307/84a3cbf8/attachment.bin 

From ndellofano at apple.com  Fri Mar  7 11:37:18 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 7 Mar 2008 11:37:18 -0800
Subject: [zfs-discuss] DTrace w/ OS X ZFS
In-Reply-To: <33644d3c0803061059g3fb24355i81ab4ae44267094d@mail.gmail.com>
References: <33644d3c0803061059g3fb24355i81ab4ae44267094d@mail.gmail.com>
Message-ID: <02AD5577-BF88-4B44-BCEE-DDBA939AC2A5@apple.com>

Currently it is not yet possible to trace any kexts with Dtrace.  This  
is an open RFE that is being worked on currently.  For now though  
sadly, you cannot Dtrace ZFS, hopefully soon though.

Noel

On Mar 6, 2008, at 10:59 AM, James Snyder wrote:

> I'm curious about making use of the dtrace probes in ZFS on OS X.  I
> can see in the source release on macosforge that some of the probes
> are commented out and others, eat least in the code there, look as if
> they should be compiled in.  I don't however see them when I do a
> dtrace -l.  I've poked around at some documentation for adding probes
> to my own code, but I've still not 100% gotten a handle on how to
> enable them on ZFS.  It sounds to me from documents like this one
> (http://blogs.sun.com/tpenta/entry/dtrace_using_placing_sdt_probes)
> that dtrace needs to be run on the generated binaries to embed the
> probes?
>
> I'm not sure that I'll actually do much debugging with it, but I
> thought I would play around with it to get a sense for what ZFS is
> doing while I'm working on top of it as well as getting a sense for
> how to use dtrace on other applications.
>
> Most of what I'm working with lately in terms of coding is in MATLAB,
> so I don't have much of an opportunity there to use probes :-)
>
> -- 
> James Snyder
> Biomedical Engineering
> Northwestern University
> jbsnyder at gmail.com
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From franzschmalzl at spamfreemail.de  Sat Mar  8 06:37:43 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sat, 8 Mar 2008 15:37:43 +0100
Subject: [zfs-discuss] encryption,filevault,homedir
Message-ID: <D8524F4D-7B9F-4958-A120-2B5181DD48AD@spamfreemail.de>

hey folks!

i have a couple of questions here


Use 2nd partition for home with zfs, is it possible ?
i'm using filevault at the moment since zfs encryption is not  
implemented atm, would i be able to put my sparsebundle onto my zfs  
partition and get it mounted on login ?
(i know i would still have a hfs+ formatted dmg inside a zfs  
partition, but at least i would have snapshots )
i know i could format my homefolder sparsebundle as zfs, would this  
confuse filevault?

and last but not least,

is zfs encryption in the works? are there any plans to support this ?


best regards

franz schmalzl 

From franzschmalzl at spamfreemail.de  Sat Mar  8 06:47:43 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sat, 8 Mar 2008 15:47:43 +0100
Subject: [zfs-discuss] follow up: encryption,filevault,homedir
In-Reply-To: <D8524F4D-7B9F-4958-A120-2B5181DD48AD@spamfreemail.de>
References: <D8524F4D-7B9F-4958-A120-2B5181DD48AD@spamfreemail.de>
Message-ID: <A6E3A98B-420A-4650-9455-81363A3B86AD@spamfreemail.de>

i forgot something

i know time machine does not work when the backup volume is formatted  
zfs
would it work when the volume it backs up is a zfs disk ?

(i know whe have snapshots, i'm just asking out of couriosity )





> hey folks!
>
> i have a couple of questions here
>
>
> Use 2nd partition for home with zfs, is it possible ?
> i'm using filevault at the moment since zfs encryption is not
> implemented atm, would i be able to put my sparsebundle onto my zfs
> partition and get it mounted on login ?
> (i know i would still have a hfs+ formatted dmg inside a zfs
> partition, but at least i would have snapshots )
> i know i could format my homefolder sparsebundle as zfs, would this
> confuse filevault?
>
> and last but not least,
>
> is zfs encryption in the works? are there any plans to support this ?
>
>
> best regards
>
> franz schmalzl
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From franzschmalzl at spamfreemail.de  Sun Mar  9 08:31:48 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sun, 9 Mar 2008 16:31:48 +0100
Subject: [zfs-discuss] and another one...
In-Reply-To: <F0ACF839-6D5F-4212-B714-ED7485A5704D@spamfreemail.de>
References: <D8524F4D-7B9F-4958-A120-2B5181DD48AD@spamfreemail.de>
	<A6E3A98B-420A-4650-9455-81363A3B86AD@spamfreemail.de>
	<F0ACF839-6D5F-4212-B714-ED7485A5704D@spamfreemail.de>
Message-ID: <30B45719-C3A2-47CD-87B7-BA45BE229E3B@spamfreemail.de>

ok disregard all my questions

answered them by myself, sorry for any inconvenience....


but another one came up regarding snapshots
(i know, don't kill me :-)


i cannot find any aswers to in the zfs admin guide, so here it comes:


state that i have 2 disks, one external and one internal

so i have my fancy zpool on my internal disk
and made some snapshots

hometank
hometank at snap1
hometank at snap2
hometank at snap3


i sent these snapshots to my external disks (into a file, because it's  
not yet zfs'ed) for backup purposes
snap 2 being incremental of 1, 3 being incremental of 2

zfs send hometank at snap1 > /<pathtomyexternaldisk>/<snapshotdir>/snap1
zfs send -i hometank at snap1 hometank at snap2  ....
and so on

now i deleted all of my snapshots on hometank because they started to  
eat up space

later i my dog came, hacked my password and deleted some of my files  
and i want to go back to the state of snap2

since snap2 is incremental of snap1, would i have to copy everything  
back?




From info at martin-hauser.net  Sun Mar  9 09:15:35 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Sun, 9 Mar 2008 17:15:35 +0100
Subject: [zfs-discuss] and another one...
In-Reply-To: <30B45719-C3A2-47CD-87B7-BA45BE229E3B@spamfreemail.de>
References: <D8524F4D-7B9F-4958-A120-2B5181DD48AD@spamfreemail.de>
	<A6E3A98B-420A-4650-9455-81363A3B86AD@spamfreemail.de>
	<F0ACF839-6D5F-4212-B714-ED7485A5704D@spamfreemail.de>
	<30B45719-C3A2-47CD-87B7-BA45BE229E3B@spamfreemail.de>
Message-ID: <7A0E56DE-B07C-4DEE-931D-965EC775CE01@martin-hauser.net>

Hello,

On Mar 9, 2008, at 16:31 PM, Franz Schmalzl wrote:

> but another one came up regarding snapshots
> (i know, don't kill me :-)

Nah, we won't ;)

> now i deleted all of my snapshots on hometank because they started to
> eat up space
>
> later i my dog came, hacked my password and deleted some of my files
> and i want to go back to the state of snap2

hmm. As far as I understand it (best let it be confirmed by somebody  
else), you have to 'zfs receive' both snap1 and snap2 and then you can  
rollback to snap2.

kind regards

Martin

-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080309/d45921ff/attachment-0001.bin 

From justin at justincooley.com  Sun Mar  9 09:41:25 2008
From: justin at justincooley.com (Justin Cooley)
Date: Sun, 9 Mar 2008 09:41:25 -0700
Subject: [zfs-discuss] ZFS as AirDisk?
Message-ID: <C7F6B74C-B29C-4E36-B84A-4F54217C86EE@justincooley.com>

I don't think I've seen this discussed but is there currently a way to  
mount a zfs disk as an AirDisk over an Airport Extreme (current  
version)?

I've tried and it doesn't seem to pick it up automatically.  I was  
also unable to get it mount over the command line (I may have been  
doing this wrong)?

Anyone had any success with this?

Thank you.


From btm at pobox.com  Sun Mar  9 10:46:05 2008
From: btm at pobox.com (Brett Ault-McCoy)
Date: Sun, 9 Mar 2008 17:46:05 +0000
Subject: [zfs-discuss] zfs crashing regularly now
Message-ID: <7bccd8dc0803091046s1de88fecx77419622ef29fea@mail.gmail.com>

I started using zfs a few weeks ago.  Until recently it's been very stable
and I haven't had any crashes or problems.  However, about two days ago it
started crashing and is now doing so somewhat regularly.  The initial crash
was due to having a zfs pool made out of some files residing on another
machine being accessed over CIFS.  The other machine was shut down while the
zfs pool was active and crashed the machine.  I no longer have that pool
(zpool destroy) but I'm still having crashes periodically.  The problem
report from the crashes is:
Sun Mar  9 17:38:07 2008
panic(cpu 0 caller 0x375768B0): "ZFS: I/O failure (write on <unknown> off 0:
zio 0x2008ef0 [L0 ZFS plain file] 20000L/20000P DVA[0]=<0:2a82fe0000:20000>
fletcher2 uncompressed BE contiguous birth=878877 fill=1
cksum=2f7d0e17f8bcf08c:48cc3c8f3c1207d2:1ce4de75a16901ab:f103c54640173d99):
error "
"5"@/Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/zfs_kext/zfs/zio.c:918
Latest stack backtrace for cpu 0:
      Backtrace:
         0x0009B2F8 0x0009BC9C 0x00029DC4 0x375768B0 0x37572340 0x37572864
0x37572918 0x37572340
         0x37575E20 0x37572340 0x375BD33C 0x37571AC4 0x375E9C08 0x000B0454
      Kernel loadable modules in backtrace (with dependencies):
         com.apple.filesystems.zfs(8.0)@0x3755a000->0x37643fff
Proceeding back via exception chain:
   Exception state (sv=0x24c56280)
      PC=0x00000000; MSR=0x0000D030; DAR=0x00000000; DSISR=0x00000000;
LR=0x00000000; R1=0x00000000; XCP=0x00000000 (Unknown)

BSD process name corresponding to current thread: kernel_task

Mac OS version:
9C31

Kernel version:
Darwin Kernel Version 9.2.0: Tue Feb  5 16:15:19 PST 2008; root:
xnu-1228.3.13~1/RELEASE_PPC
System model name: PowerMac3,5


The original crash also resulted in a corrupted file on a local HFS+ volume
that had had a pool on and I had to destroy that pool.  I assumed that was
due to HFS+ inconsistancy during the crash.

My thought on how to maybe clear the problem up is to export the pools I
have to get back to clean state and then re-import them all.

Has anyone else seen anything similar?

++Brett;
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080309/b4c1e127/attachment.html 

From i_see at macnews.de  Sun Mar  9 11:16:12 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Sun, 9 Mar 2008 19:16:12 +0100
Subject: [zfs-discuss] ZFS as read/write
In-Reply-To: <mailman.1168.1205079283.11791.zfs-discuss@lists.macosforge.org>
References: <mailman.1168.1205079283.11791.zfs-discuss@lists.macosforge.org>
Message-ID: <0A45A01E-247C-42B5-AB83-C21A36D3F086@macnews.de>

what's strange about the permissions is that zpool destroy doen NOT  
require admin privileges.
Now that is a bit weird considering FS access requires root rights by  
default,

ralf

From franzschmalzl at spamfreemail.de  Sun Mar  9 11:23:11 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sun, 9 Mar 2008 19:23:11 +0100
Subject: [zfs-discuss] and another one...
In-Reply-To: <7A0E56DE-B07C-4DEE-931D-965EC775CE01@martin-hauser.net>
References: <D8524F4D-7B9F-4958-A120-2B5181DD48AD@spamfreemail.de>
	<A6E3A98B-420A-4650-9455-81363A3B86AD@spamfreemail.de>
	<F0ACF839-6D5F-4212-B714-ED7485A5704D@spamfreemail.de>
	<30B45719-C3A2-47CD-87B7-BA45BE229E3B@spamfreemail.de>
	<7A0E56DE-B07C-4DEE-931D-965EC775CE01@martin-hauser.net>
Message-ID: <309E321C-73FD-46CF-AC69-D0C997729DC0@spamfreemail.de>

> hmm. As far as I understand it (best let it be confirmed by somebody  
> else), you have to 'zfs receive' both snap1 and snap2 and then you  
> can rollback to snap2.
>
> kind regards
>
> Martin
>


yeah but that is kinda lame


since the data contained in snapshot 1 is still there
but the snapshot is not

so i would have to copy about 50gb to my internal disk altough they  
are already present...



best regards und viele gruesse :)

franz schmalzl



On 09.03.2008, at 17:15, Martin Hauser wrote:

> Hello,
>
> On Mar 9, 2008, at 16:31 PM, Franz Schmalzl wrote:
>
>> but another one came up regarding snapshots
>> (i know, don't kill me :-)
>
> Nah, we won't ;)
>
>> now i deleted all of my snapshots on hometank because they started to
>> eat up space
>>
>> later i my dog came, hacked my password and deleted some of my files
>> and i want to go back to the state of snap2
>

From info at martin-hauser.net  Sun Mar  9 11:25:41 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Sun, 9 Mar 2008 19:25:41 +0100
Subject: [zfs-discuss] ZFS as read/write
In-Reply-To: <0A45A01E-247C-42B5-AB83-C21A36D3F086@macnews.de>
References: <mailman.1168.1205079283.11791.zfs-discuss@lists.macosforge.org>
	<0A45A01E-247C-42B5-AB83-C21A36D3F086@macnews.de>
Message-ID: <55FD06FE-2249-4B22-A231-A7DAC2796B29@martin-hauser.net>

It seems that mount/umount indeed require admin privileges while zfs  
create / destroy (which only requires talking to the zfs kernel module  
not talking to the Os itself ) does not...

so it's a error but a consistent one.



On Mar 9, 2008, at 19:16 PM, Ralf Bertling wrote:

> what's strange about the permissions is that zpool destroy doen NOT
> require admin privileges.
> Now that is a bit weird considering FS access requires root rights by
> default,
>
> ralf
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080309/adea5b42/attachment.bin 

From franzschmalzl at spamfreemail.de  Sun Mar  9 12:00:49 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sun, 9 Mar 2008 20:00:49 +0100
Subject: [zfs-discuss] and another one...
In-Reply-To: <309E321C-73FD-46CF-AC69-D0C997729DC0@spamfreemail.de>
References: <D8524F4D-7B9F-4958-A120-2B5181DD48AD@spamfreemail.de>
	<A6E3A98B-420A-4650-9455-81363A3B86AD@spamfreemail.de>
	<F0ACF839-6D5F-4212-B714-ED7485A5704D@spamfreemail.de>
	<30B45719-C3A2-47CD-87B7-BA45BE229E3B@spamfreemail.de>
	<7A0E56DE-B07C-4DEE-931D-965EC775CE01@martin-hauser.net>
	<309E321C-73FD-46CF-AC69-D0C997729DC0@spamfreemail.de>
Message-ID: <B02ADB6B-5181-4434-9A2E-C0F425B55E86@spamfreemail.de>

ok just don't understand this

to have incrementel backups via snapshots i think will have to keep  
all my snapshots, how is this supposed to work?

i mean i created one incremental snapshot earlier and sent it to  
another disk, then i deleted the snapshots...

to create another incremental backup now, i will need to recieve the  
first snapshot again, which means i will have to copy all of my data  
back from my backup disk and have it present twice in my pool...


i think i'm getting it wrong, can please someone clarify ? 

From btm at pobox.com  Sun Mar  9 13:16:05 2008
From: btm at pobox.com (Brett Ault-McCoy)
Date: Sun, 9 Mar 2008 20:16:05 +0000
Subject: [zfs-discuss] and another one...
In-Reply-To: <B02ADB6B-5181-4434-9A2E-C0F425B55E86@spamfreemail.de>
References: <D8524F4D-7B9F-4958-A120-2B5181DD48AD@spamfreemail.de>
	<A6E3A98B-420A-4650-9455-81363A3B86AD@spamfreemail.de>
	<F0ACF839-6D5F-4212-B714-ED7485A5704D@spamfreemail.de>
	<30B45719-C3A2-47CD-87B7-BA45BE229E3B@spamfreemail.de>
	<7A0E56DE-B07C-4DEE-931D-965EC775CE01@martin-hauser.net>
	<309E321C-73FD-46CF-AC69-D0C997729DC0@spamfreemail.de>
	<B02ADB6B-5181-4434-9A2E-C0F425B55E86@spamfreemail.de>
Message-ID: <7bccd8dc0803091316k29cf9d26t9a7982b5914a7d8e@mail.gmail.com>

To do a zfs send/recv of incremental data you need to start with both the
source and destination having the same snapshot on them, and it has to be
the last snapshot on the destination.  No other snapshots are necessary.
You then create a new snapshot on the source, do a "zfs send -i src at old
src at new" on the source then 'zfs recv dest' to apply the incremental to the
destination.  At that point you can delete all prior snapshots on both the
dest and src.

In short, an incremental send/recv has to be done using the last snapshot on
the destination as the incremental reference.

++Brett;


On Sun, Mar 9, 2008 at 7:00 PM, Franz Schmalzl <
franzschmalzl at spamfreemail.de> wrote:

> ok just don't understand this
>
> to have incrementel backups via snapshots i think will have to keep
> all my snapshots, how is this supposed to work?
>
> i mean i created one incremental snapshot earlier and sent it to
> another disk, then i deleted the snapshots...
>
> to create another incremental backup now, i will need to recieve the
> first snapshot again, which means i will have to copy all of my data
> back from my backup disk and have it present twice in my pool...
>
>
> i think i'm getting it wrong, can please someone clarify ?
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080309/055592d7/attachment-0001.html 

From franzschmalzl at spamfreemail.de  Sun Mar  9 13:32:18 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sun, 9 Mar 2008 21:32:18 +0100
Subject: [zfs-discuss] and another one...
Message-ID: <BEE95B44-414F-4AEE-9520-9632C98396C9@spamfreemail.de>

ook thanks...

my main problem was that i did not use a zfs disk as destination
(could have thought of that tough) *shame on me*





On 09.03.2008, at 21:16, Brett Ault-McCoy wrote:

> ------=_Part_27274_8300480.1205093765527
> Content-Type: text/plain; charset=UTF-8
> Content-Transfer-Encoding: 7bit
> Content-Disposition: inline
>
> To do a zfs send/recv of incremental data you need to start with  
> both the
> source and destination having the same snapshot on them, and it has  
> to be
> the last snapshot on the destination.  No other snapshots are  
> necessary.
> You then create a new snapshot on the source, do a "zfs send -i  
> src at old
> src at new" on the source then 'zfs recv dest' to apply the incremental  
> to the
> destination.  At that point you can delete all prior snapshots on  
> both the
> dest and src.
>
> In short, an incremental send/recv has to be done using the last  
> snapshot on
> the destination as the incremental reference.
so that means i will always have to keep at least one snapshot  
internal and external as reference and rotate them by hand ?


another question

when my backup dist starts to fill up, how will i be able to rotate my  
backups like time machine does ?

is i understand i allways have to keep the first instance of my  
filesystem complete to act as a reference for the first snapshot




>
>
> ++Brett;
>
>
> On Sun, Mar 9, 2008 at 7:00 PM, Franz Schmalzl <
> franzschmalzl at spamfreemail.de> wrote:
>
>> ok just don't understand this
>>
>> to have incrementel backups via snapshots i think will have to keep
>> all my snapshots, how is this supposed to work?
>>
>> i mean i created one incremental snapshot earlier and sent it to
>> another disk, then i deleted the snapshots...
>>
>> to create another incremental backup now, i will need to recieve the
>> first snapshot again, which means i will have to copy all of my data
>> back from my backup disk and have it present twice in my pool...
>>
>>
>> i think i'm getting it wrong, can please someone clarify ?
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
> ------=_Part_27274_8300480.1205093765527
> Content-Type: text/html; charset=UTF-8
> Content-Transfer-Encoding: 7bit
> Content-Disposition: inline
>
> To do a zfs send/recv of incremental data you need to start with  
> both the source and destination having the same snapshot on them,  
> and it has to be the last snapshot on the destination. &nbsp;No  
> other snapshots are necessary.<div>
> <br class="webkit-block-placeholder"></div><div>You then create a  
> new snapshot on the source, do a &quot;zfs send -i src at old  
> src at new&quot; on the source then &#39;zfs recv dest&#39; to apply  
> the incremental to the destination. &nbsp;At that point you can  
> delete all prior snapshots on both the dest and src.</div>
> <div><br class="webkit-block-placeholder"></div><div>In short, an  
> incremental send/recv has to be done using the last snapshot on the  
> destination as the incremental reference.</div><div><br  
> class="webkit-block-placeholder">
> </div><div>++Brett;</div><div><br><br><div class="gmail_quote">On  
> Sun, Mar 9, 2008 at 7:00 PM, Franz Schmalzl &lt;<a href="mailto:franzschmalzl at spamfreemail.de 
> ">franzschmalzl at spamfreemail.de</a>&gt; wrote:<br><blockquote  
> class="gmail_quote" style="margin:0 0 0 .8ex;border-left:1px #ccc  
> solid;padding-left:1ex;">
> ok just don&#39;t understand this<br>
> <br>
> to have incrementel backups via snapshots i think will have to  
> keep<br>
> all my snapshots, how is this supposed to work?<br>
> <br>
> i mean i created one incremental snapshot earlier and sent it to<br>
> another disk, then i deleted the snapshots...<br>
> <br>
> to create another incremental backup now, i will need to recieve  
> the<br>
> first snapshot again, which means i will have to copy all of my  
> data<br>
> back from my backup disk and have it present twice in my pool...<br>
> <br>
> <br>
> i think i&#39;m getting it wrong, can please someone clarify ?<br>
> <div><div></div><div  
> class="Wj3C7c">_______________________________________________<br>
> zfs-discuss mailing list<br>
> <a href="mailto:zfs-discuss at lists.macosforge.org">zfs-discuss at lists.macosforge.org 
> </a><br>
> <a href="http://lists.macosforge.org/mailman/listinfo.cgi/zfs- 
> discuss" target="_blank">http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss 
> </a><br>
> </div></div></blockquote></div><br></div>
>
> ------=_Part_27274_8300480.1205093765527--


From btm at pobox.com  Sun Mar  9 14:54:23 2008
From: btm at pobox.com (Brett Ault-McCoy)
Date: Sun, 9 Mar 2008 21:54:23 +0000
Subject: [zfs-discuss] and another one...
In-Reply-To: <BEE95B44-414F-4AEE-9520-9632C98396C9@spamfreemail.de>
References: <BEE95B44-414F-4AEE-9520-9632C98396C9@spamfreemail.de>
Message-ID: <7bccd8dc0803091454i39326103m2269ebc6e9e0cfc4@mail.gmail.com>

On Sun, Mar 9, 2008 at 8:32 PM, Franz Schmalzl <
franzschmalzl at spamfreemail.de> wrote:

> so that means i will always have to keep at least one snapshot
> internal and external as reference and rotate them by hand ?
>
>
> another question
>
> when my backup dist starts to fill up, how will i be able to rotate my
> backups like time machine does ?
>
> is i understand i allways have to keep the first instance of my
> filesystem complete to act as a reference for the first snapshot
>

It works something like this.  You make a snapshot of your source filesystem
and then send/recv it as a full stream to your backup destination.  This
will create a new filesystem on your backup destination that has a single
snapshot.  That snapshot references all the data in the filesystem as it is
at that point in time.

Now, at a later point in time you create a new snapshot on your source
filesystem then do an incremental send/recv using that first snapshot as the
reference.  This will create a new snapshot on your destination.  That
snapshot will reference any data from the first snapshot that still exists,
plus any new data that's been added or modified between the two snapshots.
 It will not reference any data that ceased to exist between the two
snapshots (deleted files or pieces of files that have been modified).

Now, your backup filesystem has a union of all data that existed at both
points in time that the snapshots were created with the snapshots as a way
of selecting which set you want to reference.  You can keep doing this using
the previous snapshot as the reference and your backup destination will grow
by the amount of new/modified data between snapshots.

When your destination starts to fill up you can start destroying snapshots.
 When you destroy a snapshot any data that is referenced solely by that
snapshot will go away.  So, if you destroy the origninal snapshot, data that
was deleted or modified between the first and second snapshot will go away.
 All other data will stay because it's still referenced by that second
snapshot, at least.

You do not have to keep the original instance, per se, because it's only a
set of references to data blocks, in effect.  Destroying it only deletes the
data that ceased to exist between it and the second snapshot.

When you run 'zfs list' you can see how much data a snapshot "references",
which is basically how much was in the filesystem at the time of the
snapshot.  You can also see how much data the snapshot "uses", which is the
data that only that snapshot references.  Destroying a snapshot will only
ever delete as much data as is shown in the "used" column.  However, be
destroying a snapshot you may cause the "used" value for another snapshot to
increase if it becomes the sole remaining snapshot to reference a block of
data.

++Brett;
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080309/2a7ee02f/attachment.html 

From bplist at thinkpink.com  Sun Mar  9 15:54:29 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Sun, 9 Mar 2008 15:54:29 -0700
Subject: [zfs-discuss] vmem_alloc panic is back
Message-ID: <7942E9BB-862E-4AA9-AFA0-8F4F3B993E8A@thinkpink.com>

I stopped using rsync against big trees on my ZFS file system and as a  
result had stopped getting the "vmem_alloc couldn't alloc" panic.   
Until the other day, that is.  iPhoto was re-building thumbnails(via  
NFS) and the server panic'ed.  Seems like anything that touches a  
large number of (small-ish) files creates the problem.

Any ETA for a fix for this issue?

thanks,
bri


Wed Mar  5 21:44:09 2008
panic(cpu 1 caller 0x00849974): "zfs: vmem_alloc couldn't alloc 20480  
bytes\n"@/Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/ 
zfs_kext/zfs/zfs_context.c:668
Latest stack backtrace for cpu 1:
       Backtrace:
          0x0009AD18 0x0009B6BC 0x00029DC4 0x00849974 0x0086AA3C  
0x007EDC5C 0x007FA730 0x007FB2A8
          0x007FB708 0x007FE854 0x00809894 0x00809BB8 0x007DA548  
0x00108638 0x001EB978 0x001FD690
          0x003091FC 0x000B24C8 0x6717683C
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0x7d7000->0x8c0fff
Proceeding back via exception chain:
    Exception state (sv=0x716b1500)
       PC=0x94A289DC; MSR=0x0000D030; DAR=0xF0288EE4;  
DSISR=0x42000000; LR=0x00004528; R1=0xF0288E50; XCP=0x00000030 (0xC00  
- System call)

BSD process name corresponding to current thread: nfsd

Mac OS version:
9B18

Kernel version:
Darwin Kernel Version 9.1.0: Wed Oct 31 17:48:21 PDT 2007;  
root:xnu-1228.0.2~1/RELEASE_PPC


From riscky at gmail.com  Sun Mar  9 21:13:32 2008
From: riscky at gmail.com (Riscky Abacus)
Date: Mon, 10 Mar 2008 00:13:32 -0400
Subject: [zfs-discuss] zpool status ---> kernel panic
Message-ID: <51cfc2260803092113p513bf35am5a3498da543bdaab@mail.gmail.com>

Okay I have a small issue I keep running into over the last week...
anytime I run `zpool status` with one of my drive set-ups I always
panic... I have 4 small 8GB drives set up to make one big pool 32 GB
pool for my sandbox. Any ideas?

Mon Mar 10 00:06:34 2008
panic(cpu 1 caller 0x001A8C8A): Kernel trap at 0x00361855, type
14=page fault, registers:
CR0: 0x80010033, CR2: 0x0000000c, CR3: 0x00cf2000, CR4: 0x00000660
EAX: 0x00000000, EBX: 0x36f1fde8, ECX: 0x1f000000, EDX: 0x00000000
CR2: 0x0000000c, EBP: 0x36f1fd18, ESI: 0x00000200, EDI: 0x03b56000
EFL: 0x00010202, EIP: 0x00361855, CS:  0x00000008, DS:  0x03960010
Error code: 0x00000000

Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x36f1fb38 : 0x12b0f7 (0x458124 0x36f1fb6c 0x133230 0x0)
0x36f1fb88 : 0x1a8c8a (0x461650 0x361855 0xe 0x460e00)
0x36f1fc68 : 0x19ece5 (0x36f1fc80 0x3bc23c8 0x36f1fd18 0x361855)
0x36f1fc78 : 0x361855 (0xe 0x48 0x10 0x190010)
0x36f1fd18 : 0x35840ff8 (0x0 0x4ec63 0x3000 0x0)
0x36f1fd38 : 0x358489dd (0x0 0x0 0x36f1fd58 0x1a236f)
0x36f1fd78 : 0x202bea (0x1f000000 0xcf1c5a20 0x3b56000 0x3)
0x36f1fdb8 : 0x1f6039 (0x36f1fde8 0x246 0x36f1fe18 0x1da207)
0x36f1fe18 : 0x1ec2f6 (0x5a59e20 0xcf1c5a20 0x3b56000 0x3)
0x36f1fe78 : 0x36533b (0x43cb2b0 0xcf1c5a20 0x3b56000 0x36f1ff50)
0x36f1fe98 : 0x38bcbc (0x43cb2b0 0xcf1c5a20 0x3b56000 0x36f1ff50)
0x36f1ff78 : 0x3dcea7 (0x445d2b0 0x4b8a220 0x4b8a264 0x0)
0x36f1ffc8 : 0x19f1c3 (0x47d8040 0x0 0x1a20b5 0x47d8040)
No mapping exists for frame pointer
Backtrace terminated-invalid frame pointer 0xbfffa618
      Kernel loadable modules in backtrace (with dependencies):
         com.apple.filesystems.zfs(8.0)@0x357e5000->0x358b0fff

BSD process name corresponding to current thread: zpool

Mac OS version:
9C2015

From jbsnyder at gmail.com  Mon Mar 10 14:56:14 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Mon, 10 Mar 2008 16:56:14 -0500
Subject: [zfs-discuss] DTrace w/ OS X ZFS
In-Reply-To: <02AD5577-BF88-4B44-BCEE-DDBA939AC2A5@apple.com>
References: <33644d3c0803061059g3fb24355i81ab4ae44267094d@mail.gmail.com>
	<02AD5577-BF88-4B44-BCEE-DDBA939AC2A5@apple.com>
Message-ID: <33644d3c0803101456tb2760fax1bc531fe61ce49d7@mail.gmail.com>

I assume, then, that this will need to be something integrated into a
future OS update (both kernel and dtrace userland) before this becomes
possible?

Somewhat related & unrelated:
Is there another way to get more information out of some of the panic
traces?  I've seen a few posts here where it appears that the debug
information in the code includes references to points in the ZFS
source that I suppose make it easier to track difficulties down.  What
I've had happen a number of times recently, though, are some double
faults  (rdar://problem/5786708), but the trace seems to reference
addresses that aren't communicating to me anything about how I might
narrow down the causes to provide a better test case.  I see at the
bottoms of many of the traces "Backtrace continues..."  Is there some
way to list the rest of the backtrace or choose to have that included?
 I've googled a bit and not found a reference to configuring panic
backtrace options...

Example trace below:

Fri Mar  7 12:18:42 2008
panic(cpu 0 caller 0x001A83A6): Double fault at 0x00c3d632,
thread:0x4b1cba0, trapno:0x8, err:0x0),registers:
CR0: 0x80010033, CR2: 0x3938bff8, CR3: 0x01137000, CR4: 0x00000660
EAX: 0x00077265, EBX: 0x04c2dd98, ECX: 0x04b89800, EDX: 0x04c21600
ESP: 0x3938c000, EBP: 0x3938c048, ESI: 0x00000000, EDI: 0x04b898b8
EFL: 0x00010246, EIP: 0x00c3d632

Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x4eae28 : 0x12b0f7 (0x458124 0x4eae5c 0x133230 0x0)
0x4eae78 : 0x1a83a6 (0x4610d4 0xc3d632 0x4b1cba0 0x8)
0x4eaf58 : 0x19fc73 (0x4eaf70 0x0 0x0 0x0)
0x3938c048 : 0xc3d94e (0x4c21600 0x77266 0x0 0x1)
0x3938c078 : 0xc38fde (0x4c21600 0x77266 0x0 0xca59a4)
0x3938c0d8 : 0xc6ed92 (0x45691e0 0x77266 0x0 0x0)
0x3938c188 : 0xc77598 (0x3938c33c 0x0 0x0 0x3938c33c)
0x3938c358 : 0xc6dfa1 (0x5115df80 0x216 0x3938c388 0x4c3cf50)
0x3938c398 : 0xc17e5b (0x5115df80 0x1 0x3938c428 0x4c3cf50)
0x3938c3d8 : 0x1f3eb1 (0x3938c3f4 0x216 0x3938c3fc 0x0)
0x3938c418 : 0x1db45f (0x499cd80 0x4b0d784 0x0 0x2)
0x3938c468 : 0x1db67b (0x499cd80 0x1 0x3938c498 0x19d4b1)
0x3938c4b8 : 0x1dd2e6 (0x0 0x4f43c0 0x3938c558 0x54121908)
0x3938c538 : 0xc6eca5 (0x0 0x34 0x3938c56c 0x54121894)
0x3938c5b8 : 0xc6f0ba (0x4b89c78 0x0 0x200 0x0)
0x3938c668 : 0xc77598 (0x3938c81c 0x0 0x0 0x3938c81c)
	Backtrace continues...
      Kernel loadable modules in backtrace (with dependencies):
         com.apple.filesystems.zfs(8.0)@0xc10000->0xcdbfff

BSD process name corresponding to current thread: Finder

Mac OS version:
9C2015

Kernel version:
Darwin Kernel Version 9.2.1: Tue Feb  5 23:08:45 PST 2008;
root:xnu-1228.4.20~1/RELEASE_I386
System model name: MacBook4,1 (Mac-F22788A9)



On Fri, Mar 7, 2008 at 2:37 PM, No?l Dellofano <ndellofano at apple.com> wrote:
> Currently it is not yet possible to trace any kexts with Dtrace.  This
>  is an open RFE that is being worked on currently.  For now though
>  sadly, you cannot Dtrace ZFS, hopefully soon though.
>
>  Noel
>
>
>
>  On Mar 6, 2008, at 10:59 AM, James Snyder wrote:
>
>  > I'm curious about making use of the dtrace probes in ZFS on OS X.  I
>  > can see in the source release on macosforge that some of the probes
>  > are commented out and others, eat least in the code there, look as if
>  > they should be compiled in.  I don't however see them when I do a
>  > dtrace -l.  I've poked around at some documentation for adding probes
>  > to my own code, but I've still not 100% gotten a handle on how to
>  > enable them on ZFS.  It sounds to me from documents like this one
>  > (http://blogs.sun.com/tpenta/entry/dtrace_using_placing_sdt_probes)
>  > that dtrace needs to be run on the generated binaries to embed the
>  > probes?
>  >
>  > I'm not sure that I'll actually do much debugging with it, but I
>  > thought I would play around with it to get a sense for what ZFS is
>  > doing while I'm working on top of it as well as getting a sense for
>  > how to use dtrace on other applications.
>  >
>  > Most of what I'm working with lately in terms of coding is in MATLAB,
>  > so I don't have much of an opportunity there to use probes :-)
>  >
>  > --
>  > James Snyder
>  > Biomedical Engineering
>  > Northwestern University
>  > jbsnyder at gmail.com
>  > _______________________________________________
>  > zfs-discuss mailing list
>  > zfs-discuss at lists.macosforge.org
>  > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From jbsnyder at gmail.com  Mon Mar 10 15:12:12 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Mon, 10 Mar 2008 17:12:12 -0500
Subject: [zfs-discuss] DTrace w/ OS X ZFS
In-Reply-To: <33644d3c0803101456tb2760fax1bc531fe61ce49d7@mail.gmail.com>
References: <33644d3c0803061059g3fb24355i81ab4ae44267094d@mail.gmail.com>
	<02AD5577-BF88-4B44-BCEE-DDBA939AC2A5@apple.com>
	<33644d3c0803101456tb2760fax1bc531fe61ce49d7@mail.gmail.com>
Message-ID: <33644d3c0803101512l47e25e88w5a60688656fb1773@mail.gmail.com>

Following up, I did locate the following TN to get the binary dump:
http://developer.apple.com/technotes/tn2004/tn2118.html

The only problem here is that I am running on a macbook and it is hard
to say whether I might or might not have network access at a given
time, and I'm not usually actively trying to crash it when it does
crash.  Is there a way to do a local dump, say to a dedicated spot on
disk or something?


On Mon, Mar 10, 2008 at 4:56 PM, James Snyder <jbsnyder at gmail.com> wrote:
> I assume, then, that this will need to be something integrated into a
>  future OS update (both kernel and dtrace userland) before this becomes
>  possible?
>
>  Somewhat related & unrelated:
>  Is there another way to get more information out of some of the panic
>  traces?  I've seen a few posts here where it appears that the debug
>  information in the code includes references to points in the ZFS
>  source that I suppose make it easier to track difficulties down.  What
>  I've had happen a number of times recently, though, are some double
>  faults  (rdar://problem/5786708), but the trace seems to reference
>  addresses that aren't communicating to me anything about how I might
>  narrow down the causes to provide a better test case.  I see at the
>  bottoms of many of the traces "Backtrace continues..."  Is there some
>  way to list the rest of the backtrace or choose to have that included?
>   I've googled a bit and not found a reference to configuring panic
>  backtrace options...
>
>  Example trace below:
>
>  Fri Mar  7 12:18:42 2008
>  panic(cpu 0 caller 0x001A83A6): Double fault at 0x00c3d632,
>  thread:0x4b1cba0, trapno:0x8, err:0x0),registers:
>  CR0: 0x80010033, CR2: 0x3938bff8, CR3: 0x01137000, CR4: 0x00000660
>  EAX: 0x00077265, EBX: 0x04c2dd98, ECX: 0x04b89800, EDX: 0x04c21600
>  ESP: 0x3938c000, EBP: 0x3938c048, ESI: 0x00000000, EDI: 0x04b898b8
>  EFL: 0x00010246, EIP: 0x00c3d632
>
>  Backtrace, Format - Frame : Return Address (4 potential args on stack)
>  0x4eae28 : 0x12b0f7 (0x458124 0x4eae5c 0x133230 0x0)
>  0x4eae78 : 0x1a83a6 (0x4610d4 0xc3d632 0x4b1cba0 0x8)
>  0x4eaf58 : 0x19fc73 (0x4eaf70 0x0 0x0 0x0)
>  0x3938c048 : 0xc3d94e (0x4c21600 0x77266 0x0 0x1)
>  0x3938c078 : 0xc38fde (0x4c21600 0x77266 0x0 0xca59a4)
>  0x3938c0d8 : 0xc6ed92 (0x45691e0 0x77266 0x0 0x0)
>  0x3938c188 : 0xc77598 (0x3938c33c 0x0 0x0 0x3938c33c)
>  0x3938c358 : 0xc6dfa1 (0x5115df80 0x216 0x3938c388 0x4c3cf50)
>  0x3938c398 : 0xc17e5b (0x5115df80 0x1 0x3938c428 0x4c3cf50)
>  0x3938c3d8 : 0x1f3eb1 (0x3938c3f4 0x216 0x3938c3fc 0x0)
>  0x3938c418 : 0x1db45f (0x499cd80 0x4b0d784 0x0 0x2)
>  0x3938c468 : 0x1db67b (0x499cd80 0x1 0x3938c498 0x19d4b1)
>  0x3938c4b8 : 0x1dd2e6 (0x0 0x4f43c0 0x3938c558 0x54121908)
>  0x3938c538 : 0xc6eca5 (0x0 0x34 0x3938c56c 0x54121894)
>  0x3938c5b8 : 0xc6f0ba (0x4b89c78 0x0 0x200 0x0)
>  0x3938c668 : 0xc77598 (0x3938c81c 0x0 0x0 0x3938c81c)
>         Backtrace continues...
>       Kernel loadable modules in backtrace (with dependencies):
>          com.apple.filesystems.zfs(8.0)@0xc10000->0xcdbfff
>
>  BSD process name corresponding to current thread: Finder
>
>  Mac OS version:
>  9C2015
>
>  Kernel version:
>  Darwin Kernel Version 9.2.1: Tue Feb  5 23:08:45 PST 2008;
>  root:xnu-1228.4.20~1/RELEASE_I386
>  System model name: MacBook4,1 (Mac-F22788A9)
>
>
>
>
>
>  On Fri, Mar 7, 2008 at 2:37 PM, No?l Dellofano <ndellofano at apple.com> wrote:
>  > Currently it is not yet possible to trace any kexts with Dtrace.  This
>  >  is an open RFE that is being worked on currently.  For now though
>  >  sadly, you cannot Dtrace ZFS, hopefully soon though.
>  >
>  >  Noel
>  >
>  >
>  >
>  >  On Mar 6, 2008, at 10:59 AM, James Snyder wrote:
>  >
>  >  > I'm curious about making use of the dtrace probes in ZFS on OS X.  I
>  >  > can see in the source release on macosforge that some of the probes
>  >  > are commented out and others, eat least in the code there, look as if
>  >  > they should be compiled in.  I don't however see them when I do a
>  >  > dtrace -l.  I've poked around at some documentation for adding probes
>  >  > to my own code, but I've still not 100% gotten a handle on how to
>  >  > enable them on ZFS.  It sounds to me from documents like this one
>  >  > (http://blogs.sun.com/tpenta/entry/dtrace_using_placing_sdt_probes)
>  >  > that dtrace needs to be run on the generated binaries to embed the
>  >  > probes?
>  >  >
>  >  > I'm not sure that I'll actually do much debugging with it, but I
>  >  > thought I would play around with it to get a sense for what ZFS is
>  >  > doing while I'm working on top of it as well as getting a sense for
>  >  > how to use dtrace on other applications.
>  >  >
>  >  > Most of what I'm working with lately in terms of coding is in MATLAB,
>  >  > so I don't have much of an opportunity there to use probes :-)
>  >  >
>  >  > --
>  >  > James Snyder
>  >  > Biomedical Engineering
>  >  > Northwestern University
>  >  > jbsnyder at gmail.com
>  >  > _______________________________________________
>  >  > zfs-discuss mailing list
>  >  > zfs-discuss at lists.macosforge.org
>  >  > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>  >
>  >
>
>
>
>  --
>  James Snyder
>  Biomedical Engineering
>  Northwestern University
>  jbsnyder at gmail.com
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From kane at inius.com  Tue Mar 11 09:23:44 2008
From: kane at inius.com (Kane Dijkman)
Date: Tue, 11 Mar 2008 09:23:44 -0700
Subject: [zfs-discuss] invalid vdev configuration after crash
In-Reply-To: <1C7C9931-DB88-469B-942A-45396F96FAE3@gmail.com>
References: <20080131211653.6921619D317@lists.macosforge.org>
	<1C7C9931-DB88-469B-942A-45396F96FAE3@gmail.com>
Message-ID: <A4330014-C10B-43B4-AAEE-FAF632B282ED@inius.com>

I had to do a hard shut down (hold the power button until the system  
turned off) on my computer with a zfs raidz yesterday.

After restarting I have not been able to get my zfs drive back up. I  
can see all the disks with diskutil list but when I do a zpool status  
or list I get no pools available and import -f or import -af just give  
me a can't import "invalid vdev configuration" error.

Is there any way to recover from this?

Thanks,
Kane

From ndellofano at apple.com  Tue Mar 11 13:30:26 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue, 11 Mar 2008 13:30:26 -0700
Subject: [zfs-discuss] invalid vdev configuration after crash
In-Reply-To: <A4330014-C10B-43B4-AAEE-FAF632B282ED@inius.com>
References: <20080131211653.6921619D317@lists.macosforge.org>
	<1C7C9931-DB88-469B-942A-45396F96FAE3@gmail.com>
	<A4330014-C10B-43B4-AAEE-FAF632B282ED@inius.com>
Message-ID: <A4CB19DC-64AA-48E4-B946-A3FE5D3842EE@apple.com>

This is truely odd.  Can you send your 'zpool import' output?  So all  
the drives are back up on the system and working?  Also when you  
created the pool did you use the full disk names or the actual slice  
name, a.k.a. disk2s2' ?

Noel

On Mar 11, 2008, at 9:23 AM, Kane Dijkman wrote:

> I had to do a hard shut down (hold the power button until the system
> turned off) on my computer with a zfs raidz yesterday.
>
> After restarting I have not been able to get my zfs drive back up. I
> can see all the disks with diskutil list but when I do a zpool status
> or list I get no pools available and import -f or import -af just give
> me a can't import "invalid vdev configuration" error.
>
> Is there any way to recover from this?
>
> Thanks,
> Kane
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From franzschmalzl at spamfreemail.de  Tue Mar 11 14:04:51 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Tue, 11 Mar 2008 22:04:51 +0100
Subject: [zfs-discuss] flash sound
Message-ID: <248E8395-CE82-4C84-8DDD-AFE35D1FD862@spamfreemail.de>

ok strange things happen around here...

since my conversion to a homefolder on zfs, sound in flash player does  
not work anymore....

how is this even related to the underlying fielsystem ?



From ndellofano at apple.com  Tue Mar 11 14:08:48 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue, 11 Mar 2008 14:08:48 -0700
Subject: [zfs-discuss] flash sound
In-Reply-To: <248E8395-CE82-4C84-8DDD-AFE35D1FD862@spamfreemail.de>
References: <248E8395-CE82-4C84-8DDD-AFE35D1FD862@spamfreemail.de>
Message-ID: <4E2F1F49-563E-41C6-8ED2-F9326D8006B7@apple.com>

bizarro.  This is nothing I've ever encountered.  I'm not sure how  
flash player would have anything directly to do with the  
filesystem.... weird

Noel
On Mar 11, 2008, at 2:04 PM, Franz Schmalzl wrote:

> ok strange things happen around here...
>
> since my conversion to a homefolder on zfs, sound in flash player does
> not work anymore....
>
> how is this even related to the underlying fielsystem ?
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From franzschmalzl at spamfreemail.de  Tue Mar 11 14:26:35 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Tue, 11 Mar 2008 22:26:35 +0100
Subject: [zfs-discuss] flash sound
In-Reply-To: <063B4D00-6F05-49D8-852D-1A7346590661@spamfreemail.de>
References: <248E8395-CE82-4C84-8DDD-AFE35D1FD862@spamfreemail.de>
	<4E2F1F49-563E-41C6-8ED2-F9326D8006B7@apple.com>
	<063B4D00-6F05-49D8-852D-1A7346590661@spamfreemail.de>
Message-ID: <5741FCE9-3B63-40A1-9A4A-F4A54474CCF3@spamfreemail.de>

ok i *think* i know why, but i don't know how to fix this

my zfs setup

NAME                 USED  AVAIL  REFER  MOUNTPOINT
hometank            59,4G   301G  8,01M  /Users
hometank/ruebezahl  59,4G   301G  59,4G  /Users/ruebezahl

osx is seeing some differences here, for example my home folder is no  
longer called homefolder but hometank
but it logs in correctly and uses hometank/ruebezahl as home
also my homefolder is viewed as an alias, and if  open it it takes me  
to my zfs volume...
maybe this confuses the flash player, i tried a reinstall but it won't  
let me because it says it can't create a new folder inside my  
preferences folder
permissions are correct, and it touches a file called version.txt.
for some odd reason it won't create the new folder
maybe flash player is confused by the way osx  links to my homefolder  
because it is stored on a zfs volume ?

earliere my homefolder was directly on hometank
(hometank/ruebezahl did not exist back than, flash player worked)


hmmmmm


On 11.03.2008, at 22:13, Franz Schmalzl wrote:

> it is indeed ;-)
>
> as you say, flash player should not even notice the underlaying fs
>
>
> i'm gonna play around a bit and keep you posted
>
>
>> bizarro.  This is nothing I've ever encountered.  I'm not sure how  
>> flash player would have anything directly to do with the  
>> filesystem.... weird
>>
>> Noel
>> On Mar 11, 2008, at 2:04 PM, Franz Schmalzl wrote:
>>
>>> ok strange things happen around here...
>>>
>>> since my conversion to a homefolder on zfs, sound in flash player  
>>> does
>>> not work anymore....
>>>
>>> how is this even related to the underlying fielsystem ?
>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>


From lists at loveturtle.net  Tue Mar 11 14:28:36 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Tue, 11 Mar 2008 17:28:36 -0400
Subject: [zfs-discuss] flash sound
In-Reply-To: <4E2F1F49-563E-41C6-8ED2-F9326D8006B7@apple.com>
References: <248E8395-CE82-4C84-8DDD-AFE35D1FD862@spamfreemail.de>
	<4E2F1F49-563E-41C6-8ED2-F9326D8006B7@apple.com>
Message-ID: <47D6F984.80909@loveturtle.net>

I have no idea if they are related but I am having the exact same 
problem, for a while now I've been trying to figure out why the hell I 
don't have sound in youtube since updating to 10.5 (I put my homedir on 
ZFS on day one).

Tonight or tomorrow I'll copy it over to HFS and see if it does infact 
have anything to do with being on ZFS...

No?l Dellofano wrote:
> bizarro.  This is nothing I've ever encountered.  I'm not sure how  
> flash player would have anything directly to do with the  
> filesystem.... weird
>
> Noel
> On Mar 11, 2008, at 2:04 PM, Franz Schmalzl wrote:
>
>   
>> ok strange things happen around here...
>>
>> since my conversion to a homefolder on zfs, sound in flash player does
>> not work anymore....
>>
>> how is this even related to the underlying fielsystem ?
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>     
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>   


From franzschmalzl at spamfreemail.de  Tue Mar 11 14:38:34 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Tue, 11 Mar 2008 22:38:34 +0100
Subject: [zfs-discuss] Fwd:  flash sound
References: <F7DC08AD-2480-4BB6-97B7-45BDBBC040F0@spamfreemail.de>
Message-ID: <11ABFD9D-AB8F-425C-90E0-02B8C3F670EC@spamfreemail.de>



guys i tell you, zfs is possesed by daemons :)
> On 11.03.2008, at 22:28, Dillon Kass wrote:
>
>> I have no idea if they are related but I am having the exact same
>> problem, for a while now I've been trying to figure out why the  
>> hell I
>> don't have sound in youtube since updating to 10.5 (I put my  
>> homedir on
>> ZFS on day one).
>>
>> Tonight or tomorrow I'll copy it over to HFS and see if it does  
>> infact
>> have anything to do with being on ZFS...
>>
>> No?l Dellofano wrote:
>>> bizarro.  This is nothing I've ever encountered.  I'm not sure how
>>> flash player would have anything directly to do with the
>>> filesystem.... weird
>>>
>>> Noel
>>> On Mar 11, 2008, at 2:04 PM, Franz Schmalzl wrote:
>>>
>>>
>>>> ok strange things happen around here...
>>>>
>>>> since my conversion to a homefolder on zfs, sound in flash player  
>>>> does
>>>> not work anymore....
>>>>
>>>> how is this even related to the underlying fielsystem ?
>>>>
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From franzschmalzl at spamfreemail.de  Tue Mar 11 15:26:49 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Tue, 11 Mar 2008 23:26:49 +0100
Subject: [zfs-discuss] flash sound
Message-ID: <C6923D3D-EEAA-47C9-B1AE-37ED37388165@spamfreemail.de>

ok this is very odd, but i have got the solution

again my zfs setup

NAME                 USED  AVAIL  REFER  MOUNTPOINT
hometank            58,4G   302G  16,6M  /Users
hometank/ruebezahl  58,4G   302G  58,4G  /Users/ruebezahl

when my homefolder is storden in a zfs filesystem inside my zpool
flash sound is not working

when my homefolder is stored on the root of my zpool it works


( i tried with a test user, first created his homefolder on the root  
of hometank ---> flash sound worked; then created another zfs  
filesystem and created a new user inside ---> sound was gone )

noel, can you explain ?



From ndellofano at apple.com  Tue Mar 11 16:45:45 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue, 11 Mar 2008 16:45:45 -0700
Subject: [zfs-discuss] flash sound
In-Reply-To: <C6923D3D-EEAA-47C9-B1AE-37ED37388165@spamfreemail.de>
References: <C6923D3D-EEAA-47C9-B1AE-37ED37388165@spamfreemail.de>
Message-ID: <8B9C2E4E-A545-4999-9BC2-A39797011804@apple.com>

not immediately, other then nested filesystems in zfs pools are still  
confusing Finder and the upper layers since they aren't really sure  
how to handle them.  My guess is this has somethign to do with that  
problem that upper layers where the flash is trying to run.  I will  
add this information to the bug I have tracking issues like this with  
userland and nested filesystems.

apologies for the weirdnes.... still getting stuff like this hammered  
out..

Noel

On Mar 11, 2008, at 3:26 PM, Franz Schmalzl wrote:

> ok this is very odd, but i have got the solution
>
> again my zfs setup
>
> NAME                 USED  AVAIL  REFER  MOUNTPOINT
> hometank            58,4G   302G  16,6M  /Users
> hometank/ruebezahl  58,4G   302G  58,4G  /Users/ruebezahl
>
> when my homefolder is storden in a zfs filesystem inside my zpool
> flash sound is not working
>
> when my homefolder is stored on the root of my zpool it works
>
>
> ( i tried with a test user, first created his homefolder on the root  
> of hometank ---> flash sound worked; then created another zfs  
> filesystem and created a new user inside ---> sound was gone )
>
> noel, can you explain ?
>
>


From franzschmalzl at spamfreemail.de  Tue Mar 11 16:58:44 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Wed, 12 Mar 2008 00:58:44 +0100
Subject: [zfs-discuss] Fwd: flash sound
References: <C16EA8D7-A2D9-449E-9AF9-3E538E9EEC6D@spamfreemail.de>
Message-ID: <3AB5583B-14F7-4796-8C76-C1CC0645153C@spamfreemail.de>



> not immediately, other then nested filesystems in zfs pools are  
> still confusing Finder and the upper layers since they aren't really  
> sure how to handle them.  My guess is this has somethign to do with  
> that problem that upper layers where the flash is trying to run.  I  
> will add this information to the bug I have tracking issues like  
> this with userland and nested filesystems.

yap, that was my first guess too..

it's not just flash by the way, firefox is confused to ( whon  
installing addons e.g.)

as you said.. osx doesn't handle it the best way at the moment

>
>
> apologies for the weirdnes.... still getting stuff like this  
> hammered out..
>

no problem :)

kind regards
franz
>
>
>> Noel
>>
>> On Mar 11, 2008, at 3:26 PM, Franz Schmalzl wrote:
>>
>>> ok this is very odd, but i have got the solution
>>>
>>> again my zfs setup
>>>
>>> NAME                 USED  AVAIL  REFER  MOUNTPOINT
>>> hometank            58,4G   302G  16,6M  /Users
>>> hometank/ruebezahl  58,4G   302G  58,4G  /Users/ruebezahl
>>>
>>> when my homefolder is storden in a zfs filesystem inside my zpool
>>> flash sound is not working
>>>
>>> when my homefolder is stored on the root of my zpool it works
>>>
>>>
>>> ( i tried with a test user, first created his homefolder on the  
>>> root of hometank ---> flash sound worked; then created another zfs  
>>> filesystem and created a new user inside ---> sound was gone )
>>>
>>> noel, can you explain ?
>>>
>>>
>>
>


From kane at inius.com  Tue Mar 11 19:46:31 2008
From: kane at inius.com (Kane Dijkman)
Date: Tue, 11 Mar 2008 19:46:31 -0700
Subject: [zfs-discuss] invalid vdev configuration after crash
In-Reply-To: <A4CB19DC-64AA-48E4-B946-A3FE5D3842EE@apple.com>
References: <20080131211653.6921619D317@lists.macosforge.org>
	<1C7C9931-DB88-469B-942A-45396F96FAE3@gmail.com>
	<A4330014-C10B-43B4-AAEE-FAF632B282ED@inius.com>
	<A4CB19DC-64AA-48E4-B946-A3FE5D3842EE@apple.com>
Message-ID: <D15B74A5-D10A-47DF-8A0E-C8774E95C992@inius.com>

As of this morning when I sent this (before leaving for work and after  
the system was up for 8 hours) I had the following

Lumiere:~ kane$ zpool import -af
cannot import 'zfsMirror': invalid vdev configuration

as the only message.

I just got home and tried again and got the following

Lumiere:~ kane$ zpool import
   pool: zfsMirror
     id: 5643149568963753982
  state: FAULTED
action: The pool cannot be imported due to damaged devices or data.
config:

	zfsMirror                 UNAVAIL  insufficient replicas
	  raidz1                  UNAVAIL  corrupted data
	    disk7s2               ONLINE
	    disk8s2               ONLINE
	    disk12s2              ONLINE
	    10120111723985263653  ONLINE
	    disk6s2               ONLINE


So the system is currently offline and not working.

I have no idea what 10120111723985263653 is. When things were working  
this was just a normal disk?s? name.

Looking at my list of drives with diskutil list it appears that the  
following is this weird numbered drive
/dev/disk10
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *279.5 Gi    
disk10
    1:                        EFI                         200.0 Mi    
disk10s1
    2:                        ZFS zfsMirror               279.1 Gi    
disk10s2


The pool only ever had these 5 disks in it.


When I created the pool I used the slice names


Is there a way to recover from this fubarred state?

Also, a little more info that may or may not be important. All my  
drives are in an external firewire mega case. One of the the drives  
for a normal apple raid had died so I took the case apart, removed  
that drive and then moved another drive into its place (so it was  
connected elsewhere in the firewire ide chain). It was one of the  
drives from this pool. But after that was done I booted up and the  
pool showed up and everything worked as normal without any issues that  
I am aware of. It wasnt until a few restarts later that I had the  
crash that has me in the current situation.

Everything on this pool is mirrored on another raid, so I can easily  
just destroy and recreate it. But I want to make sure there is no  
other useful info I can give you before I do.

Thanks,
Kane






On Mar 11, 2008, at 1:30 PM, No?l Dellofano wrote:

> This is truely odd.  Can you send your 'zpool import' output?  So  
> all the drives are back up on the system and working?  Also when you  
> created the pool did you use the full disk names or the actual slice  
> name, a.k.a. disk2s2' ?
>
> Noel
>
> On Mar 11, 2008, at 9:23 AM, Kane Dijkman wrote:
>
>> I had to do a hard shut down (hold the power button until the system
>> turned off) on my computer with a zfs raidz yesterday.
>>
>> After restarting I have not been able to get my zfs drive back up. I
>> can see all the disks with diskutil list but when I do a zpool status
>> or list I get no pools available and import -f or import -af just  
>> give
>> me a can't import "invalid vdev configuration" error.
>>
>> Is there any way to recover from this?
>>
>> Thanks,
>> Kane
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


----------------------------------------------------------------------------------
Does the movement of the trees make the wind blow?


From bwaters at nrao.edu  Wed Mar 12 08:23:41 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Wed, 12 Mar 2008 09:23:41 -0600
Subject: [zfs-discuss] ZFS and external enclosure
In-Reply-To: <3239.134.106.119.111.1205333109.squirrel@webmail.uni-oldenburg.de>
References: <3239.134.106.119.111.1205333109.squirrel@webmail.uni-oldenburg.de>
Message-ID: <65B251A9-63C3-4460-A3C0-3771687813E5@nrao.edu>


On Mar 12, 2008, at 8:45 AM, Carsten Engelberts wrote:

> I want to do the same ZFS-Enclosure-thing ;-).


Great! We can share info!


> I have an MBP with eSATA-Card (with SIL3132-Chip)

I have one too, it works great with the external enclosure (hardware  
port multiplier)


> I want to upgrade next year to 4x1TB drives Do you think "your"  
> enclosure (btw. thanks for the link) is capable of that?

Yes. I use Seagates. I had ordered two Western Digital "green" (low  
power) drives, one worked after some nudging, the other never worked,  
on three different computers, five different enclosures, three  
operating systems. I don't know why. I send both back and got Seagate  
7200.11 1 TB drives. Too bad, as I think those low-power Western  
Digital drives would be a slam-dunk for me: low heat, power, single- 
user.


> Another question is , what is the difference between MST5X1PM and  
> MST5XHPM

Hmm, let's look at that link again:
http://www.addonics.com/products/raid_system/mst4.asp

OK - the "HPM" can be set up to do the RAID on the *enclosure*, rather  
than the *host computer*. You have to use a Sil3132 or 3134 controller  
to set up the RAID; your eSATA Card should work (but I don't know  
about RAID utilities for Mac OS X Leopard, you might need Windows via  
Boot Camp to set it up).  Once the RAID is set up, however, the  
enclosure will look like a single (large) drive, that you could use  
with any computer.
For ZFS, that's not what you want.
You want ZFS to provision the raidz array for you. For that, the host  
computer needs to see four individual drives.
The MST5X1PM uses a Port Multiplier, too, but can't do RAID on the PM:  
the host controller has one eSATA cable to the enclosure but sees four  
drives. SATA controllers that don't implement the port-multiplier spec  
will be confused and won't work. Your eSATA-Card should work fine.


Last week I discovered this:
http://www.newegg.com/Product/Product.aspx?Item=N82E16822204074
Must... not... hit... purchase... button! (gasp)

For $1000, you get four 1 TB SATA drives, the same port multiplier, a  
better enclosure (backplane rather than cables, supports hot-swap I  
think), the eSATA cable, and the PCIe host controller.  That's  
stunning. That's going to be $400-$600 less than doing it yourself.
But I haven't tested it yet, so I don't know. If someone wants to buy  
me one, I'll be happy to test it! :-)

- boyd


From jbsnyder at gmail.com  Wed Mar 12 09:10:36 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Wed, 12 Mar 2008 11:10:36 -0500
Subject: [zfs-discuss] Fwd: flash sound
In-Reply-To: <3AB5583B-14F7-4796-8C76-C1CC0645153C@spamfreemail.de>
References: <C16EA8D7-A2D9-449E-9AF9-3E538E9EEC6D@spamfreemail.de>
	<3AB5583B-14F7-4796-8C76-C1CC0645153C@spamfreemail.de>
Message-ID: <33644d3c0803120910l78936148gb162b78b2dd6c5c5@mail.gmail.com>

I've experienced the firefox issue as well.  I was going to submit a
report regarding it, and could do so still if few other people are
having this problem, but it sounds like I'm not the only one.

Regarding Flash sound, I'm NOT having this problem (Sound works in
Firefox & Safari).  If it helps at all, here are my zfs mounts:

NAME                       USED  AVAIL  REFER  MOUNTPOINT
Aquarium                  36.8G  60.7G  56.5K  none
Aquarium/jsnyder          35.4G  60.7G  23.1G  /Users/jsnyder
Aquarium/jsnyder/Sources  10.3G  60.7G  10.2G  /Users/jsnyder/Sources
Aquarium/opt              1.40G  60.7G  1.39G  /opt

My flash version is:
9,0,115,0

I'm working out of /Users/jsnyder...

Not to pester, but might there be another source or binary update
coming in the near future?  If not, I'm still quite happy with what
I've got :-)  I assume 102A is still the most current available.



On Tue, Mar 11, 2008 at 6:58 PM, Franz Schmalzl
<franzschmalzl at spamfreemail.de> wrote:
>
>
>  > not immediately, other then nested filesystems in zfs pools are
>  > still confusing Finder and the upper layers since they aren't really
>  > sure how to handle them.  My guess is this has somethign to do with
>  > that problem that upper layers where the flash is trying to run.  I
>  > will add this information to the bug I have tracking issues like
>  > this with userland and nested filesystems.
>
>  yap, that was my first guess too..
>
>  it's not just flash by the way, firefox is confused to ( whon
>  installing addons e.g.)
>
>  as you said.. osx doesn't handle it the best way at the moment
>
>
>  >
>  >
>  > apologies for the weirdnes.... still getting stuff like this
>  > hammered out..
>  >
>
>  no problem :)
>
>  kind regards
>  franz
>
>
> >
>  >
>  >> Noel
>  >>
>  >> On Mar 11, 2008, at 3:26 PM, Franz Schmalzl wrote:
>  >>
>  >>> ok this is very odd, but i have got the solution
>  >>>
>  >>> again my zfs setup
>  >>>
>  >>> NAME                 USED  AVAIL  REFER  MOUNTPOINT
>  >>> hometank            58,4G   302G  16,6M  /Users
>  >>> hometank/ruebezahl  58,4G   302G  58,4G  /Users/ruebezahl
>  >>>
>  >>> when my homefolder is storden in a zfs filesystem inside my zpool
>  >>> flash sound is not working
>  >>>
>  >>> when my homefolder is stored on the root of my zpool it works
>  >>>
>  >>>
>  >>> ( i tried with a test user, first created his homefolder on the
>  >>> root of hometank ---> flash sound worked; then created another zfs
>  >>> filesystem and created a new user inside ---> sound was gone )
>  >>>
>  >>> noel, can you explain ?
>  >>>
>  >>>
>  >>
>  >
>
>  _______________________________________________
>  zfs-discuss mailing list
>  zfs-discuss at lists.macosforge.org
>  http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From bwaters at nrao.edu  Wed Mar 12 09:51:57 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Wed, 12 Mar 2008 10:51:57 -0600
Subject: [zfs-discuss] ZFS and external enclosure
In-Reply-To: <5AAADC35-E6D9-48AF-87EF-AC7BE7F74F61@spamfreemail.de>
References: <3239.134.106.119.111.1205333109.squirrel@webmail.uni-oldenburg.de>
	<65B251A9-63C3-4460-A3C0-3771687813E5@nrao.edu>
	<5AAADC35-E6D9-48AF-87EF-AC7BE7F74F61@spamfreemail.de>
Message-ID: <5D79E40D-D0E6-4639-803D-273BD7AC66E3@nrao.edu>


On Mar 12, 2008, at 10:21 AM, Franz Schmalzl wrote:
> hey guys, i thought i just join your conversation...
>
> i also plan to get an external jbod case, 4 -5 drives would be great
>
> but the problem i am facing is that i have an imac, so e-sata isn't  
> an option for me...
>
> do you know any good firewire cases ?


You might try this USB-to-eSATA adapter:

http://www.newegg.com/Product/Product.aspx?Item=N82E16812174001

I don't know if it will work well or not. I know that many times  
people who report bad luck with external drives are Windows users, and  
Windows XP might have bad trouble with USB drives because of the way  
it uses its integrated file cache. OK, forget I said that -- I have no  
idea.

But the USB adapter isn't much money, and you could test it with  
someone's SATA drive to see if it worked for you. If so, you could go  
ahead and get an eSATA enclosure and be OK for later use.

Firewire can be a good choice, but you wouldn't do port multiplier,  
you'd just daisy-chain, and you generally you can't hot-swap in the  
middle of a chain (you break the chain, and *all* disks down the chain  
go away).  For a four- or five-disk chain this should be a problem, I  
think.

I wouldn't bother with a dedicated "pro" enclosure for firewire. I'd  
just get four of these, either 500GB or 1TB:

http://www.newegg.com/Product/Product.aspx?Item=N82E16822204073
http://www.newegg.com/Product/Product.aspx?Item=N82E16822204066


Then I'd daisy-chain them via the Firewire ports. You can get short  
FireWire cables:
http://www.cooldrives.com/86to6pinfird.html

Such a setup will cost slightly more, and there's more cables running  
around, but you have lots of options for connecting the drives.

- boyd


From franzschmalzl at spamfreemail.de  Wed Mar 12 10:09:22 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Wed, 12 Mar 2008 18:09:22 +0100
Subject: [zfs-discuss] ZFS and external enclosure
In-Reply-To: <5D79E40D-D0E6-4639-803D-273BD7AC66E3@nrao.edu>
References: <3239.134.106.119.111.1205333109.squirrel@webmail.uni-oldenburg.de>
	<65B251A9-63C3-4460-A3C0-3771687813E5@nrao.edu>
	<5AAADC35-E6D9-48AF-87EF-AC7BE7F74F61@spamfreemail.de>
	<5D79E40D-D0E6-4639-803D-273BD7AC66E3@nrao.edu>
Message-ID: <B75D4AC5-058B-4696-AAF0-83AA29147FC0@spamfreemail.de>

ahh i had bad experiences with esata to usb (not very stable)
and of course it's usb *yuck*

as of breaking the chain...

atm moment i have 3 "icy dock cases" wich support hotswap ( you can  
still plug out the drive and the case remains running, so the daisy  
chain won't brake. i tried it)
http://www.raidsonic.de/de/pages/products/external_cases.php?we_objectID=4656

they are pretty good, and cost around 70 Euros, with firewire 800  
around 100
(around 100 and 150 $ respectively)

very thin and because they are made of alu, they requiere no extra  
cooling fan

they are made by a german company, so they are easy to get for me  
since i live in austria, but they are are available in the USA as well

and thank's for your answer, i will most likely stick to these cases  
and just stack them next to each other like you proposed...

regards
franz




>
>
> You might try this USB-to-eSATA adapter:
>
> http://www.newegg.com/Product/Product.aspx?Item=N82E16812174001
>
> I don't know if it will work well or not. I know that many times  
> people who report bad luck with external drives are Windows users,  
> and Windows XP might have bad trouble with USB drives because of the  
> way it uses its integrated file cache. OK, forget I said that -- I  
> have no idea.
>
> But the USB adapter isn't much money, and you could test it with  
> someone's SATA drive to see if it worked for you. If so, you could  
> go ahead and get an eSATA enclosure and be OK for later use.
>
> Firewire can be a good choice, but you wouldn't do port multiplier,  
> you'd just daisy-chain, and you generally you can't hot-swap in the  
> middle of a chain (you break the chain, and *all* disks down the  
> chain go away).  For a four- or five-disk chain this should be a  
> problem, I think.
>
> I wouldn't bother with a dedicated "pro" enclosure for firewire. I'd  
> just get four of these, either 500GB or 1TB:
>
> http://www.newegg.com/Product/Product.aspx?Item=N82E16822204073
> http://www.newegg.com/Product/Product.aspx?Item=N82E16822204066
>
>
> Then I'd daisy-chain them via the Firewire ports. You can get short  
> FireWire cables:
> http://www.cooldrives.com/86to6pinfird.html
>
> Such a setup will cost slightly more, and there's more cables  
> running around, but you have lots of options for connecting the  
> drives.
>
> - boyd
>


From bwaters at nrao.edu  Wed Mar 12 11:21:53 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Wed, 12 Mar 2008 12:21:53 -0600
Subject: [zfs-discuss] ZFS and external enclosure
In-Reply-To: <B75D4AC5-058B-4696-AAF0-83AA29147FC0@spamfreemail.de>
References: <3239.134.106.119.111.1205333109.squirrel@webmail.uni-oldenburg.de>
	<65B251A9-63C3-4460-A3C0-3771687813E5@nrao.edu>
	<5AAADC35-E6D9-48AF-87EF-AC7BE7F74F61@spamfreemail.de>
	<5D79E40D-D0E6-4639-803D-273BD7AC66E3@nrao.edu>
	<B75D4AC5-058B-4696-AAF0-83AA29147FC0@spamfreemail.de>
Message-ID: <E247B9E8-29BC-4CF9-B532-86AD5B22CD69@nrao.edu>


On Mar 12, 2008, at 11:09 AM, Franz Schmalzl wrote:
> atm moment i have 3 "icy dock cases" wich support hotswap ( you can  
> still plug out the drive and the case remains running, so the daisy  
> chain won't brake. i tried it)
> http://www.raidsonic.de/de/pages/products/external_cases.php?we_objectID=4656

Those might be better than the ones I had:
http://www.macgurus.com/productpages/firewire/U6SATA.php

You pull the tray, you break the chain. And you might fry the Oxford  
912 chip... as I did...  bad luck for me. (They still work as USB  
enclosures.)

For U.S customers, NewEgg has that newer Icy Dock, it does look nice!
http://www.newegg.com/Product/Product.aspx?Item=N82E16817198004



From bwaters at nrao.edu  Wed Mar 12 11:32:19 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Wed, 12 Mar 2008 12:32:19 -0600
Subject: [zfs-discuss] ZFS and external enclosure
In-Reply-To: <E247B9E8-29BC-4CF9-B532-86AD5B22CD69@nrao.edu>
References: <3239.134.106.119.111.1205333109.squirrel@webmail.uni-oldenburg.de>
	<65B251A9-63C3-4460-A3C0-3771687813E5@nrao.edu>
	<5AAADC35-E6D9-48AF-87EF-AC7BE7F74F61@spamfreemail.de>
	<5D79E40D-D0E6-4639-803D-273BD7AC66E3@nrao.edu>
	<B75D4AC5-058B-4696-AAF0-83AA29147FC0@spamfreemail.de>
	<E247B9E8-29BC-4CF9-B532-86AD5B22CD69@nrao.edu>
Message-ID: <20A06501-A80E-499B-AEF7-0B40866EB586@nrao.edu>


On Mar 12, 2008, at 12:21 PM, Boyd Waters wrote:
> Those might be better than the ones I had:
> http://www.macgurus.com/productpages/firewire/U6SATA.php
>
> You pull the tray, you break the chain. And you might fry the Oxford
> 912 chip... as I did...  bad luck for me. (They still work as USB
> enclosures.)

(
one more thing... the failure is not MacGuru's fault, I purchased  
these from another vendor, but cannot find the product at that  
vendor's site any more. ICY Dock/Cremax is OEM for lots of things,  
resold and supported by many others. When my problem occurred, I got  
*great* support from WiebeTech, they were concerned about the failure  
even though they had not sold me the devices!

anyhow, that's why I think that it is important to share notes on  
external drive systems.

but I realize that this isn't a ZFS issue.

that's enough from me for one day! and thanks for listening!
)


From katie.loch at gmail.com  Wed Mar 12 19:05:13 2008
From: katie.loch at gmail.com (Katie Locher)
Date: Wed Mar 12 20:02:08 2008
Subject: [zfs-discuss] ZFS and DTrace
Message-ID: <ee26ea0d0803121905p5e50a761ja14f5869dbb62c33@mail.gmail.com>

I'm looking for some information on instrumenting ZFS behavior with DTrace
on Leopard.  I have a ZFS pool set up, but all the information I can find on
instrumenting ZFS with DTrace seems to be Solaris specific.  For example,
the FBT provider has a ZFS module on Solaris but not on my version of
Leopard.  The fsinfo provider doesn't appear on Leopard.  Has anybody done
any work with DTrace and ZFS on Leopard?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080312/f08cfcab/attachment.html
From jbsnyder at gmail.com  Thu Mar 13 08:27:28 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Thu Mar 13 08:27:05 2008
Subject: [zfs-discuss] Firefox Plugin Install Related to Preallocation?
Message-ID: <33644d3c0803130827tbb680f5wfdbd8b5d65bae051@mail.gmail.com>

Along the lines of the firefox addon installation issue, I've pked
around with dtruss & errinfo a little, and it seems like at least one
error that is taking place at the time of plugin install on Firefox
3b4 is the following:

fcntl(0x2F, 0x2A, 0xFFFFFFFFBFFFBCF0)		 = -1 Err#45

45 is operation unsupported, and 0x2A is F_PREALLOCATE.  I've not done
much file IO work in C beyond the basics so I haven't made a test case
for this.  I was wondering if perhaps this syscall, or perhaps one of
the modes isn't implemented yet for ZFS?

The firefox source itself doesn't seem to have any references to
F_PREALLOCATE, so I suppose one if it's libraries might be using it?

I also see in zfs_kext/zfs/zfs_vfsops.c:

     * statvfs() should really be called statufs(), because it assumes
     * static metadata.  ZFS doesn't preallocate files, so the best
     * we can do is report the max that could possibly fit in f_files,
     * and that minus the number actually used in f_ffree.

I just thought I would see what I could dig up in a short amount of
time with dtrace and a little grepping...

On Wed, Mar 12, 2008 at 11:10 AM, James Snyder <jbsnyder@gmail.com> wrote:
> I've experienced the firefox issue as well.  I was going to submit a
>  report regarding it, and could do so still if few other people are
>  having this problem, but it sounds like I'm not the only one.
>
>  Regarding Flash sound, I'm NOT having this problem (Sound works in
>  Firefox & Safari).  If it helps at all, here are my zfs mounts:
>
>
>  NAME                       USED  AVAIL  REFER  MOUNTPOINT
>  Aquarium                  36.8G  60.7G  56.5K  none
>  Aquarium/jsnyder          35.4G  60.7G  23.1G  /Users/jsnyder
>  Aquarium/jsnyder/Sources  10.3G  60.7G  10.2G  /Users/jsnyder/Sources
>  Aquarium/opt              1.40G  60.7G  1.39G  /opt
>
>  My flash version is:
>  9,0,115,0
>
>  I'm working out of /Users/jsnyder...
>
>  Not to pester, but might there be another source or binary update
>  coming in the near future?  If not, I'm still quite happy with what
>  I've got :-)  I assume 102A is still the most current available.
>
>
>
>
>
>  On Tue, Mar 11, 2008 at 6:58 PM, Franz Schmalzl
>  <franzschmalzl@spamfreemail.de> wrote:
>  >
>  >
>  >  > not immediately, other then nested filesystems in zfs pools are
>  >  > still confusing Finder and the upper layers since they aren't really
>  >  > sure how to handle them.  My guess is this has somethign to do with
>  >  > that problem that upper layers where the flash is trying to run.  I
>  >  > will add this information to the bug I have tracking issues like
>  >  > this with userland and nested filesystems.
>  >
>  >  yap, that was my first guess too..
>  >
>  >  it's not just flash by the way, firefox is confused to ( whon
>  >  installing addons e.g.)
>  >
>  >  as you said.. osx doesn't handle it the best way at the moment
>  >
>  >
>  >  >
>  >  >
>  >  > apologies for the weirdnes.... still getting stuff like this
>  >  > hammered out..
>  >  >
>  >
>  >  no problem :)
>  >
>  >  kind regards
>  >  franz
>  >
>  >
>  > >
>  >  >
>  >  >> Noel
>  >  >>
>  >  >> On Mar 11, 2008, at 3:26 PM, Franz Schmalzl wrote:
>  >  >>
>  >  >>> ok this is very odd, but i have got the solution
>  >  >>>
>  >  >>> again my zfs setup
>  >  >>>
>  >  >>> NAME                 USED  AVAIL  REFER  MOUNTPOINT
>  >  >>> hometank            58,4G   302G  16,6M  /Users
>  >  >>> hometank/ruebezahl  58,4G   302G  58,4G  /Users/ruebezahl
>  >  >>>
>  >  >>> when my homefolder is storden in a zfs filesystem inside my zpool
>  >  >>> flash sound is not working
>  >  >>>
>  >  >>> when my homefolder is stored on the root of my zpool it works
>  >  >>>
>  >  >>>
>  >  >>> ( i tried with a test user, first created his homefolder on the
>  >  >>> root of hometank ---> flash sound worked; then created another zfs
>  >  >>> filesystem and created a new user inside ---> sound was gone )
>  >  >>>
>  >  >>> noel, can you explain ?
>  >  >>>
>  >  >>>
>  >  >>
>  >  >
>  >
>  >  _______________________________________________
>  >  zfs-discuss mailing list
>  >  zfs-discuss@lists.macosforge.org
>  >  http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>  >
>
>
>
>  --
>  James Snyder
>  Biomedical Engineering
>  Northwestern University
>  jbsnyder@gmail.com
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
From ndellofano at apple.com  Thu Mar 13 09:57:16 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu Mar 13 09:58:21 2008
Subject: [zfs-discuss] ZFS and DTrace
In-Reply-To: <ee26ea0d0803121905p5e50a761ja14f5869dbb62c33@mail.gmail.com>
References: <ee26ea0d0803121905p5e50a761ja14f5869dbb62c33@mail.gmail.com>
Message-ID: <79B6DB59-F01A-4D85-9CD3-05E6AD988C97@apple.com>

Currently you cannot dtrace Kexts(which ZFS is) in Leopard, hence you  
won't be able to dtrace ZFS on Leopard. The Dtrace team is working on  
this functionality...

Noel

On Mar 12, 2008, at 7:05 PM, Katie Locher wrote:

> I'm looking for some information on instrumenting ZFS behavior with  
> DTrace on Leopard.  I have a ZFS pool set up, but all the  
> information I can find on instrumenting ZFS with DTrace seems to be  
> Solaris specific.  For example, the FBT provider has a ZFS module on  
> Solaris but not on my version of Leopard.  The fsinfo provider  
> doesn't appear on Leopard.  Has anybody done any work with DTrace  
> and ZFS on Leopard?
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From i_see at macnews.de  Thu Mar 13 11:31:37 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Thu Mar 13 11:31:25 2008
Subject: [zfs-discuss] invalid vdev configuration after crash
In-Reply-To: <mailman.1486.1205340668.11791.zfs-discuss@lists.macosforge.org>
References: <mailman.1486.1205340668.11791.zfs-discuss@lists.macosforge.org>
Message-ID: <8DFCB52A-338E-432E-866D-28DCA1A65D75@macnews.de>

Hi list, hi Kane
I had similar problems during my early experiments with ZFS, but  
managed to get all my data out the pool (however, I was unable to  
fully recover it. No?l attributed the problems to one of the disks not  
having an GUID partition scheme, but I am not sure that was really the  
cause of the problem.

You get the weird number because disk10 is not recognized by zfs as  
being the disk correctly, i.e. the disk/pool identifiers do not match.
What worked for me was disconnecting the disk, bringing the pool up in  
a degraded state.
Unfortunately, I was unable to add the disk later without getting a  
kenel panic on the next zpool status, so I just copied all data from  
the broken pool and started over.
AFTER rescuing all data, you might try to copy / wipe the first/last  
MB of the disk using dd. That way, zfs would probably recognize the  
identification a broken and rewrite it from the other disks. Maybe  
No?l can recall my case and give you further instructions.
Hope this helps,
	ralf

Am 12.03.2008 um 17:51 schrieb zfs-discuss-request@lists.macosforge.org:

> As of this morning when I sent this (before leaving for work and  
> after the system was up for 8 hours) I had the following
>
> Lumiere:~ kane$ zpool import -af
> cannot import 'zfsMirror': invalid vdev configuration
>
> as the only message.
>
> I just got home and tried again and got the following
>
> Lumiere:~ kane$ zpool import
>  pool: zfsMirror
>    id: 5643149568963753982
> state: FAULTED
> action: The pool cannot be imported due to damaged devices or data.
> config:
>
> 	zfsMirror                 UNAVAIL  insufficient replicas
> 	  raidz1                  UNAVAIL  corrupted data
> 	    disk7s2               ONLINE
> 	    disk8s2               ONLINE
> 	    disk12s2              ONLINE
> 	    10120111723985263653  ONLINE
> 	    disk6s2               ONLINE
>
>
> So the system is currently offline and not working.
>
> I have no idea what 10120111723985263653 is. When things were  
> working this was just a normal disk?s? name.
>
> Looking at my list of drives with diskutil list it appears that the  
> following is this weird numbered drive
> /dev/disk10
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:      GUID_partition_scheme                        *279.5 Gi    
> disk10
>   1:                        EFI                         200.0 Mi    
> disk10s1
>   2:                        ZFS zfsMirror               279.1 Gi    
> disk10s2
>
>
> The pool only ever had these 5 disks in it.
>
>
> When I created the pool I used the slice names
>
>
> Is there a way to recover from this fubarred state?
>
> Also, a little more info that may or may not be important. All my  
> drives are in an external firewire mega case. One of the the drives  
> for a normal apple raid had died so I took the case apart, removed  
> that drive and then moved another drive into its place (so it was  
> connected elsewhere in the firewire ide chain). It was one of the  
> drives from this pool. But after that was done I booted up and the  
> pool showed up and everything worked as normal without any issues  
> that I am aware of. It wasnt until a few restarts later that I had  
> the crash that has me in the current situation.
>
> Everything on this pool is mirrored on another raid, so I can easily  
> just destroy and recreate it. But I want to make sure there is no  
> other useful info I can give you before I do.
>
> Thanks,
> Kane

From kane at inius.com  Thu Mar 13 14:35:21 2008
From: kane at inius.com (Kane Dijkman)
Date: Thu Mar 13 14:35:01 2008
Subject: [zfs-discuss] invalid vdev configuration after crash
In-Reply-To: <8DFCB52A-338E-432E-866D-28DCA1A65D75@macnews.de>
References: <mailman.1486.1205340668.11791.zfs-discuss@lists.macosforge.org>
	<8DFCB52A-338E-432E-866D-28DCA1A65D75@macnews.de>
Message-ID: <6BD78997-75A7-4F82-B5AC-4A59167E02F8@inius.com>

Ralf,

Thanks for the info. This is really good to know. As the name of the  
pool, zfsMirror, indicates its a mirror, so nothing was lost or at  
risk. I've blown the old one away and am creating a new one so I can  
copy things back over. But this is really good info to know should  
this happen again.

Thanks,
Kane


On Mar 13, 2008, at 11:31 AM, Ralf Bertling wrote:

> Hi list, hi Kane
> I had similar problems during my early experiments with ZFS, but  
> managed to get all my data out the pool (however, I was unable to  
> fully recover it. No?l attributed the problems to one of the disks  
> not having an GUID partition scheme, but I am not sure that was  
> really the cause of the problem.
>
> You get the weird number because disk10 is not recognized by zfs as  
> being the disk correctly, i.e. the disk/pool identifiers do not match.
> What worked for me was disconnecting the disk, bringing the pool up  
> in a degraded state.
> Unfortunately, I was unable to add the disk later without getting a  
> kenel panic on the next zpool status, so I just copied all data from  
> the broken pool and started over.
> AFTER rescuing all data, you might try to copy / wipe the first/last  
> MB of the disk using dd. That way, zfs would probably recognize the  
> identification a broken and rewrite it from the other disks. Maybe  
> No?l can recall my case and give you further instructions.
> Hope this helps,
> 	ralf
>
> Am 12.03.2008 um 17:51 schrieb zfs-discuss-request@lists.macosforge.org 
> :
>
>> As of this morning when I sent this (before leaving for work and  
>> after the system was up for 8 hours) I had the following
>>
>> Lumiere:~ kane$ zpool import -af
>> cannot import 'zfsMirror': invalid vdev configuration
>>
>> as the only message.
>>
>> I just got home and tried again and got the following
>>
>> Lumiere:~ kane$ zpool import
>> pool: zfsMirror
>>   id: 5643149568963753982
>> state: FAULTED
>> action: The pool cannot be imported due to damaged devices or data.
>> config:
>>
>> 	zfsMirror                 UNAVAIL  insufficient replicas
>> 	  raidz1                  UNAVAIL  corrupted data
>> 	    disk7s2               ONLINE
>> 	    disk8s2               ONLINE
>> 	    disk12s2              ONLINE
>> 	    10120111723985263653  ONLINE
>> 	    disk6s2               ONLINE
>>
>>
>> So the system is currently offline and not working.
>>
>> I have no idea what 10120111723985263653 is. When things were  
>> working this was just a normal disk?s? name.
>>
>> Looking at my list of drives with diskutil list it appears that the  
>> following is this weird numbered drive
>> /dev/disk10
>>  #:                       TYPE NAME                    SIZE        
>> IDENTIFIER
>>  0:      GUID_partition_scheme                        *279.5 Gi    
>> disk10
>>  1:                        EFI                         200.0 Mi    
>> disk10s1
>>  2:                        ZFS zfsMirror               279.1 Gi    
>> disk10s2
>>
>>
>> The pool only ever had these 5 disks in it.
>>
>>
>> When I created the pool I used the slice names
>>
>>
>> Is there a way to recover from this fubarred state?
>>
>> Also, a little more info that may or may not be important. All my  
>> drives are in an external firewire mega case. One of the the drives  
>> for a normal apple raid had died so I took the case apart, removed  
>> that drive and then moved another drive into its place (so it was  
>> connected elsewhere in the firewire ide chain). It was one of the  
>> drives from this pool. But after that was done I booted up and the  
>> pool showed up and everything worked as normal without any issues  
>> that I am aware of. It wasnt until a few restarts later that I had  
>> the crash that has me in the current situation.
>>
>> Everything on this pool is mirrored on another raid, so I can  
>> easily just destroy and recreate it. But I want to make sure there  
>> is no other useful info I can give you before I do.
>>
>> Thanks,
>> Kane
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss


---------------------------------------------------------------------------------------------
"It said Windows 2000 or better on the box, so I bought a Mac"

From ndellofano at apple.com  Thu Mar 13 15:41:41 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu Mar 13 15:42:50 2008
Subject: [zfs-discuss] zfs crashing regularly now
In-Reply-To: <7bccd8dc0803091046s1de88fecx77419622ef29fea@mail.gmail.com>
References: <7bccd8dc0803091046s1de88fecx77419622ef29fea@mail.gmail.com>
Message-ID: <B632C8AB-7F73-4269-A83A-B13F5D7C9C04@apple.com>

So the crash below is due to a corrupted disk(file in your case "LO  
ZFS plain file") or if the file disappeared suddenly and the ZFS can't  
find it to write to it.  It seems that it's possible that the file you  
backed the pool with is corrupted and therefore repeatedly crashing.   
I would recommend destroying the pool and recreating it, since you're  
backing store(vdev) has become corrupt.  Did you destroy both pools  
which were having issues or only one?
I'm a little confused on your setup, as far as which pools you  
destroyed/kept. Can you send a zpool status?

Noel

On Mar 9, 2008, at 10:46 AM, Brett Ault-McCoy wrote:

> I started using zfs a few weeks ago.  Until recently it's been very  
> stable and I haven't had any crashes or problems.  However, about  
> two days ago it started crashing and is now doing so somewhat  
> regularly.  The initial crash was due to having a zfs pool made out  
> of some files residing on another machine being accessed over CIFS.   
> The other machine was shut down while the zfs pool was active and  
> crashed the machine.  I no longer have that pool (zpool destroy) but  
> I'm still having crashes periodically.  The problem report from the  
> crashes is:
>
> Sun Mar  9 17:38:07 2008
> panic(cpu 0 caller 0x375768B0): "ZFS: I/O failure (write on  
> <unknown> off 0: zio 0x2008ef0 [L0 ZFS plain file] 20000L/20000P  
> DVA[0]=<0:2a82fe0000:20000> fletcher2 uncompressed BE contiguous  
> birth=878877 fill=1 cksum=2f7d0e17f8bcf08c: 
> 48cc3c8f3c1207d2:1ce4de75a16901ab:f103c54640173d99): error " "5"@/ 
> Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/zfs_kext/ 
> zfs/zio.c:918
> Latest stack backtrace for cpu 0:
>       Backtrace:
>          0x0009B2F8 0x0009BC9C 0x00029DC4 0x375768B0 0x37572340  
> 0x37572864 0x37572918 0x37572340
>          0x37575E20 0x37572340 0x375BD33C 0x37571AC4 0x375E9C08  
> 0x000B0454
>       Kernel loadable modules in backtrace (with dependencies):
>          com.apple.filesystems.zfs(8.0)@0x3755a000->0x37643fff
> Proceeding back via exception chain:
>    Exception state (sv=0x24c56280)
>       PC=0x00000000; MSR=0x0000D030; DAR=0x00000000;  
> DSISR=0x00000000; LR=0x00000000; R1=0x00000000; XCP=0x00000000  
> (Unknown)
>
> BSD process name corresponding to current thread: kernel_task
>
> Mac OS version:
> 9C31
>
> Kernel version:
> Darwin Kernel Version 9.2.0: Tue Feb  5 16:15:19 PST 2008;  
> root:xnu-1228.3.13~1/RELEASE_PPC
> System model name: PowerMac3,5
>
>
> The original crash also resulted in a corrupted file on a local HFS+  
> volume that had had a pool on and I had to destroy that pool.  I  
> assumed that was due to HFS+ inconsistancy during the crash.
>
> My thought on how to maybe clear the problem up is to export the  
> pools I have to get back to clean state and then re-import them all.
>
> Has anyone else seen anything similar?
>
> ++Brett;
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From ndellofano at apple.com  Thu Mar 13 16:32:43 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu Mar 13 16:34:10 2008
Subject: [zfs-discuss] Re: Firefox Plugin Install Related to Preallocation?
In-Reply-To: <33644d3c0803130827tbb680f5wfdbd8b5d65bae051@mail.gmail.com>
References: <33644d3c0803130827tbb680f5wfdbd8b5d65bae051@mail.gmail.com>
Message-ID: <B24AE435-A4B1-419D-8267-3B0002390658@apple.com>

sweet, good detective work :)
You could very well be correct in  that this could be what's making  
Firefox pissed.  ZFS doesn't support the preallocate vnop call since  
it's not a required vop, or commonly used by anyone or any filesystems  
anymore.   I have seen user land programs and libraries fail on  
Leopard if they assume that preallocate can't fail, even though it  
should be perfectly acceptable to return ENOTSUP on a preallocate  
request, as it should be being used as a hint, not a dependency.

Noel

On Mar 13, 2008, at 8:27 AM, James Snyder wrote:

> Along the lines of the firefox addon installation issue, I've pked
> around with dtruss & errinfo a little, and it seems like at least one
> error that is taking place at the time of plugin install on Firefox
> 3b4 is the following:
>
> fcntl(0x2F, 0x2A, 0xFFFFFFFFBFFFBCF0)		 = -1 Err#45
>
> 45 is operation unsupported, and 0x2A is F_PREALLOCATE.  I've not done
> much file IO work in C beyond the basics so I haven't made a test case
> for this.  I was wondering if perhaps this syscall, or perhaps one of
> the modes isn't implemented yet for ZFS?
>
> The firefox source itself doesn't seem to have any references to
> F_PREALLOCATE, so I suppose one if it's libraries might be using it?
>
> I also see in zfs_kext/zfs/zfs_vfsops.c:
>
>     * statvfs() should really be called statufs(), because it assumes
>     * static metadata.  ZFS doesn't preallocate files, so the best
>     * we can do is report the max that could possibly fit in f_files,
>     * and that minus the number actually used in f_ffree.
>
> I just thought I would see what I could dig up in a short amount of
> time with dtrace and a little grepping...
>
> On Wed, Mar 12, 2008 at 11:10 AM, James Snyder <jbsnyder@gmail.com>  
> wrote:
>> I've experienced the firefox issue as well.  I was going to submit a
>> report regarding it, and could do so still if few other people are
>> having this problem, but it sounds like I'm not the only one.
>>
>> Regarding Flash sound, I'm NOT having this problem (Sound works in
>> Firefox & Safari).  If it helps at all, here are my zfs mounts:
>>
>>
>> NAME                       USED  AVAIL  REFER  MOUNTPOINT
>> Aquarium                  36.8G  60.7G  56.5K  none
>> Aquarium/jsnyder          35.4G  60.7G  23.1G  /Users/jsnyder
>> Aquarium/jsnyder/Sources  10.3G  60.7G  10.2G  /Users/jsnyder/Sources
>> Aquarium/opt              1.40G  60.7G  1.39G  /opt
>>
>> My flash version is:
>> 9,0,115,0
>>
>> I'm working out of /Users/jsnyder...
>>
>> Not to pester, but might there be another source or binary update
>> coming in the near future?  If not, I'm still quite happy with what
>> I've got :-)  I assume 102A is still the most current available.
>>
>>
>>
>>
>>
>> On Tue, Mar 11, 2008 at 6:58 PM, Franz Schmalzl
>> <franzschmalzl@spamfreemail.de> wrote:
>>>
>>>
>>>> not immediately, other then nested filesystems in zfs pools are
>>>> still confusing Finder and the upper layers since they aren't  
>>>> really
>>>> sure how to handle them.  My guess is this has somethign to do with
>>>> that problem that upper layers where the flash is trying to run.  I
>>>> will add this information to the bug I have tracking issues like
>>>> this with userland and nested filesystems.
>>>
>>> yap, that was my first guess too..
>>>
>>> it's not just flash by the way, firefox is confused to ( whon
>>> installing addons e.g.)
>>>
>>> as you said.. osx doesn't handle it the best way at the moment
>>>
>>>
>>>>
>>>>
>>>> apologies for the weirdnes.... still getting stuff like this
>>>> hammered out..
>>>>
>>>
>>> no problem :)
>>>
>>> kind regards
>>> franz
>>>
>>>
>>>>
>>>>
>>>>> Noel
>>>>>
>>>>> On Mar 11, 2008, at 3:26 PM, Franz Schmalzl wrote:
>>>>>
>>>>>> ok this is very odd, but i have got the solution
>>>>>>
>>>>>> again my zfs setup
>>>>>>
>>>>>> NAME                 USED  AVAIL  REFER  MOUNTPOINT
>>>>>> hometank            58,4G   302G  16,6M  /Users
>>>>>> hometank/ruebezahl  58,4G   302G  58,4G  /Users/ruebezahl
>>>>>>
>>>>>> when my homefolder is storden in a zfs filesystem inside my zpool
>>>>>> flash sound is not working
>>>>>>
>>>>>> when my homefolder is stored on the root of my zpool it works
>>>>>>
>>>>>>
>>>>>> ( i tried with a test user, first created his homefolder on the
>>>>>> root of hometank ---> flash sound worked; then created another  
>>>>>> zfs
>>>>>> filesystem and created a new user inside ---> sound was gone )
>>>>>>
>>>>>> noel, can you explain ?
>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>
>>
>>
>> --
>> James Snyder
>> Biomedical Engineering
>> Northwestern University
>> jbsnyder@gmail.com
>>
>
>
>
> -- 
> James Snyder
> Biomedical Engineering
> Northwestern University
> jbsnyder@gmail.com

From ndellofano at apple.com  Thu Mar 13 16:46:29 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu Mar 13 16:48:33 2008
Subject: [zfs-discuss] Fwd: flash sound
In-Reply-To: <33644d3c0803120910l78936148gb162b78b2dd6c5c5@mail.gmail.com>
References: <C16EA8D7-A2D9-449E-9AF9-3E538E9EEC6D@spamfreemail.de>
	<3AB5583B-14F7-4796-8C76-C1CC0645153C@spamfreemail.de>
	<33644d3c0803120910l78936148gb162b78b2dd6c5c5@mail.gmail.com>
Message-ID: <C1B57320-2B14-49D8-ABE2-AC290C7B52F3@apple.com>

> Not to pester, but might there be another source or binary update
> coming in the near future?  If not, I'm still quite happy with what
> I've got :-)  I assume 102A is still the most current available.

Yes!
A new one is coming shortly which has a bunch of goodness in it  
including mmap coherency as well.  i'm going to give it a once over on  
my user installed machine and then post it.  It should be up soon.   
Thanks for your patience! :)

Noel

On Mar 12, 2008, at 9:10 AM, James Snyder wrote:

> I've experienced the firefox issue as well.  I was going to submit a
> report regarding it, and could do so still if few other people are
> having this problem, but it sounds like I'm not the only one.
>
> Regarding Flash sound, I'm NOT having this problem (Sound works in
> Firefox & Safari).  If it helps at all, here are my zfs mounts:
>
> NAME                       USED  AVAIL  REFER  MOUNTPOINT
> Aquarium                  36.8G  60.7G  56.5K  none
> Aquarium/jsnyder          35.4G  60.7G  23.1G  /Users/jsnyder
> Aquarium/jsnyder/Sources  10.3G  60.7G  10.2G  /Users/jsnyder/Sources
> Aquarium/opt              1.40G  60.7G  1.39G  /opt
>
> My flash version is:
> 9,0,115,0
>
> I'm working out of /Users/jsnyder...
>
> Not to pester, but might there be another source or binary update
> coming in the near future?  If not, I'm still quite happy with what
> I've got :-)  I assume 102A is still the most current available.
>
>
>
> On Tue, Mar 11, 2008 at 6:58 PM, Franz Schmalzl
> <franzschmalzl@spamfreemail.de> wrote:
>>
>>
>>> not immediately, other then nested filesystems in zfs pools are
>>> still confusing Finder and the upper layers since they aren't really
>>> sure how to handle them.  My guess is this has somethign to do with
>>> that problem that upper layers where the flash is trying to run.  I
>>> will add this information to the bug I have tracking issues like
>>> this with userland and nested filesystems.
>>
>> yap, that was my first guess too..
>>
>> it's not just flash by the way, firefox is confused to ( whon
>> installing addons e.g.)
>>
>> as you said.. osx doesn't handle it the best way at the moment
>>
>>
>>>
>>>
>>> apologies for the weirdnes.... still getting stuff like this
>>> hammered out..
>>>
>>
>> no problem :)
>>
>> kind regards
>> franz
>>
>>
>>>
>>>
>>>> Noel
>>>>
>>>> On Mar 11, 2008, at 3:26 PM, Franz Schmalzl wrote:
>>>>
>>>>> ok this is very odd, but i have got the solution
>>>>>
>>>>> again my zfs setup
>>>>>
>>>>> NAME                 USED  AVAIL  REFER  MOUNTPOINT
>>>>> hometank            58,4G   302G  16,6M  /Users
>>>>> hometank/ruebezahl  58,4G   302G  58,4G  /Users/ruebezahl
>>>>>
>>>>> when my homefolder is storden in a zfs filesystem inside my zpool
>>>>> flash sound is not working
>>>>>
>>>>> when my homefolder is stored on the root of my zpool it works
>>>>>
>>>>>
>>>>> ( i tried with a test user, first created his homefolder on the
>>>>> root of hometank ---> flash sound worked; then created another zfs
>>>>> filesystem and created a new user inside ---> sound was gone )
>>>>>
>>>>> noel, can you explain ?
>>>>>
>>>>>
>>>>
>>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
>
>
> -- 
> James Snyder
> Biomedical Engineering
> Northwestern University
> jbsnyder@gmail.com
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From jbsnyder at gmail.com  Thu Mar 13 17:13:22 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Thu Mar 13 17:13:01 2008
Subject: [zfs-discuss] Fwd: flash sound
In-Reply-To: <C1B57320-2B14-49D8-ABE2-AC290C7B52F3@apple.com>
References: <C16EA8D7-A2D9-449E-9AF9-3E538E9EEC6D@spamfreemail.de>
	<3AB5583B-14F7-4796-8C76-C1CC0645153C@spamfreemail.de>
	<33644d3c0803120910l78936148gb162b78b2dd6c5c5@mail.gmail.com>
	<C1B57320-2B14-49D8-ABE2-AC290C7B52F3@apple.com>
Message-ID: <33644d3c0803131713w60e4197fpd6cfe2df4b6e4d33@mail.gmail.com>

Sweet :-) I'm looking forward to it.

I'm happy to continue abusing builds in daily use, especially if it
might shave a panic or two off.

I do have one other quick question, and that is this:  If we do have
trouble with something and can reproduce it what is the best place to
report that?  Should an Apple bug report be filed, should we post
here, should we use the track bug tracker?  Do things from the system
crash reporter make their way through?

Additionally, I noticed that ztest isn't isn't one of the build
targets in the older source release.  I wouldn't mind running a test
or two other than beating it up during daily usage if there's any use
to running such things on an end-user system.  I'm not familiar with
the unit tests included in there, and so don't know if they are things
that would be useful outside of doing internal regression testing or
not.

Best.

-jsnyder

On Thu, Mar 13, 2008 at 6:46 PM, No?l Dellofano <ndellofano@apple.com> wrote:
> > Not to pester, but might there be another source or binary update
>  > coming in the near future?  If not, I'm still quite happy with what
>  > I've got :-)  I assume 102A is still the most current available.
>
>  Yes!
>  A new one is coming shortly which has a bunch of goodness in it
>  including mmap coherency as well.  i'm going to give it a once over on
>  my user installed machine and then post it.  It should be up soon.
>  Thanks for your patience! :)
>
>  Noel
>
>
>
>  On Mar 12, 2008, at 9:10 AM, James Snyder wrote:
>
>  > I've experienced the firefox issue as well.  I was going to submit a
>  > report regarding it, and could do so still if few other people are
>  > having this problem, but it sounds like I'm not the only one.
>  >
>  > Regarding Flash sound, I'm NOT having this problem (Sound works in
>  > Firefox & Safari).  If it helps at all, here are my zfs mounts:
>  >
>  > NAME                       USED  AVAIL  REFER  MOUNTPOINT
>  > Aquarium                  36.8G  60.7G  56.5K  none
>  > Aquarium/jsnyder          35.4G  60.7G  23.1G  /Users/jsnyder
>  > Aquarium/jsnyder/Sources  10.3G  60.7G  10.2G  /Users/jsnyder/Sources
>  > Aquarium/opt              1.40G  60.7G  1.39G  /opt
>  >
>  > My flash version is:
>  > 9,0,115,0
>  >
>  > I'm working out of /Users/jsnyder...
>  >
>  > Not to pester, but might there be another source or binary update
>  > coming in the near future?  If not, I'm still quite happy with what
>  > I've got :-)  I assume 102A is still the most current available.
>  >
>  >
>  >
>  > On Tue, Mar 11, 2008 at 6:58 PM, Franz Schmalzl
>  > <franzschmalzl@spamfreemail.de> wrote:
>  >>
>  >>
>  >>> not immediately, other then nested filesystems in zfs pools are
>  >>> still confusing Finder and the upper layers since they aren't really
>  >>> sure how to handle them.  My guess is this has somethign to do with
>  >>> that problem that upper layers where the flash is trying to run.  I
>  >>> will add this information to the bug I have tracking issues like
>  >>> this with userland and nested filesystems.
>  >>
>  >> yap, that was my first guess too..
>  >>
>  >> it's not just flash by the way, firefox is confused to ( whon
>  >> installing addons e.g.)
>  >>
>  >> as you said.. osx doesn't handle it the best way at the moment
>  >>
>  >>
>  >>>
>  >>>
>  >>> apologies for the weirdnes.... still getting stuff like this
>  >>> hammered out..
>  >>>
>  >>
>  >> no problem :)
>  >>
>  >> kind regards
>  >> franz
>  >>
>  >>
>  >>>
>  >>>
>  >>>> Noel
>  >>>>
>  >>>> On Mar 11, 2008, at 3:26 PM, Franz Schmalzl wrote:
>  >>>>
>  >>>>> ok this is very odd, but i have got the solution
>  >>>>>
>  >>>>> again my zfs setup
>  >>>>>
>  >>>>> NAME                 USED  AVAIL  REFER  MOUNTPOINT
>  >>>>> hometank            58,4G   302G  16,6M  /Users
>  >>>>> hometank/ruebezahl  58,4G   302G  58,4G  /Users/ruebezahl
>  >>>>>
>  >>>>> when my homefolder is storden in a zfs filesystem inside my zpool
>  >>>>> flash sound is not working
>  >>>>>
>  >>>>> when my homefolder is stored on the root of my zpool it works
>  >>>>>
>  >>>>>
>  >>>>> ( i tried with a test user, first created his homefolder on the
>  >>>>> root of hometank ---> flash sound worked; then created another zfs
>  >>>>> filesystem and created a new user inside ---> sound was gone )
>  >>>>>
>  >>>>> noel, can you explain ?
>  >>>>>
>  >>>>>
>  >>>>
>  >>>
>  >>
>  >> _______________________________________________
>  >> zfs-discuss mailing list
>  >> zfs-discuss@lists.macosforge.org
>  >> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>  >>
>  >
>  >
>  >
>  > --
>  > James Snyder
>  > Biomedical Engineering
>  > Northwestern University
>  > jbsnyder@gmail.com
>  > _______________________________________________
>  > zfs-discuss mailing list
>  > zfs-discuss@lists.macosforge.org
>  > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
From ndellofano at apple.com  Fri Mar 14 10:22:56 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri Mar 14 10:23:28 2008
Subject: [zfs-discuss] Fwd: flash sound
In-Reply-To: <33644d3c0803131713w60e4197fpd6cfe2df4b6e4d33@mail.gmail.com>
References: <C16EA8D7-A2D9-449E-9AF9-3E538E9EEC6D@spamfreemail.de>
	<3AB5583B-14F7-4796-8C76-C1CC0645153C@spamfreemail.de>
	<33644d3c0803120910l78936148gb162b78b2dd6c5c5@mail.gmail.com>
	<C1B57320-2B14-49D8-ABE2-AC290C7B52F3@apple.com>
	<33644d3c0803131713w60e4197fpd6cfe2df4b6e4d33@mail.gmail.com>
Message-ID: <9692B351-21FD-435C-B2B7-D4CCF9359C6B@apple.com>

So if you have a bug you can reproduce, send it here to this list,  
with what it is and how to reproduce it.  Bugs filed through the wiki  
are on a separate world then the rest of Apple's bugs so don't merge  
into the normal bug database.  So send the bugs here, and I'll file a  
bug with the info and report the info to the group here :)

For ztest, you don't see it because we haven't ported it yet :) It's  
on the list of things to get done since it would definately be a good  
addition to testing and help us and you guys catch more bugs.

Thanks much for beating on ZFS and for your patience!

Noel

On Mar 13, 2008, at 5:13 PM, James Snyder wrote:

> Sweet :-) I'm looking forward to it.
>
> I'm happy to continue abusing builds in daily use, especially if it
> might shave a panic or two off.
>
> I do have one other quick question, and that is this:  If we do have
> trouble with something and can reproduce it what is the best place to
> report that?  Should an Apple bug report be filed, should we post
> here, should we use the track bug tracker?  Do things from the system
> crash reporter make their way through?
>
> Additionally, I noticed that ztest isn't isn't one of the build
> targets in the older source release.  I wouldn't mind running a test
> or two other than beating it up during daily usage if there's any use
> to running such things on an end-user system.  I'm not familiar with
> the unit tests included in there, and so don't know if they are things
> that would be useful outside of doing internal regression testing or
> not.
>
> Best.
>
> -jsnyder
>
> On Thu, Mar 13, 2008 at 6:46 PM, No?l Dellofano  
> <ndellofano@apple.com> wrote:
>>> Not to pester, but might there be another source or binary update
>>> coming in the near future?  If not, I'm still quite happy with what
>>> I've got :-)  I assume 102A is still the most current available.
>>
>> Yes!
>> A new one is coming shortly which has a bunch of goodness in it
>> including mmap coherency as well.  i'm going to give it a once over  
>> on
>> my user installed machine and then post it.  It should be up soon.
>> Thanks for your patience! :)
>>
>> Noel
>>
>>
>>
>> On Mar 12, 2008, at 9:10 AM, James Snyder wrote:
>>
>>> I've experienced the firefox issue as well.  I was going to submit a
>>> report regarding it, and could do so still if few other people are
>>> having this problem, but it sounds like I'm not the only one.
>>>
>>> Regarding Flash sound, I'm NOT having this problem (Sound works in
>>> Firefox & Safari).  If it helps at all, here are my zfs mounts:
>>>
>>> NAME                       USED  AVAIL  REFER  MOUNTPOINT
>>> Aquarium                  36.8G  60.7G  56.5K  none
>>> Aquarium/jsnyder          35.4G  60.7G  23.1G  /Users/jsnyder
>>> Aquarium/jsnyder/Sources  10.3G  60.7G  10.2G  /Users/jsnyder/ 
>>> Sources
>>> Aquarium/opt              1.40G  60.7G  1.39G  /opt
>>>
>>> My flash version is:
>>> 9,0,115,0
>>>
>>> I'm working out of /Users/jsnyder...
>>>
>>> Not to pester, but might there be another source or binary update
>>> coming in the near future?  If not, I'm still quite happy with what
>>> I've got :-)  I assume 102A is still the most current available.
>>>
>>>
>>>
>>> On Tue, Mar 11, 2008 at 6:58 PM, Franz Schmalzl
>>> <franzschmalzl@spamfreemail.de> wrote:
>>>>
>>>>
>>>>> not immediately, other then nested filesystems in zfs pools are
>>>>> still confusing Finder and the upper layers since they aren't  
>>>>> really
>>>>> sure how to handle them.  My guess is this has somethign to do  
>>>>> with
>>>>> that problem that upper layers where the flash is trying to  
>>>>> run.  I
>>>>> will add this information to the bug I have tracking issues like
>>>>> this with userland and nested filesystems.
>>>>
>>>> yap, that was my first guess too..
>>>>
>>>> it's not just flash by the way, firefox is confused to ( whon
>>>> installing addons e.g.)
>>>>
>>>> as you said.. osx doesn't handle it the best way at the moment
>>>>
>>>>
>>>>>
>>>>>
>>>>> apologies for the weirdnes.... still getting stuff like this
>>>>> hammered out..
>>>>>
>>>>
>>>> no problem :)
>>>>
>>>> kind regards
>>>> franz
>>>>
>>>>
>>>>>
>>>>>
>>>>>> Noel
>>>>>>
>>>>>> On Mar 11, 2008, at 3:26 PM, Franz Schmalzl wrote:
>>>>>>
>>>>>>> ok this is very odd, but i have got the solution
>>>>>>>
>>>>>>> again my zfs setup
>>>>>>>
>>>>>>> NAME                 USED  AVAIL  REFER  MOUNTPOINT
>>>>>>> hometank            58,4G   302G  16,6M  /Users
>>>>>>> hometank/ruebezahl  58,4G   302G  58,4G  /Users/ruebezahl
>>>>>>>
>>>>>>> when my homefolder is storden in a zfs filesystem inside my  
>>>>>>> zpool
>>>>>>> flash sound is not working
>>>>>>>
>>>>>>> when my homefolder is stored on the root of my zpool it works
>>>>>>>
>>>>>>>
>>>>>>> ( i tried with a test user, first created his homefolder on the
>>>>>>> root of hometank ---> flash sound worked; then created another  
>>>>>>> zfs
>>>>>>> filesystem and created a new user inside ---> sound was gone )
>>>>>>>
>>>>>>> noel, can you explain ?
>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>
>>>
>>>
>>> --
>>> James Snyder
>>> Biomedical Engineering
>>> Northwestern University
>>> jbsnyder@gmail.com
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>>
>
>
>
> -- 
> James Snyder
> Biomedical Engineering
> Northwestern University
> jbsnyder@gmail.com

From btm at pobox.com  Fri Mar 14 16:53:58 2008
From: btm at pobox.com (Brett Ault-McCoy)
Date: Fri Mar 14 16:53:32 2008
Subject: [zfs-discuss] zfs crashing regularly now
In-Reply-To: <B632C8AB-7F73-4269-A83A-B13F5D7C9C04@apple.com>
References: <7bccd8dc0803091046s1de88fecx77419622ef29fea@mail.gmail.com>
	<B632C8AB-7F73-4269-A83A-B13F5D7C9C04@apple.com>
Message-ID: <7bccd8dc0803141653p23398f4r4c74fca0a9eb5788@mail.gmail.com>

Well, the machine it was on is out of commission now and one of the drives
is dead (got knocked off the table).
Anyway, the problem went away after I 'zpool export'd all of the pools and
re-imported them.

Thanks for the info.

++Brett;


On Thu, Mar 13, 2008 at 10:41 PM, No?l Dellofano <ndellofano@apple.com>
wrote:

> So the crash below is due to a corrupted disk(file in your case "LO
> ZFS plain file") or if the file disappeared suddenly and the ZFS can't
> find it to write to it.  It seems that it's possible that the file you
> backed the pool with is corrupted and therefore repeatedly crashing.
> I would recommend destroying the pool and recreating it, since you're
> backing store(vdev) has become corrupt.  Did you destroy both pools
> which were having issues or only one?
> I'm a little confused on your setup, as far as which pools you
> destroyed/kept. Can you send a zpool status?
>
> Noel
>
> On Mar 9, 2008, at 10:46 AM, Brett Ault-McCoy wrote:
>
> > I started using zfs a few weeks ago.  Until recently it's been very
> > stable and I haven't had any crashes or problems.  However, about
> > two days ago it started crashing and is now doing so somewhat
> > regularly.  The initial crash was due to having a zfs pool made out
> > of some files residing on another machine being accessed over CIFS.
> > The other machine was shut down while the zfs pool was active and
> > crashed the machine.  I no longer have that pool (zpool destroy) but
> > I'm still having crashes periodically.  The problem report from the
> > crashes is:
> >
> > Sun Mar  9 17:38:07 2008
> > panic(cpu 0 caller 0x375768B0): "ZFS: I/O failure (write on
> > <unknown> off 0: zio 0x2008ef0 [L0 ZFS plain file] 20000L/20000P
> > DVA[0]=<0:2a82fe0000:20000> fletcher2 uncompressed BE contiguous
> > birth=878877 fill=1 cksum=2f7d0e17f8bcf08c:
> > 48cc3c8f3c1207d2:1ce4de75a16901ab:f103c54640173d99): error " "5"@/
> > Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/zfs_kext/
> > zfs/zio.c:918
> > Latest stack backtrace for cpu 0:
> >       Backtrace:
> >          0x0009B2F8 0x0009BC9C 0x00029DC4 0x375768B0 0x37572340
> > 0x37572864 0x37572918 0x37572340
> >          0x37575E20 0x37572340 0x375BD33C 0x37571AC4 0x375E9C08
> > 0x000B0454
> >       Kernel loadable modules in backtrace (with dependencies):
> >          com.apple.filesystems.zfs(8.0)@0x3755a000->0x37643fff
> > Proceeding back via exception chain:
> >    Exception state (sv=0x24c56280)
> >       PC=0x00000000; MSR=0x0000D030; DAR=0x00000000;
> > DSISR=0x00000000; LR=0x00000000; R1=0x00000000; XCP=0x00000000
> > (Unknown)
> >
> > BSD process name corresponding to current thread: kernel_task
> >
> > Mac OS version:
> > 9C31
> >
> > Kernel version:
> > Darwin Kernel Version 9.2.0: Tue Feb  5 16:15:19 PST 2008;
> > root:xnu-1228.3.13~1/RELEASE_PPC
> > System model name: PowerMac3,5
> >
> >
> > The original crash also resulted in a corrupted file on a local HFS+
> > volume that had had a pool on and I had to destroy that pool.  I
> > assumed that was due to HFS+ inconsistancy during the crash.
> >
> > My thought on how to maybe clear the problem up is to export the
> > pools I have to get back to clean state and then re-import them all.
> >
> > Has anyone else seen anything similar?
> >
> > ++Brett;
> >
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss@lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080314/a41bd02f/attachment.html
From eric at hulteen.com  Sat Mar 15 16:12:05 2008
From: eric at hulteen.com (Eric A Hulteen)
Date: Sat Mar 15 16:11:34 2008
Subject: [zfs-discuss] Filename conventions
In-Reply-To: <F7A6F611-7E0C-4C70-AFEB-85A934004073@apple.com>
References: <42DBD20A-F51D-47AC-AB31-1B6FFC933FDE@scilytics.com>
	<F7A6F611-7E0C-4C70-AFEB-85A934004073@apple.com>
Message-ID: <a06240803c401ff0929b0@[10.0.1.2]>

   What character constraints does ZFS have that 
Mac OS Extended (Journaled) does not?

   I'm trying to back up (copy) Mac OS X files to 
ZFS.  I tried to use a NAS device for this but 
ran into trouble because files with 
Macintosh-legal characters in their names 
couldn't be copied to SMB volumes.  I assumed 
that this wouldn't be a problem with ZFS, but 
unfortunately it appears that is.

   I was copying a folder to a ZFS pool (5 x 1TB, 
raidz2) and I got a Finder error (i.e., the file 
"05 Not This Time.mp3" can't be coped because the 
name is "too long" or contains "characters the 
disk cannot display").  I tried again (35 MB 
total) and the Finder's "Copy" window stayed open 
for four hours (reporting "Estimating time 
remaining...").  I was unable to quit/stop the 
Copy and had to Force Quit the Finder (which 
failed and I had to hold the Power button for 
eight seconds).

   I don't see how the names could be "too long". 
The files have no more than 24 characters in 
their names (so even if every character was four 
bytes it still wouldn't be too long).  The files 
are French music titles that were looked up at 
CDDB when I ripped a music CD (and there are no 
more than two or three accented characters per 
title).  These are the folder and filenames:

Jazz a?? La Franc??aise (parent folder, a.k.a. 
"Jazz\ a\314\200\ La\ Franc\314\247aise")
     01 A La Franc??aise.mp3
     02 Garnerama.mp3
     03 Bach To Swing.mp3
     04 Etude In Blue.mp3
     05 Not This Time.mp3
     06 Blue Kiss From Brazil.mp3
     07 Fiance??es En Folie.mp3

   Is the limit on the whole pathname?

/Volumes/Kate_backup_372/2008/2008_03_08/katemcjones/Music/iTunes/iTunes 
Music/Claude Bolling Trio/Jazz ? La Fran?aise/05 
Not This Time.mp3

Eric
__________

>Not quite sure exactly what kind of conventions you're referring to.  
>In general, we export UTF-8 kNFD trough the API but store kNFC on disk.
>There is a difference in the way names are 
>looked up and dealt with in ZFS then HFS.  In 
>ZFS everything is stored via hash in the ZAP 
>which has a 256 byte limit.  HFS can store 256 
>Unicode chars which when converted to UTF-8 can 
>exceed 256 bytes.  Hence perhaps you have some 
>extremely long filenames that have a lot of 
>accented or foreign chars in your data.
>
>
>Noel
>
>On Jan 17, 2008, at 4:40 PM, Michael Alexander wrote:
>
>>
>>What are the file name conventions under MacOSX 
>>and in general. While ZFS works very well in my 
>>testing so far, some double byte character file 
>>names seem to be a problem relative to HFS+, 
>>but the pattern is not obvious yet. -m.a.
>>
>>
>>_______________________________________________
>>zfs-discuss mailing list
>>zfs-discuss@lists.macosforge.org
>>http://lists.macosforge.org/mailman/listinfo/zfs-discuss
From gabor at berczi.be  Sun Mar 16 07:13:15 2008
From: gabor at berczi.be (=?ISO-8859-1?Q?B=E9rczi_G=E1bor?=)
Date: Sun Mar 16 07:13:13 2008
Subject: [zfs-discuss] can't delete from full volume
Message-ID: <67A7235B-A467-4368-9EC5-DE2944AF5086@berczi.be>

NAME    USED  AVAIL  REFER  MOUNTPOINT
store  80.2G      0  80.2G  /Volumes/store

Filesystem 1024-blocks     Used Available Capacity  Mounted on
store         84122986 84122986         0   100%    /Volumes/store

rm: iphone_sdk.dmg: No space left on device

-- 
Gabucino

From i_see at macnews.de  Sun Mar 16 07:41:55 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Sun Mar 16 07:41:25 2008
Subject: [zfs-discuss] Filename conventions
In-Reply-To: <20080316141312.C39732268BE@lists.macosforge.org>
References: <20080316141312.C39732268BE@lists.macosforge.org>
Message-ID: <DEBD6642-CE17-41D4-B4A4-70AF88924BBF@macnews.de>

Hi list,
I had similar problems when copying my iTunes Library,
it seemed to happen extremely often (but not always) for Folder names  
that contained special characters (such as ????). The Finde than did  
not copy Files and subfolders reliably ,sometimes issuing eror  
messages, sometimes not.
This does appear to be a Finder rather then a ZFS issue though,as  
using cp-R generally worked well on the same Folders.
I would guess there are some assumptions about Filename-storage in  
Finder that are not true for ZFS, particulary I would ahve a look at  
the length computation, if I had access to Finder's source code ;-)

Hope this helps to narrow down the bug.
As a workaround,you can always store an HFS+-formatted image on your  
ZFS Volume, for me that works well with all applications so far.
Am 16.03.2008 um 15:13 schrieb zfs-discuss-request@lists.macosforge.org:

> Von: Eric A Hulteen <eric@hulteen.com>
> Datum: 16. M?rz 2008 00:12:05 MEZ
> An: zfs-discuss@lists.macosforge.org
> Betreff: Re: [zfs-discuss] Filename conventions
>
>
>  What character constraints does ZFS have that Mac OS Extended  
> (Journaled) does not?
>
>  I'm trying to back up (copy) Mac OS X files to ZFS.  I tried to use  
> a NAS device for this but ran into trouble because files with  
> Macintosh-legal characters in their names couldn't be copied to SMB  
> volumes.  I assumed that this wouldn't be a problem with ZFS, but  
> unfortunately it appears that is.
>
>  I was copying a folder to a ZFS pool (5 x 1TB, raidz2) and I got a  
> Finder error (i.e., the file "05 Not This Time.mp3" can't be coped  
> because the name is "too long" or contains "characters the disk  
> cannot display").  I tried again (35 MB total) and the Finder's  
> "Copy" window stayed open for four hours (reporting "Estimating time  
> remaining...").  I was unable to quit/stop the Copy and had to Force  
> Quit the Finder (which failed and I had to hold the Power button for  
> eight seconds).
>
>  I don't see how the names could be "too long". The files have no  
> more than 24 characters in their names (so even if every character  
> was four bytes it still wouldn't be too long).  The files are French  
> music titles that were looked up at CDDB when I ripped a music CD  
> (and there are no more than two or three accented characters per  
> title).  These are the folder and filenames:
>
> Jazz a?? La Franc??aise (parent folder, a.k.a. "Jazz\ a\314\200\ La\  
> Franc\314\247aise")
>    01 A La Franc??aise.mp3
>    02 Garnerama.mp3
>    03 Bach To Swing.mp3
>    04 Etude In Blue.mp3
>    05 Not This Time.mp3
>    06 Blue Kiss From Brazil.mp3
>    07 Fiance??es En Folie.mp3
>
>  Is the limit on the whole pathname?
>
> /Volumes/Kate_backup_372/2008/2008_03_08/katemcjones/Music/iTunes/ 
> iTunes Music/Claude Bolling Trio/Jazz ? La Fran?aise/05 Not This  
> Time.mp3
>
> Eric

From zfs at hessmann.de  Mon Mar 17 04:21:11 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Mon Mar 17 04:20:54 2008
Subject: [zfs-discuss] rsync Invalid checksum error
Message-ID: <019EBDA5-4FC9-4A6C-974B-F964F7C23AD7@hessmann.de>

Dear all,


any update on Ticket #2 (http://trac.macosforge.org/projects/zfs/ticket/2)?

I'm trying to copy my home directory (HFS+) to a remote 10.5.2 client  
with ZFS RAID.Z via rsync/ssh (-avESz --no-whole-file --partial -- 
stats --progress --inplace --delete --ignore-errors), but get errors  
like:

Invalid checksum length 9175040
rsync error: protocol incompatibility (code 2) at /SourceCache/rsync/ 
rsync-30/rsync/sender.c(60)

about every 200-500 files, which result in the termination of the  
process.

Retrying always works with the same file, but the error returns a few  
hundred files later.

Thank you for any information.


Best regards,

Christian
From info at martin-hauser.net  Mon Mar 17 05:32:25 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Mon Mar 17 06:06:30 2008
Subject: [zfs-discuss] new release?
Message-ID: <B21E341F-503A-45B9-AA00-88214EF7437B@martin-hauser.net>

Hello everyone,

I'm not trying to push anyone, but as Noel mentioned there would soon  
be a new release of zfs with mmap() support and co, where would I  
check for that new release? is it found on macosforge? and announced  
on this list?

Thanks in advance and keep uop the good work

kind regards

Martin
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080317/7bc668a7/PGP.bin
From bwaters at nrao.edu  Mon Mar 17 08:55:34 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Mon Mar 17 08:55:31 2008
Subject: [zfs-discuss] rsync Invalid checksum error
In-Reply-To: <019EBDA5-4FC9-4A6C-974B-F964F7C23AD7@hessmann.de>
References: <019EBDA5-4FC9-4A6C-974B-F964F7C23AD7@hessmann.de>
Message-ID: <B319F49A-F62B-4533-A117-66D59C6B2A0F@nrao.edu>

Have you tried rsync 3.0?

It is very easy to build from source code, and I run into fewer  
problems when I'm using it.

See

http://www.samba.org/rsync/
http://www.samba.org/ftp/rsync/src/rsync-3.0.0.tar.gz




On Mar 17, 2008, at 5:21 AM, Christian He?mann wrote:
> Dear all,
>
>
> any update on Ticket #2 (http://trac.macosforge.org/projects/zfs/ticket/2)?
>
> I'm trying to copy my home directory (HFS+) to a remote 10.5.2  
> client with ZFS RAID.Z via rsync/ssh (-avESz --no-whole-file -- 
> partial --stats --progress --inplace --delete --ignore-errors), but  
> get errors like:
>
> Invalid checksum length 9175040
> rsync error: protocol incompatibility (code 2) at /SourceCache/rsync/ 
> rsync-30/rsync/sender.c(60)
>
> about every 200-500 files, which result in the termination of the  
> process.
>
> Retrying always works with the same file, but the error returns a  
> few hundred files later.
>
> Thank you for any information.
>
>
> Best regards,
>
> Christian
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From zfs at hessmann.de  Mon Mar 17 09:12:30 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Mon Mar 17 09:11:56 2008
Subject: [zfs-discuss] rsync Invalid checksum error
In-Reply-To: <B319F49A-F62B-4533-A117-66D59C6B2A0F@nrao.edu>
References: <019EBDA5-4FC9-4A6C-974B-F964F7C23AD7@hessmann.de>
	<B319F49A-F62B-4533-A117-66D59C6B2A0F@nrao.edu>
Message-ID: <55CD4B19-85B3-4CF8-85EA-393435AC6A0D@hessmann.de>

On 17.03.2008, at 16:55, Boyd Waters wrote:

> Have you tried rsync 3.0?
> It is very easy to build from source code, and I run into fewer  
> problems when I'm using it.

This might be a stupid question, but can the "vanilla" rsync from  
source handle all the OS X oddities and extensions?
The one thing I know is that there is no need for rsyncX in Leopard  
anymore, but I don't know whether this is due to a special version  
Apple ships with Leopard or due to a source merge in rsync itself...


Best regards,

Christian
From bwaters at nrao.edu  Mon Mar 17 09:15:15 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Mon Mar 17 09:15:58 2008
Subject: [zfs-discuss] rsync Invalid checksum error
In-Reply-To: <55CD4B19-85B3-4CF8-85EA-393435AC6A0D@hessmann.de>
References: <019EBDA5-4FC9-4A6C-974B-F964F7C23AD7@hessmann.de>
	<B319F49A-F62B-4533-A117-66D59C6B2A0F@nrao.edu>
	<55CD4B19-85B3-4CF8-85EA-393435AC6A0D@hessmann.de>
Message-ID: <DC2C200B-F545-49FB-B742-31A233DAEE6C@nrao.edu>


On Mar 17, 2008, at 10:12 AM, Christian He?mann wrote:
> can the "vanilla" rsync from source handle all the OS X oddities and  
> extensions?

A very *good* question, and the answer is

YES!!

No more silly patches! No more Mac-specific rsync!

It has the "-E" extended attribute flag. And so on.

I haven't done extensive testing with resource forks, but I think it  
works.


From lists at loveturtle.net  Mon Mar 17 09:19:00 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Mon Mar 17 09:18:25 2008
Subject: [zfs-discuss] rsync Invalid checksum error
In-Reply-To: <B319F49A-F62B-4533-A117-66D59C6B2A0F@nrao.edu>
References: <019EBDA5-4FC9-4A6C-974B-F964F7C23AD7@hessmann.de>
	<B319F49A-F62B-4533-A117-66D59C6B2A0F@nrao.edu>
Message-ID: <47DE99F4.5000003@loveturtle.net>

3.0 has been in macports for a little while now too. if you have that 
installed you can just selfupdate and install rsync

Boyd Waters wrote:
> Have you tried rsync 3.0?
>
> It is very easy to build from source code, and I run into fewer 
> problems when I'm using it.
>
> See
>
> http://www.samba.org/rsync/
> http://www.samba.org/ftp/rsync/src/rsync-3.0.0.tar.gz
>
>
>
>
> On Mar 17, 2008, at 5:21 AM, Christian He?mann wrote:
>> Dear all,
>>
>>
>> any update on Ticket #2 
>> (http://trac.macosforge.org/projects/zfs/ticket/2)?
>>
>> I'm trying to copy my home directory (HFS+) to a remote 10.5.2 client 
>> with ZFS RAID.Z via rsync/ssh (-avESz --no-whole-file --partial 
>> --stats --progress --inplace --delete --ignore-errors), but get 
>> errors like:
>>
>> Invalid checksum length 9175040
>> rsync error: protocol incompatibility (code 2) at 
>> /SourceCache/rsync/rsync-30/rsync/sender.c(60)
>>
>> about every 200-500 files, which result in the termination of the 
>> process.
>>
>> Retrying always works with the same file, but the error returns a few 
>> hundred files later.
>>
>> Thank you for any information.
>>
>>
>> Best regards,
>>
>> Christian
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
   
From jbsnyder at gmail.com  Mon Mar 17 09:25:23 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Mon Mar 17 09:24:56 2008
Subject: [zfs-discuss] rsync Invalid checksum error
In-Reply-To: <47DE99F4.5000003@loveturtle.net>
References: <019EBDA5-4FC9-4A6C-974B-F964F7C23AD7@hessmann.de>
	<B319F49A-F62B-4533-A117-66D59C6B2A0F@nrao.edu>
	<47DE99F4.5000003@loveturtle.net>
Message-ID: <33644d3c0803170925n2d2bb8c9w1f7b39ec8e4e8639@mail.gmail.com>

Yup, I just noticed that.  It hasn't been that long though, the main
source release was made only about 2 weeks ago.

I've been having good experiences with it so far.  I love the rolling
approach to comparing source/destination.  I've not had any issues
with it on ZFS, and in particular I've been using the release
candidates for some time because I'd heard that rsync 3 & zfs played
better together.

I've not tried testing resource forks afterwards, however.  I can just
vouch for it working with rsyncing quite a bit of data (hundreds of
gigabytes) on OS X, FreeBSD & Linux for types of files that typically
don't require those extended properties.

On Mon, Mar 17, 2008 at 11:19 AM, Dillon Kass <lists@loveturtle.net> wrote:
> 3.0 has been in macports for a little while now too. if you have that
>  installed you can just selfupdate and install rsync
>
>
>
>  Boyd Waters wrote:
>  > Have you tried rsync 3.0?
>  >
>  > It is very easy to build from source code, and I run into fewer
>  > problems when I'm using it.
>  >
>  > See
>  >
>  > http://www.samba.org/rsync/
>  > http://www.samba.org/ftp/rsync/src/rsync-3.0.0.tar.gz
>  >
>  >
>  >
>  >
>  > On Mar 17, 2008, at 5:21 AM, Christian He?mann wrote:
>  >> Dear all,
>  >>
>  >>
>  >> any update on Ticket #2
>  >> (http://trac.macosforge.org/projects/zfs/ticket/2)?
>  >>
>  >> I'm trying to copy my home directory (HFS+) to a remote 10.5.2 client
>  >> with ZFS RAID.Z via rsync/ssh (-avESz --no-whole-file --partial
>  >> --stats --progress --inplace --delete --ignore-errors), but get
>  >> errors like:
>  >>
>  >> Invalid checksum length 9175040
>  >> rsync error: protocol incompatibility (code 2) at
>  >> /SourceCache/rsync/rsync-30/rsync/sender.c(60)
>  >>
>  >> about every 200-500 files, which result in the termination of the
>  >> process.
>  >>
>  >> Retrying always works with the same file, but the error returns a few
>  >> hundred files later.
>  >>
>  >> Thank you for any information.
>  >>
>  >>
>  >> Best regards,
>  >>
>  >> Christian
>  >> _______________________________________________
>  >> zfs-discuss mailing list
>  >> zfs-discuss@lists.macosforge.org
>  >> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>  >
>  > _______________________________________________
>  > zfs-discuss mailing list
>  > zfs-discuss@lists.macosforge.org
>  > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
>  _______________________________________________
>  zfs-discuss mailing list
>  zfs-discuss@lists.macosforge.org
>  http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
From zfs at hessmann.de  Mon Mar 17 09:54:09 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Mon Mar 17 09:53:32 2008
Subject: [zfs-discuss] rsync Invalid checksum error
In-Reply-To: <DC2C200B-F545-49FB-B742-31A233DAEE6C@nrao.edu>
References: <019EBDA5-4FC9-4A6C-974B-F964F7C23AD7@hessmann.de>
	<B319F49A-F62B-4533-A117-66D59C6B2A0F@nrao.edu>
	<55CD4B19-85B3-4CF8-85EA-393435AC6A0D@hessmann.de>
	<DC2C200B-F545-49FB-B742-31A233DAEE6C@nrao.edu>
Message-ID: <437A0BDF-F475-4F14-9BD5-0541943E5C7A@hessmann.de>

On 17.03.2008, at 17:15, Boyd Waters wrote:

>> can the "vanilla" rsync from source handle all the OS X oddities  
>> and extensions?
> A very *good* question, and the answer is
> YES!!
> No more silly patches! No more Mac-specific rsync!
> It has the "-E" extended attribute flag. And so on.
> I haven't done extensive testing with resource forks, but I think it  
> works.

Good to know.
Interestingly, -E is described as "preserve the file's executability",  
but maybe it really copies all extended attributes.

One thing I can already say is that it doesn't create ._-files on ZFS  
anymore, as did 2.6.6 (or whatever is shipped with 10.5.2).

Thank you.


Best regards,

Christian
From jbsnyder at gmail.com  Mon Mar 17 10:05:44 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Mon Mar 17 10:08:33 2008
Subject: [zfs-discuss] rsync Invalid checksum error
In-Reply-To: <437A0BDF-F475-4F14-9BD5-0541943E5C7A@hessmann.de>
References: <019EBDA5-4FC9-4A6C-974B-F964F7C23AD7@hessmann.de>
	<B319F49A-F62B-4533-A117-66D59C6B2A0F@nrao.edu>
	<55CD4B19-85B3-4CF8-85EA-393435AC6A0D@hessmann.de>
	<DC2C200B-F545-49FB-B742-31A233DAEE6C@nrao.edu>
	<437A0BDF-F475-4F14-9BD5-0541943E5C7A@hessmann.de>
Message-ID: <33644d3c0803171005v4ee0398fm1be64c8abdbd34d5@mail.gmail.com>

Perhaps what was meant was the -X flag:
-X, --xattrs                preserve extended attributes

?  I haven't bothered with using it so far.  I'm not sure that there
are extended attributes that I need to preserve on ZFS for my home
directory?  I may be wrong :-)

On Mon, Mar 17, 2008 at 11:54 AM, Christian He?mann <zfs@hessmann.de> wrote:
> On 17.03.2008, at 17:15, Boyd Waters wrote:
>
>  >> can the "vanilla" rsync from source handle all the OS X oddities
>  >> and extensions?
>  > A very *good* question, and the answer is
>  > YES!!
>  > No more silly patches! No more Mac-specific rsync!
>  > It has the "-E" extended attribute flag. And so on.
>  > I haven't done extensive testing with resource forks, but I think it
>  > works.
>
>  Good to know.
>  Interestingly, -E is described as "preserve the file's executability",
>  but maybe it really copies all extended attributes.
>
>  One thing I can already say is that it doesn't create ._-files on ZFS
>  anymore, as did 2.6.6 (or whatever is shipped with 10.5.2).
>
>  Thank you.
>
>
>  Best regards,
>
>  Christian
>
>
> _______________________________________________
>  zfs-discuss mailing list
>  zfs-discuss@lists.macosforge.org
>  http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
From ndellofano at apple.com  Mon Mar 17 12:03:20 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon Mar 17 12:03:37 2008
Subject: [zfs-discuss] new release?
In-Reply-To: <B21E341F-503A-45B9-AA00-88214EF7437B@martin-hauser.net>
References: <B21E341F-503A-45B9-AA00-88214EF7437B@martin-hauser.net>
Message-ID: <73374D20-6940-4B1A-929A-B22E1CC7DE7D@apple.com>

check macosforge, I'll always keep the latest and greatest bits up  
there.  I'll also send out an email announcement to this list when the  
new bits are available and uploaded.

Noel :)

On Mar 17, 2008, at 5:32 AM, Martin Hauser wrote:

> Hello everyone,
>
> I'm not trying to push anyone, but as Noel mentioned there would  
> soon be a new release of zfs with mmap() support and co, where would  
> I check for that new release? is it found on macosforge? and  
> announced on this list?
>
> Thanks in advance and keep uop the good work
>
> kind regards
>
> Martin
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From blustig at apple.com  Mon Mar 17 12:12:30 2008
From: blustig at apple.com (Barry Lustig)
Date: Mon Mar 17 12:13:18 2008
Subject: [zfs-discuss] can't delete from full volume
In-Reply-To: <67A7235B-A467-4368-9EC5-DE2944AF5086@berczi.be>
References: <67A7235B-A467-4368-9EC5-DE2944AF5086@berczi.be>
Message-ID: <2E79EDB1-9D0E-4B11-BC3B-99A036348702@apple.com>


On Mar 16, 2008, at 7:13 AM, B?rczi G?bor wrote:
> NAME    USED  AVAIL  REFER  MOUNTPOINT
> store  80.2G      0  80.2G  /Volumes/store
>
> Filesystem 1024-blocks     Used Available Capacity  Mounted on
> store         84122986 84122986         0   100%    /Volumes/store
>
> rm: iphone_sdk.dmg: No space left on device

This is a known issue.  To workaround:  "cp /dev/null file" to zero  
out the file and then remove it.


barry

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4148 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080317/8502012a/smime.bin
From alex.blewitt at gmail.com  Tue Mar 18 18:48:55 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Wed, 19 Mar 2008 01:48:55 +0000
Subject: [zfs-discuss] ZFS 102 A installer
Message-ID: <636fd28e0803181848o473c3ae0k9fd6a7acdbb72c3d@mail.gmail.com>

Based on the binaries available at
http://trac.macosforge.org/projects/zfs/wiki/downloads, I put together
a Mac OS X installer package and blogged about it:

http://alblue.blogspot.com/2008/03/zfs-on-mac.html

I've included the Apple License; I think that's OK since I'm not
modifying the code, but if someone believes this isn't in the license
then I can remove the download. Alternatively, if you want to host the
installer on the MacOSForge downloads page instead to encourage
adoption, please feel free to do so!

Alex

From franzschmalzl at spamfreemail.de  Wed Mar 19 11:01:16 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Wed, 19 Mar 2008 19:01:16 +0100
Subject: [zfs-discuss] raidz
Message-ID: <7271394B-C1ED-479C-8A54-58EB4FD4A64E@spamfreemail.de>

hi noel

did find the cause of the corrupted raidz arrays mentioned earlier by  
two people?

i'm planning to build a raidz in near future and i'd like to know if  
there are any bugs which could end up in a data loss...
since i won't have any backup of the raid array
( not enough harddrives here..)

best regards

franz schmalzl

From ndellofano at apple.com  Wed Mar 19 11:50:50 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed, 19 Mar 2008 11:50:50 -0700
Subject: [zfs-discuss] raidz
In-Reply-To: <7271394B-C1ED-479C-8A54-58EB4FD4A64E@spamfreemail.de>
References: <7271394B-C1ED-479C-8A54-58EB4FD4A64E@spamfreemail.de>
Message-ID: <24423687-6BA1-47C6-B3AD-E79F38DF824D@apple.com>

Hey Franz,

Can you refresh my memory of which two corruptions you're referring  
to?  There was a raidz bug that existed pre zfs-102A drop so I think a  
few people experienced corruption due to that.  Currently though,  
there are no bugs which would cause raidz data loss at all.

The only other thing I can think of to mention is some people had  
either failed to format their disk for ZFS first (diskutil  
partitionDisk ZFS...) or they created their pool using 'disk2' instead  
of 'disk2s2', and because of the way diskutil works that will cause  
confusion on bringup such that the drive isn't brought up properly,  
causing people to be unable to import their pools.


Noel

On Mar 19, 2008, at 11:01 AM, Franz Schmalzl wrote:

> hi noel
>
> did find the cause of the corrupted raidz arrays mentioned earlier  
> by two people?
>
> i'm planning to build a raidz in near future and i'd like to know if  
> there are any bugs which could end up in a data loss...
> since i won't have any backup of the raid array
> ( not enough harddrives here..)
>
> best regards
>
> franz schmalzl


From ndellofano at apple.com  Wed Mar 19 12:01:27 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed, 19 Mar 2008 12:01:27 -0700
Subject: [zfs-discuss] raidz
In-Reply-To: <A6125087-891E-40E0-B648-D4D1BAA1337F@spamfreemail.de>
References: <7271394B-C1ED-479C-8A54-58EB4FD4A64E@spamfreemail.de>
	<24423687-6BA1-47C6-B3AD-E79F38DF824D@apple.com>
	<A6125087-891E-40E0-B648-D4D1BAA1337F@spamfreemail.de>
Message-ID: <45DBBF68-A6C0-4BEA-AF98-E8D3FA8A20C5@apple.com>

>
> so it is crucial to do those two steps ?
> diskutil partitionDisk ZFS
> and use a slice (i.e disk2s2)
> not a whole disk  ?


Yes, it is very important you partition the drive for ZFS and that it  
has a GPT partition ala:
# diskutil partitionDisk disk2 GPTFormat ZFS %noformat% 100%

and when you create the actual pool for zfs, make sure you use the  
drive and the actual slice and not the whole drive:
#sudo zpool create wombat disk2s2

We're working on making this more seemless and not so easy to mess  
up.  But for now, everyone should just be aware of this issue :)

thanks!
Noel

On Mar 19, 2008, at 11:55 AM, Franz Schmalzl wrote:

>
> On 19.03.2008, at 19:50, No?l Dellofano wrote:
>
>> Hey Franz,
>>
>> Can you refresh my memory of which two corruptions you're referring  
>> to?  There was a raidz bug that existed pre zfs-102A drop so I  
>> think a few people experienced corruption due to that.  Currently  
>> though, there are no bugs which would cause raidz data loss at all.
>>
>> The only other thing I can think of to mention is some people had  
>> either failed to format their disk for ZFS first (diskutil  
>> partitionDisk ZFS...) or they created their pool using 'disk2'  
>> instead of 'disk2s2', and because of the way diskutil works that  
>> will cause confusion on bringup such that the drive isn't brought  
>> up properly, causing people to be unable to import their pools.
>
> oh sorry, i was regarding to those cases..
> i thought it was a zfs bug, not wrong preparation
>
>
> so it is crucial to do those two steps ?
> diskutil partitionDisk ZFS
> and use a slice (i.e disk2s2)
> not a whole disk  ?
>
>>
>>
>>
>> Noel
>>
>> On Mar 19, 2008, at 11:01 AM, Franz Schmalzl wrote:
>>
>>> hi noel
>>>
>>> did find the cause of the corrupted raidz arrays mentioned earlier  
>>> by two people?
>>>
>>> i'm planning to build a raidz in near future and i'd like to know  
>>> if there are any bugs which could end up in a data loss...
>>> since i won't have any backup of the raid array
>>> ( not enough harddrives here..)
>>>
>>> best regards
>>>
>>> franz schmalzl
>>
>


From kane at inius.com  Wed Mar 19 12:05:33 2008
From: kane at inius.com (Kane Dijkman)
Date: Wed, 19 Mar 2008 12:05:33 -0700
Subject: [zfs-discuss] raidz
In-Reply-To: <24423687-6BA1-47C6-B3AD-E79F38DF824D@apple.com>
References: <7271394B-C1ED-479C-8A54-58EB4FD4A64E@spamfreemail.de>
	<24423687-6BA1-47C6-B3AD-E79F38DF824D@apple.com>
Message-ID: <7E6F0F89-FDA5-4361-B182-F077F85DC402@inius.com>

What happens if the user does not format the disk for ZFS first? Is  
there a way to tell? I believe the last time I setup my raidz I  
assumed that it would format any disks that needed to be formatted as  
part of the creation process.

Reading through http://trac.macosforge.org/projects/zfs/wiki/get_the_party_started 
  for instructions I did not get the impression that it is required to  
manually format each disk for ZFS if they are already GPT. Is this  
wrong?

If I take a disk that was formatted GPT, but with JHFS+, and add it to  
a raidz I am creating will there be a problem?

Kane


On Mar 19, 2008, at 11:50 AM, No?l Dellofano wrote:
> Hey Franz,
>
> Can you refresh my memory of which two corruptions you're referring
> to?  There was a raidz bug that existed pre zfs-102A drop so I think a
> few people experienced corruption due to that.  Currently though,
> there are no bugs which would cause raidz data loss at all.
>
> The only other thing I can think of to mention is some people had
> either failed to format their disk for ZFS first (diskutil
> partitionDisk ZFS...) or they created their pool using 'disk2' instead
> of 'disk2s2', and because of the way diskutil works that will cause
> confusion on bringup such that the drive isn't brought up properly,
> causing people to be unable to import their pools.
>
>
> Noel
>
> On Mar 19, 2008, at 11:01 AM, Franz Schmalzl wrote:
>
>> hi noel
>>
>> did find the cause of the corrupted raidz arrays mentioned earlier
>> by two people?
>>
>> i'm planning to build a raidz in near future and i'd like to know if
>> there are any bugs which could end up in a data loss...
>> since i won't have any backup of the raid array
>> ( not enough harddrives here..)
>>
>> best regards
>>
>> franz schmalzl
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


---------------------------------------------------------------------------------
If life gives you lemons, squeeze the juice into a watergun and shoot  
other people in the eyes.


From bwaters at nrao.edu  Wed Mar 19 12:16:06 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Wed, 19 Mar 2008 13:16:06 -0600
Subject: [zfs-discuss] raidz
In-Reply-To: <45DBBF68-A6C0-4BEA-AF98-E8D3FA8A20C5@apple.com>
References: <7271394B-C1ED-479C-8A54-58EB4FD4A64E@spamfreemail.de>
	<24423687-6BA1-47C6-B3AD-E79F38DF824D@apple.com>
	<A6125087-891E-40E0-B648-D4D1BAA1337F@spamfreemail.de>
	<45DBBF68-A6C0-4BEA-AF98-E8D3FA8A20C5@apple.com>
Message-ID: <9969E6D0-37F7-43FA-95A5-4AA2C9680464@nrao.edu>

> Currently though,
> there are no bugs which would cause raidz data loss at all.


I have two small raidz arrays, four disks each, one is 4x500GB the  
other 4x1TB. I have *hammered* on these arrays with large files,  
millions of small files, rsync transfers, backups, etc. Though there  
have been many kernel panics along the way, I have never experienced  
any data loss.

I did it the way it says in the README:

> # diskutil partitionDisk disk{2,3,4,5} GPTFormat ZFS %noformat% 100%
> #sudo zpool create wombat raidz disk{2,3,4,5}s2




From franzschmalzl at spamfreemail.de  Wed Mar 19 12:48:09 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Wed, 19 Mar 2008 20:48:09 +0100
Subject: [zfs-discuss] raidz
In-Reply-To: <9969E6D0-37F7-43FA-95A5-4AA2C9680464@nrao.edu>
References: <7271394B-C1ED-479C-8A54-58EB4FD4A64E@spamfreemail.de>
	<24423687-6BA1-47C6-B3AD-E79F38DF824D@apple.com>
	<A6125087-891E-40E0-B648-D4D1BAA1337F@spamfreemail.de>
	<45DBBF68-A6C0-4BEA-AF98-E8D3FA8A20C5@apple.com>
	<9969E6D0-37F7-43FA-95A5-4AA2C9680464@nrao.edu>
Message-ID: <A78EDE51-82FC-49A3-9F41-80F0E1D766E5@spamfreemail.de>

what does the %noformat% mean ?


On 19.03.2008, at 20:16, Boyd Waters wrote:

>> Currently though,
>> there are no bugs which would cause raidz data loss at all.
>
>
> I have two small raidz arrays, four disks each, one is 4x500GB the
> other 4x1TB. I have *hammered* on these arrays with large files,
> millions of small files, rsync transfers, backups, etc. Though there
> have been many kernel panics along the way, I have never experienced
> any data loss.
>
> I did it the way it says in the README:
>
>> # diskutil partitionDisk disk{2,3,4,5} GPTFormat ZFS %noformat% 100%
>> #sudo zpool create wombat raidz disk{2,3,4,5}s2
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From franzschmalzl at spamfreemail.de  Wed Mar 19 13:33:53 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Wed, 19 Mar 2008 21:33:53 +0100
Subject: [zfs-discuss] time machine
Message-ID: <6A996909-F83E-403A-8DE1-2F165F1C9C7A@spamfreemail.de>

first: thanks to noel and boyd for their assistance on raidz


anyone tried using a sparsebundle on top of a zpool as timemachine disk?

or, is it even possible to tell time machine to backup to  
sparsebundles ?

thanks and best regards
franz schmalzl

From dirkschelfhout at mac.com  Wed Mar 19 13:50:34 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Wed, 19 Mar 2008 21:50:34 +0100
Subject: [zfs-discuss] raidz
In-Reply-To: <A78EDE51-82FC-49A3-9F41-80F0E1D766E5@spamfreemail.de>
References: <7271394B-C1ED-479C-8A54-58EB4FD4A64E@spamfreemail.de>
	<24423687-6BA1-47C6-B3AD-E79F38DF824D@apple.com>
	<A6125087-891E-40E0-B648-D4D1BAA1337F@spamfreemail.de>
	<45DBBF68-A6C0-4BEA-AF98-E8D3FA8A20C5@apple.com>
	<9969E6D0-37F7-43FA-95A5-4AA2C9680464@nrao.edu>
	<A78EDE51-82FC-49A3-9F41-80F0E1D766E5@spamfreemail.de>
Message-ID: <58247F09-2397-43E7-B97B-EAE08D7CAC24@mac.com>

see man diskutil
On 19 Mar 2008, at 20:48, Franz Schmalzl wrote:

> what does the %noformat% mean ?
>
>
> On 19.03.2008, at 20:16, Boyd Waters wrote:
>
>>> Currently though,
>>> there are no bugs which would cause raidz data loss at all.
>>
>>
>> I have two small raidz arrays, four disks each, one is 4x500GB the
>> other 4x1TB. I have *hammered* on these arrays with large files,
>> millions of small files, rsync transfers, backups, etc. Though there
>> have been many kernel panics along the way, I have never experienced
>> any data loss.
>>
>> I did it the way it says in the README:
>>
>>> # diskutil partitionDisk disk{2,3,4,5} GPTFormat ZFS %noformat% 100%
>>> #sudo zpool create wombat raidz disk{2,3,4,5}s2
>>
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Wed Mar 19 16:31:39 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed, 19 Mar 2008 16:31:39 -0700
Subject: [zfs-discuss] ZFS as AirDisk?
In-Reply-To: <C7F6B74C-B29C-4E36-B84A-4F54217C86EE@justincooley.com>
References: <C7F6B74C-B29C-4E36-B84A-4F54217C86EE@justincooley.com>
Message-ID: <E1D445B7-1172-4079-8820-0F0237E84900@apple.com>

No, actually ZFS as an Air Disk  is currently not supported yet.

Noel

On Mar 9, 2008, at 9:41 AM, Justin Cooley wrote:

> I don't think I've seen this discussed but is there currently a way to
> mount a zfs disk as an AirDisk over an Airport Extreme (current
> version)?
>
> I've tried and it doesn't seem to pick it up automatically.  I was
> also unable to get it mount over the command line (I may have been
> doing this wrong)?
>
> Anyone had any success with this?
>
> Thank you.
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From zfs at deadlycomputer.com  Wed Mar 19 21:11:11 2008
From: zfs at deadlycomputer.com (steve)
Date: Thu, 20 Mar 2008 00:11:11 -0400
Subject: [zfs-discuss] Abort Trap
Message-ID: <abfbdd440803192111x3f179cf9w13c68278a95c077d@mail.gmail.com>

I'm trying to create a zpool in Leopard 10.5.1 and I keep getting the "Abort
Trap" error.  I downloaded it from the ADC site, and used the instructions
here: http://www.lildude.co.uk/howto-create-a-zfs-filesystem-os-x-leopard/to
get it working in
10.5.1.

How do I go about fixing this?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080320/f450d251/attachment-0001.html 

From chris.shuman at gmail.com  Wed Mar 19 22:40:58 2008
From: chris.shuman at gmail.com (Chris Shuman)
Date: Thu, 20 Mar 2008 00:40:58 -0500
Subject: [zfs-discuss] Volume Won't Automount
Message-ID: <6F7A72EF-5EE5-470A-89B6-ACA0A38BEC6B@gmail.com>

My mirrored pool will not mount on reboot. I have to do a 'sudo zpool  
import -f zdata' to mount it every time. I checked the FAQs, but did  
not see any known issues. I also get a dialog box for each disk in the  
mirror indicating an unrecognized disk; asking if I want to format or  
ignore? Of course I always ignore, but is there a way to suppress  
those messages? It looks to me as if both of my issues are related, I  
just don't know how to fix them.

Chris

From info at martin-hauser.net  Thu Mar 20 02:39:55 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Thu, 20 Mar 2008 10:39:55 +0100
Subject: [zfs-discuss] Volume Won't Automount
In-Reply-To: <6F7A72EF-5EE5-470A-89B6-ACA0A38BEC6B@gmail.com>
References: <6F7A72EF-5EE5-470A-89B6-ACA0A38BEC6B@gmail.com>
Message-ID: <7107A0DC-80D4-4EBD-9D48-0A02E471B689@martin-hauser.net>

Hello Chris,

On Mar 20, 2008, at 06:40 AM, Chris Shuman wrote:
> My mirrored pool will not mount on reboot. I have to do a 'sudo zpool
> import -f zdata' to mount it every time. I checked the FAQs, but did
> not see any known issues. I also get a dialog box for each disk in the
> mirror indicating an unrecognized disk; asking if I want to format or
> ignore? Of course I always ignore, but is there a way to suppress
> those messages? It looks to me as if both of my issues are related, I
> just don't know how to fix them.

The only reason I've ever seen this problem was as I didn't have the  
drive formated with ZFS (that is not having done diskutil  
partitiondisk /dev/disk2 GPTFormat ZFS %noformat% 100%

I'm not quite sure if you should do that with something that has  
already data on as it might destroy your data. Noel has written  
somewhere else that they are working on making the process of setup  
easier not to mess up.

regards

Martin
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080320/a3663452/attachment.bin 

From danchr at daimi.au.dk  Thu Mar 20 09:46:38 2008
From: danchr at daimi.au.dk (Dan Villiom Podlaski Christiansen)
Date: Thu, 20 Mar 2008 17:46:38 +0100
Subject: [zfs-discuss] new release?
In-Reply-To: <73374D20-6940-4B1A-929A-B22E1CC7DE7D@apple.com>
References: <B21E341F-503A-45B9-AA00-88214EF7437B@martin-hauser.net>
	<73374D20-6940-4B1A-929A-B22E1CC7DE7D@apple.com>
Message-ID: <5841F875-A255-4325-8F98-0EF28CADA872@daimi.au.dk>

On 17 Mar 2008, at 20:03, No?l Dellofano wrote:
> check macosforge, I'll always keep the latest and greatest bits up  
> there.  I'll also send out an email announcement to this list when  
> the new bits are available and uploaded.

Just wondering; are there any plans to open the development up a bit  
more? Currently, the Subversion repository hasn't been modified in two  
months, and I'm sure you've done some work since then ;-) Also, having  
the working tree public might allow someone else to help out.

--

Dan Villiom Podlaski Christiansen
stud. scient., danchr at daimi.au.dk




From bwaters at nrao.edu  Thu Mar 20 09:56:33 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Thu, 20 Mar 2008 10:56:33 -0600
Subject: [zfs-discuss] Abort Trap
In-Reply-To: <abfbdd440803192111x3f179cf9w13c68278a95c077d@mail.gmail.com>
References: <abfbdd440803192111x3f179cf9w13c68278a95c077d@mail.gmail.com>
Message-ID: <9ABD195A-464C-47F0-B89A-A28FEA75893B@nrao.edu>


On Mar 19, 2008, at 10:11 PM, steve wrote:
> I downloaded it from the ADC site,

Ouch! I think that one is old!

Get it from this site:

http://trac.macosforge.org/projects/zfs/wiki/downloads

From alex.blewitt at gmail.com  Thu Mar 20 13:43:55 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Thu, 20 Mar 2008 20:43:55 +0000
Subject: [zfs-discuss] Abort Trap
In-Reply-To: <9ABD195A-464C-47F0-B89A-A28FEA75893B@nrao.edu>
References: <abfbdd440803192111x3f179cf9w13c68278a95c077d@mail.gmail.com>
	<9ABD195A-464C-47F0-B89A-A28FEA75893B@nrao.edu>
Message-ID: <636fd28e0803201343m10837cqd413532a357edb47@mail.gmail.com>

On Thu, Mar 20, 2008 at 4:56 PM, Boyd Waters <bwaters at nrao.edu> wrote:
>
>  On Mar 19, 2008, at 10:11 PM, steve wrote:
>  > I downloaded it from the ADC site,
>
>  Ouch! I think that one is old!

Certainly is!

>  Get it from this site:
>
>  http://trac.macosforge.org/projects/zfs/wiki/downloads

I'd strongly echo that. If you want a package, I've wrapped it as a
double-clickable ZFS.pkg here if you're interested:

http://alblue.blogspot.com/2008/03/zfs-on-mac.html

Alex
>  _______________________________________________
>  zfs-discuss mailing list
>  zfs-discuss at lists.macosforge.org
>  http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

From carengel at pmnet.uni-oldenburg.de  Thu Mar 20 17:44:38 2008
From: carengel at pmnet.uni-oldenburg.de (Carsten Engelberts)
Date: Fri, 21 Mar 2008 01:44:38 +0100
Subject: [zfs-discuss] No progress - scrub
Message-ID: <8923ED72-F4C0-4334-BECB-F8D164BED7E8@pmnet.uni-oldenburg.de>

Hi,
i'm using a 4-drive raidz since today. So after copying a bunch of  
files i did a #zpool scrub tank. All went fine for the first seconds  
but then the progress freezed. Here is the output.


---------------------------
pool: tank
state: ONLINE
status: The pool is formatted using an older on-disk format.  The pool  
can
	still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
	pool will no longer be accessible on older software versions.
scrub: scrub in progress, 3,20% done, 13h54m to go
config:

	NAME         STATE     READ WRITE CKSUM
	tank         ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk2s2  ONLINE       0     0     0
	    disk3s2  ONLINE       0     0     0
	    disk4s2  ONLINE       0     0     0
	    disk5s2  ONLINE       0     0     0

errors: No known data errors

-------------------------

No progress until now.  What is going wrong?

Cheers
Carsten



From james-zfsosx at jrv.org  Thu Mar 20 20:57:37 2008
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Thu, 20 Mar 2008 22:57:37 -0500
Subject: [zfs-discuss] How to erase cached zpool member info?
Message-ID: <47E33231.40107@jrv.org>

How do I erase the cached zpool information?

I have tremendous trouble reusing disks under OSX/ZFS if they have ever 
been used for ZFS elsewhere: kernel panics, etc.  I'm suspicious the 
problem may actually bogus state in some zfs cache since "zpool import" 
gives me nonsensical output:

bash-3.2# zpool import
  pool: xxx-1
    id: 1218651719753128900
 state: FAULTED
status: The pool was last accessed by another system.
action: The pool cannot be imported due to damaged devices or data.
   see: http://www.sun.com/msg/ZFS-8000-EY
config:

    xxx-1                   UNAVAIL  insufficient replicas
      raidz1                 UNAVAIL  insufficient replicas
        da0                  UNAVAIL  cannot open
        9526628723727662261  UNAVAIL  cannot open
        da1                  UNAVAIL  cannot open
        disk2                UNAVAIL  cannot open
        disk4                UNAVAIL  cannot open
bash-3.2#


No pool with that name has every been attached to this host.  There was 
such a pool attached to a FreeBSD host and then the disks recycled.

There is a file /etc/zfs/zpool.cache but this doesn't appear to contain 
useful data:

bash-3.2# od -x /etc/zfs/zpool.cache
0000000      0001    0000    0000    0000    0000    0100    0000    0000
0000020      0000    0000                                               
0000024
bash-3.2#

Once I get a zpool created it seems to work well, and I have never had a 
corruption problem.  But there are disk-full boundary cases to deal 
with: it appears possible to fill a disk such that an unprivileged user 
cannot recover (I've gotten a zpool full to the point that chmod failed 
but rm seems to still work).

From ndellofano at apple.com  Fri Mar 21 09:37:59 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 21 Mar 2008 09:37:59 -0700
Subject: [zfs-discuss] No progress - scrub
In-Reply-To: <8923ED72-F4C0-4334-BECB-F8D164BED7E8@pmnet.uni-oldenburg.de>
References: <8923ED72-F4C0-4334-BECB-F8D164BED7E8@pmnet.uni-oldenburg.de>
Message-ID: <7D6E90D6-0BA3-4336-811F-E2D27CA057A3@apple.com>

strange :(
So you started a scrub on the pool and each time you do a zpool  
status, the scrub is at the same point in its progress?  Or did the  
actual thread you started the scrub on hang during the command?

Noel

On Mar 20, 2008, at 5:44 PM, Carsten Engelberts wrote:

> Hi,
> i'm using a 4-drive raidz since today. So after copying a bunch of
> files i did a #zpool scrub tank. All went fine for the first seconds
> but then the progress freezed. Here is the output.
>
>
> ---------------------------
> pool: tank
> state: ONLINE
> status: The pool is formatted using an older on-disk format.  The pool
> can
> 	still be used, but some features are unavailable.
> action: Upgrade the pool using 'zpool upgrade'.  Once this is done,  
> the
> 	pool will no longer be accessible on older software versions.
> scrub: scrub in progress, 3,20% done, 13h54m to go
> config:
>
> 	NAME         STATE     READ WRITE CKSUM
> 	tank         ONLINE       0     0     0
> 	  raidz1     ONLINE       0     0     0
> 	    disk2s2  ONLINE       0     0     0
> 	    disk3s2  ONLINE       0     0     0
> 	    disk4s2  ONLINE       0     0     0
> 	    disk5s2  ONLINE       0     0     0
>
> errors: No known data errors
>
> -------------------------
>
> No progress until now.  What is going wrong?
>
> Cheers
> Carsten
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Fri Mar 21 09:58:52 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 21 Mar 2008 09:58:52 -0700
Subject: [zfs-discuss] How to erase cached zpool member info?
In-Reply-To: <47E33231.40107@jrv.org>
References: <47E33231.40107@jrv.org>
Message-ID: <605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>

Yeah so looks like you're having a hostil label takeover issue :)

  So for OSX, we actually don't use the /etc/zfs/zpool.cache file to  
store cached pool information like Solaris does.  This is because in  
Solaris, device paths and ids are static entities and never change,  
even across reboots.  Hence they can store disk path info for each  
pool for quick lookups but in OSX we can't since we generate device  
ids and paths automatically. They can change at a moments notice.

So, with that info, in OSX we simply sniff each disk we see and if it  
looks like a ZFS drive then we sniff the label out for it's info.   
Note that on ZFS drives we write 4 lables to them:

[ L0 ] [L1] ........................... [L2][L3]

these labels hold the vdevs for the pool, the pool guid name of the  
pool, and lots of other handy info.  It seems that this information  
never got cleared out, i.e. you didn't do a zpool destroy on this pool  
in it's last system?? If you did, apologies, as this shouldn't be an  
issue and I'll check it out.

If you didn't do a zpool destroy and are trying to reuse the disks for  
a new ZFS pool you will need to clear the labels out by hand  
(reformatting will only catch the labels at the front, not the back,  
hence doing this by hand ensures a clean drive).  We are working on a  
new utility which will clear the labels for you in one fell swoop.

In the meantime I recommend using dd to clear the labels out.  Each  
ZFS label is 256K.  Each disk looks like this:

<------ 200 MB EFI label -------> <----512K L0, L1 ZFS labels --- 
 >  ................................................... <--512K L2, L3  
ZFS labels--> {end}

I use dd and usually clear the first and last megs or so just to be  
complete.  Something like
#sudo dd if=/dev/zero of=/dev/disk2 bs=1m skip=200 count=1
#sudo dd if=/dev/zero of=/dev/disk2 bs=1m skip=153599

once for the front and once for the back.  For the back you just need  
to know how big your drive is and subtract off the 1m and write to the  
end of the drive.  If you need help or questions, feel free to email me.

Noel



On Mar 20, 2008, at 8:57 PM, James R. Van Artsdalen wrote:

> How do I erase the cached zpool information?
>
> I have tremendous trouble reusing disks under OSX/ZFS if they have  
> ever
> been used for ZFS elsewhere: kernel panics, etc.  I'm suspicious the
> problem may actually bogus state in some zfs cache since "zpool  
> import"
> gives me nonsensical output:
>
> bash-3.2# zpool import
>  pool: xxx-1
>    id: 1218651719753128900
> state: FAULTED
> status: The pool was last accessed by another system.
> action: The pool cannot be imported due to damaged devices or data.
>   see: http://www.sun.com/msg/ZFS-8000-EY
> config:
>
>    xxx-1                   UNAVAIL  insufficient replicas
>      raidz1                 UNAVAIL  insufficient replicas
>        da0                  UNAVAIL  cannot open
>        9526628723727662261  UNAVAIL  cannot open
>        da1                  UNAVAIL  cannot open
>        disk2                UNAVAIL  cannot open
>        disk4                UNAVAIL  cannot open
> bash-3.2#
>
>
> No pool with that name has every been attached to this host.  There  
> was
> such a pool attached to a FreeBSD host and then the disks recycled.
>
> There is a file /etc/zfs/zpool.cache but this doesn't appear to  
> contain
> useful data:
>
> bash-3.2# od -x /etc/zfs/zpool.cache
> 0000000      0001    0000    0000    0000    0000    0100    0000     
> 0000
> 0000020      0000    0000
> 0000024
> bash-3.2#
>
> Once I get a zpool created it seems to work well, and I have never  
> had a
> corruption problem.  But there are disk-full boundary cases to deal
> with: it appears possible to fill a disk such that an unprivileged  
> user
> cannot recover (I've gotten a zpool full to the point that chmod  
> failed
> but rm seems to still work).
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Fri Mar 21 10:03:12 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 21 Mar 2008 10:03:12 -0700
Subject: [zfs-discuss] Volume Won't Automount
In-Reply-To: <7107A0DC-80D4-4EBD-9D48-0A02E471B689@martin-hauser.net>
References: <6F7A72EF-5EE5-470A-89B6-ACA0A38BEC6B@gmail.com>
	<7107A0DC-80D4-4EBD-9D48-0A02E471B689@martin-hauser.net>
Message-ID: <2FD9B79B-60DF-45E8-9294-3A3675662DED@apple.com>

Hey Chris,

As Martin mentioned below, this could be the case.  It also could be  
that you used the whole disk name instead of the disk2s2 when creating  
your mirrored pool?  This is also a common cause for the pool not  
importing on reboot.  I will add this to the FAQ as well so everyone  
knows.  Did you do something like
zpool create mypool mirror disk1 disk2


? If so, that is why it's not importing.  For now, you need to specify  
the entire disk name(disks2 disk2s2) or diskutil gets confused when  
it's importing....

Noel

On Mar 20, 2008, at 2:39 AM, Martin Hauser wrote:

> Hello Chris,
>
> On Mar 20, 2008, at 06:40 AM, Chris Shuman wrote:
>> My mirrored pool will not mount on reboot. I have to do a 'sudo zpool
>> import -f zdata' to mount it every time. I checked the FAQs, but did
>> not see any known issues. I also get a dialog box for each disk in  
>> the
>> mirror indicating an unrecognized disk; asking if I want to format or
>> ignore? Of course I always ignore, but is there a way to suppress
>> those messages? It looks to me as if both of my issues are related, I
>> just don't know how to fix them.
>
> The only reason I've ever seen this problem was as I didn't have the  
> drive formated with ZFS (that is not having done diskutil  
> partitiondisk /dev/disk2 GPTFormat ZFS %noformat% 100%
>
> I'm not quite sure if you should do that with something that has  
> already data on as it might destroy your data. Noel has written  
> somewhere else that they are working on making the process of setup  
> easier not to mess up.
>
> regards
>
> Martin
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Fri Mar 21 10:05:14 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 21 Mar 2008 10:05:14 -0700
Subject: [zfs-discuss] Abort Trap
In-Reply-To: <636fd28e0803201343m10837cqd413532a357edb47@mail.gmail.com>
References: <abfbdd440803192111x3f179cf9w13c68278a95c077d@mail.gmail.com>
	<9ABD195A-464C-47F0-B89A-A28FEA75893B@nrao.edu>
	<636fd28e0803201343m10837cqd413532a357edb47@mail.gmail.com>
Message-ID: <BEB9DD60-CA3E-437B-930E-99F8975B723F@apple.com>

Thanks everyone!

and apologies, I thought they took the old one on the ADC site down so  
we wouldn't have this confusion, apparently not :(  I'll check on  
that...

Noel

On Mar 20, 2008, at 1:43 PM, Alex Blewitt wrote:

> On Thu, Mar 20, 2008 at 4:56 PM, Boyd Waters <bwaters at nrao.edu> wrote:
>>
>> On Mar 19, 2008, at 10:11 PM, steve wrote:
>>> I downloaded it from the ADC site,
>>
>> Ouch! I think that one is old!
>
> Certainly is!
>
>> Get it from this site:
>>
>> http://trac.macosforge.org/projects/zfs/wiki/downloads
>
> I'd strongly echo that. If you want a package, I've wrapped it as a
> double-clickable ZFS.pkg here if you're interested:
>
> http://alblue.blogspot.com/2008/03/zfs-on-mac.html
>
> Alex
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From thompson.chris.lists at gmail.com  Fri Mar 21 13:49:05 2008
From: thompson.chris.lists at gmail.com (Chris Thompson)
Date: Fri, 21 Mar 2008 14:49:05 -0600
Subject: [zfs-discuss] How to erase cached zpool member info?
In-Reply-To: <605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>
References: <47E33231.40107@jrv.org>
	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>
Message-ID: <FFF41898-9AC2-4D36-84D5-F5E71370C130@gmail.com>

This information should definitely be in the Readme file or FAQ.  It's  
the sort of thing that would have people tearing their hair out  
otherwise... most people would expect formatting a disk to clear all  
information.

-- Chris

On 21-Mar-08, at 10:58 AM, No?l Dellofano wrote:

> If you didn't do a zpool destroy and are trying to reuse the disks for
> a new ZFS pool you will need to clear the labels out by hand
> (reformatting will only catch the labels at the front, not the back,
> hence doing this by hand ensures a clean drive).  We are working on a
> new utility which will clear the labels for you in one fell swoop.
>
> In the meantime I recommend using dd to clear the labels out.  Each
> ZFS label is 256K.  Each disk looks like this:
>
> <------ 200 MB EFI label -------> <----512K L0, L1 ZFS labels ---
>> ................................................... <--512K L2, L3
> ZFS labels--> {end}
>
> I use dd and usually clear the first and last megs or so just to be
> complete.  Something like
> #sudo dd if=/dev/zero of=/dev/disk2 bs=1m skip=200 count=1
> #sudo dd if=/dev/zero of=/dev/disk2 bs=1m skip=153599
>
> once for the front and once for the back.  For the back you just need
> to know how big your drive is and subtract off the 1m and write to the
> end of the drive.  If you need help or questions, feel free to email  
> me.
>
> Noel


From james-zfsosx at jrv.org  Fri Mar 21 14:53:36 2008
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Fri, 21 Mar 2008 16:53:36 -0500
Subject: [zfs-discuss] How to erase cached zpool member info?
In-Reply-To: <605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>
References: <47E33231.40107@jrv.org>
	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>
Message-ID: <47E42E60.6040506@jrv.org>

No?l Dellofano wrote:
> Yeah so looks like you're having a hostil label takeover issue :)
>
>  So for OSX, we actually don't use the /etc/zfs/zpool.cache file to 
> store cached pool information like Solaris does.
>

"zfs import" reports bogus info even if the disks are physically 
detached from the system.  It's getting bad data somewhere other than 
the disks themselves at this point.


From franzschmalzl at spamfreemail.de  Sat Mar 22 06:17:42 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sat, 22 Mar 2008 14:17:42 +0100
Subject: [zfs-discuss] Time machine with ZFS backend
Message-ID: <70E03D30-1456-464C-B7DB-0ACCE5B42519@spamfreemail.de>

Hi list!

I just managed to get Time machine working on my external raidz...
it's a bit dirty, but works

Steps:

Create a zfs volume
Share over the network via afp
open up ssh tunnel to localhost, since afp won't let you connect to  
localhost
(ssh user at localhost -L randomportnumber:localhost:548 )
Command+K, apf://localhost:randomportnumber)

(( of course randomportnumber hast to be a random port number ;), like  
1234 )

Your "network" share should get mounted  and you can use it for time  
machine witch creates i'ts sparsebundles...

p.s.

don't forget to apply

defaults write com.apple.systempreferences  
TMShowUnsupportedNetworkVolumes 1


regards

franz schmalzl 

From katie.loch at gmail.com  Sat Mar 22 18:27:00 2008
From: katie.loch at gmail.com (Katie Locher)
Date: Sat, 22 Mar 2008 21:27:00 -0400
Subject: [zfs-discuss] ZFS and DTrace
In-Reply-To: <79B6DB59-F01A-4D85-9CD3-05E6AD988C97@apple.com>
References: <ee26ea0d0803121905p5e50a761ja14f5869dbb62c33@mail.gmail.com>
	<79B6DB59-F01A-4D85-9CD3-05E6AD988C97@apple.com>
Message-ID: <ee26ea0d0803221827k472a72fcl7bdd0f8fa48f5c81@mail.gmail.com>

Is this true even when adding SDT probes as described by the dtrace man page
on OS X?  I'm surprised.  Is there a stated reason why?
--Katie

On Thu, Mar 13, 2008 at 12:57 PM, No?l Dellofano <ndellofano at apple.com>
wrote:

> Currently you cannot dtrace Kexts(which ZFS is) in Leopard, hence you
> won't be able to dtrace ZFS on Leopard. The Dtrace team is working on
> this functionality...
>
> Noel
>
> On Mar 12, 2008, at 7:05 PM, Katie Locher wrote:
>
> > I'm looking for some information on instrumenting ZFS behavior with
> > DTrace on Leopard.  I have a ZFS pool set up, but all the
> > information I can find on instrumenting ZFS with DTrace seems to be
> > Solaris specific.  For example, the FBT provider has a ZFS module on
> > Solaris but not on my version of Leopard.  The fsinfo provider
> > doesn't appear on Leopard.  Has anybody done any work with DTrace
> > and ZFS on Leopard?
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss at lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080322/d21717e6/attachment.html 

From canadrian at electricteaparty.net  Tue Mar 25 16:26:50 2008
From: canadrian at electricteaparty.net (Thornton Adrian)
Date: Tue, 25 Mar 2008 17:26:50 -0600
Subject: [zfs-discuss] Increasing the capacity of a RAID-Z pool
Message-ID: <B42F7FC6-0F82-4D0D-98DB-808A2989E31D@electricteaparty.net>

Hoping I can get a clear answer on this one - there seems to be  
conflicting info drifting around the intertubes. I have a RAID-Z pool  
of eight 320gb disks, whose capacity is currently about 2tb. I would  
like to increase the capacity of the pool. I understand that I can not  
add more disks to the pool, but it sounds as if I can swap out disks  
for larger ones and resilver to achieve the same end.

Initially, I thought I understood that all I had to do was swap out  
one disk and resilver, export the pool, then import it again to see  
the greater capacity. I have done this (with two disks in fact), but  
the capacity of the pool has not changed. So does this mean I need to  
swap out ALL disks in the pool for larger ones before I will see an  
increase? Can anyone say with confidence that if I shell out for six  
more 750gb drives and go through the long process of resilvering that  
I will definitely see an increase in pool size?

Thanks in advance for your help on this.

- Canadrian

From james-zfsosx at jrv.org  Tue Mar 25 20:26:41 2008
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Tue, 25 Mar 2008 22:26:41 -0500
Subject: [zfs-discuss] Increasing the capacity of a RAID-Z pool
In-Reply-To: <B42F7FC6-0F82-4D0D-98DB-808A2989E31D@electricteaparty.net>
References: <B42F7FC6-0F82-4D0D-98DB-808A2989E31D@electricteaparty.net>
Message-ID: <47E9C271.8080903@jrv.org>

Thornton Adrian wrote:
> Hoping I can get a clear answer on this one - there seems to be  
> conflicting info drifting around the intertubes. I have a RAID-Z pool  
> of eight 320gb disks, whose capacity is currently about 2tb. I would  
> like to increase the capacity of the pool. I understand that I can not  
> add more disks to the pool, but it sounds as if I can swap out disks  
> for larger ones and resilver to achieve the same end.
>   
No, that's not the right way to think of it.

You have a pool with one vdev, and that vdev is a RAIDZ.  The pool can 
be expanded by adding vdevs but the RAIDZ cannot be expanded - indeed no 
vdev can be expanded in storage capacity as far as I know.  To add more 
capacity to the pool you have to add a new vdev.

(in other words you can add drives to the pool, just not to the existing 
RAIDZ: you can create a new MIRROR or RAIDZ vdev and add that vdev to 
the pool.  The concepts of MIRROR and RAIDZ only apply to individual 
vdevs, not to the pool itself)


From i_see at macnews.de  Tue Mar 25 23:42:33 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Wed, 26 Mar 2008 07:42:33 +0100
Subject: [zfs-discuss] Increasing the capacity of a RAID-Z pool
In-Reply-To: <mailman.910.1206501937.1359.zfs-discuss@lists.macosforge.org>
References: <mailman.910.1206501937.1359.zfs-discuss@lists.macosforge.org>
Message-ID: <5F852B1F-CAAF-46BF-8B1F-D150C0668BAE@macnews.de>

Hi,
as I understood it, for now you can.
1. add vdevs (raidz or mirrored, or even simply drives or files if you  
don't care about your data)
2. swap out all disks in a vdev with larger ones. The capacity of   
vdev is currently determined by the smallest device.
if you have 8 disks in a raidz1 vdev you'll have roughly 7*(capacity  
of the smallest disk), if you use raidz2, its 6*(capacity of the  
smallest disk)
As far as I understood the zfs-team is working on methods to do more  
flexible re-organization of pools, since for normal operations you  
basicly need the double (triple) capacity to do zfs send and receive  
of the data in order to rearrange drives.

However the good news is, there is no need to export and import the  
pool to see the capacity grow. ZFS performs all maintenance operations  
online.
(This is just for clarification - I'd rather have a offline "Alter  
vdev" tool than nothing, although in the long run it will be very nice  
to "evacuate" ,restripe, etc disks/vdevs while working online.)
Hope this helps,
	ralf
PS:  No?l or anyone with special access. I also missed this info in  
the FAQ. Is there any timeframe as to when we are likely to see  
improvement on that situation?
>
> Thornton Adrian wrote:
>> Hoping I can get a clear answer on this one - there seems to be   
>> conflicting info drifting around the intertubes. I have a RAID-Z  
>> pool  of eight 320gb disks, whose capacity is currently about 2tb.  
>> I would  like to increase the capacity of the pool. I understand  
>> that I can not  add more disks to the pool, but it sounds as if I  
>> can swap out disks  for larger ones and resilver to achieve the  
>> same end.
>>
> No, that's not the right way to think of it.
>
> You have a pool with one vdev, and that vdev is a RAIDZ.  The pool  
> can be expanded by adding vdevs but the RAIDZ cannot be expanded -  
> indeed no vdev can be expanded in storage capacity as far as I  
> know.  To add more capacity to the pool you have to add a new vdev.
>
> (in other words you can add drives to the pool, just not to the  
> existing RAIDZ: you can create a new MIRROR or RAIDZ vdev and add  
> that vdev to the pool.  The concepts of MIRROR and RAIDZ only apply  
> to individual vdevs, not to the pool itself)


From zorg at sogeeky.net  Sun Mar 30 16:47:45 2008
From: zorg at sogeeky.net (Mr. Zorg ...)
Date: Sun Mar 30 16:47:18 2008
Subject: [zfs-discuss] How to force a resilver of a faulted disc?
Message-ID: <fbaadd320803301647j2b41b1d7sad5bd2db7734f39b@mail.gmail.com>

I'm using a raidz on an external drive array, and one of the power
supplies failed on me last night.  The drive itself is fine, just the
power supply went bad.  I replaced it with a spare from some other
disks I had, but not before the pool became hopelessly out of sync and
considered the disk to be faulted.  I tried bringing it back online,
but it still wouldn't use it.  I know the disc is good, so how can I
force it to resilver it?  Not wanting to wait for too long, I hooked
up a spare disc and did a replace, which it's doing now.  I then
intend to replace the spare disc with the original one.  But that
takes a long time, just wondering if there was a better way?

$ zpool status
  pool: Storage
 state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
    attempt was made to correct the error.  Applications are unaffected.
action: Determine if the device needs to be replaced, and clear the errors
    using 'zpool clear' or replace the device with 'zpool replace'.
   see: http://www.sun.com/msg/ZFS-8000-9P
 scrub: resilver completed with 0 errors on Sun Mar 30 03:09:29 2008
config:

    NAME         STATE     READ WRITE CKSUM
    Storage      ONLINE       0     0     0
      raidz1     ONLINE       0     0     0
        disk6s2  ONLINE       0     0     0
        disk5s2  ONLINE       0     0     0
        disk4s2  ONLINE       0     0     0
        disk3s2  ONLINE       0     0    26
        disk1s2  ONLINE       0     0     0
        disk2s2  ONLINE       0     0     0

errors: No known data errors
From jbsnyder at gmail.com  Mon Mar 31 09:32:26 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Mon Mar 31 09:31:27 2008
Subject: [zfs-discuss] How to force a resilver of a faulted disc?
In-Reply-To: <fbaadd320803301647j2b41b1d7sad5bd2db7734f39b@mail.gmail.com>
References: <fbaadd320803301647j2b41b1d7sad5bd2db7734f39b@mail.gmail.com>
Message-ID: <33644d3c0803310932j6cb77dd4s289f6f0c8780bef7@mail.gmail.com>

I believe you'll just want to initiate a zpool scrub <poolname>, and
that will do it for you.  The scrub will end up going over all the
data in the pool, unfortunately, but it won't re-silver portions of
the disk with no data.

>From the zpool man page:

           Scrubbing  and resilvering are very similar operations. The differ-
           ence is that resilvering only examines data that ZFS  knows  to  be
           out  of  date (for example, when attaching a new device to a mirror
           or replacing an existing device), whereas  scrubbing  examines  all
           data to discover silent errors due to hardware faults or disk fail-
           ure.

If someone else knows of a better approach, I'd be glad to know.  My
understanding is that under normal conditions if the drive is exported
and imported it should only resilver portions that need it?  I also
thought that it should be able to resilver even if the device were
hot-unplugged, or were missing since data on disk are "always
consistent."  I'm more soliciting commentary in these last couple
sentences than trying to inform :-)  I've had to scrub disks in the
past while experimenting with mirror on OS X under situations where I
thought it would automatically resilver for me.

On Sun, Mar 30, 2008 at 6:47 PM, Mr. Zorg ... <zorg@sogeeky.net> wrote:
> I'm using a raidz on an external drive array, and one of the power
>  supplies failed on me last night.  The drive itself is fine, just the
>  power supply went bad.  I replaced it with a spare from some other
>  disks I had, but not before the pool became hopelessly out of sync and
>  considered the disk to be faulted.  I tried bringing it back online,
>  but it still wouldn't use it.  I know the disc is good, so how can I
>  force it to resilver it?  Not wanting to wait for too long, I hooked
>  up a spare disc and did a replace, which it's doing now.  I then
>  intend to replace the spare disc with the original one.  But that
>  takes a long time, just wondering if there was a better way?
>
>  $ zpool status
>   pool: Storage
>   state: ONLINE
>  status: One or more devices has experienced an unrecoverable error.  An
>     attempt was made to correct the error.  Applications are unaffected.
>  action: Determine if the device needs to be replaced, and clear the errors
>     using 'zpool clear' or replace the device with 'zpool replace'.
>    see: http://www.sun.com/msg/ZFS-8000-9P
>   scrub: resilver completed with 0 errors on Sun Mar 30 03:09:29 2008
>  config:
>
>     NAME         STATE     READ WRITE CKSUM
>     Storage      ONLINE       0     0     0
>       raidz1     ONLINE       0     0     0
>         disk6s2  ONLINE       0     0     0
>         disk5s2  ONLINE       0     0     0
>         disk4s2  ONLINE       0     0     0
>         disk3s2  ONLINE       0     0    26
>         disk1s2  ONLINE       0     0     0
>         disk2s2  ONLINE       0     0     0
>
>  errors: No known data errors
>  _______________________________________________
>  zfs-discuss mailing list
>  zfs-discuss@lists.macosforge.org
>  http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
From ndellofano at apple.com  Mon Mar 31 13:23:20 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon Mar 31 13:23:49 2008
Subject: [zfs-discuss] How to erase cached zpool member info?
In-Reply-To: <47E42E60.6040506@jrv.org>
References: <47E33231.40107@jrv.org>
	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>
	<47E42E60.6040506@jrv.org>
Message-ID: <6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>

yikes.  Ok that's truely disturbing.  I'll look into it.  Is there any  
pattern to this you've noticed?  so if you have a pool on a usb drive,  
and you export the pool and detatch it from the system, 'zpool import'  
still shows this pool as available for import?

Noel

On Mar 21, 2008, at 2:53 PM, James R. Van Artsdalen wrote:

> No?l Dellofano wrote:
>> Yeah so looks like you're having a hostil label takeover issue :)
>>
>> So for OSX, we actually don't use the /etc/zfs/zpool.cache file to  
>> store cached pool information like Solaris does.
>>
>
> "zfs import" reports bogus info even if the disks are physically  
> detached from the system.  It's getting bad data somewhere other  
> than the disks themselves at this point.
>

