<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [zfs-discuss] Performance and Spare...
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=Re%3A%20%5Bzfs-discuss%5D%20Performance%20and%20Spare...&In-Reply-To=%3C6494C722-A223-44F9-A342-CFB8FED20586%40gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001499.html">
   <LINK REL="Next"  HREF="001501.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[zfs-discuss] Performance and Spare...</H1>
    <B>Richard McClellan</B> 
    <A HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=Re%3A%20%5Bzfs-discuss%5D%20Performance%20and%20Spare...&In-Reply-To=%3C6494C722-A223-44F9-A342-CFB8FED20586%40gmail.com%3E"
       TITLE="[zfs-discuss] Performance and Spare...">richmc at gmail.com
       </A><BR>
    <I>Fri Apr 10 22:12:52 PDT 2009</I>
    <P><UL>
        <LI>Previous message: <A HREF="001499.html">[zfs-discuss] Performance and Spare...
</A></li>
        <LI>Next message: <A HREF="001501.html">[zfs-discuss] ZFS and the globalSAN iSCSI initiator - i/o error?
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1500">[ date ]</a>
              <a href="thread.html#1500">[ thread ]</a>
              <a href="subject.html#1500">[ subject ]</a>
              <a href="author.html#1500">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>
On Apr 10, 2009, at 08:54 , Ted Simbajon wrote:

&gt;<i> Well here's another detail I forgot to mention.
</I>&gt;<i>
</I>&gt;<i> 2 of the Storage Server (2x[16Disk:2 Escalade]) runs ATTO 3300 which  
</I>&gt;<i> is
</I>&gt;<i> 2Gbps FC card, because of this, this will bring my entire  
</I>&gt;<i> infrastructure
</I>&gt;<i> down to 2Gbps.
</I>
You should be seeing much higher bandwidth throughput than you quoted  
(~25MB/sec).

&gt;<i>
</I>&gt;<i> But still, it should be faster than 1 single disk 1394a write speed.  
</I>&gt;<i> Now, im
</I>&gt;<i> not sure of I want to redo this again since im not sure if I can fix  
</I>&gt;<i> it from
</I>&gt;<i> the start.
</I>&gt;<i>
</I>
What do you mean &quot;fix it from the start&quot;?  Setting up your pool with  
~8 vdevs should take you about 15 minutes if you go slowly/methodically.

&gt;<i> At almost at 6TB mark!. :)
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> ----
</I>&gt;<i> Hydra:~ hydra$ zpool iostat 1
</I>&gt;<i>               capacity     operations    bandwidth
</I>&gt;<i> pool         used  avail   read  write   read  write
</I>&gt;<i> ----------  -----  -----  -----  -----  -----  -----
</I>&gt;<i> Array       5.99T  4.67T      3    163   254K  17.2M
</I>&gt;<i> Array       5.99T  4.67T      0    189      0  23.5M
</I>&gt;<i> Array       5.99T  4.67T      4    283  71.0K  20.0M
</I>&gt;<i> Array       5.99T  4.67T     20     70  2.49M  7.85M
</I>&gt;<i> Array       5.99T  4.67T     13    206  1.24M  12.9M
</I>&gt;<i> Array       5.99T  4.67T      4      0   635K      0
</I>&gt;<i> Array       5.99T  4.67T      0    148      0  18.1M
</I>&gt;<i> Array       5.99T  4.67T      0    177      0  21.9M
</I>&gt;<i> Array       5.99T  4.67T      0    242      0  28.5M
</I>&gt;<i> Array       5.99T  4.67T      0    186      0  23.1M
</I>&gt;<i> Array       5.99T  4.67T      0    234      0  28.8M
</I>&gt;<i> Array       5.99T  4.67T      7    155   701K  6.24M
</I>&gt;<i> Array       5.99T  4.67T      0      0   127K      0
</I>&gt;<i> Array       5.99T  4.67T      2     41   381K  4.48M
</I>&gt;<i> Array       5.99T  4.67T      0    229      0  28.2M
</I>&gt;<i> Array       5.99T  4.67T      0    178      0  22.1M
</I>&gt;<i> Array       5.99T  4.67T      0    234      0  27.9M
</I>&gt;<i> Array       5.99T  4.67T      0    180      0  22.4M
</I>&gt;<i> Array       5.99T  4.67T      6    288   137K  22.0M
</I>&gt;<i> Array       5.99T  4.67T      1      0   1017      0
</I>&gt;<i> Array       5.99T  4.67T     12      0  1.60M      0
</I>&gt;<i> Array       5.99T  4.67T      0    166      0  20.6M
</I>&gt;<i> Array       5.99T  4.67T      0    227      0  27.4M
</I>&gt;<i> Array       5.99T  4.67T      0    185      0  23.0M
</I>&gt;<i> Array       5.99T  4.67T      0    234      0  27.8M
</I>&gt;<i> Array       5.99T  4.67T      0    190      0  23.3M
</I>&gt;<i> Array       5.99T  4.67T     10    150   707K  4.99M
</I>&gt;<i> Array       5.99T  4.67T     11      0  1.49M      0
</I>&gt;<i> Array       5.99T  4.67T      1     58   254K  7.33M
</I>&gt;<i> ----
</I>&gt;<i>
</I>&gt;<i> My only biggest concern is the two disk outside of my Raidz2.
</I>&gt;<i>
</I>&gt;<i> Can someone confirm that there are data on these 2 simple disk?.
</I>&gt;<i> To me it looks like, these 2 disk is a part of &quot;Array&quot; which Array is
</I>&gt;<i> composed of 3 Volumes, 1(raidz2) and 2(simple volume).
</I>&gt;<i>
</I>
I've never made a pool with disk vdevs, but if they are treated like  
mirror or raidz vdevs than a `zpool status` should list both the  
vdev--&quot;disk&quot;--and the hard drive in it--&quot;disk10s2&quot;--as the raidz2  
vdevs are listed.  So the output of `zpool status` would like  
something like

...
Array
   raidz2
     diskNsM
     ...
   disk
     disk10s2
   disk
     disk157s2


You could also try adding -v to iostat (i.e., `zpool iostat -v Array  
1`) and see how the load is spread across all of the devices. If the  
hard drives in the raidz2 pool are getting a smaller fraction of IOPS,  
reads, writes, etc., than the supposed &quot;disk&quot; vdevs, then you may have  
three vdevs comprising the pools.

Regardless, it's not a good idea to have just one vdev in the zpool,  
so redo it :-)

I missed the thread on disk labels changing when the computer starts  
up/initiates connections to the SAN boxes.  Does the order vary within  
a storage group (e.g., disks in SAN A are randomly out of order) or  
across all of the storage groups (one time SAN B is seen first so it's  
drives get the lower numbers, next time SAN A gets the lower numbers)  
or are they all mixed up?  I've found with iSCSI targets that I have  
to be careful about the order in which the targets are presented to  
the OS.

		Rich


&gt;<i> This might be my problem with my write preformance...
</I>&gt;<i>
</I>&gt;<i> ----
</I>&gt;<i> Hydra:~ hydra$ zpool status
</I>&gt;<i>  pool: Array
</I>&gt;<i> state: ONLINE
</I>&gt;<i> scrub: scrub stopped with 0 errors on Wed Apr  8 16:26:00 2009
</I>&gt;<i> config:
</I>&gt;<i>
</I>&gt;<i>    NAME           STATE     READ WRITE CKSUM
</I>&gt;<i>    Array          ONLINE       0     0     0
</I>&gt;<i>      raidz2       ONLINE       0     0     0
</I>&gt;<i>        disk9s2    ONLINE       0     0     0
</I>&gt;<i>        disk5s2    ONLINE       0     0     0
</I>&gt;<i>        disk12s2   ONLINE       0     0     0
</I>&gt;<i>        disk8s2    ONLINE       0     0     0
</I>&gt;<i>        disk18s2   ONLINE       0     0     0
</I>&gt;<i>        disk19s2   ONLINE       0     0     0
</I>&gt;<i>        disk20s2   ONLINE       0     0     0
</I>&gt;<i>        disk28s2   ONLINE       0     0     0
</I>&gt;<i>        disk25s2   ONLINE       0     0     0
</I>&gt;<i>        disk21s2   ONLINE       0     0     0
</I>&gt;<i>        disk29s2   ONLINE       0     0     0
</I>&gt;<i>        disk33s2   ONLINE       0     0     0
</I>&gt;<i>        disk36s2   ONLINE       0     0     0
</I>&gt;<i>        disk35s2   ONLINE       0     0     0
</I>&gt;<i>        disk42s2   ONLINE       0     0     0
</I>&gt;<i>        disk39s2   ONLINE       0     0     0
</I>&gt;<i>        disk56s2   ONLINE       0     0     0
</I>&gt;<i>        disk41s2   ONLINE       0     0     0
</I>&gt;<i>        disk50s2   ONLINE       0     0     0
</I>&gt;<i>        disk44s2   ONLINE       0     0     0
</I>&gt;<i>        disk49s2   ONLINE       0     0     0
</I>&gt;<i>        disk58s2   ONLINE       0     0     0
</I>&gt;<i>        disk51s2   ONLINE       0     0     0
</I>&gt;<i>        disk69s2   ONLINE       0     0     0
</I>&gt;<i>        disk63s2   ONLINE       0     0     0
</I>&gt;<i>        disk60s2   ONLINE       0     0     0
</I>&gt;<i>        disk71s2   ONLINE       0     0     0
</I>&gt;<i>        disk82s2   ONLINE       0     0     0
</I>&gt;<i>        disk77s2   ONLINE       0     0     0
</I>&gt;<i>        disk70s2   ONLINE       0     0     0
</I>&gt;<i>        disk90s2   ONLINE       0     0     0
</I>&gt;<i>        disk62s2   ONLINE       0     0     0
</I>&gt;<i>        disk81s2   ONLINE       0     0     0
</I>&gt;<i>        disk74s2   ONLINE       0     0     0
</I>&gt;<i>        disk94s2   ONLINE       0     0     0
</I>&gt;<i>        disk87s2   ONLINE       0     0     0
</I>&gt;<i>        disk83s2   ONLINE       0     0     0
</I>&gt;<i>        disk84s2   ONLINE       0     0     0
</I>&gt;<i>        disk101s2  ONLINE       0     0     0
</I>&gt;<i>        disk92s2   ONLINE       0     0     0
</I>&gt;<i>        disk91s2   ONLINE       0     0     0
</I>&gt;<i>        disk104s2  ONLINE       0     0     0
</I>&gt;<i>        disk124s2  ONLINE       0     0     0
</I>&gt;<i>        disk112s2  ONLINE       0     0     0
</I>&gt;<i>        disk110s2  ONLINE       0     0     0
</I>&gt;<i>        disk120s2  ONLINE       0     0     0
</I>&gt;<i>        disk115s2  ONLINE       0     0     0
</I>&gt;<i>        disk130s2  ONLINE       0     0     0
</I>&gt;<i>        disk135s2  ONLINE       0     0     0
</I>&gt;<i>        disk150s2  ONLINE       0     0     0
</I>&gt;<i>        disk132s2  ONLINE       0     0     0
</I>&gt;<i>        disk129s2  ONLINE       0     0     0
</I>&gt;<i>        disk122s2  ONLINE       0     0     0
</I>&gt;<i>        disk121s2  ONLINE       0     0     0
</I>&gt;<i>        disk131s2  ONLINE       0     0     0
</I>&gt;<i>        disk165s2  ONLINE       0     0     0
</I>&gt;<i>        disk137s2  ONLINE       0     0     0
</I>&gt;<i>        disk133s2  ONLINE       0     0     0
</I>&gt;<i>        disk145s2  ONLINE       0     0     0
</I>&gt;<i>        disk164s2  ONLINE       0     0     0
</I>&gt;<i>        disk141s2  ONLINE       0     0     0
</I>&gt;<i>        disk148s2  ONLINE       0     0     0
</I>&gt;<i>      disk10s2     ONLINE       0     0     0
</I>&gt;<i>      disk157s2    ONLINE       0     0     0
</I>&gt;<i>
</I>&gt;<i> errors: No known data errors
</I>&gt;<i> -------
</I>&gt;<i>
</I>&gt;<i> Thanks.
</I>&gt;<i>
</I>&gt;<i> ---
</I>&gt;<i> Ted
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;&gt;<i> From: &quot;erik.ableson&quot; &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">eableson at me.com</A>&gt;
</I>&gt;&gt;<i> Date: Fri, 10 Apr 2009 10:03:25 +0200
</I>&gt;&gt;<i> To: Ted Simbajon &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">ted at philotv.com</A>&gt;
</I>&gt;&gt;<i> Cc: &lt;<A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">zfs-discuss at lists.macosforge.org</A>&gt;
</I>&gt;&gt;<i> Subject: Re: [zfs-discuss] Performance and Spare...
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Hmmm - given that environment, I would expect much better
</I>&gt;&gt;<i> performance.  That said, if the objective is to furnish shared  
</I>&gt;&gt;<i> storage
</I>&gt;&gt;<i> to more than one machine, I would have a tendency towards using
</I>&gt;&gt;<i> OpenSolaris for managing the disks and then publish the data via  
</I>&gt;&gt;<i> iSCSI
</I>&gt;&gt;<i> or NFS to the Mac machines.  The current ZFS build on the Mac is a  
</I>&gt;&gt;<i> few
</I>&gt;&gt;<i> version behind and the OpenSolaris build is much more mature. You'll
</I>&gt;&gt;<i> be able to boost your performance with the addition of SSD cache
</I>&gt;&gt;<i> devices if required which is not (from what I understand) a priority
</I>&gt;&gt;<i> in the OS X build.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> For comparison on a small scale, I can pull down 128 Mb/s write and
</I>&gt;&gt;<i> 156MB/s read on a small install composed of 4 SATA disks on a generic
</I>&gt;&gt;<i> PC with a 2.6Ghz Core 2 Duo.  Bottlenecks aside (like the IDE buses  
</I>&gt;&gt;<i> in
</I>&gt;&gt;<i> the Escalades), your FC infrastructure should be able to handle about
</I>&gt;&gt;<i> 400MB/s and you certainly have enough disks to attain that kind of
</I>&gt;&gt;<i> performance. With just the 4 disks I can saturate a GbE connection,  
</I>&gt;&gt;<i> so
</I>&gt;&gt;<i> with 64 (properly configured) you should be able to saturate a 4Gbps
</I>&gt;&gt;<i> FC connection.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Example on my home server:
</I>&gt;&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">root at shemhazai</A>:~# time dd if=/dev/zero of=/siovale/bench/bigfile  
</I>&gt;&gt;<i> bs=8k
</I>&gt;&gt;<i> count=750000
</I>&gt;&gt;<i> 750000+0 records in
</I>&gt;&gt;<i> 750000+0 records out
</I>&gt;&gt;<i> 6144000000 bytes (6.1 GB) copied, 47.9947 s, 128 MB/s
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">root at shemhazai</A>:~# time dd of=/dev/null if=/siovale/bench/bigfile  
</I>&gt;&gt;<i> bs=8k
</I>&gt;&gt;<i> count=750000
</I>&gt;&gt;<i> 750000+0 records in
</I>&gt;&gt;<i> 750000+0 records out
</I>&gt;&gt;<i> 6144000000 bytes (6.1 GB) copied, 39.4134 s, 156 MB/s
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Cheers,
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Erik
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> On 9 avr. 09, at 17:50, Ted Simbajon wrote:
</I>&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> There are 64x 170GB &quot;IDE&quot; disk and 8 of these disk is being
</I>&gt;&gt;&gt;<i> controlled by a
</I>&gt;&gt;&gt;<i> &quot;3ware Escalade&quot; which is housed in a 3U server. 1 server houses 2
</I>&gt;&gt;&gt;<i> controller which sums to 16 Disk per server and I have 4 of these
</I>&gt;&gt;&gt;<i> servers.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> All these storage servers are connected to a Qlogic Sandbox 1400
</I>&gt;&gt;&gt;<i> (4Gbps
</I>&gt;&gt;&gt;<i> &quot;bits&quot;) via an ATTO 3300 (2Gbps) HBA. The MAC machine (G5 2x2.5Ghz
</I>&gt;&gt;&gt;<i> w/ 7GB
</I>&gt;&gt;&gt;<i> RAM) is connected to the same Sandbox via an ATTO FC-42XS card. On
</I>&gt;&gt;&gt;<i> top of
</I>&gt;&gt;&gt;<i> the 64x170GB &quot;IDE&quot; Disk Targets that one of the ports of the ATTO
</I>&gt;&gt;&gt;<i> FC-42XS
</I>&gt;&gt;&gt;<i> sees, it also sees a 128x500GB &quot;SATA&quot; disk Targets which is an &quot;Avid
</I>&gt;&gt;&gt;<i> Unity&quot;
</I>&gt;&gt;&gt;<i> via a Qlogic 52xx Sandbox.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> So, keeping those in-mind. I looked at the old Logs and it kindda
</I>&gt;&gt;&gt;<i> matches
</I>&gt;&gt;&gt;<i> what I have right now (ZFS RAIDZ2) which is about ~25 - 30 MB/s.
</I>&gt;&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> _______________________________________________
</I>&gt;<i> zfs-discuss mailing list
</I>&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">zfs-discuss at lists.macosforge.org</A>
</I>&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss</A>
</I>
</PRE>



<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001499.html">[zfs-discuss] Performance and Spare...
</A></li>
	<LI>Next message: <A HREF="001501.html">[zfs-discuss] ZFS and the globalSAN iSCSI initiator - i/o error?
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1500">[ date ]</a>
              <a href="thread.html#1500">[ thread ]</a>
              <a href="subject.html#1500">[ subject ]</a>
              <a href="author.html#1500">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">More information about the zfs-discuss
mailing list</a><br>
</body></html>
