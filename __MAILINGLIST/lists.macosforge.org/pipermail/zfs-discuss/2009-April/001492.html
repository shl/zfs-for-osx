<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [zfs-discuss] Performance and Spare...
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=Re%3A%20%5Bzfs-discuss%5D%20Performance%20and%20Spare...&In-Reply-To=%3CC6036769.F77F%25Ted%40philotv.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001497.html">
   <LINK REL="Next"  HREF="001493.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[zfs-discuss] Performance and Spare...</H1>
    <B>Ted Simbajon</B> 
    <A HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=Re%3A%20%5Bzfs-discuss%5D%20Performance%20and%20Spare...&In-Reply-To=%3CC6036769.F77F%25Ted%40philotv.com%3E"
       TITLE="[zfs-discuss] Performance and Spare...">Ted at philotv.com
       </A><BR>
    <I>Thu Apr  9 08:50:49 PDT 2009</I>
    <P><UL>
        <LI>Previous message: <A HREF="001497.html">[zfs-discuss] Performance and Spare...
</A></li>
        <LI>Next message: <A HREF="001493.html">[zfs-discuss] Performance and Spare...
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1492">[ date ]</a>
              <a href="thread.html#1492">[ thread ]</a>
              <a href="subject.html#1492">[ subject ]</a>
              <a href="author.html#1492">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Here is the rundown on my hardware.

There are 64x 170GB &quot;IDE&quot; disk and 8 of these disk is being controlled by a
&quot;3ware Escalade&quot; which is housed in a 3U server. 1 server houses 2
controller which sums to 16 Disk per server and I have 4 of these servers.

All these storage servers are connected to a Qlogic Sandbox 1400 (4Gbps
&quot;bits&quot;) via an ATTO 3300 (2Gbps) HBA. The MAC machine (G5 2x2.5Ghz w/ 7GB
RAM) is connected to the same Sandbox via an ATTO FC-42XS card. On top of
the 64x170GB &quot;IDE&quot; Disk Targets that one of the ports of the ATTO FC-42XS
sees, it also sees a 128x500GB &quot;SATA&quot; disk Targets which is an &quot;Avid Unity&quot;
via a Qlogic 52xx Sandbox.

So, keeping those in-mind. I looked at the old Logs and it kindda matches
what I have right now (ZFS RAIDZ2) which is about ~25 - 30 MB/s.

Now, my biggest concern are the two disk outside of my Raidz2

&gt;<i>Yes.  Did you &quot;zpool add&quot; these later?  The data therein is not protected.
</I>Unfortunately, there is no way to &quot;unadd&quot; these.

No, I added All the Drives into the &quot;Array&quot; Raidz2 at once, all 64 of them,
but didn't really noticed them until last night. Is there something I can do
to correct them?

With regards to the &quot;mirroring&quot;, I can't since it's going to have about 9+TB
of data after all these writings.


&gt;<i>Yes, but there are also configurations which are worse than others. For
</I>example, having more than about 9-10 vdevs in a raidz2 set is likely to be a bad
idea for someone expecting decent performance.

Here is where it shows that I'm a New user, can I create a Volume &quot;Array&quot;
with ~10TB of storage consisting of 6vdevs where each vdev is a raidz2
consisting of 10-12 disk, so a total of 6vdevs?



With great respect to you guys!


----------------------
Ted Simbajon
Avid ACSR-MAC / IT Tech Support Engr.
Philo TV / Lieberman Productions
415-321-4551






</PRE>




<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001497.html">[zfs-discuss] Performance and Spare...
</A></li>
	<LI>Next message: <A HREF="001493.html">[zfs-discuss] Performance and Spare...
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1492">[ date ]</a>
              <a href="thread.html#1492">[ thread ]</a>
              <a href="subject.html#1492">[ subject ]</a>
              <a href="author.html#1492">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">More information about the zfs-discuss
mailing list</a><br>
</body></html>
