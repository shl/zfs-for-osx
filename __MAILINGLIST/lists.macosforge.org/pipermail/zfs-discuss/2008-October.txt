From werner.donne at re.be  Wed Oct  1 00:18:26 2008
From: werner.donne at re.be (=?ISO-8859-1?Q?Werner_Donn=E9?=)
Date: Wed, 1 Oct 2008 09:18:26 +0200
Subject: [zfs-discuss] late mount
In-Reply-To: <EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
	<48E2180C.4090102@loveturtle.net>
	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
Message-ID: <82213C42-A6D4-44D6-84E7-8FCFDB6AC11E@re.be>

Hi,

I have my home directory on a pool of three external USB drives and I
turned off power saving from the beginning. However, I reckon power
saving is on by default and I wonder at which point in the boot sequence
the preference becomes active. I say this because the disks are turned
off and on several times during the boot cycle.

Regards,

Werner.

On 30 Sep 2008, at 22:31, No?l Dellofano wrote:

> This is strange.  A group of us at Apple keep our home directories on
> ZFS on our laptops on the internal drive.  Are all of you guys using
> the internal drives, or USB drives for your home directories? or some
> other type of detachable (non internal bay) drive?  There has been
> some issues in the past noted with external drives since a lot of them
> have power saving features and such so the drive sometimes isn't
> actually spun up,  or takes a bit to get spun up.
>
> Noel
>
> On Sep 30, 2008, at 5:14 AM, Dillon Kass wrote:
>
>> I just use the login window to add a few more seconds to the login
>> time.
>> Usually by the time the login window loads and i type my username and
>> password + take a single breath my home dir has mounted and i can
>> safely
>> log in.
>>
>> I also have cron snapshot my home dir by hourly so that if i do log  
>> in
>> too quick and lose my settings i can just log out and rollback.
>>
>>
>> Werner Donn? wrote:
>>> Hi,
>>>
>>> This is how it started for me too. Since then it has deteriorated
>>> gradually up to the point, today, where after twenty reboots I still
>>> have no online pool. I wonder if ZFS is suitable for USB-disks on  
>>> Mac
>>> OS X. If it isn't I'm going to get a Solaris box and move the pool  
>>> to
>>> it.
>>>
>>> Regards,
>>>
>>> Werner.
>>>
>>> On 28 Sep 2008, at 17:46, ruebezahl wrote:
>>>
>>>
>>>> hey there!
>>>>
>>>> guys: PANIC !
>>>>
>>>> It seems my internal zpool which contains my homefolder gets  
>>>> mounted
>>>> very late in the booting process, maybe not even until i trie to
>>>> login.
>>>> which results in my library/wallpaper and so on not getting loaded
>>>> properly
>>>>
>>>> when logging in as another user first, and then logging in again it
>>>> works ?
>>>>
>>>> is there a way to force osx to mount my zpool at boot time ?
>>>>
>>>> i tried via a launchd entry, but failed.
>>>>
>>>>
>>>> regards
>>>>
>>>> franz
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>
>>> --
>>> Werner Donn?  --  Re                                     http://www.pincette.biz
>>> Engelbeekstraat 8                                               http://www.re.be
>>> BE-3300 Tienen
>>> tel: (+32) 486 425803	e-mail: werner.donne at re.be
>>>
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

--
Werner Donn?  --  Re                                     http://www.pincette.biz
Engelbeekstraat 8                                               http://www.re.be
BE-3300 Tienen
tel: (+32) 486 425803	e-mail: werner.donne at re.be






From werner.donne at re.be  Wed Oct  1 00:24:59 2008
From: werner.donne at re.be (=?ISO-8859-1?Q?Werner_Donn=E9?=)
Date: Wed, 1 Oct 2008 09:24:59 +0200
Subject: [zfs-discuss] late mount
In-Reply-To: <48E2A79D.9020100@loveturtle.net>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>	<48E2180C.4090102@loveturtle.net>	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>	<46CAEFC3-A3EA-4A7A-B696-3084D92F93FC@durin42.com>
	<20080930150928.f989f8f6.aorchid@mac.com>
	<48E2A79D.9020100@loveturtle.net>
Message-ID: <544759C7-19E0-4E4A-9BEF-B22C78246A21@re.be>

Hi,

On 01 Oct 2008, at 00:26, Dillon Kass wrote:

> From what I've seen on this list the "not importing on boot" is
> generally because diskutil wasn't used first to prepare the disk  
> before
> adding it to your zpool. Even if you did use diskutil to prepare the
> disk if you inserted the entire disk and not the s2 slice it would
> probably act this way as well.
>

I have the problem despite the s2 slices being added to the pool.

> note that you can also log in as the user >console from loginwindow  
> and
> it will drop you to the command line. This is a little easier than
> creating a second user.
>

That is a good idea.

Werner.

> Aric Gregson wrote:
>> On Tue, 30 Sep 2008 15:44:07 -0500
>> Augie Fackler<lists at durin42.com>  wrote:
>>
>>
>>> I've got a ZFS partition on the internal in my MBP, and I can
>>> generally count on having to manually use zpool to import it when I
>>> reboot.
>>>
>>
>> We are having a similar problem here on a MacPro. Our 'home' for the
>> main user is on a ZFS (only) formatted, mirrored internal drive. When
>> we reboot, we have to log in as another user on the HFS+ boot drive
>> (created just for this purpose) and manually force import the ZFS
>> pools. Every single time.
>>
>> I'd love to help debug this and figure out what the problem is. Not
>> sure where to look, but will if pointed in the right direction....
>>
>> thanks, aric
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

--
Werner Donn?  --  Re                                     http://www.pincette.biz
Engelbeekstraat 8                                               http://www.re.be
BE-3300 Tienen
tel: (+32) 486 425803	e-mail: werner.donne at re.be






From mcamou at tecnoguru.com  Wed Oct  1 00:26:27 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Wed, 1 Oct 2008 09:26:27 +0200
Subject: [zfs-discuss] Advice for ZFS config
In-Reply-To: <24082E9C-3D6E-4BD9-8FE6-F6807295A010@gmail.com>
References: <3FDB5662-FC5E-4DA1-84F3-0215EF30BDF4@tecnoguru.com>
	<24082E9C-3D6E-4BD9-8FE6-F6807295A010@gmail.com>
Message-ID: <762437f0810010026kf98c44i29fe32bbd7cd0de2@mail.gmail.com>

I hadn't thought about it this way but it's true, since the ZFS layer
doesn't know anything about how the disk is actually laid out it would
assume that A.{1,2} and B.{1,2} are actually 4 separate disks which would
probably result in lots of thrashing and waear-and-tear.

I really don't know about whether the RAIDZs can serve requests concurrently
since I would assume that would be hidden by the ZFS layer. My assumption so
far is that I have a single ZFS volume with 2 pools below it so the OS would
see a single filesystem. Perhaps this assumption is wrong?

Anyway now that I see it this way I think option #1 will be best, like I
said mainly to save on wear-and-tear. Since this is a small network (2
users, 5 workstations) performance is not too much of an issue as long as I
can get enough bandwidth to get video without stuttering (since this will
also be my media center).

Any more thoughts will be very welcome.

Thanks,
-Mario.

On Wed, Oct 1, 2008 at 2:08 AM, Kona Blend <kona8lend at gmail.com> wrote:

> I would recommend case #1 because the performance characteristic will be
> that writes will behave like striping across 2 spindles, and reads will
> behave at worse like striping across 2 spindles and at best like striping
> across 3 spindles. And up to 2 drives (1 drive from each set) may fail
> without losing data.
>
> Case #2 sounds pretty bad; as though it will simply behave like 1 spindle
> for any reads or writes. Drive failure survival is murky. You can survive at
> least 1 drive failure, or 2, depending on the combinations. As I understand
> it:
>
> A = 1TB, A.1 = 500, A.2 = 500
> B = 1TB, B.1 = 500, B.2 = 500
> C = 500GB
> D = 500GB
> E = 500GB
>
> RAIDZ { A.1 + B.1 + C + D } + RAIDZ { A.2 + B.2 + E }
>
> Is there a case where the RAIDZs can service the requests concurrently?
> RAIDZ does full stripe-writes, thus drives A,B act as contention points
> between the concat'd RAIDZs.
>
> --Kona Blend
>
>
> On 30-Sep-08, at 7:02 PM, Mario Camou wrote:
>
>  Hi all,
>>
>> I am planning to turn a Mac Mini into a disk server / media center to
>> replace a Linux machine I have running with MythTV. I am planning to
>> use ZFS for the filesystem. The thing is, I'm not exactly sure which
>> will be the best hard drive config.
>>
>> I currently have about 1.7TB of data in several independent USB disks.
>> I want to create a single ZFS volume. I have 2x1TB disks and 3x500GB
>> disks.
>>
>> My question is regarding how to best organize the disks into pools to
>> get a bit of data security (of which I currently have none). I see two
>> options:
>>
>> 1. Mirror the 2x1TB drives and RAIDZ the 3x500GB drives, giving 2TB of
>> data space
>>
>> 2. Partition the 2x!TB drives into 2x500GB partitions each. RAIDZ one
>> partition from each drive with 2x500GB hard drives, and also RAIDZ the
>> other partition in each drive with 1x500GB drive. That would give me
>> 2.5TB of data.
>>
>> I would prefer to use approach 2) since it gives me more data space
>> but I'd like your thoughts on why this might be a bad idea, or on
>> other ideas on how to lay out the pools.
>>
>> Thanks in advance,
>> -Mario.
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
>


-- 
The impossible has, on occasion, let me down
                                                       --R.U. Sirius
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081001/3566e595/attachment.html 

From franzschmalzl at spamfreemail.de  Wed Oct  1 02:12:41 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Wed, 1 Oct 2008 11:12:41 +0200
Subject: [zfs-discuss] late mount
In-Reply-To: <EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
	<48E2180C.4090102@loveturtle.net>
	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
Message-ID: <B3D8D44D-4860-4439-8025-E681F0A8D2B8@spamfreemail.de>

I invested some time the last days to fix this issue and came to the  
conclusion that ( at least in my case it was not really zfs' fault)
I had 4 partitions on my one and only internal harddrive ( 500gb in an  
imac)
EFI
HFS+
ZFS
NTFS

Apparently Windows fucked up my partition table, which at first i did  
not notice.
When doing a diskutil list it would crash with "Diskmanagement set-uid  
tool failure"
Diskutility GUI would crash when erasing volumes or trying to  
partition my internal harddrive.

I tried to erase everything and set everything up from scratch, worked  
fine until i installed windows again *and* synced gpt and mbr using  
rEFIt ( mayb it's rEFIt's fault ? )
same thing again, zfs partition was not properly mounted at boot,  
diskmanagement setuid tool failure.

I now ended up restoring everything again from backups, but without  
any windows partiton.
Again, everything works like a charm now.


So i think i ran into some partition table/poor osx management of  
partititon issues


My thanks for your time No?l

Franz


On 30.09.2008, at 22:31, No?l Dellofano wrote:

> This is strange.  A group of us at Apple keep our home directories on
> ZFS on our laptops on the internal drive.  Are all of you guys using
> the internal drives, or USB drives for your home directories? or some
> other type of detachable (non internal bay) drive?  There has been
> some issues in the past noted with external drives since a lot of them
> have power saving features and such so the drive sometimes isn't
> actually spun up,  or takes a bit to get spun up.
>
> Noel
>
> On Sep 30, 2008, at 5:14 AM, Dillon Kass wrote:
>
>> I just use the login window to add a few more seconds to the login
>> time.
>> Usually by the time the login window loads and i type my username and
>> password + take a single breath my home dir has mounted and i can
>> safely
>> log in.
>>
>> I also have cron snapshot my home dir by hourly so that if i do log  
>> in
>> too quick and lose my settings i can just log out and rollback.
>>
>>
>> Werner Donn? wrote:
>>> Hi,
>>>
>>> This is how it started for me too. Since then it has deteriorated
>>> gradually up to the point, today, where after twenty reboots I still
>>> have no online pool. I wonder if ZFS is suitable for USB-disks on  
>>> Mac
>>> OS X. If it isn't I'm going to get a Solaris box and move the pool  
>>> to
>>> it.
>>>
>>> Regards,
>>>
>>> Werner.
>>>
>>> On 28 Sep 2008, at 17:46, ruebezahl wrote:
>>>
>>>
>>>> hey there!
>>>>
>>>> guys: PANIC !
>>>>
>>>> It seems my internal zpool which contains my homefolder gets  
>>>> mounted
>>>> very late in the booting process, maybe not even until i trie to
>>>> login.
>>>> which results in my library/wallpaper and so on not getting loaded
>>>> properly
>>>>
>>>> when logging in as another user first, and then logging in again it
>>>> works ?
>>>>
>>>> is there a way to force osx to mount my zpool at boot time ?
>>>>
>>>> i tried via a launchd entry, but failed.
>>>>
>>>>
>>>> regards
>>>>
>>>> franz
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>
>>> --
>>> Werner Donn?  --  Re                                     http://www.pincette.biz
>>> Engelbeekstraat 8                                               http://www.re.be
>>> BE-3300 Tienen
>>> tel: (+32) 486 425803	e-mail: werner.donne at re.be
>>>
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From andy at aligature.com  Wed Oct  1 04:30:57 2008
From: andy at aligature.com (Andrew Webber)
Date: Wed, 1 Oct 2008 07:30:57 -0400
Subject: [zfs-discuss] late mount
In-Reply-To: <B3D8D44D-4860-4439-8025-E681F0A8D2B8@spamfreemail.de>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
	<48E2180C.4090102@loveturtle.net>
	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
	<B3D8D44D-4860-4439-8025-E681F0A8D2B8@spamfreemail.de>
Message-ID: <60b50dc10810010430i4733e6e4ob720667ffb5696c8@mail.gmail.com>

Did you use the boot camp utility to install Windows?  It should have done
the *right* thing with respect to repartitioning your drive.



On Wed, Oct 1, 2008 at 5:12 AM, ruebezahl <franzschmalzl at spamfreemail.de>wrote:

> I invested some time the last days to fix this issue and came to the
> conclusion that ( at least in my case it was not really zfs' fault)
> I had 4 partitions on my one and only internal harddrive ( 500gb in an
> imac)
> EFI
> HFS+
> ZFS
> NTFS
>
> Apparently Windows fucked up my partition table, which at first i did
> not notice.
> When doing a diskutil list it would crash with "Diskmanagement set-uid
> tool failure"
> Diskutility GUI would crash when erasing volumes or trying to
> partition my internal harddrive.
>
> I tried to erase everything and set everything up from scratch, worked
> fine until i installed windows again *and* synced gpt and mbr using
> rEFIt ( mayb it's rEFIt's fault ? )
> same thing again, zfs partition was not properly mounted at boot,
> diskmanagement setuid tool failure.
>
> I now ended up restoring everything again from backups, but without
> any windows partiton.
> Again, everything works like a charm now.
>
>
> So i think i ran into some partition table/poor osx management of
> partititon issues
>
>
> My thanks for your time No?l
>
> Franz
>
>
> On 30.09.2008, at 22:31, No?l Dellofano wrote:
>
> > This is strange.  A group of us at Apple keep our home directories on
> > ZFS on our laptops on the internal drive.  Are all of you guys using
> > the internal drives, or USB drives for your home directories? or some
> > other type of detachable (non internal bay) drive?  There has been
> > some issues in the past noted with external drives since a lot of them
> > have power saving features and such so the drive sometimes isn't
> > actually spun up,  or takes a bit to get spun up.
> >
> > Noel
> >
> > On Sep 30, 2008, at 5:14 AM, Dillon Kass wrote:
> >
> >> I just use the login window to add a few more seconds to the login
> >> time.
> >> Usually by the time the login window loads and i type my username and
> >> password + take a single breath my home dir has mounted and i can
> >> safely
> >> log in.
> >>
> >> I also have cron snapshot my home dir by hourly so that if i do log
> >> in
> >> too quick and lose my settings i can just log out and rollback.
> >>
> >>
> >> Werner Donn? wrote:
> >>> Hi,
> >>>
> >>> This is how it started for me too. Since then it has deteriorated
> >>> gradually up to the point, today, where after twenty reboots I still
> >>> have no online pool. I wonder if ZFS is suitable for USB-disks on
> >>> Mac
> >>> OS X. If it isn't I'm going to get a Solaris box and move the pool
> >>> to
> >>> it.
> >>>
> >>> Regards,
> >>>
> >>> Werner.
> >>>
> >>> On 28 Sep 2008, at 17:46, ruebezahl wrote:
> >>>
> >>>
> >>>> hey there!
> >>>>
> >>>> guys: PANIC !
> >>>>
> >>>> It seems my internal zpool which contains my homefolder gets
> >>>> mounted
> >>>> very late in the booting process, maybe not even until i trie to
> >>>> login.
> >>>> which results in my library/wallpaper and so on not getting loaded
> >>>> properly
> >>>>
> >>>> when logging in as another user first, and then logging in again it
> >>>> works ?
> >>>>
> >>>> is there a way to force osx to mount my zpool at boot time ?
> >>>>
> >>>> i tried via a launchd entry, but failed.
> >>>>
> >>>>
> >>>> regards
> >>>>
> >>>> franz
> >>>>
> >>>>
> >>>>
> >>>> _______________________________________________
> >>>> zfs-discuss mailing list
> >>>> zfs-discuss at lists.macosforge.org
> >>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >>>>
> >>>
> >>> --
> >>> Werner Donn?  --  Re
> http://www.pincette.biz
> >>> Engelbeekstraat 8
> http://www.re.be
> >>> BE-3300 Tienen
> >>> tel: (+32) 486 425803       e-mail: werner.donne at re.be
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> _______________________________________________
> >>> zfs-discuss mailing list
> >>> zfs-discuss at lists.macosforge.org
> >>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >>>
> >>
> >> _______________________________________________
> >> zfs-discuss mailing list
> >> zfs-discuss at lists.macosforge.org
> >> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss at lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081001/8c83777d/attachment-0001.html 

From zfs at hessmann.de  Wed Oct  1 05:46:40 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Wed, 1 Oct 2008 14:46:40 +0200
Subject: [zfs-discuss] added storage and rsync --inplace
Message-ID: <AD5E4AC4-4292-459F-9B16-56828035BD2B@hessmann.de>

Good morning everyone,


as I might have mentioned before, I added another 3-way RAID.Z to my  
Pool.
The (by far) biggest use of this Pool is as a backup storage for quite  
a few ZFS-less OS X clients, which back up their home directories with  
rsync. Each rsync session finishes with a snapshot command.

The switches I'll use with rsync are the following:

rsync -aENHAXvz --fileflags --stats --progress --inplace --delete

AFAIK, --inplace replaces a file without first creating a new file and  
replacing the old one with it after a successful transfer later. This  
seems to be important for efficient ZFS snapshot creation - right?

According to various ZFS documentation, additional storage is only  
used for new data and COW'ed old data.
I'm not quite sure whether an inplace-replacement of files gets  
written on old or new data, but it seems the new RAID.Z is not used as  
much as it could be:

bash-3.2$ zpool iostat -v
                 capacity     operations    bandwidth
pool          used  avail   read  write   read  write
-----------  -----  -----  -----  -----  -----  -----
tank         2.01T   373G     22      3   212K  28.3K
   raidz1     2.01T  32.9G     21      1   195K  6.61K
     disk7s2      -      -      2      1   170K  3.41K
     disk5s2      -      -      2      1   169K  3.41K
     disk6s2      -      -      2      1   169K  3.41K
   raidz1      1.8G   340G      1      2  17.2K  21.7K
     disk3s2      -      -      0      1  12.2K  11.0K
     disk4s2      -      -      0      1  12.3K  11.0K
     disk2s2      -      -      0      1  12.2K  11.0K
-----------  -----  -----  -----  -----  -----  -----

I've added disks 2-4 (which, incidentally, have been 8-10 at the time  
I've added them) 6 days ago - during that time, as you can see, less  
than 2GB have been used on these disks, while the available capacity  
on the first three disks got reduced from ~70GB to <33GB.

Should I be worried that I'll run out of space on the old disks?

Thank you.


Best regards,

Christian

From zfs at hessmann.de  Wed Oct  1 05:52:30 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Wed, 1 Oct 2008 14:52:30 +0200
Subject: [zfs-discuss] iTunes doesn't find specific files on ZFS
In-Reply-To: <E0B498CA-4477-4C5F-9CC0-2545D42B04C3@hexaneinc.com>
References: <E0B498CA-4477-4C5F-9CC0-2545D42B04C3@hexaneinc.com>
Message-ID: <95B6E8CC-D1E7-420D-AB68-50AA2859DCA0@hessmann.de>

On 26.09.2008, at 20:27, Matthew Rezny wrote:

> I have seen this problem myself and I believe I know the source of the
> problem. The good news is that once you have all instances fixed, it
> stays fixed. The other good news is that if you start with ZFS it
> doesn't affect you. The bad news is that almost all of us started with
> HFS+ and migrated our iTunes Library to ZFS.
[...]
> With HFS+, the file system is case preserving, but not case sensitive.
> So, if you rename a file such that only the case has changed, you can
> still open it with the old name. With ZFS, which is truly case
> sensitive, the attempt to open with the old name, differing only by
> case, fails.

Matthew,

thank you for your input. It sounds plausible to be a case sensitive  
problem, since I also started with a HFS+ library. Although I could  
have sworn I've had the problem with the same file more than once...

I'll try to keep a list of all the files and check for duplicates.


Best regards,

Christian


From franzschmalzl at spamfreemail.de  Wed Oct  1 07:38:20 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Wed, 1 Oct 2008 16:38:20 +0200
Subject: [zfs-discuss] late mount
In-Reply-To: <60b50dc10810010430i4733e6e4ob720667ffb5696c8@mail.gmail.com>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
	<48E2180C.4090102@loveturtle.net>
	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
	<B3D8D44D-4860-4439-8025-E681F0A8D2B8@spamfreemail.de>
	<60b50dc10810010430i4733e6e4ob720667ffb5696c8@mail.gmail.com>
Message-ID: <D07A5C3F-BD7E-4470-9B27-3318F72506FC@spamfreemail.de>

nope, wouldn't work because the guys at apple assume users ought to  
have only one partition on their harddrive...

but as far is i understand boot camp assistant does nothing different  
than repartition and sync gpt and mbr
which i have done manually



franz





> Did you use the boot camp utility to install Windows?  It should  
> have done the *right* thing with respect to repartitioning your drive.
>
>
>
> On Wed, Oct 1, 2008 at 5:12 AM, ruebezahl <franzschmalzl at spamfreemail.de 
> > wrote:
> I invested some time the last days to fix this issue and came to the
> conclusion that ( at least in my case it was not really zfs' fault)
> I had 4 partitions on my one and only internal harddrive ( 500gb in an
> imac)
> EFI
> HFS+
> ZFS
> NTFS
>
> Apparently Windows fucked up my partition table, which at first i did
> not notice.
> When doing a diskutil list it would crash with "Diskmanagement set-uid
> tool failure"
> Diskutility GUI would crash when erasing volumes or trying to
> partition my internal harddrive.
>
> I tried to erase everything and set everything up from scratch, worked
> fine until i installed windows again *and* synced gpt and mbr using
> rEFIt ( mayb it's rEFIt's fault ? )
> same thing again, zfs partition was not properly mounted at boot,
> diskmanagement setuid tool failure.
>
> I now ended up restoring everything again from backups, but without
> any windows partiton.
> Again, everything works like a charm now.
>
>
> So i think i ran into some partition table/poor osx management of
> partititon issues
>
>
> My thanks for your time No?l
>
> Franz
>
>
> On 30.09.2008, at 22:31, No?l Dellofano wrote:
>
> > This is strange.  A group of us at Apple keep our home directories  
> on
> > ZFS on our laptops on the internal drive.  Are all of you guys using
> > the internal drives, or USB drives for your home directories? or  
> some
> > other type of detachable (non internal bay) drive?  There has been
> > some issues in the past noted with external drives since a lot of  
> them
> > have power saving features and such so the drive sometimes isn't
> > actually spun up,  or takes a bit to get spun up.
> >
> > Noel
> >
> > On Sep 30, 2008, at 5:14 AM, Dillon Kass wrote:
> >
> >> I just use the login window to add a few more seconds to the login
> >> time.
> >> Usually by the time the login window loads and i type my username  
> and
> >> password + take a single breath my home dir has mounted and i can
> >> safely
> >> log in.
> >>
> >> I also have cron snapshot my home dir by hourly so that if i do log
> >> in
> >> too quick and lose my settings i can just log out and rollback.
> >>
> >>
> >> Werner Donn? wrote:
> >>> Hi,
> >>>
> >>> This is how it started for me too. Since then it has deteriorated
> >>> gradually up to the point, today, where after twenty reboots I  
> still
> >>> have no online pool. I wonder if ZFS is suitable for USB-disks on
> >>> Mac
> >>> OS X. If it isn't I'm going to get a Solaris box and move the pool
> >>> to
> >>> it.
> >>>
> >>> Regards,
> >>>
> >>> Werner.
> >>>
> >>> On 28 Sep 2008, at 17:46, ruebezahl wrote:
> >>>
> >>>
> >>>> hey there!
> >>>>
> >>>> guys: PANIC !
> >>>>
> >>>> It seems my internal zpool which contains my homefolder gets
> >>>> mounted
> >>>> very late in the booting process, maybe not even until i trie to
> >>>> login.
> >>>> which results in my library/wallpaper and so on not getting  
> loaded
> >>>> properly
> >>>>
> >>>> when logging in as another user first, and then logging in  
> again it
> >>>> works ?
> >>>>
> >>>> is there a way to force osx to mount my zpool at boot time ?
> >>>>
> >>>> i tried via a launchd entry, but failed.
> >>>>
> >>>>
> >>>> regards
> >>>>
> >>>> franz
> >>>>
> >>>>
> >>>>
> >>>> _______________________________________________
> >>>> zfs-discuss mailing list
> >>>> zfs-discuss at lists.macosforge.org
> >>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >>>>
> >>>
> >>> --
> >>> Werner Donn?  --  Re                                     http://www.pincette.biz
> >>> Engelbeekstraat 8                                               http://www.re.be
> >>> BE-3300 Tienen
> >>> tel: (+32) 486 425803       e-mail: werner.donne at re.be
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> _______________________________________________
> >>> zfs-discuss mailing list
> >>> zfs-discuss at lists.macosforge.org
> >>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >>>
> >>
> >> _______________________________________________
> >> zfs-discuss mailing list
> >> zfs-discuss at lists.macosforge.org
> >> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss at lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From franzschmalzl at spamfreemail.de  Wed Oct  1 07:42:14 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Wed, 1 Oct 2008 16:42:14 +0200
Subject: [zfs-discuss] weird zfs thread synchronization issue? (perhaps
	mmap is the culprit?)
In-Reply-To: <33644d3c0804241221q23ffbec3l92e5e8398fa0e92d@mail.gmail.com>
References: <33644d3c0804222055t29a6293fsa6f23fefc8dc5aff@mail.gmail.com>
	<420D0A27-82E1-494F-B990-E107B6AB3B24@martin-hauser.net>
	<33644d3c0804230909k66b85244j88d34c3143552979@mail.gmail.com>
	<33644d3c0804241221q23ffbec3l92e5e8398fa0e92d@mail.gmail.com>
Message-ID: <BE8D208C-AD96-4001-99B4-B321586292C3@spamfreemail.de>

very very late follow up :)

i lately used musicbrainz picard to tag some mp3 and m4a files, some  
of them got fucked up which only happend on my zfs volumes

possibly related ?


franz



From dirkschelfhout at mac.com  Wed Oct  1 08:01:12 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Wed, 01 Oct 2008 17:01:12 +0200
Subject: [zfs-discuss] panic doing rsync
Message-ID: <7CFFEFCD-3229-430E-A2FA-1048DCA981CE@mac.com>

2 times in a row.
I'll do a scrub now.
hmmmm. It seems I ran out off disk space on my zfs raidz.
But that shouldn't give cause for a panic ?
Dirk

Wed Oct  1 16:57:22 2008
panic(cpu 3 caller 0x00A12CD9): "[ZFS]: assertion failed in /Volumes/ 
pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/zfs_znode.c  
line 1004: zp->z_dbuf_held == 0"@/Volumes/pixie_dust/home/ndellofano/ 
zfs-work/zfs-119/zfs_kext/zfs/zfs_znode.c:1004
Backtrace (CPU 3), Frame : Return Address (4 potential args on stack)
0x758e7578 : 0x12b0fa (0x459234 0x758e75ac 0x133243 0x0)
0x758e75c8 : 0xa12cd9 (0xa6c0ec 0xa6bc6c 0x3ec 0xa6bd3c)
0x758e7608 : 0x9bd15b (0x8900edc0 0x1 0x758e7658 0x141de3)
0x758e7648 : 0x1f3f6a (0x758e7664 0x206 0x758e766c 0x4f6f40)
0x758e7688 : 0x1db5ea (0x9fc00a0 0xb11f364 0x0 0x0)
0x758e76d8 : 0x1db806 (0x9fc00a0 0x1 0x758e7708 0x19d6a0)
0x758e7728 : 0x1dd3b8 (0x0 0x4f63c0 0x4049e261 0x0)
0x758e7798 : 0x31ea94 (0x0 0x34 0x758e77ec 0x8c05a2c)
0x758e7838 : 0x327009 (0x9538804 0x9eb4bd0 0x758e7f08 0x758e7968)
0x758e7998 : 0x1f2c9c (0x758e79b8 0x2 0x758e79e8 0x44fe1e)
0x758e79e8 : 0x1d47aa (0x9eb4bd0 0x758e7df4 0x758e7f08 0xb11f364)
0x758e7a78 : 0x1d552f (0x758e7ddc 0x100 0x758e7dfc 0x0)
0x758e7b38 : 0x1e413c (0x758e7ddc 0x0 0x0 0x0)
0x758e7d88 : 0x1e476b (0xbfffd7e0 0x0 0x0 0x0)
0x758e7f48 : 0x1e4804 (0xbfffd7e0 0x0 0x0 0x0)
0x758e7f78 : 0x3ddd6e (0x91b02b0 0xb11f260 0xb11f2a4 0x0)
	Backtrace continues...
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0x9b5000->0xa80fff

BSD process name corresponding to current thread: rsync

Mac OS version:
9F33

Kernel version:
Darwin Kernel Version 9.5.0: Wed Sep  3 11:29:43 PDT 2008;  
root:xnu-1228.7.58~1/RELEASE_I386
System model name: MacPro1,1 (Mac-F4208DC8)


From dirkschelfhout at mac.com  Wed Oct  1 08:09:01 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Wed, 01 Oct 2008 17:09:01 +0200
Subject: [zfs-discuss] panic doing rsync
In-Reply-To: <7CFFEFCD-3229-430E-A2FA-1048DCA981CE@mac.com>
References: <7CFFEFCD-3229-430E-A2FA-1048DCA981CE@mac.com>
Message-ID: <72951523-C4FC-40D1-B3F1-2F06F9077186@mac.com>

unmounting the raidz just did the same :
Wed Oct  1 17:07:07 2008
panic(cpu 1 caller 0x00A12CD9): "[ZFS]: assertion failed in /Volumes/ 
pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/zfs_znode.c  
line 1004: zp->z_dbuf_held == 0"@/Volumes/pixie_dust/home/ndellofano/ 
zfs-work/zfs-119/zfs_kext/zfs/zfs_znode.c:1004
Backtrace (CPU 1), Frame : Return Address (4 potential args on stack)
0x8903fb48 : 0x12b0fa (0x459234 0x8903fb7c 0x133243 0x0)
0x8903fb98 : 0xa12cd9 (0xa6c0ec 0xa6bc6c 0x3ec 0xa6bd3c)
0x8903fbd8 : 0x9bd15b (0x8c47ab18 0x1 0x0 0x0)
0x8903fc18 : 0x1f3f6a (0x8903fc34 0x202 0x8903fc3c 0x4f6f40)
0x8903fc58 : 0x1db5ea (0x9d154d0 0xb009ec4 0x0 0x0)
0x8903fca8 : 0x1db806 (0x9d154d0 0x1 0x8903fcd8 0x1d3cdb)
0x8903fcf8 : 0x1de41d (0x0 0x1 0x0 0xb009ec4)
0x8903fd58 : 0x1dfd65 (0x9375000 0x0 0x19 0x1f5ead)
0x8903fda8 : 0x1e0067 (0x9375000 0x0 0x1 0xb009ec4)
0x8903fdd8 : 0x1e020c (0x9375000 0x0 0xb009ec4 0x0)
0x8903ff78 : 0x3ddd6e (0x9d787b0 0xb009dc0 0xb009e04 0x0)
0x8903ffc8 : 0x19f3b3 (0xa273d40 0x0 0x1a20b5 0xa273d40)
No mapping exists for frame pointer
Backtrace terminated-invalid frame pointer 0xbfffef58
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0x9b5000->0xa80fff

BSD process name corresponding to current thread: zfs

Mac OS version:
9F33

Kernel version:
Darwin Kernel Version 9.5.0: Wed Sep  3 11:29:43 PDT 2008;  
root:xnu-1228.7.58~1/RELEASE_I386
System model name: MacPro1,1 (Mac-F4208DC8)

On 01 Oct 2008, at 17:01, Dirk Schelfhout wrote:

> 2 times in a row.
> I'll do a scrub now.
> hmmmm. It seems I ran out off disk space on my zfs raidz.
> But that shouldn't give cause for a panic ?
> Dirk
>
> Wed Oct  1 16:57:22 2008
> panic(cpu 3 caller 0x00A12CD9): "[ZFS]: assertion failed in /Volumes/
> pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/zfs_znode.c
> line 1004: zp->z_dbuf_held == 0"@/Volumes/pixie_dust/home/ndellofano/
> zfs-work/zfs-119/zfs_kext/zfs/zfs_znode.c:1004
> Backtrace (CPU 3), Frame : Return Address (4 potential args on stack)
> 0x758e7578 : 0x12b0fa (0x459234 0x758e75ac 0x133243 0x0)
> 0x758e75c8 : 0xa12cd9 (0xa6c0ec 0xa6bc6c 0x3ec 0xa6bd3c)
> 0x758e7608 : 0x9bd15b (0x8900edc0 0x1 0x758e7658 0x141de3)
> 0x758e7648 : 0x1f3f6a (0x758e7664 0x206 0x758e766c 0x4f6f40)
> 0x758e7688 : 0x1db5ea (0x9fc00a0 0xb11f364 0x0 0x0)
> 0x758e76d8 : 0x1db806 (0x9fc00a0 0x1 0x758e7708 0x19d6a0)
> 0x758e7728 : 0x1dd3b8 (0x0 0x4f63c0 0x4049e261 0x0)
> 0x758e7798 : 0x31ea94 (0x0 0x34 0x758e77ec 0x8c05a2c)
> 0x758e7838 : 0x327009 (0x9538804 0x9eb4bd0 0x758e7f08 0x758e7968)
> 0x758e7998 : 0x1f2c9c (0x758e79b8 0x2 0x758e79e8 0x44fe1e)
> 0x758e79e8 : 0x1d47aa (0x9eb4bd0 0x758e7df4 0x758e7f08 0xb11f364)
> 0x758e7a78 : 0x1d552f (0x758e7ddc 0x100 0x758e7dfc 0x0)
> 0x758e7b38 : 0x1e413c (0x758e7ddc 0x0 0x0 0x0)
> 0x758e7d88 : 0x1e476b (0xbfffd7e0 0x0 0x0 0x0)
> 0x758e7f48 : 0x1e4804 (0xbfffd7e0 0x0 0x0 0x0)
> 0x758e7f78 : 0x3ddd6e (0x91b02b0 0xb11f260 0xb11f2a4 0x0)
> 	Backtrace continues...
>       Kernel loadable modules in backtrace (with dependencies):
>          com.apple.filesystems.zfs(8.0)@0x9b5000->0xa80fff
>
> BSD process name corresponding to current thread: rsync
>
> Mac OS version:
> 9F33
>
> Kernel version:
> Darwin Kernel Version 9.5.0: Wed Sep  3 11:29:43 PDT 2008;
> root:xnu-1228.7.58~1/RELEASE_I386
> System model name: MacPro1,1 (Mac-F4208DC8)
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dirkschelfhout at mac.com  Wed Oct  1 08:14:48 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Wed, 01 Oct 2008 17:14:48 +0200
Subject: [zfs-discuss] can't delete from full volume
In-Reply-To: <2E79EDB1-9D0E-4B11-BC3B-99A036348702@apple.com>
References: <67A7235B-A467-4368-9EC5-DE2944AF5086@berczi.be>
	<2E79EDB1-9D0E-4B11-BC3B-99A036348702@apple.com>
Message-ID: <1415E29F-9159-4FB9-903E-BFDE1C6FDEC4@mac.com>


On 17 Mar 2008, at 20:12, Barry Lustig wrote:

>
> On Mar 16, 2008, at 7:13 AM, B?rczi G?bor wrote:
>> NAME    USED  AVAIL  REFER  MOUNTPOINT
>> store  80.2G      0  80.2G  /Volumes/store
>>
>> Filesystem 1024-blocks     Used Available Capacity  Mounted on
>> store         84122986 84122986         0   100%    /Volumes/store
>>
>> rm: iphone_sdk.dmg: No space left on device
>
> This is a known issue.  To workaround:  "cp /dev/null file" to zero  
> out the file and then remove it.
This doesn't work for me.
Any other methods ?
>
>
>
> barry
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss


From hanche at math.ntnu.no  Wed Oct  1 08:31:12 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Wed, 01 Oct 2008 17:31:12 +0200 (CEST)
Subject: [zfs-discuss] can't delete from full volume
In-Reply-To: <1415E29F-9159-4FB9-903E-BFDE1C6FDEC4@mac.com>
References: <67A7235B-A467-4368-9EC5-DE2944AF5086@berczi.be>
	<2E79EDB1-9D0E-4B11-BC3B-99A036348702@apple.com>
	<1415E29F-9159-4FB9-903E-BFDE1C6FDEC4@mac.com>
Message-ID: <20081001.173112.630216169365598116.hanche@math.ntnu.no>

+ Dirk Schelfhout <dirkschelfhout at mac.com>:

> >> rm: iphone_sdk.dmg: No space left on device
> >
> > This is a known issue.  To workaround:  "cp /dev/null file" to zero  
> > out the file and then remove it.
> This doesn't work for me.

What happens when you try? (If the same file exists in a snapshot then
this method may not work, as it doesn't free up any space.)

> Any other methods ?

You might consider destroying an old snapshot if you have any.

- Harald

From dirkschelfhout at mac.com  Wed Oct  1 09:06:27 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Wed, 01 Oct 2008 18:06:27 +0200
Subject: [zfs-discuss] can't delete from full volume
In-Reply-To: <20081001.173112.630216169365598116.hanche@math.ntnu.no>
References: <67A7235B-A467-4368-9EC5-DE2944AF5086@berczi.be>
	<2E79EDB1-9D0E-4B11-BC3B-99A036348702@apple.com>
	<1415E29F-9159-4FB9-903E-BFDE1C6FDEC4@mac.com>
	<20081001.173112.630216169365598116.hanche@math.ntnu.no>
Message-ID: <DD504B84-B929-4623-9716-20B95B4EF0D5@mac.com>


On 01 Oct 2008, at 17:31, Harald Hanche-Olsen wrote:

> + Dirk Schelfhout <dirkschelfhout at mac.com>:
>
>>>> rm: iphone_sdk.dmg: No space left on device
>>>
>>> This is a known issue.  To workaround:  "cp /dev/null file" to zero
>>> out the file and then remove it.
>> This doesn't work for me.
>
> What happens when you try? (If the same file exists in a snapshot then
> this method may not work, as it doesn't free up any space.)
>
>> Any other methods ?
>
> You might consider destroying an old snapshot if you have any.
This gave me some free space.
I tried a rollback before, to just where I took a snapshot before  
rsyncing, but that gave me a panic as well.
Tx,
>
>
> - Harald


From koonce at universe42.com  Wed Oct  1 10:13:05 2008
From: koonce at universe42.com (Brett Koonce)
Date: Wed, 1 Oct 2008 12:13:05 -0500
Subject: [zfs-discuss] read errors
Message-ID: <CF18DCA9-C82A-400B-A013-FFDCF0A5FD09@universe42.com>

I built a little raidz array with 4 500GB hard drives in a Sans  
Digital TRU-4 usb box.  I copied over some data (~700GB), ran a zfs  
scrub and got this:

$ zpool status
   pool: clover
  state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
	attempt was made to correct the error.  Applications are unaffected.
action: Determine if the device needs to be replaced, and clear the  
errors
	using 'zpool clear' or replace the device with 'zpool replace'.
    see: http://www.sun.com/msg/ZFS-8000-9P
  scrub: scrub in progress, 28.97% done, 12h15m to go
config:

	NAME         STATE     READ WRITE CKSUM
	clover       ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk1s2  ONLINE       0     0     0
	    disk2s2  ONLINE       0     0     0
	    disk3s2  ONLINE       0     0     0
	    disk4s2  ONLINE       9     0     0

I decided to clear the errors and went along my merry way.  So a  
couple days ago I decided to scrub things again, and got the exact  
same results (9 read errors on disk4 near the 30% mark).

So, my questions:

I was under the impression that zfs would attempt to repair the  
errors.  Or rather, is the fact that I got the same numbers twice a  
weird coincidence?  (i.e. are they the same 9 errors?)

My understanding (correct me if I'm wrong) is that zfs will only  
attempt to fix things on a COW operation.  In that case, if I was to  
find the file in question, duplicate/replace it from my other backups,  
and delete the original, would this theoretically get around the bad  
sectors?  Is there a way to get that information out of zfs?  The  
system.log only mentions the errors happened.

I guess I should just pony up the fifty bucks for a new drive.  But  
I'd like to know how this works.  Thanks in advance,
-Brett

From aorchid at mac.com  Wed Oct  1 10:17:12 2008
From: aorchid at mac.com (Aric Gregson)
Date: Wed, 01 Oct 2008 10:17:12 -0700
Subject: [zfs-discuss] late mount
In-Reply-To: <48E2D622.4010504@loveturtle.net>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
	<48E2180C.4090102@loveturtle.net>
	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
	<46CAEFC3-A3EA-4A7A-B696-3084D92F93FC@durin42.com>
	<20080930150928.f989f8f6.aorchid@mac.com>
	<48E2A79D.9020100@loveturtle.net>
	<20080930155309.71cbf62e.aorchid@mac.com>
	<48E2D622.4010504@loveturtle.net>
Message-ID: <48E3B098.50201@mac.com>

Dillon Kass wrote:
> Sure, You should be able to just look at the disks in zpool status
> They will either say like disk0s2 or disk0
>    

OK. Here it is. I suppose this is incorrect?

   pool: peipool
  state: ONLINE
  scrub: resilver completed with 0 errors on Thu Sep 25 20:09:07 2008
config:

     NAME        STATE     READ WRITE CKSUM
     peipool     ONLINE       0     0     0
       mirror    ONLINE       0     0     0
         disk1   ONLINE       0     0     0
         disk2   ONLINE       0     0     0

errors: No known data errors

From zorg at sogeeky.net  Wed Oct  1 10:30:20 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Wed, 1 Oct 2008 10:30:20 -0700
Subject: [zfs-discuss] read errors
In-Reply-To: <CF18DCA9-C82A-400B-A013-FFDCF0A5FD09@universe42.com>
References: <CF18DCA9-C82A-400B-A013-FFDCF0A5FD09@universe42.com>
Message-ID: <66D94218-9595-4CEF-B82B-800EC163592B@sogeeky.net>

Your data is intact. The fact that it says applications are unaffected  
tells you that. It is able to work around the bad data and reconstruct  
from the parity info inherent on raidz. But it is telling you that the  
drive with the errors on it has some bad sectors. Most modern drives  
keep spare sectors in reserve and automatically use them when  
necessary, so this means you've probably exhausted those reserves.  
Pick up a new drive and use the zfs commands to replace the bad drive  
with the new one. Then pull the bad drive. The hard part is figuring  
out which drive number is which physical drive. I usually just shut  
down all apps and turn them off one by one until I see the one I want  
disappear. :)

I had something similar when the powersupply of an external drive was  
going bad causing the disk to periodically spin down. The drive itself  
was fine. But I am guessing that's not your problem since ther errors  
seem to happen in the same spot.

On Oct 1, 2008, at 10:13 AM, Brett Koonce <koonce at universe42.com> wrote:

> I built a little raidz array with 4 500GB hard drives in a Sans
> Digital TRU-4 usb box.  I copied over some data (~700GB), ran a zfs
> scrub and got this:
>
> $ zpool status
>   pool: clover
>  state: ONLINE
> status: One or more devices has experienced an unrecoverable error.   
> An
>    attempt was made to correct the error.  Applications are  
> unaffected.
> action: Determine if the device needs to be replaced, and clear the
> errors
>    using 'zpool clear' or replace the device with 'zpool replace'.
>    see: http://www.sun.com/msg/ZFS-8000-9P
>  scrub: scrub in progress, 28.97% done, 12h15m to go
> config:
>
>    NAME         STATE     READ WRITE CKSUM
>    clover       ONLINE       0     0     0
>      raidz1     ONLINE       0     0     0
>        disk1s2  ONLINE       0     0     0
>        disk2s2  ONLINE       0     0     0
>        disk3s2  ONLINE       0     0     0
>        disk4s2  ONLINE       9     0     0
>
> I decided to clear the errors and went along my merry way.  So a
> couple days ago I decided to scrub things again, and got the exact
> same results (9 read errors on disk4 near the 30% mark).
>
> So, my questions:
>
> I was under the impression that zfs would attempt to repair the
> errors.  Or rather, is the fact that I got the same numbers twice a
> weird coincidence?  (i.e. are they the same 9 errors?)
>
> My understanding (correct me if I'm wrong) is that zfs will only
> attempt to fix things on a COW operation.  In that case, if I was to
> find the file in question, duplicate/replace it from my other backups,
> and delete the original, would this theoretically get around the bad
> sectors?  Is there a way to get that information out of zfs?  The
> system.log only mentions the errors happened.
>
> I guess I should just pony up the fifty bucks for a new drive.  But
> I'd like to know how this works.  Thanks in advance,
> -Brett
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From zorg at sogeeky.net  Wed Oct  1 10:32:27 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Wed, 1 Oct 2008 10:32:27 -0700
Subject: [zfs-discuss] late mount
In-Reply-To: <48E3B098.50201@mac.com>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
	<48E2180C.4090102@loveturtle.net>
	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
	<46CAEFC3-A3EA-4A7A-B696-3084D92F93FC@durin42.com>
	<20080930150928.f989f8f6.aorchid@mac.com>
	<48E2A79D.9020100@loveturtle.net>
	<20080930155309.71cbf62e.aorchid@mac.com>
	<48E2D622.4010504@loveturtle.net> <48E3B098.50201@mac.com>
Message-ID: <459D3ED1-5BD5-41AE-A445-492C2D482E7E@sogeeky.net>

Yep. That's incorrect. Now the good news: since that is a mirror, you  
can remove one disk at a time, fix it, add it back and let of  
resilver. Then do the next one.

On Oct 1, 2008, at 10:17 AM, Aric Gregson <aorchid at mac.com> wrote:

> Dillon Kass wrote:
>> Sure, You should be able to just look at the disks in zpool status
>> They will either say like disk0s2 or disk0
>>
>
> OK. Here it is. I suppose this is incorrect?
>
>   pool: peipool
>  state: ONLINE
>  scrub: resilver completed with 0 errors on Thu Sep 25 20:09:07 2008
> config:
>
>     NAME        STATE     READ WRITE CKSUM
>     peipool     ONLINE       0     0     0
>       mirror    ONLINE       0     0     0
>         disk1   ONLINE       0     0     0
>         disk2   ONLINE       0     0     0
>
> errors: No known data errors
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From lists at loveturtle.net  Wed Oct  1 10:35:31 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Wed, 01 Oct 2008 13:35:31 -0400
Subject: [zfs-discuss] late mount
In-Reply-To: <48E3B098.50201@mac.com>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
	<48E2180C.4090102@loveturtle.net>
	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
	<46CAEFC3-A3EA-4A7A-B696-3084D92F93FC@durin42.com>
	<20080930150928.f989f8f6.aorchid@mac.com>
	<48E2A79D.9020100@loveturtle.net>
	<20080930155309.71cbf62e.aorchid@mac.com>
	<48E2D622.4010504@loveturtle.net> <48E3B098.50201@mac.com>
Message-ID: <48E3B4E3.7050708@loveturtle.net>

Right, Using a whole disk for zfs on my desktop looks like this

   pool: z
  state: ONLINE
  scrub: none requested
config:

         NAME         STATE     READ WRITE CKSUM
         z            ONLINE       0     0     0
           mirror     ONLINE       0     0     0
             disk3s2  ONLINE       0     0     0
             disk2s2  ONLINE       0     0     0

errors: No known data errors
(and diskutil list output)
/dev/disk2
    #:                       TYPE NAME                    SIZE       
IDENTIFIER
    0:      GUID_partition_scheme                        *298.1 Gi   disk2
    1:                        EFI                         200.0 Mi   disk2s1
    2:                        ZFS z                       297.8 Gi   disk2s2

The problem now is that you're kind of in a pickle, you could detach one 
of your disks and repartition it with diskutil and try to add the s2 
slice but the problem is that it's going to be a little smaller than 
your other (whole) disk and zfs will not allow you to add a smaller 
device to the mirror. You could try but I'm pretty certian it wont work.

So I suppose your only other option would be to backup your filesystems 
with zfs send (too bad we don't have zfs send -R in the osx version of 
zfs) and then create a new pool and restore it.

If you're low on space and can't hold zfs dumps of all your filesystems 
you could just detach a disk from the mirror and create a new pool with 
it and then zfs send | zfs recv it over to the new pool. although I 
think I kind of remember there being some bug with zfs on osx where you 
couldn't pipe zfs send to zfs recv for some reason even though zfs send 
 > file zfs recv < file worked just fine....

Well either way, I'm pretty sure that is why your pool isn't importing 
on boot and you could at least verify it by detaching a disk and 
creating a second pool with the s2 slice after reruning diskutil and 
rebooting and see if it's imported on boot. Then go from there..

Aric Gregson wrote:
> OK. Here it is. I suppose this is incorrect?
>
>   pool: peipool
>  state: ONLINE
>  scrub: resilver completed with 0 errors on Thu Sep 25 20:09:07 2008
> config:
>
>     NAME        STATE     READ WRITE CKSUM
>     peipool     ONLINE       0     0     0
>       mirror    ONLINE       0     0     0
>         disk1   ONLINE       0     0     0
>         disk2   ONLINE       0     0     0
>
> errors: No known data errors


From William.Winnett at Sun.COM  Wed Oct  1 10:42:52 2008
From: William.Winnett at Sun.COM (Bill Winnett)
Date: Wed, 01 Oct 2008 13:42:52 -0400
Subject: [zfs-discuss] Another Panic to report
Message-ID: <08BB7FD7-3EFE-4262-AEDD-45EDB9474586@sun.com>


Hi,

Just had another panic, not really doing anything.  Issued a zpool  
import -f <filesystem>, received panic below.  I've done this maybe a  
hundred times before with the same set of disks and filesystem.
No problems before, why it happens today, beats me.

Leopard 10.5.4, zfs 119

Wed Oct  1 13:37:15 2008
panic(cpu 0 caller 0x00C71BD7): "[ZFS]: assertion failed in /Volumes/ 
pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/dnode_sync.c  
line 397: pass < 100"@/Volumes/pixie_dust/home/ndellofano/zfs-work/ 
zfs-119/zfs_kext/zfs/dnode_sync.c:397
Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x66cc3a68 : 0x12b0fa (0x4592a4 0x66cc3a9c 0x133243 0x0)
0x66cc3ab8 : 0xc71bd7 (0xcde4f4 0xcddd40 0x18d 0xcde4e8)
0x66cc3bd8 : 0xc6a65d (0x79e9b90 0x0 0x1 0x0)
0x66cc3c18 : 0xc6a866 (0x66cc3c58 0x0 0x66cc3c78 0xc65098)
0x66cc3c78 : 0xca9cda (0x0 0xace8c00 0x66cc3cb8 0x6a78420)
0x66cc3c98 : 0xc8cc24 (0x7e08a00 0xcf09c0 0xcf09c0 0x6a78420)
0x66cc3cb8 : 0xc91c74 (0x6a78420 0xcf445c 0x48e3b30c 0x0)
0x66cc3cf8 : 0xca739b (0x6a78f60 0x23ac6252 0x99dd400 0x0)
0x66cc3d38 : 0xca9a23 (0x99dd000 0x0 0x0 0x0)
0x66cc3d78 : 0x20300f (0x1f000000 0xcf1c5a06 0x99dd000 0x3)
0x66cc3db8 : 0x1f63a2 (0x66cc3de8 0x246 0x66cc3e18 0x1da567)
0x66cc3e18 : 0x1ec67c (0x77a7560 0xcf1c5a06 0x99dd000 0x3)
0x66cc3e78 : 0x3661ff (0x71b0a50 0xcf1c5a06 0x99dd000 0x66cc3f50)
0x66cc3e98 : 0x38cb8f (0x71b0a50 0xcf1c5a06 0x99dd000 0x66cc3f50)
0x66cc3f78 : 0x3ddde2 (0x71bb4d0 0x7b395a0 0x7b395e4 0x0)
0x66cc3fc8 : 0x19f2c3 (0x7446dc0 0x0 0x1a20b5 0x7446dc0)
	Backtrace continues...
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0xc46000->0xd11fff

BSD process name corresponding to current thread: zpool

Mac OS version:
9E17

Kernel version:
Darwin Kernel Version 9.4.0: Mon Jun  9 19:30:53 PDT 2008;  
root:xnu-1228.5.20~1/RELEASE_I386
System model name: iMac7,1 (Mac-F4238CC8)

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081001/7d824123/attachment-0001.html 

From ndellofano at apple.com  Wed Oct  1 14:11:02 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed, 1 Oct 2008 14:11:02 -0700
Subject: [zfs-discuss] late mount
In-Reply-To: <20080930155309.71cbf62e.aorchid@mac.com>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
	<48E2180C.4090102@loveturtle.net>
	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
	<46CAEFC3-A3EA-4A7A-B696-3084D92F93FC@durin42.com>
	<20080930150928.f989f8f6.aorchid@mac.com>
	<48E2A79D.9020100@loveturtle.net>
	<20080930155309.71cbf62e.aorchid@mac.com>
Message-ID: <5F266F83-2A2D-48C8-B406-8CE684DEA580@apple.com>

try doing 'zpool status', if you put in a whole drive, you'll get  
status on the whole drive instead of the 's2' slice which is the  
reason you're pool isn't importing on boot.
Also, everyone can double check they partitioned their drive properly  
by doing a 'diskutil list'.

As Dillon mentioned, the only two reasons (at least I've ever seen)  
for a pool to not import on it's own is if it wasn't formated for ZFS  
first with diskutil, or if you passed ZFS the whole disk instead of  
the s2 slice.

the slowness in bringing up, especially for Dillon is likely due to  
the fact he has two drives.  The problem is diskutil is racing  
launchd, and as you've all seemed to witness, well, launchd usually  
wins.  So diskutil can't bring up the pool until all the devices are  
found and it has to go through on boot and sniff every single device  
it can find, regardless of what it is.  We're working on fixing this  
issue currently with making ZFS a first class IOkit citizen.  So hang  
tight.

I currently have my internal drive partitioned, small part for HFS+  
where the os sits, and the rest for ZFS which I use for all my data  
and my home directory.


Noel

On Sep 30, 2008, at 3:53 PM, Aric Gregson wrote:

> On Tue, 30 Sep 2008 18:26:37 -0400
> Dillon Kass <lists at loveturtle.net> wrote:
>
>> From what I've seen on this list the "not importing on boot" is
>> generally because diskutil wasn't used first to prepare the disk
>> before adding it to your zpool. Even if you did use diskutil to
>> prepare the disk if you inserted the entire disk and not the s2 slice
>> it would probably act this way as well.
>
> That is what I keep reading and hearing, but I created them only a
> couple of months ago following the recipe on the Mac ZFS site, so I
> suspect that I created them properly. Although, I may have inserted  
> the
> entire disk, as this is what I do in solaris. Unfortunately, there is
> no real way to figure that out now, or is there?
>
>> note that you can also log in as the user >console from loginwindow
>> and it will drop you to the command line. This is a little easier
>> than creating a second user.
>
> Thanks, I'll try that in the future.
>
> -- 
> Aric Gregson
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Wed Oct  1 14:19:23 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed, 1 Oct 2008 14:19:23 -0700
Subject: [zfs-discuss] late mount
In-Reply-To: <48E3B4E3.7050708@loveturtle.net>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
	<48E2180C.4090102@loveturtle.net>
	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
	<46CAEFC3-A3EA-4A7A-B696-3084D92F93FC@durin42.com>
	<20080930150928.f989f8f6.aorchid@mac.com>
	<48E2A79D.9020100@loveturtle.net>
	<20080930155309.71cbf62e.aorchid@mac.com>
	<48E2D622.4010504@loveturtle.net> <48E3B098.50201@mac.com>
	<48E3B4E3.7050708@loveturtle.net>
Message-ID: <F58994C5-1066-4FCB-9A26-A943CA336D58@apple.com>

So the detach, reattach should work since actually what ZFS is doing  
now is seeing the 'disk0' appending 's2' on for it's usage (whole disk  
support is disabled right now, hence why you have to partition  
yourself).  However since we still store the name you gave us "disk0"  
then diskutil doesn't know that drive which it sees as disk0s2   
'matches' the one in your pool.  So stuff gets all screwy and long  
story short, you're pool won't get imported.

give the detatch/reattch a try though since I believe it should work.  
(make sure your drives are formatted for ZFS of course).

Noel

On Oct 1, 2008, at 10:35 AM, Dillon Kass wrote:

> Right, Using a whole disk for zfs on my desktop looks like this
>
>   pool: z
>  state: ONLINE
>  scrub: none requested
> config:
>
>         NAME         STATE     READ WRITE CKSUM
>         z            ONLINE       0     0     0
>           mirror     ONLINE       0     0     0
>             disk3s2  ONLINE       0     0     0
>             disk2s2  ONLINE       0     0     0
>
> errors: No known data errors
> (and diskutil list output)
> /dev/disk2
>    #:                       TYPE NAME                    SIZE
> IDENTIFIER
>    0:      GUID_partition_scheme                        *298.1 Gi    
> disk2
>    1:                        EFI                         200.0 Mi    
> disk2s1
>    2:                        ZFS z                       297.8 Gi    
> disk2s2
>
> The problem now is that you're kind of in a pickle, you could detach  
> one
> of your disks and repartition it with diskutil and try to add the s2
> slice but the problem is that it's going to be a little smaller than
> your other (whole) disk and zfs will not allow you to add a smaller
> device to the mirror. You could try but I'm pretty certian it wont  
> work.
>
> So I suppose your only other option would be to backup your  
> filesystems
> with zfs send (too bad we don't have zfs send -R in the osx version of
> zfs) and then create a new pool and restore it.
>
> If you're low on space and can't hold zfs dumps of all your  
> filesystems
> you could just detach a disk from the mirror and create a new pool  
> with
> it and then zfs send | zfs recv it over to the new pool. although I
> think I kind of remember there being some bug with zfs on osx where  
> you
> couldn't pipe zfs send to zfs recv for some reason even though zfs  
> send
>> file zfs recv < file worked just fine....
>
> Well either way, I'm pretty sure that is why your pool isn't importing
> on boot and you could at least verify it by detaching a disk and
> creating a second pool with the s2 slice after reruning diskutil and
> rebooting and see if it's imported on boot. Then go from there..
>
> Aric Gregson wrote:
>> OK. Here it is. I suppose this is incorrect?
>>
>>  pool: peipool
>> state: ONLINE
>> scrub: resilver completed with 0 errors on Thu Sep 25 20:09:07 2008
>> config:
>>
>>    NAME        STATE     READ WRITE CKSUM
>>    peipool     ONLINE       0     0     0
>>      mirror    ONLINE       0     0     0
>>        disk1   ONLINE       0     0     0
>>        disk2   ONLINE       0     0     0
>>
>> errors: No known data errors
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From franzschmalzl at spamfreemail.de  Thu Oct  2 07:30:22 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Thu, 2 Oct 2008 16:30:22 +0200
Subject: [zfs-discuss] Leopard Server
Message-ID: <C10B6941-1D5E-43B3-9356-140000B34101@spamfreemail.de>

Hey there!

I'm planning to get a macmini and set it up with leopard server to add  
some more geekness to this room.
Question: do the zfs bits work with leo server ?

thanks

franz

  

From tangles at me.com  Thu Oct  2 16:59:43 2008
From: tangles at me.com (Raoul Callaghan)
Date: Fri, 03 Oct 2008 09:59:43 +1000
Subject: [zfs-discuss] Leopard Server
In-Reply-To: <C10B6941-1D5E-43B3-9356-140000B34101@spamfreemail.de>
References: <C10B6941-1D5E-43B3-9356-140000B34101@spamfreemail.de>
Message-ID: <6AAF7EDF-9ABF-42DB-A298-336BC4583079@me.com>

Works here with leo server.
A PPC Digital Audio mobo that's residing inside an ANS500 with an 8 x  
250GB raidz setup.

Raoul Callaghan
I.T. Manager
Australian Mathematical Sciences Institute
111 Barry Street
The University of Melbourne
Victoria 3010 Australia
p: 03 8344 1783
f:  03 9349 4106
e: raoul at amsi.org.au

"... The only problem with Microsoft is they just have no taste, they  
have absolutely no taste and what that means is, and I don't mean that  
in a small way, I mean that in a big way.
In the sense that: they, they don't think of original ideas, and they  
don't bring much culture into their product.
And you say why is that important? well proportionally spaced fonts  
come from typesetting and beautiful books, that's where one gets the  
idea.  If it weren't for the mac they would never have that in their  
products.
And, so I guess I am saddened, not by Microsoft's success, I have no  
problem with their success. They've earned their success; for the most  
part.
I have a problem with the fact that they make just really third grade  
products ..."

Steve Jobs, circa 1980's
http://www.youtube.com/watch?v=oBISzVRmYIM

On 03/10/2008, at 12:30 AM, ruebezahl wrote:

> Hey there!
>
> I'm planning to get a macmini and set it up with leopard server to add
> some more geekness to this room.
> Question: do the zfs bits work with leo server ?
>
> thanks
>
> franz
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081003/beb11704/attachment.html 

From ndellofano at apple.com  Fri Oct  3 19:48:33 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 3 Oct 2008 19:48:33 -0700
Subject: [zfs-discuss] Leopard Server
In-Reply-To: <C10B6941-1D5E-43B3-9356-140000B34101@spamfreemail.de>
References: <C10B6941-1D5E-43B3-9356-140000B34101@spamfreemail.de>
Message-ID: <85178754-78F6-4657-9186-DF4990B00CE3@apple.com>

yes, the bits will work fine on leo server.  One bug I know of is that  
ZFS filesystms aren't showing up in the Server admin share  panel, but  
other then that you should be good...

Noel

On Oct 2, 2008, at 7:30 AM, ruebezahl wrote:

> Hey there!
>
> I'm planning to get a macmini and set it up with leopard server to add
> some more geekness to this room.
> Question: do the zfs bits work with leo server ?
>
> thanks
>
> franz
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Fri Oct  3 20:01:27 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 3 Oct 2008 20:01:27 -0700
Subject: [zfs-discuss] Initialization of a pool during boot
In-Reply-To: <62451018-5A7C-42E5-9B61-772382FBC5C3@re.be>
References: <52204649-1CCD-4FD5-8D5C-863DD09032BE@re.be>
	<C681E1A8-F432-47D8-8370-EC5F95FADF56@apple.com>
	<62451018-5A7C-42E5-9B61-772382FBC5C3@re.be>
Message-ID: <BE5DAE74-0270-4D73-8258-C5F2625E13DB@apple.com>

whats your zpool status?  Did you use slice names when you added your  
disks to your pool?  (disk0s2, disk1s2, etc).
I'm wondering if you have partial disk names and diskutil doesn't  
realize until it's too late that the disks are ZFS and hence you're  
racing hopeing they get discovered fast enough they all come up.  Sigh
this will be fixed when we get citizenship in IOkit....

Noel

On Sep 16, 2008, at 12:07 AM, Werner Donn? wrote:

> Hi No?l,
>
> I did indeed. This is the output of diskutil:
>
> werner at re-mac-4> diskutil list
> /dev/disk0
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:      GUID_partition_scheme                        *74.5 Gi     
> disk0
>   1:                        EFI                         200.0 Mi    
> disk0s1
>   2:                  Apple_HFS Macintosh HD            74.1 Gi     
> disk0s2
> /dev/disk1
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:      GUID_partition_scheme                        *465.8 Gi    
> disk1
>   1:                        EFI                         200.0 Mi    
> disk1s1
>   2:                        ZFS re                      465.4 Gi    
> disk1s2
> /dev/disk2
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:      GUID_partition_scheme                        *465.8 Gi    
> disk2
>   1:                        EFI                         200.0 Mi    
> disk2s1
>   2:                        ZFS re                      465.4 Gi    
> disk2s2
> /dev/disk3
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:      GUID_partition_scheme                        *465.8 Gi    
> disk3
>   1:                        EFI                         200.0 Mi    
> disk3s1
>   2:                        ZFS re                      465.4 Gi    
> disk3s2
>
> The situation is getting worse. Now after the first boot I get a  
> message
> box saying there is an uninitialized disk, with the proposal to  
> initialize,
> ignore or eject it. After that I got a few panics.
>
> After six or seven reboots, some of which with a power cycle, I get  
> a perfectly
> working zpool. The stages in between give me a degraded or a slow  
> pool. It is
> as if the pool is not mounted in time for some other initialization  
> parts.
>
> Best regards,
>
> Werner.
>
> On 16 Sep 2008, at 04:38, No?l Dellofano wrote:
>
>> this sounds like the exact symptom of an unformatted disk.
>>
>> Did you use "diskutil partitionDisk disk0 ...." to format the disks  
>> for ZFS before you created your ZFS pool with them?
>>
>> Noel
>>
>> On Sep 9, 2008, at 12:07 AM, Werner Donn? wrote:
>>
>>> Hi,
>>>
>>> There is something strange about the initialization of a raidz  
>>> pool on
>>> a set of three USB disks. I use ZFS 119 on a Mac Mini with Mac OS X
>>> 10.5.4.
>>> During the boot sequence the disks go on and off several times. I
>>> usually
>>> have to boot two or three times before the pool works properly. In  
>>> most
>>> cases the pool is in a degraded state with one disk that is not
>>> available.
>>> Strangely enough it is not always the same disk.
>>>
>>> In other cases the pool is completely online, but very slow.  
>>> Normally
>>> there is always activity on all disks at the same time, but in this
>>> case when two of them are very active the third is not or much  
>>> less and
>>> vice versa.
>>>
>>> Once the pool is online in a normal way it is very fast and shows no
>>> problems for the rest of the day.
>>>
>>> Best regards,
>>>
>>> Werner.
>>> --
>>> Werner Donn?  --  Re                                     http://www.pincette.biz
>>> Engelbeekstraat 8                                               http://www.re.be
>>> BE-3300 Tienen
>>> tel: (+32) 486 425803	e-mail: werner.donne at re.be
>>>
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
> --
> Werner Donn?  --  Re                                     http://www.pincette.biz
> Engelbeekstraat 8                                               http://www.re.be
> BE-3300 Tienen
> tel: (+32) 486 425803	e-mail: werner.donne at re.be
>
>
>
>
>


From darwinskernel at gmail.com  Sat Oct  4 08:28:08 2008
From: darwinskernel at gmail.com (Charles Darwin)
Date: Sat, 4 Oct 2008 11:28:08 -0400
Subject: [zfs-discuss] Funny Behaviour (Permissions)
Message-ID: <C9E8AFF7-0EA7-4590-8AAA-5F0301EAE449@gmail.com>

~ pm$ ls -le /Volumes/neo/Users/pm/Music/
ls: .DS_Store: Permission denied
ls: iTunes: Permission denied
~ pm$ sudo ls -le /Volumes/neo/Users/pm/Music/
total 33
-rw-rw-rw-  1 pm  staff  6148 30 Sep 08:58 .DS_Store
drw-rw-rw-  3 pm  staff     4 30 Sep 08:59 iTunes
~ pm$ diskutil list
/dev/disk0
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    3:                        ZFS neo                     81.3 Gi     
disk0s3

Am I missing something here?

Charles

From alex.blewitt at gmail.com  Sat Oct  4 08:40:14 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Sat, 4 Oct 2008 16:40:14 +0100
Subject: [zfs-discuss] Funny Behaviour (Permissions)
In-Reply-To: <C9E8AFF7-0EA7-4590-8AAA-5F0301EAE449@gmail.com>
References: <C9E8AFF7-0EA7-4590-8AAA-5F0301EAE449@gmail.com>
Message-ID: <7E97BB97-6765-4D78-B00F-E7363BA51E3D@gmail.com>

You don't have execute permissions on the iTunes directory, which is  
needed for scanning a directory's contents.

Do you have execute permissions on the current directory?

Also, bear in mind that ACLs override the POSIX permissions so check  
you haven't got anything odd set there.

Alex

Sent from my (new) iPhone

On 4 Oct 2008, at 16:28, Charles Darwin <darwinskernel at gmail.com> wrote:

> ~ pm$ ls -le /Volumes/neo/Users/pm/Music/
> ls: .DS_Store: Permission denied
> ls: iTunes: Permission denied
> ~ pm$ sudo ls -le /Volumes/neo/Users/pm/Music/
> total 33
> -rw-rw-rw-  1 pm  staff  6148 30 Sep 08:58 .DS_Store
> drw-rw-rw-  3 pm  staff     4 30 Sep 08:59 iTunes
> ~ pm$ diskutil list
> /dev/disk0
>    #:                       TYPE NAME                    SIZE
> IDENTIFIER
>    3:                        ZFS neo                     81.3 Gi
> disk0s3
>
> Am I missing something here?
>
> Charles
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From zfs at hessmann.de  Sat Oct  4 12:57:25 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Sat, 4 Oct 2008 21:57:25 +0200
Subject: [zfs-discuss] zfs send/recv
Message-ID: <DE5CB0FF-F9DB-4414-97D5-D223F646E20E@hessmann.de>

Good morning group,


from the changelog I got the impression zfs send/recv works - so what  
am I doing wrong?

=====
bash-3.2$ zfs list |grep iTunes
music.hessi/iTunes                              101G  23.4G  94.2G  / 
Volumes/music.hessi/iTunes
music.hessi/iTunes at 20080713131900              93.3M      -  83.2G  -
music.hessi/iTunes at 20080718084100              59.8M      -  85.6G  -
music.hessi/iTunes at 20080722235800              60.6M      -  86.3G  -
music.hessi/iTunes at 20080813213320              65.9M      -  90.3G  -
music.hessi/iTunes at 20080909232045              1.51G      -  95.7G  -
tank/backup/music.hessi/iTunes                 83.1G  81.0G  83.1G  / 
Volumes/tank/backup/music.hessi/iTunes
tank/backup/music.hessi/iTunes at 20080713131900  91.9K      -  83.1G  -
bash-3.2$ zfs send music.hessi/iTunes at 20080718084100 |zfs recv -d tank/ 
backup/music.hessi/iTunes
internal error: Bad file descriptor
cannot receive: invalid stream (failed to read first record)
bash-3.2$ zfs send music.hessi/iTunes at 20080718084100 |zfs recv -d tank/ 
backup/music.hessi
internal error: Bad file descriptor
cannot receive: invalid stream (failed to read first record)
=====

tank/backup/music.hessi/iTunes at 20080713131900 is a snapshot created  
with send/recv but via file (send > file, recv < file). I tried to  
create this snapshot with send | recv as well, which didn't work (same  
error).

Since sending a snapshot via a file always seems to create a file with  
all the data in it (not only the changes), I would really like to  
change my backup routine to pipe the changes through send/recv in one  
go. I presume it will only transfer changes on byte level, right?

My fault or not yet implemented in OS X?


Best regards,

Christian

From alex.blewitt at gmail.com  Sat Oct  4 13:29:06 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Sat, 4 Oct 2008 21:29:06 +0100
Subject: [zfs-discuss] zfs send/recv
In-Reply-To: <DE5CB0FF-F9DB-4414-97D5-D223F646E20E@hessmann.de>
References: <DE5CB0FF-F9DB-4414-97D5-D223F646E20E@hessmann.de>
Message-ID: <636fd28e0810041329u434b69a2obfa4b3f4a9ce85e2@mail.gmail.com>

Not fully - you have to send the zfs send to a file, and then from a
file into a zfs recv.

zfs send > /tmp/foo
zfs recv < /tmp/foo

I think Noel posted something a few days ago with a more elegant
piping solution.

Alex

On Sat, Oct 4, 2008 at 8:57 PM, Christian He?mann <zfs at hessmann.de> wrote:
> Good morning group,
>
>
> from the changelog I got the impression zfs send/recv works - so what
> am I doing wrong?
>
> =====
> bash-3.2$ zfs list |grep iTunes
> music.hessi/iTunes                              101G  23.4G  94.2G  /
> Volumes/music.hessi/iTunes
> music.hessi/iTunes at 20080713131900              93.3M      -  83.2G  -
> music.hessi/iTunes at 20080718084100              59.8M      -  85.6G  -
> music.hessi/iTunes at 20080722235800              60.6M      -  86.3G  -
> music.hessi/iTunes at 20080813213320              65.9M      -  90.3G  -
> music.hessi/iTunes at 20080909232045              1.51G      -  95.7G  -
> tank/backup/music.hessi/iTunes                 83.1G  81.0G  83.1G  /
> Volumes/tank/backup/music.hessi/iTunes
> tank/backup/music.hessi/iTunes at 20080713131900  91.9K      -  83.1G  -
> bash-3.2$ zfs send music.hessi/iTunes at 20080718084100 |zfs recv -d tank/
> backup/music.hessi/iTunes
> internal error: Bad file descriptor
> cannot receive: invalid stream (failed to read first record)
> bash-3.2$ zfs send music.hessi/iTunes at 20080718084100 |zfs recv -d tank/
> backup/music.hessi
> internal error: Bad file descriptor
> cannot receive: invalid stream (failed to read first record)
> =====
>
> tank/backup/music.hessi/iTunes at 20080713131900 is a snapshot created
> with send/recv but via file (send > file, recv < file). I tried to
> create this snapshot with send | recv as well, which didn't work (same
> error).
>
> Since sending a snapshot via a file always seems to create a file with
> all the data in it (not only the changes), I would really like to
> change my backup routine to pipe the changes through send/recv in one
> go. I presume it will only transfer changes on byte level, right?
>
> My fault or not yet implemented in OS X?
>
>
> Best regards,
>
> Christian
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

From info at martin-hauser.net  Sat Oct  4 13:29:28 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Sat, 4 Oct 2008 22:29:28 +0200
Subject: [zfs-discuss] zfs send/recv
In-Reply-To: <DE5CB0FF-F9DB-4414-97D5-D223F646E20E@hessmann.de>
References: <DE5CB0FF-F9DB-4414-97D5-D223F646E20E@hessmann.de>
Message-ID: <77320334-3B0A-4ECA-A21E-89B610C7E066@martin-hauser.net>

Hello,


On Oct 4, 2008, at 21:57 PM, Christian He?mann wrote:

> bash-3.2$ zfs send music.hessi/iTunes at 20080718084100 |zfs recv -d  
> tank/
> backup/music.hessi/iTunes
> internal error: Bad file descriptor
> cannot receive: invalid stream (failed to read first record)
> bash-3.2$ zfs send music.hessi/iTunes at 20080718084100 |zfs recv -d  
> tank/
> backup/music.hessi
> internal error: Bad file descriptor
> cannot receive: invalid stream (failed to read first record)
> =====
>
> tank/backup/music.hessi/iTunes at 20080713131900 is a snapshot created
> with send/recv but via file (send > file, recv < file). I tried to
> create this snapshot with send | recv as well, which didn't work (same
> error).
>
> Since sending a snapshot via a file always seems to create a file with
> all the data in it (not only the changes), I would really like to
> change my backup routine to pipe the changes through send/recv in one
> go. I presume it will only transfer changes on byte level, right?

Currently there is no support for sending stuff via the pipe. If you  
want to skip the 'file' part, you can create yourself a fifo (mkfifo  
fifoname) and use that instead. It will use no extra space on your  
filesystem on sending data to that fifo, yet you'll need to connect  
both ends to it like you'd do with a file:

mkfifo zfs_recv_pipe
zfs send > zfs_recv_pipe
zfs recv < zfs_recv_pipe

and there you go. The difference between a fifo and using the | are  
only marginal and in syntax. Also you want to clean up your fifo later  
using rm zfs_recv_fifo

Hope that helps

Martin

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 2272 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081004/65fa789e/attachment.bin 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081004/65fa789e/attachment-0001.bin 

From mcamou at tecnoguru.com  Sun Oct  5 03:53:51 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Sun, 5 Oct 2008 12:53:51 +0200
Subject: [zfs-discuss] ZFS hanging when a disk in a mirror goes offline
Message-ID: <762437f0810050353g67cd8d7dif4fd869531c5c142@mail.gmail.com>

I'm running zfs-119 downloaded yesterday from zfs.macosforge.org.

In the end I went with the "sane" option: 2x1TB mirrored and 3x500GB
RAIDZ'd.

I first added the mirror to the pool, since 2 of the 500GB disks currently
have data that will be moved into ZFS. I started copying with cpio but after
a while the copy hung.

I found out that one of my 1TB disks suddenly turns itself off for no
apparent reason. I get a bunch of IOUSBKit error messages in dmesg, and
zpool status tells me that "One or more devices could not be opened.
Sufficient replicas exist for the pool to continue functioning in a degraded
state". The device in question shows as UNAVAIL and the pool and the mirror
as DEGRADED.

Why does the copy hang if the pool is still usable?

Now I turn the offending device off and on again. Thing is, OS X assigns it
a new device name. So I do an ls /dev/disk*, find out which is the latest
disk (from just a couple of seconds ago). If before it was disk4s2, it turns
out it's now disk7s2 (I have other disks on this machine). So I do a zpool
online plogiston /dev/disk7s2...which hangs. I next do a zpool
status...which hangs. I have to do a hard shutdown (holding down the power
button). I disconnect all other external hard drives to correctly identify
the parts of the pool and restart. I only have 2 external drives: disk1s2
(which is now online) and disk2s2 (which doesn't appear in zpool status, it
still says disk4s2). So I try a zpool online /dev/disk2s2 and it tells me
that device is not part of the pool. So now I seem to have half a mirror
(whichfor some reason is resilvering) and another disk which is completely
unavailable.

How could I recover from this situation?

At this point I could recreate the pool since I still have the original data
in the 2x500GB disks, but in the end these will be RAIDZ'ed with another
500GB and added to the pool. I of course need to get the faulty 1TB disk
replaced but at this point I don't think I can trust ZFS to give it all my
data. I would really like to know what's going on

-- 
The impossible has, on occasion, let me down
                                                       --R.U. Sirius
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081005/e19e45f6/attachment-0001.html 

From dirkschelfhout at mac.com  Mon Oct  6 00:41:04 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Mon, 06 Oct 2008 09:41:04 +0200
Subject: [zfs-discuss] increased partiiton sizes for raidz
Message-ID: <A7AB4299-58D3-4A38-BF33-9645D9300558@mac.com>

Hi,

I have a macpro in which I use 3 disks for a n hfs+ stripe and for a  
raidz.
My raidz was getting full, so I wanted to move more files to the raidz  
and make the hfs+ stripe smaller.

1 by 1 I repartitioned the disks and let zfs resilver the raidz. this  
worked fine and after a reboot, it showed the extra size on raidz.

But now diskutil won't let me recreate the hfs+ stripe.

This is what 1 disk looks like now :
root# diskutil list disk1
/dev/disk1
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *465.8 Gi    
disk1
    1:                        EFI                         200.0 Mi    
disk1s1
    2:                  Apple_HFS raidHfs                 49.9 Gi     
disk1s2
    3:                        ZFS backup                  415.4 Gi    
disk1s3
Macintosh:~ root#
This is what I get from disk utility : ( the zfs raid is not mounted  
at this time )
from /var/log/system.log :
root# diskutil list disk1
/dev/disk1
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *465.8 Gi    
disk1
    1:                        EFI                         200.0 Mi    
disk1s1
    2:                  Apple_HFS raidHfs                 49.9 Gi     
disk1s2
    3:                        ZFS backup                  415.4 Gi    
disk1s3
Macintosh:~ root#
So it is trying to create a booter partition but fails.
Any help welcome,

Dirk


From dirkschelfhout at mac.com  Mon Oct  6 00:48:16 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Mon, 06 Oct 2008 09:48:16 +0200
Subject: [zfs-discuss] increased partiiton sizes for raidz
In-Reply-To: <A7AB4299-58D3-4A38-BF33-9645D9300558@mac.com>
References: <A7AB4299-58D3-4A38-BF33-9645D9300558@mac.com>
Message-ID: <71343D43-CB37-42C2-BD47-8771F38F0F1E@mac.com>

The bottom part should be :
Oct  6 09:12:54 Macintosh Disk Utility[273]: Creating RAID
Oct  6 09:12:54 Macintosh Disk Utility[273]:     Filesystem: Mac OS  
Extended (Case-sensitive, Journaled)
Oct  6 09:12:54 Macintosh Disk Utility[273]:     RAID type: Striped  
RAID Set
Oct  6 09:12:54 Macintosh Disk Utility[273]:     RAID set name: ?work?
Oct  6 09:12:54 Macintosh Disk Utility[273]:     RAID chunk size: 64K
Oct  6 09:12:54 Macintosh Disk Utility[273]:     3 members
Oct  6 09:12:54 Macintosh Disk Utility[273]:             raidHfs  
(disk1s2)
Oct  6 09:12:54 Macintosh Disk Utility[273]:             raidHfs  
(disk2s2)
Oct  6 09:12:54 Macintosh Disk Utility[273]:             raidHfs  
(disk3s2)
Oct  6 09:12:54 Macintosh Disk Utility[273]: Adding booter for RAID  
partition 'disk1s2 raidHfs'
Oct  6 09:13:11 Macintosh Disk Utility[273]: WARNING: There was an  
error, "Resource busy" (16), updating the booter on disk member  
disk1s2 raidHfs
Oct  6 09:13:11 Macintosh Disk Utility[273]: Error creating RAID:  
Could not add a RAID disk to a RAID.
Oct  6 09:13:11 Macintosh Disk Utility[273]: RAID set ?work? not  
created successfully.
Oct  6 09:13:13 Macintosh Disk Utility[273]:

On 06 Oct 2008, at 09:41, Dirk Schelfhout wrote:

> Hi,
>
> I have a macpro in which I use 3 disks for a n hfs+ stripe and for a
> raidz.
> My raidz was getting full, so I wanted to move more files to the raidz
> and make the hfs+ stripe smaller.
>
> 1 by 1 I repartitioned the disks and let zfs resilver the raidz. this
> worked fine and after a reboot, it showed the extra size on raidz.
>
> But now diskutil won't let me recreate the hfs+ stripe.
>
> This is what 1 disk looks like now :
> root# diskutil list disk1
> /dev/disk1
>    #:                       TYPE NAME                    SIZE
> IDENTIFIER
>    0:      GUID_partition_scheme                        *465.8 Gi
> disk1
>    1:                        EFI                         200.0 Mi
> disk1s1
>    2:                  Apple_HFS raidHfs                 49.9 Gi
> disk1s2
>    3:                        ZFS backup                  415.4 Gi
> disk1s3
> Macintosh:~ root#
> This is what I get from disk utility : ( the zfs raid is not mounted
> at this time )
> from /var/log/system.log :
> root# diskutil list disk1
> /dev/disk1
>    #:                       TYPE NAME                    SIZE
> IDENTIFIER
>    0:      GUID_partition_scheme                        *465.8 Gi
> disk1
>    1:                        EFI                         200.0 Mi
> disk1s1
>    2:                  Apple_HFS raidHfs                 49.9 Gi
> disk1s2
>    3:                        ZFS backup                  415.4 Gi
> disk1s3
> Macintosh:~ root#
> So it is trying to create a booter partition but fails.
> Any help welcome,
>
> Dirk
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From werner.donne at re.be  Mon Oct  6 04:11:09 2008
From: werner.donne at re.be (=?ISO-8859-1?Q?Werner_Donn=E9?=)
Date: Mon, 6 Oct 2008 13:11:09 +0200
Subject: [zfs-discuss] ZFS hanging when a disk in a mirror goes offline
In-Reply-To: <762437f0810050353g67cd8d7dif4fd869531c5c142@mail.gmail.com>
References: <762437f0810050353g67cd8d7dif4fd869531c5c142@mail.gmail.com>
Message-ID: <E930FBEC-027D-4FA0-ADC1-AD7ED3A4C85C@re.be>

The 1TB disk isn't necessarily faulty. I have this problem all the time
and it is not always with the same disk. After a number of reboots or
manual export/imports the pool is mounted properly and the so called
faulty disk remains perfectly OK for the rest of the day.

Werner.

On 05 Oct 2008, at 12:53, Mario Camou wrote:

> I'm running zfs-119 downloaded yesterday from zfs.macosforge.org.
>
> In the end I went with the "sane" option: 2x1TB mirrored and 3x500GB  
> RAIDZ'd.
>
> I first added the mirror to the pool, since 2 of the 500GB disks  
> currently have data that will be moved into ZFS. I started copying  
> with cpio but after a while the copy hung.
>
> I found out that one of my 1TB disks suddenly turns itself off for  
> no apparent reason. I get a bunch of IOUSBKit error messages in  
> dmesg, and zpool status tells me that "One or more devices could not  
> be opened. Sufficient replicas exist for the pool to continue  
> functioning in a degraded state". The device in question shows as  
> UNAVAIL and the pool and the mirror as DEGRADED.
>
> Why does the copy hang if the pool is still usable?
>
> Now I turn the offending device off and on again. Thing is, OS X  
> assigns it a new device name. So I do an ls /dev/disk*, find out  
> which is the latest disk (from just a couple of seconds ago). If  
> before it was disk4s2, it turns out it's now disk7s2 (I have other  
> disks on this machine). So I do a zpool online plogiston /dev/ 
> disk7s2...which hangs. I next do a zpool status...which hangs. I  
> have to do a hard shutdown (holding down the power button). I  
> disconnect all other external hard drives to correctly identify the  
> parts of the pool and restart. I only have 2 external drives:  
> disk1s2 (which is now online) and disk2s2 (which doesn't appear in  
> zpool status, it still says disk4s2). So I try a zpool online /dev/ 
> disk2s2 and it tells me that device is not part of the pool. So now  
> I seem to have half a mirror (whichfor some reason is resilvering)  
> and another disk which is completely unavailable.
>
> How could I recover from this situation?
>
> At this point I could recreate the pool since I still have the  
> original data in the 2x500GB disks, but in the end these will be  
> RAIDZ'ed with another 500GB and added to the pool. I of course need  
> to get the faulty 1TB disk replaced but at this point I don't think  
> I can trust ZFS to give it all my data. I would really like to know  
> what's going on
>
> -- 
> The impossible has, on occasion, let me down
>                                                        --R.U. Sirius
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

--
Werner Donn?  --  Re                                     http://www.pincette.biz
Engelbeekstraat 8                                               http://www.re.be
BE-3300 Tienen
tel: (+32) 486 425803	e-mail: werner.donne at re.be






From mattsnow at gmail.com  Mon Oct  6 08:21:01 2008
From: mattsnow at gmail.com (Matt Snow)
Date: Mon, 6 Oct 2008 08:21:01 -0700
Subject: [zfs-discuss] ZFS hanging when a disk in a mirror goes offline
In-Reply-To: <762437f0810050353g67cd8d7dif4fd869531c5c142@mail.gmail.com>
References: <762437f0810050353g67cd8d7dif4fd869531c5c142@mail.gmail.com>
Message-ID: <6879ebc80810060821l5bde7f3ai61cda11a4ad40cae@mail.gmail.com>

silly question, but did you create the ZFS GPT/EFI partition/labels using
CLI diskutil before running zpool create? I had a similar experience with
USB disks and not using diskutil prior to zpool create as noted in the FAQ.

It is normal in OS X for the disks to enumerate differently each boot, or
re-inserting a device.

..Matt

On Sun, Oct 5, 2008 at 3:53 AM, Mario Camou <mcamou at tecnoguru.com> wrote:

> I'm running zfs-119 downloaded yesterday from zfs.macosforge.org.
>
> In the end I went with the "sane" option: 2x1TB mirrored and 3x500GB
> RAIDZ'd.
>
> I first added the mirror to the pool, since 2 of the 500GB disks currently
> have data that will be moved into ZFS. I started copying with cpio but after
> a while the copy hung.
>
> I found out that one of my 1TB disks suddenly turns itself off for no
> apparent reason. I get a bunch of IOUSBKit error messages in dmesg, and
> zpool status tells me that "One or more devices could not be opened.
> Sufficient replicas exist for the pool to continue functioning in a degraded
> state". The device in question shows as UNAVAIL and the pool and the mirror
> as DEGRADED.
>
> Why does the copy hang if the pool is still usable?
>
> Now I turn the offending device off and on again. Thing is, OS X assigns it
> a new device name. So I do an ls /dev/disk*, find out which is the latest
> disk (from just a couple of seconds ago). If before it was disk4s2, it turns
> out it's now disk7s2 (I have other disks on this machine). So I do a zpool
> online plogiston /dev/disk7s2...which hangs. I next do a zpool
> status...which hangs. I have to do a hard shutdown (holding down the power
> button). I disconnect all other external hard drives to correctly identify
> the parts of the pool and restart. I only have 2 external drives: disk1s2
> (which is now online) and disk2s2 (which doesn't appear in zpool status, it
> still says disk4s2). So I try a zpool online /dev/disk2s2 and it tells me
> that device is not part of the pool. So now I seem to have half a mirror
> (whichfor some reason is resilvering) and another disk which is completely
> unavailable.
>
> How could I recover from this situation?
>
> At this point I could recreate the pool since I still have the original
> data in the 2x500GB disks, but in the end these will be RAIDZ'ed with
> another 500GB and added to the pool. I of course need to get the faulty 1TB
> disk replaced but at this point I don't think I can trust ZFS to give it all
> my data. I would really like to know what's going on
>
> --
> The impossible has, on occasion, let me down
>                                                        --R.U. Sirius
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081006/3f29817c/attachment.html 

From wmertens at cisco.com  Mon Oct  6 12:29:34 2008
From: wmertens at cisco.com (Wout Mertens)
Date: Mon, 6 Oct 2008 21:29:34 +0200
Subject: [zfs-discuss] Filesystem operations hang
Message-ID: <6A98CA2C-9234-43BE-85B0-A2A0DA7B64EC@cisco.com>

Heyhey,

I just got a brand-spanking-new 1TB USB disk and I figured I needed  
ZFS on it. So I got the 119 binaries and created a pool as in the wiki.

I started to copy over my stuff to it using Finder, which went  
swimmingly (180GB), but after a while I noticed that things did not  
proceed at all. Currently finder is stuck at 90% of the copy and  
commands that call fstat() all hang. zpool status etc don't hang as  
you can see below. Reading the raw device with dd doesn't hang either,  
so it's somewhere above the device layer.

Not a peep in syslog. Should I be doing something special for better  
logs?

I tried doing a zpool export -f but that hung. I'll now disconnect the  
USB device and see what happens (I'm expecting a crash so I'm sending  
this before that ;-) )

I just thought I'd throw the data point out there.

Wout.


wmertens-mac-2 ~ $ zpool list
NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
Z                       931G    160G    771G    17%  ONLINE     -

wmertens-mac-2 ~ $ zpool status
   pool: Z
  state: ONLINE
status: The pool is formatted using an older on-disk format.  The pool  
can
	still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
	pool will no longer be accessible on older software versions.
  scrub: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	Z           ONLINE       0     0     0
	  disk1s2   ONLINE       0     0     0

errors: No known data errors

wmertens-mac-2 ~ $ diskutil info disk1
    Device Identifier:        disk1
    Device Node:              /dev/disk1
    Part Of Whole:            disk1
    Device / Media Name:      WDC WD10 EAVS-00D7B0 Media

    Volume Name:
    Mount Point:

    Partition Type:           GUID_partition_scheme
    Bootable:                 Not bootable
    Media Type:               Generic
    Protocol:                 USB
    SMART Status:             Not Supported

    Total Size:               931.5 Gi (1000204886016 B) (1953525168  
512-byte blocks)
    Free Space:               0.0 B (0 B) (0 512-byte blocks)

    Read Only:                No
    Ejectable:                Yes
    Whole:                    Yes
    Internal:                 No
    OS 9 Drivers:             No
    Low Level Format:         Not Supported


From wmertens at cisco.com  Mon Oct  6 12:55:23 2008
From: wmertens at cisco.com (Wout Mertens)
Date: Mon, 6 Oct 2008 21:55:23 +0200
Subject: [zfs-discuss] Filesystem operations hang
In-Reply-To: <6A98CA2C-9234-43BE-85B0-A2A0DA7B64EC@cisco.com>
References: <6A98CA2C-9234-43BE-85B0-A2A0DA7B64EC@cisco.com>
Message-ID: <CE98E1BA-040F-468B-94CD-EAF26D1E1339@cisco.com>

Update: removing the USB didn't do a thing. The only thing that  
happened was that syslog now contains:

Mon Oct  6 21:45:18 wmertens-mac-2 kernel[0] <Debug>: disk1s2: media  
is not present.
Mon Oct  6 21:45:18 wmertens-mac-2 kernel[0] <Debug>: disk1s2: media  
is not present.
Mon Oct  6 21:45:18 wmertens-mac-2 kernel[0] <Debug>: disk1s2: media  
is not present.
Mon Oct  6 21:45:18 wmertens-mac-2 kernel[0] <Debug>: disk1s2: media  
is not present.

zpool scrub hangs:
Analysis of sampling zpool (pid 13265) every 1 millisecond
Call graph:
     2278 Thread_2503
       2278 start
         2278 main
           2278 zpool_do_scrub
             2278 for_each_pool
               2278 pool_list_iter
                 2278 scrub_callback
                   2278 zpool_scrub
                     2278 zfs_ioctl
                       2278 ioctl
                         2278 ioctl

replugging the USB disk doesn't help either. Diskutil info disk1  
doesn't work anymore.

I've run out of options; rebooting.

Wout.

On Oct 6, 2008, at 9:29 PM, Wout Mertens wrote:

> Heyhey,
>
> I just got a brand-spanking-new 1TB USB disk and I figured I needed
> ZFS on it. So I got the 119 binaries and created a pool as in the  
> wiki.
>
> I started to copy over my stuff to it using Finder, which went
> swimmingly (180GB), but after a while I noticed that things did not
> proceed at all. Currently finder is stuck at 90% of the copy and
> commands that call fstat() all hang. zpool status etc don't hang as
> you can see below. Reading the raw device with dd doesn't hang either,
> so it's somewhere above the device layer.
>
> Not a peep in syslog. Should I be doing something special for better
> logs?
>
> I tried doing a zpool export -f but that hung. I'll now disconnect the
> USB device and see what happens (I'm expecting a crash so I'm sending
> this before that ;-) )
>
> I just thought I'd throw the data point out there.
>
> Wout.
>
>
> wmertens-mac-2 ~ $ zpool list
> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
> ALTROOT
> Z                       931G    160G    771G    17%  ONLINE     -
>
> wmertens-mac-2 ~ $ zpool status
>   pool: Z
>  state: ONLINE
> status: The pool is formatted using an older on-disk format.  The pool
> can
> 	still be used, but some features are unavailable.
> action: Upgrade the pool using 'zpool upgrade'.  Once this is done,  
> the
> 	pool will no longer be accessible on older software versions.
>  scrub: none requested
> config:
>
> 	NAME        STATE     READ WRITE CKSUM
> 	Z           ONLINE       0     0     0
> 	  disk1s2   ONLINE       0     0     0
>
> errors: No known data errors
>
> wmertens-mac-2 ~ $ diskutil info disk1
>    Device Identifier:        disk1
>    Device Node:              /dev/disk1
>    Part Of Whole:            disk1
>    Device / Media Name:      WDC WD10 EAVS-00D7B0 Media
>
>    Volume Name:
>    Mount Point:
>
>    Partition Type:           GUID_partition_scheme
>    Bootable:                 Not bootable
>    Media Type:               Generic
>    Protocol:                 USB
>    SMART Status:             Not Supported
>
>    Total Size:               931.5 Gi (1000204886016 B) (1953525168
> 512-byte blocks)
>    Free Space:               0.0 B (0 B) (0 512-byte blocks)
>
>    Read Only:                No
>    Ejectable:                Yes
>    Whole:                    Yes
>    Internal:                 No
>    OS 9 Drivers:             No
>    Low Level Format:         Not Supported
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From zfs at hessmann.de  Tue Oct  7 06:27:25 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Tue, 7 Oct 2008 15:27:25 +0200
Subject: [zfs-discuss] chksum error on RAID.Z - severity?
In-Reply-To: <DB9A2F12-AD43-41B9-913A-23DA8977D519@hessmann.de>
References: <DB9A2F12-AD43-41B9-913A-23DA8977D519@hessmann.de>
Message-ID: <725B7031-7338-42F8-A603-B033E8EA11A8@hessmann.de>

On 26.09.2008, at 10:26, Christian He?mann wrote:

> Me again...
>
> After adding three more disks to my RAID.Z Pool (now 2x 3 disks) I
> issued a scrub just to be sure everything is fine.
>
> Well, apparently it isn't, since zpool tells me I have chksum errors
> (85% done, 5 errors so far) on one of the additional disks (which
> isn't new and the smallest of all: old 3x 750GB, new 500/200/120GB).
> That same disk worked fine as a standalone ZFS device until yesterday,
> with scrubs without any error messages.

Well, seems I have to talk to myself, since no one wants to answer  
me. ;-)

I decided not to replace the disk and didn't encounter any further  
problems (didn't do another scrub, though, due to the bug which causes  
the scrub to restart as soon as a snapshot is added - any chance for  
its fix to be implemented in OS X soon?).

After a planned reboot and therefore new mount, I issued another  
scrub, with a nearly completely filled pool (~5GB available) and: no  
more chksum errors.

I don't believe the blocks in question were all free during that time,  
so either the disk marked them as bad (don't think so) or it was not a  
sign for a bad disk, but some other problem - maybe during write/read,  
maybe due to the kernel panics the system had to endure, maybe one of  
the ZFS bugs displaying chksum errors which aren't there...

Anyway, if anyone is interested, my personal conclusion: Don't Panic!


Best regards,

Christian

From bplist at thinkpink.com  Tue Oct  7 06:34:24 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Tue, 7 Oct 2008 06:34:24 -0700
Subject: [zfs-discuss] chksum error on RAID.Z - severity?
In-Reply-To: <725B7031-7338-42F8-A603-B033E8EA11A8@hessmann.de>
References: <DB9A2F12-AD43-41B9-913A-23DA8977D519@hessmann.de>
	<725B7031-7338-42F8-A603-B033E8EA11A8@hessmann.de>
Message-ID: <FA167D28-477F-4985-B243-E573BC664A39@thinkpink.com>

I've had this kind of transient problem as well.  In my case, one of  
the SATA cables inside my enclosure had gone bad (how the heck does  
this happen?)  I replaced the cable, rebuilt the mirror, and ZFS was  
happy.

bri

On Oct 7, 2008, at 6:27 AM, Christian He?mann wrote:

> On 26.09.2008, at 10:26, Christian He?mann wrote:
>
>> Me again...
>>
>> After adding three more disks to my RAID.Z Pool (now 2x 3 disks) I
>> issued a scrub just to be sure everything is fine.
>>
>> Well, apparently it isn't, since zpool tells me I have chksum errors
>> (85% done, 5 errors so far) on one of the additional disks (which
>> isn't new and the smallest of all: old 3x 750GB, new 500/200/120GB).
>> That same disk worked fine as a standalone ZFS device until  
>> yesterday,
>> with scrubs without any error messages.
>
> Well, seems I have to talk to myself, since no one wants to answer
> me. ;-)
>
> I decided not to replace the disk and didn't encounter any further
> problems (didn't do another scrub, though, due to the bug which causes
> the scrub to restart as soon as a snapshot is added - any chance for
> its fix to be implemented in OS X soon?).
>
> After a planned reboot and therefore new mount, I issued another
> scrub, with a nearly completely filled pool (~5GB available) and: no
> more chksum errors.
>
> I don't believe the blocks in question were all free during that time,
> so either the disk marked them as bad (don't think so) or it was not a
> sign for a bad disk, but some other problem - maybe during write/read,
> maybe due to the kernel panics the system had to endure, maybe one of
> the ZFS bugs displaying chksum errors which aren't there...
>
> Anyway, if anyone is interested, my personal conclusion: Don't Panic!
>
>
> Best regards,
>
> Christian
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dirkschelfhout at mac.com  Tue Oct  7 11:31:01 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Tue, 07 Oct 2008 20:31:01 +0200
Subject: [zfs-discuss] chksum error on RAID.Z - severity?
In-Reply-To: <FA167D28-477F-4985-B243-E573BC664A39@thinkpink.com>
References: <DB9A2F12-AD43-41B9-913A-23DA8977D519@hessmann.de>
	<725B7031-7338-42F8-A603-B033E8EA11A8@hessmann.de>
	<FA167D28-477F-4985-B243-E573BC664A39@thinkpink.com>
Message-ID: <5BF42F74-7CC3-462A-9C68-0DAE8DD9594F@mac.com>

How did you find out it was the cable ?
On 07 Oct 2008, at 15:34, Brian Pinkerton wrote:

> I've had this kind of transient problem as well.  In my case, one of
> the SATA cables inside my enclosure had gone bad (how the heck does
> this happen?)  I replaced the cable, rebuilt the mirror, and ZFS was
> happy.
>
> bri
>
> On Oct 7, 2008, at 6:27 AM, Christian He?mann wrote:
>
>> On 26.09.2008, at 10:26, Christian He?mann wrote:
>>
>>> Me again...
>>>
>>> After adding three more disks to my RAID.Z Pool (now 2x 3 disks) I
>>> issued a scrub just to be sure everything is fine.
>>>
>>> Well, apparently it isn't, since zpool tells me I have chksum errors
>>> (85% done, 5 errors so far) on one of the additional disks (which
>>> isn't new and the smallest of all: old 3x 750GB, new 500/200/120GB).
>>> That same disk worked fine as a standalone ZFS device until
>>> yesterday,
>>> with scrubs without any error messages.
>>
>> Well, seems I have to talk to myself, since no one wants to answer
>> me. ;-)
>>
>> I decided not to replace the disk and didn't encounter any further
>> problems (didn't do another scrub, though, due to the bug which  
>> causes
>> the scrub to restart as soon as a snapshot is added - any chance for
>> its fix to be implemented in OS X soon?).
>>
>> After a planned reboot and therefore new mount, I issued another
>> scrub, with a nearly completely filled pool (~5GB available) and: no
>> more chksum errors.
>>
>> I don't believe the blocks in question were all free during that  
>> time,
>> so either the disk marked them as bad (don't think so) or it was  
>> not a
>> sign for a bad disk, but some other problem - maybe during write/ 
>> read,
>> maybe due to the kernel panics the system had to endure, maybe one of
>> the ZFS bugs displaying chksum errors which aren't there...
>>
>> Anyway, if anyone is interested, my personal conclusion: Don't Panic!
>>
>>
>> Best regards,
>>
>> Christian
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From zorg at sogeeky.net  Tue Oct  7 11:41:20 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Tue, 7 Oct 2008 11:41:20 -0700
Subject: [zfs-discuss] chksum error on RAID.Z - severity?
In-Reply-To: <5BF42F74-7CC3-462A-9C68-0DAE8DD9594F@mac.com>
References: <DB9A2F12-AD43-41B9-913A-23DA8977D519@hessmann.de>
	<725B7031-7338-42F8-A603-B033E8EA11A8@hessmann.de>
	<FA167D28-477F-4985-B243-E573BC664A39@thinkpink.com>
	<5BF42F74-7CC3-462A-9C68-0DAE8DD9594F@mac.com>
Message-ID: <6E4B5E85-D98A-4EFA-8CD0-35C1193DDB8E@sogeeky.net>

I once had similar issues caused by a flaky power brick.

On Oct 7, 2008, at 11:31 AM, Dirk Schelfhout <dirkschelfhout at mac.com>  
wrote:

> How did you find out it was the cable ?
> On 07 Oct 2008, at 15:34, Brian Pinkerton wrote:
>
>> I've had this kind of transient problem as well.  In my case, one of
>> the SATA cables inside my enclosure had gone bad (how the heck does
>> this happen?)  I replaced the cable, rebuilt the mirror, and ZFS was
>> happy.
>>
>> bri
>>
>> On Oct 7, 2008, at 6:27 AM, Christian He?mann wrote:
>>
>>> On 26.09.2008, at 10:26, Christian He?mann wrote:
>>>
>>>> Me again...
>>>>
>>>> After adding three more disks to my RAID.Z Pool (now 2x 3 disks) I
>>>> issued a scrub just to be sure everything is fine.
>>>>
>>>> Well, apparently it isn't, since zpool tells me I have chksum  
>>>> errors
>>>> (85% done, 5 errors so far) on one of the additional disks (which
>>>> isn't new and the smallest of all: old 3x 750GB, new  
>>>> 500/200/120GB).
>>>> That same disk worked fine as a standalone ZFS device until
>>>> yesterday,
>>>> with scrubs without any error messages.
>>>
>>> Well, seems I have to talk to myself, since no one wants to answer
>>> me. ;-)
>>>
>>> I decided not to replace the disk and didn't encounter any further
>>> problems (didn't do another scrub, though, due to the bug which
>>> causes
>>> the scrub to restart as soon as a snapshot is added - any chance for
>>> its fix to be implemented in OS X soon?).
>>>
>>> After a planned reboot and therefore new mount, I issued another
>>> scrub, with a nearly completely filled pool (~5GB available) and: no
>>> more chksum errors.
>>>
>>> I don't believe the blocks in question were all free during that
>>> time,
>>> so either the disk marked them as bad (don't think so) or it was
>>> not a
>>> sign for a bad disk, but some other problem - maybe during write/
>>> read,
>>> maybe due to the kernel panics the system had to endure, maybe one  
>>> of
>>> the ZFS bugs displaying chksum errors which aren't there...
>>>
>>> Anyway, if anyone is interested, my personal conclusion: Don't  
>>> Panic!
>>>
>>>
>>> Best regards,
>>>
>>> Christian
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From mcamou at tecnoguru.com  Tue Oct  7 14:14:54 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Tue, 7 Oct 2008 23:14:54 +0200
Subject: [zfs-discuss] ZFS hanging when a disk in a mirror goes offline
In-Reply-To: <6879ebc80810060821l5bde7f3ai61cda11a4ad40cae@mail.gmail.com>
References: <762437f0810050353g67cd8d7dif4fd869531c5c142@mail.gmail.com>
	<6879ebc80810060821l5bde7f3ai61cda11a4ad40cae@mail.gmail.com>
Message-ID: <81216894-EA8D-419E-B05F-11644697263C@tecnoguru.com>

I see the thing about the disks, it can happen in other OSes too. I  
did do the diskutil command-line stuff beforehand.

Thanks,
-Mario.

On 6 Oct 2008, at 17:21, Matt Snow wrote:

> silly question, but did you create the ZFS GPT/EFI partition/labels  
> using CLI diskutil before running zpool create? I had a similar  
> experience with USB disks and not using diskutil prior to zpool  
> create as noted in the FAQ.
>
> It is normal in OS X for the disks to enumerate differently each  
> boot, or re-inserting a device.
>
> ..Matt
>
> On Sun, Oct 5, 2008 at 3:53 AM, Mario Camou <mcamou at tecnoguru.com>  
> wrote:
> I'm running zfs-119 downloaded yesterday from zfs.macosforge.org.
>
> In the end I went with the "sane" option: 2x1TB mirrored and 3x500GB  
> RAIDZ'd.
>
> I first added the mirror to the pool, since 2 of the 500GB disks  
> currently have data that will be moved into ZFS. I started copying  
> with cpio but after a while the copy hung.
>
> I found out that one of my 1TB disks suddenly turns itself off for  
> no apparent reason. I get a bunch of IOUSBKit error messages in  
> dmesg, and zpool status tells me that "One or more devices could not  
> be opened. Sufficient replicas exist for the pool to continue  
> functioning in a degraded state". The device in question shows as  
> UNAVAIL and the pool and the mirror as DEGRADED.
>
> Why does the copy hang if the pool is still usable?
>
> Now I turn the offending device off and on again. Thing is, OS X  
> assigns it a new device name. So I do an ls /dev/disk*, find out  
> which is the latest disk (from just a couple of seconds ago). If  
> before it was disk4s2, it turns out it's now disk7s2 (I have other  
> disks on this machine). So I do a zpool online plogiston /dev/ 
> disk7s2...which hangs. I next do a zpool status...which hangs. I  
> have to do a hard shutdown (holding down the power button). I  
> disconnect all other external hard drives to correctly identify the  
> parts of the pool and restart. I only have 2 external drives:  
> disk1s2 (which is now online) and disk2s2 (which doesn't appear in  
> zpool status, it still says disk4s2). So I try a zpool online /dev/ 
> disk2s2 and it tells me that device is not part of the pool. So now  
> I seem to have half a mirror (whichfor some reason is resilvering)  
> and another disk which is completely unavailable.
>
> How could I recover from this situation?
>
> At this point I could recreate the pool since I still have the  
> original data in the 2x500GB disks, but in the end these will be  
> RAIDZ'ed with another 500GB and added to the pool. I of course need  
> to get the faulty 1TB disk replaced but at this point I don't think  
> I can trust ZFS to give it all my data. I would really like to know  
> what's going on
>
> -- 
> The impossible has, on occasion, let me down
>                                                        --R.U. Sirius
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081007/04d8636e/attachment.html 

From mcamou at tecnoguru.com  Tue Oct  7 16:18:40 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Wed, 8 Oct 2008 01:18:40 +0200
Subject: [zfs-discuss] ZFS hanging when a disk in a mirror goes offline
In-Reply-To: <E930FBEC-027D-4FA0-ADC1-AD7ED3A4C85C@re.be>
References: <762437f0810050353g67cd8d7dif4fd869531c5c142@mail.gmail.com>
	<E930FBEC-027D-4FA0-ADC1-AD7ED3A4C85C@re.be>
Message-ID: <BF112351-0191-4027-8E12-441B3E258127@tecnoguru.com>

I've never had this problem with any of my other disks, I went in and  
exchanged it for a MyBook 1TB (the other one was IOMega).

My concern here is with the fact that the system hangs and there seems  
to be no way to get it back up short of a hard shutdown (i.e., power  
switch). I am sometimes travelling for several weeks/months and the  
system has to be able to run unattended. I realize there has to be  
manual intervention in the case of a disk failure (d'oh!) but I would  
assume that if one disk of a mirror goes temporarily offline and comes  
back up ZFS should detect it and continue working. I would also assume  
that in this case the system should continue to work "normally" (as  
far as processes that are reading/writing to the pool). However, the  
cpio process is hanging hard and can't be stopped by any means short  
of the power switch. A reboot command won't work because the cpio  
command won't die and thus will interrupt the reboot.

And then after the reboot I haven't found a way of bringing the faulty  
disk back into the pool. In this case, do I need to do a full reformat  
of the disk and add it back to the mirror as if it was a completely  
new volume?

-Mario.

On 6 Oct 2008, at 13:11, Werner Donn? wrote:

> The 1TB disk isn't necessarily faulty. I have this problem all the  
> time
> and it is not always with the same disk. After a number of reboots or
> manual export/imports the pool is mounted properly and the so called
> faulty disk remains perfectly OK for the rest of the day.
>
> Werner.
>
> On 05 Oct 2008, at 12:53, Mario Camou wrote:
>
>> I'm running zfs-119 downloaded yesterday from zfs.macosforge.org.
>>
>> In the end I went with the "sane" option: 2x1TB mirrored and  
>> 3x500GB RAIDZ'd.
>>
>> I first added the mirror to the pool, since 2 of the 500GB disks  
>> currently have data that will be moved into ZFS. I started copying  
>> with cpio but after a while the copy hung.
>>
>> I found out that one of my 1TB disks suddenly turns itself off for  
>> no apparent reason. I get a bunch of IOUSBKit error messages in  
>> dmesg, and zpool status tells me that "One or more devices could  
>> not be opened. Sufficient replicas exist for the pool to continue  
>> functioning in a degraded state". The device in question shows as  
>> UNAVAIL and the pool and the mirror as DEGRADED.
>>
>> Why does the copy hang if the pool is still usable?
>>
>> Now I turn the offending device off and on again. Thing is, OS X  
>> assigns it a new device name. So I do an ls /dev/disk*, find out  
>> which is the latest disk (from just a couple of seconds ago). If  
>> before it was disk4s2, it turns out it's now disk7s2 (I have other  
>> disks on this machine). So I do a zpool online plogiston /dev/ 
>> disk7s2...which hangs. I next do a zpool status...which hangs. I  
>> have to do a hard shutdown (holding down the power button). I  
>> disconnect all other external hard drives to correctly identify the  
>> parts of the pool and restart. I only have 2 external drives:  
>> disk1s2 (which is now online) and disk2s2 (which doesn't appear in  
>> zpool status, it still says disk4s2). So I try a zpool online /dev/ 
>> disk2s2 and it tells me that device is not part of the pool. So now  
>> I seem to have half a mirror (whichfor some reason is resilvering)  
>> and another disk which is completely unavailable.
>>
>> How could I recover from this situation?
>>
>> At this point I could recreate the pool since I still have the  
>> original data in the 2x500GB disks, but in the end these will be  
>> RAIDZ'ed with another 500GB and added to the pool. I of course need  
>> to get the faulty 1TB disk replaced but at this point I don't think  
>> I can trust ZFS to give it all my data. I would really like to know  
>> what's going on
>>
>> -- 
>> The impossible has, on occasion, let me down
>>                                                       --R.U. Sirius
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> --
> Werner Donn?  --  Re                                     http://www.pincette.biz
> Engelbeekstraat 8                                               http://www.re.be
> BE-3300 Tienen
> tel: (+32) 486 425803	e-mail: werner.donne at re.be
>
>
>
>
>


From bplist at thinkpink.com  Wed Oct  8 09:18:56 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Wed, 8 Oct 2008 09:18:56 -0700
Subject: [zfs-discuss] chksum error on RAID.Z - severity?
In-Reply-To: <5BF42F74-7CC3-462A-9C68-0DAE8DD9594F@mac.com>
References: <DB9A2F12-AD43-41B9-913A-23DA8977D519@hessmann.de>
	<725B7031-7338-42F8-A603-B033E8EA11A8@hessmann.de>
	<FA167D28-477F-4985-B243-E573BC664A39@thinkpink.com>
	<5BF42F74-7CC3-462A-9C68-0DAE8DD9594F@mac.com>
Message-ID: <ABFDDBC0-8D56-4B93-A12F-CC0B302D172E@thinkpink.com>

I went through a long process of replacing each component in the chain  
until I had no checksum errors.  I started with the disk, then the  
tray, then the port multiplier, and finally the cable.  Through all of  
it, ZFS was running on the good disk with no errors, and finally  
succeeded in resilvering the bad one when it was working correctly.

bri

On Oct 7, 2008, at 11:31 AM, Dirk Schelfhout wrote:

> How did you find out it was the cable ?
> On 07 Oct 2008, at 15:34, Brian Pinkerton wrote:
>
>> I've had this kind of transient problem as well.  In my case, one of
>> the SATA cables inside my enclosure had gone bad (how the heck does
>> this happen?)  I replaced the cable, rebuilt the mirror, and ZFS was
>> happy.
>>
>> bri
>>
>> On Oct 7, 2008, at 6:27 AM, Christian He?mann wrote:
>>
>>> On 26.09.2008, at 10:26, Christian He?mann wrote:
>>>
>>>> Me again...
>>>>
>>>> After adding three more disks to my RAID.Z Pool (now 2x 3 disks) I
>>>> issued a scrub just to be sure everything is fine.
>>>>
>>>> Well, apparently it isn't, since zpool tells me I have chksum  
>>>> errors
>>>> (85% done, 5 errors so far) on one of the additional disks (which
>>>> isn't new and the smallest of all: old 3x 750GB, new  
>>>> 500/200/120GB).
>>>> That same disk worked fine as a standalone ZFS device until
>>>> yesterday,
>>>> with scrubs without any error messages.
>>>
>>> Well, seems I have to talk to myself, since no one wants to answer
>>> me. ;-)
>>>
>>> I decided not to replace the disk and didn't encounter any further
>>> problems (didn't do another scrub, though, due to the bug which
>>> causes
>>> the scrub to restart as soon as a snapshot is added - any chance for
>>> its fix to be implemented in OS X soon?).
>>>
>>> After a planned reboot and therefore new mount, I issued another
>>> scrub, with a nearly completely filled pool (~5GB available) and: no
>>> more chksum errors.
>>>
>>> I don't believe the blocks in question were all free during that
>>> time,
>>> so either the disk marked them as bad (don't think so) or it was
>>> not a
>>> sign for a bad disk, but some other problem - maybe during write/
>>> read,
>>> maybe due to the kernel panics the system had to endure, maybe one  
>>> of
>>> the ZFS bugs displaying chksum errors which aren't there...
>>>
>>> Anyway, if anyone is interested, my personal conclusion: Don't  
>>> Panic!
>>>
>>>
>>> Best regards,
>>>
>>> Christian
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From zfs at hessmann.de  Wed Oct  8 13:20:30 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Wed, 8 Oct 2008 22:20:30 +0200
Subject: [zfs-discuss] ZFS-formatted sparsebundle as home directory
Message-ID: <9EEDDEF6-A4CC-4DD3-97CE-E8D6CBFD9362@hessmann.de>

Hello everybody,


I'm thinking about moving my home directory to ZFS - just for the fun  
of it.
Since one of the reasons to get me acquainted with ZFS was to get rid  
of partitions and their typical problems (not enough space on one, too  
much on the other), I'm thinking about using a ZFS formatted  
sparsebundle on my HFS+ formatted root as my home directory.

I gave it a try with an AES encrypted ZFS sparsebundle on a test user,  
which seemed to work quite fine, except for the inconvenience of  
having to mount the image from a different user each time I reboot the  
computer.
One possible solution would be a login hook as documented by Mike  
Bombich, which, at least AFAIK, will prevent me from encrypt the  
sparsebundle - or does it?

So, first of all: Bad idea or good idea to go with a sparsebundle?
Second: Any chance to combine an encrypted ZFS-formatted sparsebundle  
with an automatic mount on login? Tamper with Filevault, for example?

Thanks.


Best regards,

Christian

From alex.blewitt at gmail.com  Wed Oct  8 15:23:52 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Wed, 8 Oct 2008 23:23:52 +0100
Subject: [zfs-discuss] ZFS-formatted sparsebundle as home directory
In-Reply-To: <9EEDDEF6-A4CC-4DD3-97CE-E8D6CBFD9362@hessmann.de>
References: <9EEDDEF6-A4CC-4DD3-97CE-E8D6CBFD9362@hessmann.de>
Message-ID: <BECFF64E-BF24-4A26-B7F7-D32B87974CBB@gmail.com>

Not a lot of point in a sparsebundle IMHO. You might as well create a  
file of a fixed size and use ZFS compression. You'lll still need to  
arrange a mount on boot but are less likely to suffer catastrophic  
failures that will render the entire volume ubnountable.

If you need more space, create another file-type disk and add it to  
the pool. You can then grow as needed.

Alex

Sent from my (new) iPhone

On 8 Oct 2008, at 21:20, Christian He?mann <zfs at hessmann.de> wrote:

> Hello everybody,
>
>
> I'm thinking about moving my home directory to ZFS - just for the fun
> of it.
> Since one of the reasons to get me acquainted with ZFS was to get rid
> of partitions and their typical problems (not enough space on one, too
> much on the other), I'm thinking about using a ZFS formatted
> sparsebundle on my HFS+ formatted root as my home directory.
>
> I gave it a try with an AES encrypted ZFS sparsebundle on a test user,
> which seemed to work quite fine, except for the inconvenience of
> having to mount the image from a different user each time I reboot the
> computer.
> One possible solution would be a login hook as documented by Mike
> Bombich, which, at least AFAIK, will prevent me from encrypt the
> sparsebundle - or does it?
>
> So, first of all: Bad idea or good idea to go with a sparsebundle?
> Second: Any chance to combine an encrypted ZFS-formatted sparsebundle
> with an automatic mount on login? Tamper with Filevault, for example?
>
> Thanks.
>
>
> Best regards,
>
> Christian
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From ndellofano at apple.com  Thu Oct  9 22:15:46 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu, 9 Oct 2008 22:15:46 -0700
Subject: [zfs-discuss] MSWord 2004 V11.5.0 (080429) Problem Saving Files
	to	ZFS Partition
In-Reply-To: <48E301D3.9010106@mac.com>
References: <48E301D3.9010106@mac.com>
Message-ID: <31B63464-00F1-4FEF-AEBE-5446E8E54E4A@apple.com>

Aric, you are running zfs-119 right?  We were seeing this issue with  
Text Edit and in Terminal in zfs-117 when saving and removing files.   
However that bug is fixed in 119 and it fixed the issue for everyone  
that was seeing it before.  Perhaps MSWord is using some weirdo  
encoding or something...

Noel

On Sep 30, 2008, at 9:51 PM, Aric Gregson wrote:

> I'm wondering if anyone has seen a similar problem with this version  
> of MSWord and saving files to ZFS partitions. I have not seen this  
> error previously, I can guess as to what is happening, but I do not  
> know of a work-around.
>
> Word will save a new file the first time without problems. However,  
> once you start editing the file and choose to save your changes or  
> if autosave tries to save the file, this dialog pops up. You can hit  
> OK, then choose to save the file manually. It will then ask if you  
> want to overwrite the existing file. You choose yes and this same  
> dialogue appears again. The changes to the file are actually saved  
> at this point. It is a serious inconvenience for the user here,  
> particularly when the Autosave feature makes you confront this  
> dialogue every 15 minutes.
>
> Any ideas? (other than using OpenOffice/NeoOffice which does not  
> have this problem).
>
> thanks, aric
> <wordDialogue.tiff>_______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From johnemac at tekserve.com  Sat Oct 11 07:17:29 2008
From: johnemac at tekserve.com (John McAdams)
Date: Sat, 11 Oct 2008 10:17:29 -0400
Subject: [zfs-discuss] ideal drive array
Message-ID: <FC439836-1D7B-4D22-94B5-8B620B772891@tekserve.com>

I have been trying out ZFS on 1-4 drive combinations and have been  
thrilled with it's capabilities. But now I want to move into 14-28  
drive configurations. I am still in the testing phase so I don't want  
to go enterprise, but it would be nice to have some kind of enclosure  
to keep the cable clutter to a minimum.

I've been looking at a variety of storage towers. Most of them offer  
raid controllers which I don't want. iSCSI looks promising, but again  
most products have raid and volume management tools built in.

Ideally the system would be 2-4 eSATA enclosures of 7-8 drives with a  
single port multiplier on the back. That would connect to a 4 port PCI  
card. At least that is what I have dreamed up. Currently I'm using USB  
which is a real tangle of wires and very slow.

Does anybody have a solution for getting 14-28 drives on a host over a  
fast bus?

Best regards,
--jmca
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081011/d94ba52d/attachment.html 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 2425 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081011/d94ba52d/attachment.bin 

From richmc at gmail.com  Sat Oct 11 11:42:02 2008
From: richmc at gmail.com (Richard McClellan)
Date: Sat, 11 Oct 2008 11:42:02 -0700
Subject: [zfs-discuss] ideal drive array
In-Reply-To: <FC439836-1D7B-4D22-94B5-8B620B772891@tekserve.com>
References: <FC439836-1D7B-4D22-94B5-8B620B772891@tekserve.com>
Message-ID: <D9F12AE4-52BA-4A17-BA71-3AB500DAD070@gmail.com>

Check out the storage products from Sun such as the J4200 which holds  
12 SATA or SAS disks.  Four J4200s can be daisy chained together.

		Rich

On Oct 11, 2008, at 07:17 , John McAdams wrote:

> I have been trying out ZFS on 1-4 drive combinations and have been  
> thrilled with it's capabilities. But now I want to move into 14-28  
> drive configurations. I am still in the testing phase so I don't  
> want to go enterprise, but it would be nice to have some kind of  
> enclosure to keep the cable clutter to a minimum.
>
> I've been looking at a variety of storage towers. Most of them offer  
> raid controllers which I don't want. iSCSI looks promising, but  
> again most products have raid and volume management tools built in.
>
> Ideally the system would be 2-4 eSATA enclosures of 7-8 drives with  
> a single port multiplier on the back. That would connect to a 4 port  
> PCI card. At least that is what I have dreamed up. Currently I'm  
> using USB which is a real tangle of wires and very slow.
>
> Does anybody have a solution for getting 14-28 drives on a host over  
> a fast bus?
>
> Best regards,
> --jmca
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081011/2daefc57/attachment.html 

From lopez.on.the.lists at yellowspace.net  Sat Oct 11 12:20:56 2008
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Sat, 11 Oct 2008 21:20:56 +0200
Subject: [zfs-discuss] ZFS-formatted sparsebundle as home directory
In-Reply-To: <BECFF64E-BF24-4A26-B7F7-D32B87974CBB@gmail.com>
References: <9EEDDEF6-A4CC-4DD3-97CE-E8D6CBFD9362@hessmann.de>
	<BECFF64E-BF24-4A26-B7F7-D32B87974CBB@gmail.com>
Message-ID: <BE2EC758-5E8B-4368-85CE-2B70EA4928E2@yellowspace.net>

I don't get the advantage of it right now... maybe it's
the hangover still.. but, for the records, I did the inverse:
made a real zfs partition on my disk, and created a zfs filesystem
for my home on which the sparseimage resides.

This has the advantage that I can use snapshots and
yet it integrates fine with filevault. You have to do it once
from another user, but it works fine: just copy your
sparseimage onto the zfs filesystem and symlink it:

ln -s /Volumes/YourPool/YourHomeOnIt/username.sparseimage / 
Users/.username/username.sparseimage

What also works fine for me, is incremental backups (with zfs send -i)
to a zfs-formatted sparseimage.  This is nice because so I
can share one time machine disk between a mac without zfs and one
with it, without wasting space and/or having to reformat it.

regards,

Lorenzo


On 09.10.2008, at 00:23, Alex Blewitt wrote:

> Not a lot of point in a sparsebundle IMHO. You might as well create a
> file of a fixed size and use ZFS compression. You'lll still need to
> arrange a mount on boot but are less likely to suffer catastrophic
> failures that will render the entire volume ubnountable.
>
> If you need more space, create another file-type disk and add it to
> the pool. You can then grow as needed.
>
> Alex
>
> Sent from my (new) iPhone
>
> On 8 Oct 2008, at 21:20, Christian He?mann <zfs at hessmann.de> wrote:
>
>> Hello everybody,
>>
>>
>> I'm thinking about moving my home directory to ZFS - just for the fun
>> of it.
>> Since one of the reasons to get me acquainted with ZFS was to get rid
>> of partitions and their typical problems (not enough space on one,  
>> too
>> much on the other), I'm thinking about using a ZFS formatted
>> sparsebundle on my HFS+ formatted root as my home directory.
>>
>> I gave it a try with an AES encrypted ZFS sparsebundle on a test  
>> user,
>> which seemed to work quite fine, except for the inconvenience of
>> having to mount the image from a different user each time I reboot  
>> the
>> computer.
>> One possible solution would be a login hook as documented by Mike
>> Bombich, which, at least AFAIK, will prevent me from encrypt the
>> sparsebundle - or does it?
>>
>> So, first of all: Bad idea or good idea to go with a sparsebundle?
>> Second: Any chance to combine an encrypted ZFS-formatted sparsebundle
>> with an automatic mount on login? Tamper with Filevault, for example?
>>
>> Thanks.
>>
>>
>> Best regards,
>>
>> Christian
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From abelian.group at yahoo.com  Sat Oct 11 13:36:31 2008
From: abelian.group at yahoo.com (Abelian Group)
Date: Sat, 11 Oct 2008 13:36:31 -0700 (PDT)
Subject: [zfs-discuss] Flash L2ARC and Separate SLOG
Message-ID: <745949.92184.qm@web59613.mail.ac4.yahoo.com>



    Anybody working on incorporating the Open Solaris Flash L2ARC code that Adam Leventhal is working on and Brendan Gregg discussed here:
    
     http://blogs.sun.com/brendan/entry/test

    Also, I tried to create separate mirrored ZILs ( Intent Logs) using:

           zpool create poolname device1 device2 device3 device4 log mirror device5 device6 mirror device7 device8

          It didn't work. Any suggestions. 

          This is a nice article concerning Sun's plans for Flash in its servers ( for those who haven't read about it). 

 http://mags.acm.org/communications/200807/?pg=49


          Would be interesting to get the L2ARC and ZIL running on mac. 

Thanks


      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081011/48327bf2/attachment.html 

From bplist at thinkpink.com  Sat Oct 11 15:48:41 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Sat, 11 Oct 2008 15:48:41 -0700
Subject: [zfs-discuss] ideal drive array
In-Reply-To: <FC439836-1D7B-4D22-94B5-8B620B772891@tekserve.com>
References: <FC439836-1D7B-4D22-94B5-8B620B772891@tekserve.com>
Message-ID: <7340C492-F5E8-4638-9507-8F408DA52575@thinkpink.com>

Check out http://www.burlystorage.com/   They have a number of port- 
multiplier SATA enclosures that work well with OS X and ZFS.  However,  
they typically run 3-4 disks per port multiplier, so you'd have to get  
another SATA controller to handle the number of disks you're  
envisioning.

I also like the WiebeTech RTX series -- they have enclosures that  
don't require trays -- just stick your disk in and go.  There's no  
rackmount version as of yet.

bri

On Oct 11, 2008, at 7:17 AM, John McAdams wrote:

> I have been trying out ZFS on 1-4 drive combinations and have been  
> thrilled with it's capabilities. But now I want to move into 14-28  
> drive configurations. I am still in the testing phase so I don't  
> want to go enterprise, but it would be nice to have some kind of  
> enclosure to keep the cable clutter to a minimum.
>
> I've been looking at a variety of storage towers. Most of them offer  
> raid controllers which I don't want. iSCSI looks promising, but  
> again most products have raid and volume management tools built in.
>
> Ideally the system would be 2-4 eSATA enclosures of 7-8 drives with  
> a single port multiplier on the back. That would connect to a 4 port  
> PCI card. At least that is what I have dreamed up. Currently I'm  
> using USB which is a real tangle of wires and very slow.
>
> Does anybody have a solution for getting 14-28 drives on a host over  
> a fast bus?
>
> Best regards,
> --jmca
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081011/289e650a/attachment-0001.html 

From gstasko at mac.com  Sat Oct 11 17:49:13 2008
From: gstasko at mac.com (Greg Stasko)
Date: Sat, 11 Oct 2008 19:49:13 -0500
Subject: [zfs-discuss] novice question - where is zfs targeted?
Message-ID: <DB27678F-034C-4CE8-9CE2-5A66E072C0E7@mac.com>

I'm a pretty knowledgeable mac user - started back with the 128k  
original mac. Have had various Mac desktops and laptops over the  
years. Actually worked for Apple for 10-ish years as an SE. But  
nowhere near anything like a sysadmin or such.

That being said, I'd like to pose a very basic question. Is ZFS  
envisioned being used in conjunction with external desktop drives, USB  
or Firewire, or is it targeted at the more "professional" parts of the  
mac community using SATA/eSATA drives? If it can be used with USB or  
Firewire, should the drives be virtually identical if used to create a  
pool? All USB, all 5400 or 7200 RPM? If someone has a somewhat random  
collection of USB and Firewire drives, can they be used to put  
together a "poor man's" ZFS implementation?

Can Time Machine still backup data off of a ZFS partition? I wasn't  
sure how to interpret this:

> Can I use ZFS with TM?
> Not currently, ZFS volumes are not currently browsable through Time  
> Machine
>
I read this as I could not format a drive with ZFS and use it as a TM  
"destination". Is that correct? Can an HFS-formatted drive backup data  
on a ZFS drive? Can you restore data to a ZFS drive from an HFS/TM  
drive?

Currently, I have a collection of external drives by various vendors  
connected to my dual proc G5 (hopefully soon to be a quad core iMac,  
whenever that happens). Firewire 400 and 800 and USB2.

The idea of not having to have a half a dozen external drives mounted  
in the Finder on my desktop as my storage needs expand would be GREAT!  
If I can just figure out exactly how to make it all "play" <grin>.

Thanks in advance,

Greg

From raoul at amsi.org.au  Sat Oct 11 19:20:05 2008
From: raoul at amsi.org.au (Raoul Callaghan)
Date: Sun, 12 Oct 2008 13:20:05 +1100
Subject: [zfs-discuss] AFP and deleting items...
In-Reply-To: <000301c8df57$8b0fdac0$a12f9040$@com>
References: <000301c8df57$8b0fdac0$a12f9040$@com>
Message-ID: <1A0FB32E-316E-4774-ADDB-F10FC0250844@amsi.org.au>

Hello,

Am running 10.5.5 Server (PPC) with ZFS 119.
I have a raidz logical volume (8 x 250GB physical) being shared via AFP.

I'm experiencing an issue via AFP where I cannot delete a folder if it  
contains items.
A Finder dialog box claims the folder is in use...

If the folder is empty, I can delete via AFP no problem.

Is this related to the same issue of files not being deleted by the  
Finder locally? or is this something new?

I searched the forums before posting this but couldn't find anything  
about deletion via AFP.

Regards,

Raoul Callaghan
I.T. Manager
Australian Mathematical Sciences Institute
111 Barry Street
The University of Melbourne
Victoria 3010 Australia
p: 03 8344 1783
f:  03 9349 4106
e: raoul at amsi.org.au

"... The only problem with Microsoft is they just have no taste, they  
have absolutely no taste and what that means is, and I don't mean that  
in a small way, I mean that in a big way.
In the sense that: they, they don't think of original ideas, and they  
don't bring much culture into their product.
And you say why is that important? well proportionally spaced fonts  
come from typesetting and beautiful books, that's where one gets the  
idea.  If it weren't for the mac they would never have that in their  
products.
And, so I guess I am saddened, not by Microsoft's success, I have no  
problem with their success. They've earned their success; for the most  
part.
I have a problem with the fact that they make just really third grade  
products ..."

Steve Jobs, circa 1980's
http://www.youtube.com/watch?v=oBISzVRmYIM

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081012/8453a6f0/attachment.html 

From zorg at sogeeky.net  Sat Oct 11 19:55:00 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Sat, 11 Oct 2008 19:55:00 -0700
Subject: [zfs-discuss] AFP and deleting items...
In-Reply-To: <1A0FB32E-316E-4774-ADDB-F10FC0250844@amsi.org.au>
References: <000301c8df57$8b0fdac0$a12f9040$@com>
	<1A0FB32E-316E-4774-ADDB-F10FC0250844@amsi.org.au>
Message-ID: <6C49D130-1D5C-4D18-B81C-6007C509F787@sogeeky.net>

I too have experienced this. I believe it's a known issue that zfs  
doesn't play 100% correctly with AFP.

On Oct 11, 2008, at 7:20 PM, Raoul Callaghan <raoul at amsi.org.au> wrote:

> Hello,
>
> Am running 10.5.5 Server (PPC) with ZFS 119.
> I have a raidz logical volume (8 x 250GB physical) being shared via  
> AFP.
>
> I'm experiencing an issue via AFP where I cannot delete a folder if  
> it contains items.
> A Finder dialog box claims the folder is in use...
>
> If the folder is empty, I can delete via AFP no problem.
>
> Is this related to the same issue of files not being deleted by the  
> Finder locally? or is this something new?
>
> I searched the forums before posting this but couldn't find anything  
> about deletion via AFP.
>
> Regards,
>
> Raoul Callaghan
> I.T. Manager
> Australian Mathematical Sciences Institute
> 111 Barry Street
> The University of Melbourne
> Victoria 3010 Australia
> p: 03 8344 1783
> f:  03 9349 4106
> e: raoul at amsi.org.au
>
> "... The only problem with Microsoft is they just have no taste,  
> they have absolutely no taste and what that means is, and I don't  
> mean that in a small way, I mean that in a big way.
> In the sense that: they, they don't think of original ideas, and  
> they don't bring much culture into their product.
> And you say why is that important? well proportionally spaced fonts  
> come from typesetting and beautiful books, that's where one gets the  
> idea.  If it weren't for the mac they would never have that in their  
> products.
> And, so I guess I am saddened, not by Microsoft's success, I have no  
> problem with their success. They've earned their success; for the  
> most part.
> I have a problem with the fact that they make just really third  
> grade products ..."
>
> Steve Jobs, circa 1980's
> http://www.youtube.com/watch?v=oBISzVRmYIM
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081011/1a8779ca/attachment-0001.html 

From lopez.on.the.lists at yellowspace.net  Sun Oct 12 08:20:24 2008
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Sun, 12 Oct 2008 17:20:24 +0200
Subject: [zfs-discuss] hungry for new bits...
Message-ID: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>

hi there @mothership,

it's sunday afternoon here, so why not use some spare minutes just to  
get on your nerves... :>

any new bits in sight? especially the problem with the volume names  
within Finder and other parts (sharing prefpane, time machine  
prefpane, etc...) throughout mac os x is really a show stopper... (I  
try to survive with icons, but they're not used in every place).. also  
the incompatibility (or sort of) with spotlight.
And yes, the fact that ZFS filesystems mounted on any other place than  
the /Volumes/Pool/Fs default do not appear _at all_ in the Finder  
(just with cmd-shift-G) is no fun... Oh, and while I'm here, to add  
some salt, let me exhume those case-sensitivity flamewars: that  
filesystem-specific flag is really a must, unfortunately. I keep on  
stumbling upon apps that I have to move to my HFS /Applications to get  
them to work... (afaik this issue is likely to be a show stopper also  
on a fileserver, for example for the network Applications share...but  
then again maybe the afp handles this, haven't tried it yet..)

ok, now I'm definitively on your blacklist and I better get some  
shelter before you get to Your mailbox ;)

Sincere regards for all the work done and shared so far,

Lorenzo


From byron at mac.com  Sun Oct 12 11:08:34 2008
From: byron at mac.com (Byron Servies)
Date: Sun, 12 Oct 2008 11:08:34 -0700
Subject: [zfs-discuss] hungry for new bits...
In-Reply-To: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>
References: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>
Message-ID: <FDECDA5D-7A55-4E0F-873E-C0890D413C68@mac.com>

On Oct 12, 2008, at 8:20 AM, Lorenzo Perone wrote:

> hi there @mothership,
>
> it's sunday afternoon here, so why not use some spare minutes just to
> get on your nerves... :>
>
> any new bits in sight?

While I do not work at Apple, I am a software engineer for a big  
company and I think I can put forward a plausible explanation for what  
is happening right now.

Let's assume that Mac OSX 10.6 will be available in January at  
Macworld, as most expect, and will include some form of ZFS.  Counting  
backwards from that date:

	January - 1 month of manufacturing + shipping = December
	December - 1 month of release candidate testing = November
	November - 1 month of feature freeze = October

Now, in truth, the "1 month" portion of these calculations is too  
short.  Nonetheless I suspect the entire Apple software organization  
is deeply focused on an internal deadline that is coming up very,  
very, quickly.

Having spent the past 3 months doing exactly the same thing, I  wish  
them good luck.  Don't forget sleep and exercise!

Byron

From geerds at bago.net  Sun Oct 12 11:16:36 2008
From: geerds at bago.net (Joergen Geerds)
Date: Sun, 12 Oct 2008 14:16:36 -0400
Subject: [zfs-discuss] data loss with ZFS as scratch disk
In-Reply-To: <D61A2215-F2E9-4865-8A37-53CC2CCB66F7@sogeeky.net>
References: <B1A6E1B2-9705-4A2D-96CF-3BB1F2A93F13@bago.net>
	<39CBC861-A2A0-4866-A3B1-039E0B6614FD@sogeeky.net>
	<E6CA0AFA-C8AC-429A-BE96-97CCF4DDF718@bago.net>
	<8A2A1846-5F5B-40CE-A57C-291B7D8BDB28@sogeeky.net>
	<BB4982DC-6385-4112-AE99-1BCDC294619F@bago.net>
	<010BED38-B836-4B21-95DC-83BF43A4F46F@bago.net>
	<D61A2215-F2E9-4865-8A37-53CC2CCB66F7@sogeeky.net>
Message-ID: <37E2FD3B-D122-4E2A-8308-910E9F417C4D@bago.net>

Any word if this issue was ever fixed/addressed or at least looked at  
recently?

Joergen Geerds
23-34 30th Drive
Astoria, NY 11102
+1.646.479.0977
mail at nypano.com
http://luminous-newyork.com



On Aug 4, 2008, at 23:09 , Mr. Zorg wrote:

> I know there were issues with mmap coherency at one point, but that  
> was supposedly fixed in zfs-111...  Needs another look perhaps, N?el?
>
> On Aug 4, 2008, at 7:26 PM, Joergen Geerds <geerds at bago.net> wrote:
>
>> Joost from PTgui told me that he's using mmap, mflush and munmap.  
>> This may be the reason why the problem only appears with PTGui.
>> I hope this helps a bit.
>>
>> Joergen Geerds
>> http://luminous-newyork.com
>> http://newyorkpanorama.com
>>
>> On Aug 4, 2008, at 15:12 , Joergen Geerds wrote:
>>
>>> I did another test, with 3 250GB disks, and created a clean,  
>>> working raidz set that reported 460GB space:
>>> pool: bigraid
>>> state: ONLINE
>>> scrub: none requested
>>> config:
>>>
>>> 	NAME         STATE     READ WRITE CKSUM
>>> 	bigraid      ONLINE       0     0     0
>>> 	  raidz1     ONLINE       0     0     0
>>> 	    disk3s2  ONLINE       0     0     0
>>> 	    disk2s2  ONLINE       0     0     0
>>> 	    disk1s2  ONLINE       0     0     0
>>>
>>> errors: No known data errors
>>>
>>> and yet, PTgui (using the ZFS as temp folder) still has some  
>>> serious issues with data corruption when writing the PSB from it's  
>>> temp files:
>>> http://nypano.com/clients/ptgui8b6-scratchdisk-error-2.jpg
>>>
>>> photoshop, on the other hand, has no issues using it for temp files.
>>>
>>> any ideas? are there any non-kosher ways to read/write files to a  
>>> ZFS that could destroy data along the way?
>>>
>>> Joergen Geerds
>>> http://luminous-newyork.com
>>> http://newyorkpanorama.com
>>>
>>> On Aug 4, 2008, at 13:43 , Mr. Zorg wrote:
>>>
>>>> That would be good, yes. It is very much like raid5. You CAN use  
>>>> two disks, it's just pointless.
>>>>
>>>> On Aug 4, 2008, at 10:35 AM, Joergen Geerds <geerds at bago.net>  
>>>> wrote:
>>>>
>>>>> I added the history of the pool on the bottom. after the first  
>>>>> setup gave me only 230GB, I properly destroyed the pool, and  
>>>>> redid it with one disk, and then adding the second to create a  
>>>>> raid (i do assume that it would do some sort of stripping). if  
>>>>> this is not the proper way to go, then the FAQ needs some work  
>>>>> (i.e. "you can't build a raid from 2 disks"). maybe the FAQ  
>>>>> should say that the raidz is something like a RAID5, not a  
>>>>> RAID0, and therefore needs 3 disks minimum.
>>>>>
>>>>> can anyone shed a bit more light on this?
>>>>>
>>>>> Joergen Geerds
>>>>> http://luminous-newyork.com
>>>>> http://newyorkpanorama.com
>>>>>
>>>>> On Aug 4, 2008, at 12:03 , Mr. Zorg wrote:
>>>>>
>>>>>> 230GB is correct. In a raidz configuration you only get n-1  
>>>>>> disks worth of storage since one disk is reserved for parity.  
>>>>>> It really doesn't make sense to use less than three disks for  
>>>>>> raidz, with just two you may as well use a mirror configuration.
>>>>>>
>>>>>> It is possible that the corruption is a result of you adding  
>>>>>> disk2s2 to the pool a second time. It may have been using it  
>>>>>> both as a data disk and a parity disk resulting in scrambled  
>>>>>> data. Of course, it should not have allowed that.
>>>>>>
>>>>>> I don't have any spare disks, can anyone else verify this  
>>>>>> behavior?
>>>>>>
>>>>>> On Aug 4, 2008, at 8:09 AM, Joergen Geerds <geerds at bago.net>  
>>>>>> wrote:
>>>>>>
>>>>>>> I installed the latest build of ZFS (July 16, 2008), and  
>>>>>>> partinioned my external FW800 case (2 sata disks) as a ZFS  
>>>>>>> raid pool.
>>>>>>>
>>>>>>> First bug:
>>>>>>> On my first attempt, I did follow the "Getting started" steps:
>>>>>>> zpool create bigraid raidz disk1s2 disk2s2
>>>>>>>
>>>>>>> This resulted in a raidz pool that was only reporting 230GB  
>>>>>>> instead of 460GB to the OS (but zpool iostat reported 460GB in  
>>>>>>> size)
>>>>>>> So I redid the setup, created the pool with just one disk, and  
>>>>>>> then "zpool add bigraid raidz  disk2s2"
>>>>>>> This gave me a pool with 460 GB. It would be great if you  
>>>>>>> could look into this.
>>>>>>>
>>>>>>> Second bug:
>>>>>>> photoshop seems to love the additional speed of the ZFS  
>>>>>>> scratch disk, it felt really faster than usual (no hard data  
>>>>>>> to back it up) and no problems.
>>>>>>> So I decided to do a quick test with PTgui 8b6 (using the ZFS  
>>>>>>> bigraid as PTgui temp folder, but reading/writing all input/ 
>>>>>>> output from the internal HFS+). The output is a layered PSB  
>>>>>>> file of about 1GB in this case. The PSB was shredded.
>>>>>>> See http://nypano.com/clients/ptgui8b6-scratchdisk-error.jpg
>>>>>>> I was able to verify that it is indeed the ZFS scratchdisk  
>>>>>>> that is the culprit (moving the ptgui temp folder to the  
>>>>>>> internal disk results in normal output, but PTgui 7.8 with the  
>>>>>>> ZFS as temp folder shreds the PSB also).
>>>>>>>
>>>>>>> Joost from PTgui claims that system & ZFS is at fault, not  
>>>>>>> PTgui. Any ideas what went wrong?
>>>>>>>
>>>>>>>
>>>>>>> System info:
>>>>>>> Imac 2.16GHz, 10.5.4, 3GB, 500GB internal HFS+, 2x250GB ext  
>>>>>>> FW800
>>>>>>>
>>>>>>> pool: bigraid
>>>>>>> state: ONLINE
>>>>>>> scrub: none requested
>>>>>>> config:
>>>>>>>
>>>>>>> 	NAME        STATE     READ WRITE CKSUM
>>>>>>> 	bigraid     ONLINE       0     0     0
>>>>>>> 	  disk1s2   ONLINE       0     0     0
>>>>>>> 	  disk2s2   ONLINE       0     0     0
>>>>>>>
>>>>>>> errors: No known data errors
>>>>>>>
>>>>>>> History for 'bigraid':
>>>>>>> 2008-08-03.18:42:02 zpool create bigraid /dev/disk1s2
>>>>>>> 2008-08-03.18:46:03 zpool upgrade bigraid 8
>>>>>>> 2008-08-03.18:47:10 zpool add bigraid disk2s2
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> Joergen Geerds
>>>>>>> http://luminous-newyork.com
>>>>>>> http://newyorkpanorama.com
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> zfs-discuss mailing list
>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> /div>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081012/1dff9741/attachment-0001.html 

From bwaters at nrao.edu  Sun Oct 12 14:03:55 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Sun, 12 Oct 2008 15:03:55 -0600
Subject: [zfs-discuss] hungry for new bits...
In-Reply-To: <FDECDA5D-7A55-4E0F-873E-C0890D413C68@mac.com>
References: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>
	<FDECDA5D-7A55-4E0F-873E-C0890D413C68@mac.com>
Message-ID: <59D9F21C-BBB1-4942-9642-A41C23F0FDD4@nrao.edu>


On Oct 12, 2008, at 12:08 PM, Byron Servies wrote:

> Having spent the past 3 months doing exactly the same thing, I  wish
> them good luck.  Don't forget sleep and exercise!


Oh, man. What you said. I completely agree with that estimate!

And as I understand, you could compile the bits yourself, as the  
source code is posted on the site.



From ndellofano at apple.com  Tue Oct 14 18:17:15 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue, 14 Oct 2008 18:17:15 -0700
Subject: [zfs-discuss] hungry for new bits...
In-Reply-To: <59D9F21C-BBB1-4942-9642-A41C23F0FDD4@nrao.edu>
References: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>
	<FDECDA5D-7A55-4E0F-873E-C0890D413C68@mac.com>
	<59D9F21C-BBB1-4942-9642-A41C23F0FDD4@nrao.edu>
Message-ID: <819D142E-D01C-47B7-A596-EB0765EB747D@apple.com>

Hey guys!

Yeah, so the whole schedule ramping up party you allude to is  
happening, hence my slowness on the email of late.  Sorry.

So on to some answers for you.  So I was holding releasing new bits  
until we sync with the new Solaris base, which will have those tasty  
case insensitivity options you hunger for.  Problem being, it's not  
stable, and we're still debugging it.  So I didn't want to release  
shady bits to everyone, hence I haven't released any.  I do have a few  
fixes in line for you for some issues.  The trash emptying for example  
is fixed, the wacky names of filesystems over AFP and NFS in Finder  
we've got fixes for, and a few others.  Spotlight, I hear you, we are  
working on.  Finder issues, also being worked on.  Many issues involve  
more then just the ZFS code to change (opting in to preference panes  
and such) so fixed bits I get to you still may not do exactly what you  
want as other parts of the system has to be adjusted as well.

and never fear, telling my what drives you nuts is good, it helps me  
know what's broken and what you guys use so I can fix it :) So info is  
good and I promise to not black list you :)


Noel

On Oct 12, 2008, at 2:03 PM, Boyd Waters wrote:

>>> hi there @mothership,
>>>
>>> it's sunday afternoon here, so why not use some spare minutes just  
>>> to
>>> get on your nerves... :>
>>>
>>> any new bits in sight? especially the problem with the volume names
>>> within Finder and other parts (sharing prefpane, time machine
>>> prefpane, etc...) throughout mac os x is really a show stopper... (I
>>> try to survive with icons, but they're not used in every place)..  
>>> also
>>> the incompatibility (or sort of) with spotlight.
>>> And yes, the fact that ZFS filesystems mounted on any other place  
>>> than
>>> the /Volumes/Pool/Fs default do not appear _at all_ in the Finder
>>> (just with cmd-shift-G) is no fun... Oh, and while I'm here, to add
>>> some salt, let me exhume those case-sensitivity flamewars: that
>>> filesystem-specific flag is really a must, unfortunately. I keep on
>>> stumbling upon apps that I have to move to my HFS /Applications to  
>>> get
>>> them to work... (afaik this issue is likely to be a show stopper  
>>> also
>>> on a fileserver, for example for the network Applications  
>>> share...but
>>> then again maybe the afp handles this, haven't tried it yet..)
>>>
>>> ok, now I'm definitively on your blacklist and I better get some
>>> shelter before you get to Your mailbox ;)
>>>
>>> Sincere regards for all the work done and shared so far,
>>>
>>> Lorenzo
>>>
> _______________________________________________
> On Oct 12, 2008, at 12:08 PM, Byron Servies wrote:
>
>> Having spent the past 3 months doing exactly the same thing, I  wish
>> them good luck.  Don't forget sleep and exercise!
>
>
> Oh, man. What you said. I completely agree with that estimate!
>
> And as I understand, you could compile the bits yourself, as the
> source code is posted on the site.
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From jason at jasonrm.net  Tue Oct 14 19:29:12 2008
From: jason at jasonrm.net (Jason Richard McNeil)
Date: Tue, 14 Oct 2008 19:29:12 -0700
Subject: [zfs-discuss] hungry for new bits...
In-Reply-To: <819D142E-D01C-47B7-A596-EB0765EB747D@apple.com>
References: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>
	<FDECDA5D-7A55-4E0F-873E-C0890D413C68@mac.com>
	<59D9F21C-BBB1-4942-9642-A41C23F0FDD4@nrao.edu>
	<819D142E-D01C-47B7-A596-EB0765EB747D@apple.com>
Message-ID: <3D24322D-9CC8-4B91-8896-8ED11D955F73@mac.com>

No?l, thank you very much for what you do. I know I am looking forward  
to more bits as I have a new MacBook Pro on the way as of today and I  
would very much like to start off with a ZFS home. :)

I have had some issues with a multi drive pool on my mac pro thou that  
I'll follow up with later tonight.

Sorry for shortness, back to Spanish class, ;)

jrm
jason at jasonrm.net

On Oct 14, 2008, at 6:17 PM, No?l Dellofano <ndellofano at apple.com>  
wrote:

> Hey guys!
>
> Yeah, so the whole schedule ramping up party you allude to is
> happening, hence my slowness on the email of late.  Sorry.
>
> So on to some answers for you.  So I was holding releasing new bits
> until we sync with the new Solaris base, which will have those tasty
> case insensitivity options you hunger for.  Problem being, it's not
> stable, and we're still debugging it.  So I didn't want to release
> shady bits to everyone, hence I haven't released any.  I do have a few
> fixes in line for you for some issues.  The trash emptying for example
> is fixed, the wacky names of filesystems over AFP and NFS in Finder
> we've got fixes for, and a few others.  Spotlight, I hear you, we are
> working on.  Finder issues, also being worked on.  Many issues involve
> more then just the ZFS code to change (opting in to preference panes
> and such) so fixed bits I get to you still may not do exactly what you
> want as other parts of the system has to be adjusted as well.
>
> and never fear, telling my what drives you nuts is good, it helps me
> know what's broken and what you guys use so I can fix it :) So info is
> good and I promise to not black list you :)
>
>
> Noel
>
> On Oct 12, 2008, at 2:03 PM, Boyd Waters wrote:
>
>>>> hi there @mothership,
>>>>
>>>> it's sunday afternoon here, so why not use some spare minutes just
>>>> to
>>>> get on your nerves... :>
>>>>
>>>> any new bits in sight? especially the problem with the volume names
>>>> within Finder and other parts (sharing prefpane, time machine
>>>> prefpane, etc...) throughout mac os x is really a show stopper...  
>>>> (I
>>>> try to survive with icons, but they're not used in every place)..
>>>> also
>>>> the incompatibility (or sort of) with spotlight.
>>>> And yes, the fact that ZFS filesystems mounted on any other place
>>>> than
>>>> the /Volumes/Pool/Fs default do not appear _at all_ in the Finder
>>>> (just with cmd-shift-G) is no fun... Oh, and while I'm here, to add
>>>> some salt, let me exhume those case-sensitivity flamewars: that
>>>> filesystem-specific flag is really a must, unfortunately. I keep on
>>>> stumbling upon apps that I have to move to my HFS /Applications to
>>>> get
>>>> them to work... (afaik this issue is likely to be a show stopper
>>>> also
>>>> on a fileserver, for example for the network Applications
>>>> share...but
>>>> then again maybe the afp handles this, haven't tried it yet..)
>>>>
>>>> ok, now I'm definitively on your blacklist and I better get some
>>>> shelter before you get to Your mailbox ;)
>>>>
>>>> Sincere regards for all the work done and shared so far,
>>>>
>>>> Lorenzo
>>>>
>> _______________________________________________
>> On Oct 12, 2008, at 12:08 PM, Byron Servies wrote:
>>
>>> Having spent the past 3 months doing exactly the same thing, I  wish
>>> them good luck.  Don't forget sleep and exercise!
>>
>>
>> Oh, man. What you said. I completely agree with that estimate!
>>
>> And as I understand, you could compile the bits yourself, as the
>> source code is posted on the site.
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From jason at jasonrm.net  Tue Oct 14 22:40:00 2008
From: jason at jasonrm.net (Jason Richard McNeil)
Date: Tue, 14 Oct 2008 22:40:00 -0700
Subject: [zfs-discuss] Faulted Drives,
	fixed by export/import... but what causes it?
Message-ID: <D25D225A-09E0-47E7-A3A2-733A778D1500@jasonrm.net>

As I mentioned in my previous email, I've only had one issue outside  
of the well known issues on the wiki.

I've held off on this one for a bit thinking that it might just be me  
and doing something wrong, but it's happening enough that it is, well  
frankly starting to scare me. I think that it only happens when I  
either restart or cold boot after a while. I have almost constant read/ 
write to the pool, and I run scrubs rather frequently, but unless I'm  
recovering from the following issues, I never have had an error.

General system info:
Mac Pro , 2.66GHz CPU , 6GB MEM
2 - 500GB Internal Drives (not part of the pool, RAID1)
2 - 500GB Internal Drives
4 - 640GB External Drives in a FirmTek SeriTek/5PM enclosure connected  
to a Highpoint RocketRAID 2314

Cold boot. I'm quite certain that all the external drives are powered  
up and spinning before I turn on my system, and here is the symptom  
that I see.

   pool: Indigo
  state: DEGRADED
status: One or more devices could not be used because the label is  
missing or
	invalid.  Sufficient replicas exist for the pool to continue
	functioning in a degraded state.
action: Replace the device using 'zpool replace'.
    see: http://www.sun.com/msg/ZFS-8000-4J
  scrub: resilver completed with 0 errors on Sat Oct 11 01:30:46 2008
config:

	NAME                      STATE     READ WRITE CKSUM
	Indigo                    DEGRADED     0     0     0
	  mirror                  DEGRADED     0     0     0
	    4541104571258798197   FAULTED      0     0     0  was /dev/disk5s2
	    disk6s2               ONLINE       0     0     0
	  mirror                  DEGRADED     0     0     0
	    disk5s2               ONLINE       0     0     0
	    18432184791024237345  FAULTED      0     0     0  was /dev/disk6s2
	  mirror                  ONLINE       0     0     0
	    disk0s2               ONLINE       0     0     0
	    disk3s2               ONLINE       0     0     0

errors: No known data errors

I am wondering if this is either an issue that is with how the  
HighPoint controller is working, or some larger issue.

The above was one of the first times that it has "misplaced" two  
drives, as usually it's only one that it is unable to find.
When it has been only one I have tried various forms of zfs replace  
(sudo, unmounting, force-unmounting). It returns an error saying the  
destination drive is already in use.

Best solution I have found so far, is to log off all users, SSH in and  
force a unmount, do an export, then import and it has come back every  
time so far. I do have to do a scrub, but it only finds checksum  
errors, which, considering that it comes up without a drive and  
activity happens while missing a drive or two, it makes sense to me so  
far.

Any ideas of what to try?

Also, I don't have auto-login set, but I do have apache2 using a  
directory of one of the mountpoints as the document root. The odd  
thing is, that sometimes I will reboot and no issues at all, so I  
haven't found an exact cause yet. The HighPoint controller does  
consider the drives to be volumes, that I then "export" each drive  
volume from the controller as JBOD (total of 4 volumes, one per disk).  
Only based on the fact that the expect drive ID's are high enough to  
be on the controller, I'm suspecting some sort of delay there...
If no one else is seeing this, then maybe it is just a odd combo of  
drives, controller, DISK->JBOD->slices, idk, something funny.

Sorry for writing a novel, but I'm try to error on the side of too  
much info... of course, please let me know what most basic I've  
forgotten to include. ;)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081014/6fe373d7/attachment.html 

From richmc at gmail.com  Tue Oct 14 23:14:20 2008
From: richmc at gmail.com (Rich McClellan)
Date: Tue, 14 Oct 2008 23:14:20 -0700
Subject: [zfs-discuss] Faulted Drives,
	fixed by export/import... but what causes it?
In-Reply-To: <D25D225A-09E0-47E7-A3A2-733A778D1500@jasonrm.net>
References: <D25D225A-09E0-47E7-A3A2-733A778D1500@jasonrm.net>
Message-ID: <203e36e30810142314r1d309a0fg4bfa985d60d9ec58@mail.gmail.com>

I've seen this behavior too.  I'm using a set of disks that are iSCSI
targets (multiple LUNs per target).

It smells like a race condition and it may have nothing to do with ZFS.

Rich

On Tue, Oct 14, 2008 at 10:40 PM, Jason Richard McNeil
<jason at jasonrm.net> wrote:
> As I mentioned in my previous email, I've only had one issue outside of the
> well known issues on the wiki.
> I've held off on this one for a bit thinking that it might just be me and
> doing something wrong, but it's happening enough that it is, well frankly
> starting to scare me. I think that it only happens when I either restart or
> cold boot after a while. I have almost constant read/write to the pool, and
> I run scrubs rather frequently, but unless I'm recovering from the following
> issues, I never have had an error.
> General system info:
> Mac Pro , 2.66GHz CPU , 6GB MEM
> 2 - 500GB Internal Drives (not part of the pool, RAID1)
> 2 - 500GB Internal Drives
> 4 - 640GB External Drives in a FirmTek SeriTek/5PM enclosure connected to a
> Highpoint RocketRAID 2314
> Cold boot. I'm quite certain that all the external drives are powered up and
> spinning before I turn on my system, and here is the symptom that I see.
>   pool: Indigo
>  state: DEGRADED
> status: One or more devices could not be used because the label is missing
> or
> invalid.  Sufficient replicas exist for the pool to continue
> functioning in a degraded state.
> action: Replace the device using 'zpool replace'.
>    see: http://www.sun.com/msg/ZFS-8000-4J
>  scrub: resilver completed with 0 errors on Sat Oct 11 01:30:46 2008
> config:
> NAME                      STATE     READ WRITE CKSUM
> Indigo                    DEGRADED     0     0     0
>  mirror                  DEGRADED     0     0     0
>    4541104571258798197   FAULTED      0     0     0  was /dev/disk5s2
>    disk6s2               ONLINE       0     0     0
>  mirror                  DEGRADED     0     0     0
>    disk5s2               ONLINE       0     0     0
>    18432184791024237345  FAULTED      0     0     0  was /dev/disk6s2
>  mirror                  ONLINE       0     0     0
>    disk0s2               ONLINE       0     0     0
>    disk3s2               ONLINE       0     0     0
> errors: No known data errors
> I am wondering if this is either an issue that is with how the HighPoint
> controller is working, or some larger issue.
> The above was one of the first times that it has "misplaced" two drives, as
> usually it's only one that it is unable to find.
> When it has been only one I have tried various forms of zfs replace (sudo,
> unmounting, force-unmounting). It returns an error saying the destination
> drive is already in use.
> Best solution I have found so far, is to log off all users, SSH in and force
> a unmount, do an export, then import and it has come back every time so far.
> I do have to do a scrub, but it only finds checksum errors, which,
> considering that it comes up without a drive and activity happens while
> missing a drive or two, it makes sense to me so far.
> Any ideas of what to try?
> Also, I don't have auto-login set, but I do have apache2 using a directory
> of one of the mountpoints as the document root. The odd thing is, that
> sometimes I will reboot and no issues at all, so I haven't found an exact
> cause yet. The HighPoint controller does consider the drives to be volumes,
> that I then "export" each drive volume from the controller as JBOD (total of
> 4 volumes, one per disk). Only based on the fact that the expect drive ID's
> are high enough to be on the controller, I'm suspecting some sort of delay
> there...
> If no one else is seeing this, then maybe it is just a odd combo of drives,
> controller, DISK->JBOD->slices, idk, something funny.
> Sorry for writing a novel, but I'm try to error on the side of too much
> info... of course, please let me know what most basic I've forgotten to
> include. ;)
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>

From james-zfsosx at jrv.org  Wed Oct 15 01:56:38 2008
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Wed, 15 Oct 2008 03:56:38 -0500
Subject: [zfs-discuss] Faulted Drives,
 fixed by export/import... but what causes it?
In-Reply-To: <D25D225A-09E0-47E7-A3A2-733A778D1500@jasonrm.net>
References: <D25D225A-09E0-47E7-A3A2-733A778D1500@jasonrm.net>
Message-ID: <48F5B046.7070005@jrv.org>

Jason Richard McNeil wrote:
> ...
> I think that it only happens when I either restart or cold boot after
> a while.
> NAME                      STATE     READ WRITE CKSUM
> Indigo                    DEGRADED     0     0     0
>  mirror                  DEGRADED     0     0     0
>    4541104571258798197   FAULTED      0     0     0  was /dev/disk5s2
>    disk6s2               ONLINE       0     0     0
>  mirror                  DEGRADED     0     0     0
>    disk5s2               ONLINE       0     0     0
>    18432184791024237345  FAULTED      0     0     0  was /dev/disk6s2
>  mirror                  ONLINE       0     0     0
>    disk0s2               ONLINE       0     0     0
>    disk3s2               ONLINE       0     0     0
>
>
> Best solution I have found so far, is to log off all users, SSH in and
> force a unmount, do an export, then import and it has come back every
> time so far.

I would guess that when the disks are connected they are detected and
scanned for mountable filesystems one by one.  As soon as
disk{6,5,0,3}s2 are ready the pool can be brought online, and if the
other two disks aren't detected yet then those vdevs are faulted.  It
may be that the OSX ZFS port needs to wait or probe more vigorously when
a pool is found but some vdev members are missing.

To my experience it always works to export the pool and then import it. 
ZFS will resilver but for raidz this is very quick.

> I do have to do a scrub, but it only finds checksum errors, which,
> considering that it comes up without a drive and activity happens
> while missing a drive or two, it makes sense to me so far.


I suspect these checksum errors to be completely unrelated to the issue
above and worthy of a great deal of concern.  Checksum errors do *not*
make sense in such a scenario: there should be *no* checksum errors on
either the online or faulted disks as a result of a vdev member not
being present and faulted when the pool is brought online.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081015/9f791833/attachment.html 

From canadrian at electricteaparty.net  Wed Oct 15 09:00:13 2008
From: canadrian at electricteaparty.net (Adrian Thornton)
Date: Wed, 15 Oct 2008 10:00:13 -0600
Subject: [zfs-discuss] hungry for new bits...
In-Reply-To: <3D24322D-9CC8-4B91-8896-8ED11D955F73@mac.com>
References: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>
	<FDECDA5D-7A55-4E0F-873E-C0890D413C68@mac.com>
	<59D9F21C-BBB1-4942-9642-A41C23F0FDD4@nrao.edu>
	<819D142E-D01C-47B7-A596-EB0765EB747D@apple.com>
	<3D24322D-9CC8-4B91-8896-8ED11D955F73@mac.com>
Message-ID: <72feba2a0810150900r5f4d04b4wa2424abbc13a6c0f@mail.gmail.com>

 Yes, No?l, I think none of us can thank you enough for all all your hard
work on this project. I know it's been said before, but it deserves saying
again. I think OS X and ZFS are about the two most exciting aspects of
computing today, and the fact that they're being put together is all down to
the amazing work by you and your team. Thanks so much!

- Adrian

On Tue, 2008-10-14 at 19:29 -0700, Jason Richard McNeil wrote:

No?l, thank you very much for what you do. I know I am looking forward
to more bits as I have a new MacBook Pro on the way as of today and I
would very much like to start off with a ZFS home. :)

I have had some issues with a multi drive pool on my mac pro thou that
I'll follow up with later tonight.

Sorry for shortness, back to Spanish class, ;)

jrmjason at jasonrm.net

On Oct 14, 2008, at 6:17 PM, No?l Dellofano <ndellofano at apple.com>
wrote:

> Hey guys!
>
> Yeah, so the whole schedule ramping up party you allude to is
> happening, hence my slowness on the email of late.  Sorry.
>
> So on to some answers for you.  So I was holding releasing new bits
> until we sync with the new Solaris base, which will have those tasty
> case insensitivity options you hunger for.  Problem being, it's not
> stable, and we're still debugging it.  So I didn't want to release
> shady bits to everyone, hence I haven't released any.  I do have a few
> fixes in line for you for some issues.  The trash emptying for example
> is fixed, the wacky names of filesystems over AFP and NFS in Finder
> we've got fixes for, and a few others.  Spotlight, I hear you, we are
> working on.  Finder issues, also being worked on.  Many issues involve
> more then just the ZFS code to change (opting in to preference panes
> and such) so fixed bits I get to you still may not do exactly what you
> want as other parts of the system has to be adjusted as well.
>
> and never fear, telling my what drives you nuts is good, it helps me
> know what's broken and what you guys use so I can fix it :) So info is
> good and I promise to not black list you :)
>
>
> Noel
>
> On Oct 12, 2008, at 2:03 PM, Boyd Waters wrote:
>
>>>> hi there @mothership,
>>>>
>>>> it's sunday afternoon here, so why not use some spare minutes just
>>>> to
>>>> get on your nerves... :>
>>>>
>>>> any new bits in sight? especially the problem with the volume names
>>>> within Finder and other parts (sharing prefpane, time machine
>>>> prefpane, etc...) throughout mac os x is really a show stopper...
>>>> (I
>>>> try to survive with icons, but they're not used in every place)..
>>>> also
>>>> the incompatibility (or sort of) with spotlight.
>>>> And yes, the fact that ZFS filesystems mounted on any other place
>>>> than
>>>> the /Volumes/Pool/Fs default do not appear _at all_ in the Finder
>>>> (just with cmd-shift-G) is no fun... Oh, and while I'm here, to add
>>>> some salt, let me exhume those case-sensitivity flamewars: that
>>>> filesystem-specific flag is really a must, unfortunately. I keep on
>>>> stumbling upon apps that I have to move to my HFS /Applications to
>>>> get
>>>> them to work... (afaik this issue is likely to be a show stopper
>>>> also
>>>> on a fileserver, for example for the network Applications
>>>> share...but
>>>> then again maybe the afp handles this, haven't tried it yet..)
>>>>
>>>> ok, now I'm definitively on your blacklist and I better get some
>>>> shelter before you get to Your mailbox ;)
>>>>
>>>> Sincere regards for all the work done and shared so far,
>>>>
>>>> Lorenzo
>>>>
>> _______________________________________________
>> On Oct 12, 2008, at 12:08 PM, Byron Servies wrote:
>>
>>> Having spent the past 3 months doing exactly the same thing, I  wish
>>> them good luck.  Don't forget sleep and exercise!
>>
>>
>> Oh, man. What you said. I completely agree with that estimate!
>>
>> And as I understand, you could compile the bits yourself, as the
>> source code is posted on the site.
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
_______________________________________________
zfs-discuss mailing
listzfs-discuss at lists.macosforge.orghttp://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081015/966336ab/attachment.html 

From franzschmalzl at spamfreemail.de  Wed Oct 15 09:07:50 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Wed, 15 Oct 2008 18:07:50 +0200
Subject: [zfs-discuss] hungry for new bits...
In-Reply-To: <72feba2a0810150900r5f4d04b4wa2424abbc13a6c0f@mail.gmail.com>
References: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>
	<FDECDA5D-7A55-4E0F-873E-C0890D413C68@mac.com>
	<59D9F21C-BBB1-4942-9642-A41C23F0FDD4@nrao.edu>
	<819D142E-D01C-47B7-A596-EB0765EB747D@apple.com>
	<3D24322D-9CC8-4B91-8896-8ED11D955F73@mac.com>
	<72feba2a0810150900r5f4d04b4wa2424abbc13a6c0f@mail.gmail.com>
Message-ID: <56772C58-F3F0-4B36-AAD7-FF2774FCC16A@spamfreemail.de>

das kann ich nur unterschreiben...


On 15.10.2008, at 18:00, Adrian Thornton wrote:

> Yes, No?l, I think none of us can thank you enough for all all your  
> hard work on this project. I know it's been said before, but it  
> deserves saying again. I think OS X and ZFS are about the two most  
> exciting aspects of computing today, and the fact that they're being  
> put together is all down to the amazing work by you and your team.  
> Thanks so much!
>
> - Adrian
>
> On Tue, 2008-10-14 at 19:29 -0700, Jason Richard McNeil wrote:
>>
>> No?l, thank you very much for what you do. I know I am looking  
>> forward
>> to more bits as I have a new MacBook Pro on the way as of today and I
>> would very much like to start off with a ZFS home. :)
>>
>> I have had some issues with a multi drive pool on my mac pro thou  
>> that
>> I'll follow up with later tonight.
>>
>> Sorry for shortness, back to Spanish class, ;)
>>
>> jrm
>> jason at jasonrm.net
>>
>> On Oct 14, 2008, at 6:17 PM, No?l Dellofano <ndellofano at apple.com>
>> wrote:
>>
>> > Hey guys!
>> >
>> > Yeah, so the whole schedule ramping up party you allude to is
>> > happening, hence my slowness on the email of late.  Sorry.
>> >
>> > So on to some answers for you.  So I was holding releasing new bits
>> > until we sync with the new Solaris base, which will have those  
>> tasty
>> > case insensitivity options you hunger for.  Problem being, it's not
>> > stable, and we're still debugging it.  So I didn't want to release
>> > shady bits to everyone, hence I haven't released any.  I do have  
>> a few
>> > fixes in line for you for some issues.  The trash emptying for  
>> example
>> > is fixed, the wacky names of filesystems over AFP and NFS in Finder
>> > we've got fixes for, and a few others.  Spotlight, I hear you, we  
>> are
>> > working on.  Finder issues, also being worked on.  Many issues  
>> involve
>> > more then just the ZFS code to change (opting in to preference  
>> panes
>> > and such) so fixed bits I get to you still may not do exactly  
>> what you
>> > want as other parts of the system has to be adjusted as well.
>> >
>> > and never fear, telling my what drives you nuts is good, it helps  
>> me
>> > know what's broken and what you guys use so I can fix it :) So  
>> info is
>> > good and I promise to not black list you :)
>> >
>> >
>> > Noel
>> >
>> > On Oct 12, 2008, at 2:03 PM, Boyd Waters wrote:
>> >
>> >>>> hi there @mothership,
>> >>>>
>> >>>> it's sunday afternoon here, so why not use some spare minutes  
>> just
>> >>>> to
>> >>>> get on your nerves... :>
>> >>>>
>> >>>> any new bits in sight? especially the problem with the volume  
>> names
>> >>>> within Finder and other parts (sharing prefpane, time machine
>> >>>> prefpane, etc...) throughout mac os x is really a show  
>> stopper...
>> >>>> (I
>> >>>> try to survive with icons, but they're not used in every  
>> place)..
>> >>>> also
>> >>>> the incompatibility (or sort of) with spotlight.
>> >>>> And yes, the fact that ZFS filesystems mounted on any other  
>> place
>> >>>> than
>> >>>> the /Volumes/Pool/Fs default do not appear _at all_ in the  
>> Finder
>> >>>> (just with cmd-shift-G) is no fun... Oh, and while I'm here,  
>> to add
>> >>>> some salt, let me exhume those case-sensitivity flamewars: that
>> >>>> filesystem-specific flag is really a must, unfortunately. I  
>> keep on
>> >>>> stumbling upon apps that I have to move to my HFS / 
>> Applications to
>> >>>> get
>> >>>> them to work... (afaik this issue is likely to be a show stopper
>> >>>> also
>> >>>> on a fileserver, for example for the network Applications
>> >>>> share...but
>> >>>> then again maybe the afp handles this, haven't tried it yet..)
>> >>>>
>> >>>> ok, now I'm definitively on your blacklist and I better get some
>> >>>> shelter before you get to Your mailbox ;)
>> >>>>
>> >>>> Sincere regards for all the work done and shared so far,
>> >>>>
>> >>>> Lorenzo
>> >>>>
>> >> _______________________________________________
>> >> On Oct 12, 2008, at 12:08 PM, Byron Servies wrote:
>> >>
>> >>> Having spent the past 3 months doing exactly the same thing, I   
>> wish
>> >>> them good luck.  Don't forget sleep and exercise!
>> >>
>> >>
>> >> Oh, man. What you said. I completely agree with that estimate!
>> >>
>> >> And as I understand, you could compile the bits yourself, as the
>> >> source code is posted on the site.
>> >>
>> >>
>> >> _______________________________________________
>> >> zfs-discuss mailing list
>> >> zfs-discuss at lists.macosforge.org
>> >> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>> >
>> > _______________________________________________
>> > zfs-discuss mailing list
>> > zfs-discuss at lists.macosforge.org
>> > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From franzschmalzl at spamfreemail.de  Wed Oct 15 09:10:13 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Wed, 15 Oct 2008 18:10:13 +0200
Subject: [zfs-discuss] hungry for new bits...
In-Reply-To: <56772C58-F3F0-4B36-AAD7-FF2774FCC16A@spamfreemail.de>
References: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>
	<FDECDA5D-7A55-4E0F-873E-C0890D413C68@mac.com>
	<59D9F21C-BBB1-4942-9642-A41C23F0FDD4@nrao.edu>
	<819D142E-D01C-47B7-A596-EB0765EB747D@apple.com>
	<3D24322D-9CC8-4B91-8896-8ED11D955F73@mac.com>
	<72feba2a0810150900r5f4d04b4wa2424abbc13a6c0f@mail.gmail.com>
	<56772C58-F3F0-4B36-AAD7-FF2774FCC16A@spamfreemail.de>
Message-ID: <6382BD79-7816-450F-8602-B48C1E96C49F@spamfreemail.de>


wuuups, sorry for the german, i was doing about 300 things  
simultaneously here :)

i wanted to point out that adrian is completely right :)


franz



On 15.10.2008, at 18:07, ruebezahl wrote:

> das kann ich nur unterschreiben...
>
>
> On 15.10.2008, at 18:00, Adrian Thornton wrote:
>
>> Yes, No?l, I think none of us can thank you enough for all all your
>> hard work on this project. I know it's been said before, but it
>> deserves saying again. I think OS X and ZFS are about the two most
>> exciting aspects of computing today, and the fact that they're being
>> put together is all down to the amazing work by you and your team.
>> Thanks so much!
>>
>> - Adrian
>>
>> On Tue, 2008-10-14 at 19:29 -0700, Jason Richard McNeil wrote:
>>>
>>> No?l, thank you very much for what you do. I know I am looking
>>> forward
>>> to more bits as I have a new MacBook Pro on the way as of today  
>>> and I
>>> would very much like to start off with a ZFS home. :)
>>>
>>> I have had some issues with a multi drive pool on my mac pro thou
>>> that
>>> I'll follow up with later tonight.
>>>
>>> Sorry for shortness, back to Spanish class, ;)
>>>
>>> jrm
>>> jason at jasonrm.net
>>>
>>> On Oct 14, 2008, at 6:17 PM, No?l Dellofano <ndellofano at apple.com>
>>> wrote:
>>>
>>>> Hey guys!
>>>>
>>>> Yeah, so the whole schedule ramping up party you allude to is
>>>> happening, hence my slowness on the email of late.  Sorry.
>>>>
>>>> So on to some answers for you.  So I was holding releasing new bits
>>>> until we sync with the new Solaris base, which will have those
>>> tasty
>>>> case insensitivity options you hunger for.  Problem being, it's not
>>>> stable, and we're still debugging it.  So I didn't want to release
>>>> shady bits to everyone, hence I haven't released any.  I do have
>>> a few
>>>> fixes in line for you for some issues.  The trash emptying for
>>> example
>>>> is fixed, the wacky names of filesystems over AFP and NFS in Finder
>>>> we've got fixes for, and a few others.  Spotlight, I hear you, we
>>> are
>>>> working on.  Finder issues, also being worked on.  Many issues
>>> involve
>>>> more then just the ZFS code to change (opting in to preference
>>> panes
>>>> and such) so fixed bits I get to you still may not do exactly
>>> what you
>>>> want as other parts of the system has to be adjusted as well.
>>>>
>>>> and never fear, telling my what drives you nuts is good, it helps
>>> me
>>>> know what's broken and what you guys use so I can fix it :) So
>>> info is
>>>> good and I promise to not black list you :)
>>>>
>>>>
>>>> Noel
>>>>
>>>> On Oct 12, 2008, at 2:03 PM, Boyd Waters wrote:
>>>>
>>>>>>> hi there @mothership,
>>>>>>>
>>>>>>> it's sunday afternoon here, so why not use some spare minutes
>>> just
>>>>>>> to
>>>>>>> get on your nerves... :>
>>>>>>>
>>>>>>> any new bits in sight? especially the problem with the volume
>>> names
>>>>>>> within Finder and other parts (sharing prefpane, time machine
>>>>>>> prefpane, etc...) throughout mac os x is really a show
>>> stopper...
>>>>>>> (I
>>>>>>> try to survive with icons, but they're not used in every
>>> place)..
>>>>>>> also
>>>>>>> the incompatibility (or sort of) with spotlight.
>>>>>>> And yes, the fact that ZFS filesystems mounted on any other
>>> place
>>>>>>> than
>>>>>>> the /Volumes/Pool/Fs default do not appear _at all_ in the
>>> Finder
>>>>>>> (just with cmd-shift-G) is no fun... Oh, and while I'm here,
>>> to add
>>>>>>> some salt, let me exhume those case-sensitivity flamewars: that
>>>>>>> filesystem-specific flag is really a must, unfortunately. I
>>> keep on
>>>>>>> stumbling upon apps that I have to move to my HFS /
>>> Applications to
>>>>>>> get
>>>>>>> them to work... (afaik this issue is likely to be a show stopper
>>>>>>> also
>>>>>>> on a fileserver, for example for the network Applications
>>>>>>> share...but
>>>>>>> then again maybe the afp handles this, haven't tried it yet..)
>>>>>>>
>>>>>>> ok, now I'm definitively on your blacklist and I better get some
>>>>>>> shelter before you get to Your mailbox ;)
>>>>>>>
>>>>>>> Sincere regards for all the work done and shared so far,
>>>>>>>
>>>>>>> Lorenzo
>>>>>>>
>>>>> _______________________________________________
>>>>> On Oct 12, 2008, at 12:08 PM, Byron Servies wrote:
>>>>>
>>>>>> Having spent the past 3 months doing exactly the same thing, I
>>> wish
>>>>>> them good luck.  Don't forget sleep and exercise!
>>>>>
>>>>>
>>>>> Oh, man. What you said. I completely agree with that estimate!
>>>>>
>>>>> And as I understand, you could compile the bits yourself, as the
>>>>> source code is posted on the site.
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From jkh at apple.com  Wed Oct 15 11:23:59 2008
From: jkh at apple.com (Jordan K. Hubbard)
Date: Wed, 15 Oct 2008 11:23:59 -0700
Subject: [zfs-discuss] hungry for new bits...
In-Reply-To: <6382BD79-7816-450F-8602-B48C1E96C49F@spamfreemail.de>
References: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>
	<FDECDA5D-7A55-4E0F-873E-C0890D413C68@mac.com>
	<59D9F21C-BBB1-4942-9642-A41C23F0FDD4@nrao.edu>
	<819D142E-D01C-47B7-A596-EB0765EB747D@apple.com>
	<3D24322D-9CC8-4B91-8896-8ED11D955F73@mac.com>
	<72feba2a0810150900r5f4d04b4wa2424abbc13a6c0f@mail.gmail.com>
	<56772C58-F3F0-4B36-AAD7-FF2774FCC16A@spamfreemail.de>
	<6382BD79-7816-450F-8602-B48C1E96C49F@spamfreemail.de>
Message-ID: <1C4728C9-98E9-456E-83F7-7B28C8945C50@apple.com>

Macht nichts.   In american english, we would simply translate that  
german phrase of yours to the rather more consise:  "What he said!"    
You may consider this your free Amerikanische Umgangssprache lesson  
for today. ;-)

On Oct 15, 2008, at 9:10 AM, ruebezahl wrote:

>
> wuuups, sorry for the german, i was doing about 300 things
> simultaneously here :)
>
> i wanted to point out that adrian is completely right :)
>
>
> franz
>
>
>
> On 15.10.2008, at 18:07, ruebezahl wrote:
>
>> das kann ich nur unterschreiben...
>>
>>
>> On 15.10.2008, at 18:00, Adrian Thornton wrote:
>>
>>> Yes, No?l, I think none of us can thank you enough for all all your
>>> hard work on this project. I know it's been said before, but it
>>> deserves saying again. I think OS X and ZFS are about the two most
>>> exciting aspects of computing today, and the fact that they're being
>>> put together is all down to the amazing work by you and your team.
>>> Thanks so much!
>>>
>>> - Adrian
>>>
>>> On Tue, 2008-10-14 at 19:29 -0700, Jason Richard McNeil wrote:
>>>>
>>>> No?l, thank you very much for what you do. I know I am looking
>>>> forward
>>>> to more bits as I have a new MacBook Pro on the way as of today
>>>> and I
>>>> would very much like to start off with a ZFS home. :)
>>>>
>>>> I have had some issues with a multi drive pool on my mac pro thou
>>>> that
>>>> I'll follow up with later tonight.
>>>>
>>>> Sorry for shortness, back to Spanish class, ;)
>>>>
>>>> jrm
>>>> jason at jasonrm.net
>>>>
>>>> On Oct 14, 2008, at 6:17 PM, No?l Dellofano <ndellofano at apple.com>
>>>> wrote:
>>>>
>>>>> Hey guys!
>>>>>
>>>>> Yeah, so the whole schedule ramping up party you allude to is
>>>>> happening, hence my slowness on the email of late.  Sorry.
>>>>>
>>>>> So on to some answers for you.  So I was holding releasing new  
>>>>> bits
>>>>> until we sync with the new Solaris base, which will have those
>>>> tasty
>>>>> case insensitivity options you hunger for.  Problem being, it's  
>>>>> not
>>>>> stable, and we're still debugging it.  So I didn't want to release
>>>>> shady bits to everyone, hence I haven't released any.  I do have
>>>> a few
>>>>> fixes in line for you for some issues.  The trash emptying for
>>>> example
>>>>> is fixed, the wacky names of filesystems over AFP and NFS in  
>>>>> Finder
>>>>> we've got fixes for, and a few others.  Spotlight, I hear you, we
>>>> are
>>>>> working on.  Finder issues, also being worked on.  Many issues
>>>> involve
>>>>> more then just the ZFS code to change (opting in to preference
>>>> panes
>>>>> and such) so fixed bits I get to you still may not do exactly
>>>> what you
>>>>> want as other parts of the system has to be adjusted as well.
>>>>>
>>>>> and never fear, telling my what drives you nuts is good, it helps
>>>> me
>>>>> know what's broken and what you guys use so I can fix it :) So
>>>> info is
>>>>> good and I promise to not black list you :)
>>>>>
>>>>>
>>>>> Noel
>>>>>
>>>>> On Oct 12, 2008, at 2:03 PM, Boyd Waters wrote:
>>>>>
>>>>>>>> hi there @mothership,
>>>>>>>>
>>>>>>>> it's sunday afternoon here, so why not use some spare minutes
>>>> just
>>>>>>>> to
>>>>>>>> get on your nerves... :>
>>>>>>>>
>>>>>>>> any new bits in sight? especially the problem with the volume
>>>> names
>>>>>>>> within Finder and other parts (sharing prefpane, time machine
>>>>>>>> prefpane, etc...) throughout mac os x is really a show
>>>> stopper...
>>>>>>>> (I
>>>>>>>> try to survive with icons, but they're not used in every
>>>> place)..
>>>>>>>> also
>>>>>>>> the incompatibility (or sort of) with spotlight.
>>>>>>>> And yes, the fact that ZFS filesystems mounted on any other
>>>> place
>>>>>>>> than
>>>>>>>> the /Volumes/Pool/Fs default do not appear _at all_ in the
>>>> Finder
>>>>>>>> (just with cmd-shift-G) is no fun... Oh, and while I'm here,
>>>> to add
>>>>>>>> some salt, let me exhume those case-sensitivity flamewars: that
>>>>>>>> filesystem-specific flag is really a must, unfortunately. I
>>>> keep on
>>>>>>>> stumbling upon apps that I have to move to my HFS /
>>>> Applications to
>>>>>>>> get
>>>>>>>> them to work... (afaik this issue is likely to be a show  
>>>>>>>> stopper
>>>>>>>> also
>>>>>>>> on a fileserver, for example for the network Applications
>>>>>>>> share...but
>>>>>>>> then again maybe the afp handles this, haven't tried it yet..)
>>>>>>>>
>>>>>>>> ok, now I'm definitively on your blacklist and I better get  
>>>>>>>> some
>>>>>>>> shelter before you get to Your mailbox ;)
>>>>>>>>
>>>>>>>> Sincere regards for all the work done and shared so far,
>>>>>>>>
>>>>>>>> Lorenzo
>>>>>>>>
>>>>>> _______________________________________________
>>>>>> On Oct 12, 2008, at 12:08 PM, Byron Servies wrote:
>>>>>>
>>>>>>> Having spent the past 3 months doing exactly the same thing, I
>>>> wish
>>>>>>> them good luck.  Don't forget sleep and exercise!
>>>>>>
>>>>>>
>>>>>> Oh, man. What you said. I completely agree with that estimate!
>>>>>>
>>>>>> And as I understand, you could compile the bits yourself, as the
>>>>>> source code is posted on the site.
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> zfs-discuss mailing list
>>>>>> zfs-discuss at lists.macosforge.org
>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From franzschmalzl at spamfreemail.de  Wed Oct 15 12:56:01 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Wed, 15 Oct 2008 21:56:01 +0200
Subject: [zfs-discuss] hungry for new bits...
In-Reply-To: <1C4728C9-98E9-456E-83F7-7B28C8945C50@apple.com>
References: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>
	<FDECDA5D-7A55-4E0F-873E-C0890D413C68@mac.com>
	<59D9F21C-BBB1-4942-9642-A41C23F0FDD4@nrao.edu>
	<819D142E-D01C-47B7-A596-EB0765EB747D@apple.com>
	<3D24322D-9CC8-4B91-8896-8ED11D955F73@mac.com>
	<72feba2a0810150900r5f4d04b4wa2424abbc13a6c0f@mail.gmail.com>
	<56772C58-F3F0-4B36-AAD7-FF2774FCC16A@spamfreemail.de>
	<6382BD79-7816-450F-8602-B48C1E96C49F@spamfreemail.de>
	<1C4728C9-98E9-456E-83F7-7B28C8945C50@apple.com>
Message-ID: <2969B25A-CBF9-4ED1-99AF-6F0F8BF009F7@spamfreemail.de>

Well, i say danke dir, and have nice abend then :)


On 15.10.2008, at 20:23, Jordan K. Hubbard wrote:

> Macht nichts.   In american english, we would simply translate that  
> german phrase of yours to the rather more consise:  "What he  
> said!"   You may consider this your free Amerikanische  
> Umgangssprache lesson for today. ;-)
>
> On Oct 15, 2008, at 9:10 AM, ruebezahl wrote:
>
>>
>> wuuups, sorry for the german, i was doing about 300 things
>> simultaneously here :)
>>
>> i wanted to point out that adrian is completely right :)
>>
>>
>> franz
>>
>>
>>
>> On 15.10.2008, at 18:07, ruebezahl wrote:
>>
>>> das kann ich nur unterschreiben...
>>>
>>>
>>> On 15.10.2008, at 18:00, Adrian Thornton wrote:
>>>
>>>> Yes, No?l, I think none of us can thank you enough for all all your
>>>> hard work on this project. I know it's been said before, but it
>>>> deserves saying again. I think OS X and ZFS are about the two most
>>>> exciting aspects of computing today, and the fact that they're  
>>>> being
>>>> put together is all down to the amazing work by you and your team.
>>>> Thanks so much!
>>>>
>>>> - Adrian
>>>>
>>>> On Tue, 2008-10-14 at 19:29 -0700, Jason Richard McNeil wrote:
>>>>>
>>>>> No?l, thank you very much for what you do. I know I am looking
>>>>> forward
>>>>> to more bits as I have a new MacBook Pro on the way as of today
>>>>> and I
>>>>> would very much like to start off with a ZFS home. :)
>>>>>
>>>>> I have had some issues with a multi drive pool on my mac pro thou
>>>>> that
>>>>> I'll follow up with later tonight.
>>>>>
>>>>> Sorry for shortness, back to Spanish class, ;)
>>>>>
>>>>> jrm
>>>>> jason at jasonrm.net
>>>>>
>>>>> On Oct 14, 2008, at 6:17 PM, No?l Dellofano <ndellofano at apple.com>
>>>>> wrote:
>>>>>
>>>>>> Hey guys!
>>>>>>
>>>>>> Yeah, so the whole schedule ramping up party you allude to is
>>>>>> happening, hence my slowness on the email of late.  Sorry.
>>>>>>
>>>>>> So on to some answers for you.  So I was holding releasing new  
>>>>>> bits
>>>>>> until we sync with the new Solaris base, which will have those
>>>>> tasty
>>>>>> case insensitivity options you hunger for.  Problem being, it's  
>>>>>> not
>>>>>> stable, and we're still debugging it.  So I didn't want to  
>>>>>> release
>>>>>> shady bits to everyone, hence I haven't released any.  I do have
>>>>> a few
>>>>>> fixes in line for you for some issues.  The trash emptying for
>>>>> example
>>>>>> is fixed, the wacky names of filesystems over AFP and NFS in  
>>>>>> Finder
>>>>>> we've got fixes for, and a few others.  Spotlight, I hear you, we
>>>>> are
>>>>>> working on.  Finder issues, also being worked on.  Many issues
>>>>> involve
>>>>>> more then just the ZFS code to change (opting in to preference
>>>>> panes
>>>>>> and such) so fixed bits I get to you still may not do exactly
>>>>> what you
>>>>>> want as other parts of the system has to be adjusted as well.
>>>>>>
>>>>>> and never fear, telling my what drives you nuts is good, it helps
>>>>> me
>>>>>> know what's broken and what you guys use so I can fix it :) So
>>>>> info is
>>>>>> good and I promise to not black list you :)
>>>>>>
>>>>>>
>>>>>> Noel
>>>>>>
>>>>>> On Oct 12, 2008, at 2:03 PM, Boyd Waters wrote:
>>>>>>
>>>>>>>>> hi there @mothership,
>>>>>>>>>
>>>>>>>>> it's sunday afternoon here, so why not use some spare minutes
>>>>> just
>>>>>>>>> to
>>>>>>>>> get on your nerves... :>
>>>>>>>>>
>>>>>>>>> any new bits in sight? especially the problem with the volume
>>>>> names
>>>>>>>>> within Finder and other parts (sharing prefpane, time machine
>>>>>>>>> prefpane, etc...) throughout mac os x is really a show
>>>>> stopper...
>>>>>>>>> (I
>>>>>>>>> try to survive with icons, but they're not used in every
>>>>> place)..
>>>>>>>>> also
>>>>>>>>> the incompatibility (or sort of) with spotlight.
>>>>>>>>> And yes, the fact that ZFS filesystems mounted on any other
>>>>> place
>>>>>>>>> than
>>>>>>>>> the /Volumes/Pool/Fs default do not appear _at all_ in the
>>>>> Finder
>>>>>>>>> (just with cmd-shift-G) is no fun... Oh, and while I'm here,
>>>>> to add
>>>>>>>>> some salt, let me exhume those case-sensitivity flamewars:  
>>>>>>>>> that
>>>>>>>>> filesystem-specific flag is really a must, unfortunately. I
>>>>> keep on
>>>>>>>>> stumbling upon apps that I have to move to my HFS /
>>>>> Applications to
>>>>>>>>> get
>>>>>>>>> them to work... (afaik this issue is likely to be a show  
>>>>>>>>> stopper
>>>>>>>>> also
>>>>>>>>> on a fileserver, for example for the network Applications
>>>>>>>>> share...but
>>>>>>>>> then again maybe the afp handles this, haven't tried it yet..)
>>>>>>>>>
>>>>>>>>> ok, now I'm definitively on your blacklist and I better get  
>>>>>>>>> some
>>>>>>>>> shelter before you get to Your mailbox ;)
>>>>>>>>>
>>>>>>>>> Sincere regards for all the work done and shared so far,
>>>>>>>>>
>>>>>>>>> Lorenzo
>>>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> On Oct 12, 2008, at 12:08 PM, Byron Servies wrote:
>>>>>>>
>>>>>>>> Having spent the past 3 months doing exactly the same thing, I
>>>>> wish
>>>>>>>> them good luck.  Don't forget sleep and exercise!
>>>>>>>
>>>>>>>
>>>>>>> Oh, man. What you said. I completely agree with that estimate!
>>>>>>>
>>>>>>> And as I understand, you could compile the bits yourself, as the
>>>>>>> source code is posted on the site.
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> zfs-discuss mailing list
>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>>
>>>>>> _______________________________________________
>>>>>> zfs-discuss mailing list
>>>>>> zfs-discuss at lists.macosforge.org
>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From zorg at sogeeky.net  Wed Oct 15 12:57:31 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Wed, 15 Oct 2008 12:57:31 -0700
Subject: [zfs-discuss] hungry for new bits...
In-Reply-To: <1C4728C9-98E9-456E-83F7-7B28C8945C50@apple.com>
References: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>
	<FDECDA5D-7A55-4E0F-873E-C0890D413C68@mac.com>
	<59D9F21C-BBB1-4942-9642-A41C23F0FDD4@nrao.edu>
	<819D142E-D01C-47B7-A596-EB0765EB747D@apple.com>
	<3D24322D-9CC8-4B91-8896-8ED11D955F73@mac.com>
	<72feba2a0810150900r5f4d04b4wa2424abbc13a6c0f@mail.gmail.com>
	<56772C58-F3F0-4B36-AAD7-FF2774FCC16A@spamfreemail.de>
	<6382BD79-7816-450F-8602-B48C1E96C49F@spamfreemail.de>
	<1C4728C9-98E9-456E-83F7-7B28C8945C50@apple.com>
Message-ID: <A9BED72B-EE92-45D5-9AB8-45B9E5ACE912@sogeeky.net>

Or, more literally "that I can only underscore". Bet you didn't know  
so many of us spoke German. :)

On Oct 15, 2008, at 11:23 AM, "Jordan K. Hubbard" <jkh at apple.com> wrote:

> Macht nichts.   In american english, we would simply translate that
> german phrase of yours to the rather more consise:  "What he said!"
> You may consider this your free Amerikanische Umgangssprache lesson
> for today. ;-)
>
> On Oct 15, 2008, at 9:10 AM, ruebezahl wrote:
>
>>
>> wuuups, sorry for the german, i was doing about 300 things
>> simultaneously here :)
>>
>> i wanted to point out that adrian is completely right :)
>>
>>
>> franz
>>
>>
>>
>> On 15.10.2008, at 18:07, ruebezahl wrote:
>>
>>> das kann ich nur unterschreiben...
>>>
>>>
>>> On 15.10.2008, at 18:00, Adrian Thornton wrote:
>>>
>>>> Yes, No?l, I think none of us can thank you enough for all all 
>>>>  your
>>>> hard work on this project. I know it's been said before, but it
>>>> deserves saying again. I think OS X and ZFS are about the two most
>>>> exciting aspects of computing today, and the fact that they're  
>>>> being
>>>> put together is all down to the amazing work by you and your team.
>>>> Thanks so much!
>>>>
>>>> - Adrian
>>>>
>>>> On Tue, 2008-10-14 at 19:29 -0700, Jason Richard McNeil wrote:
>>>>>
>>>>> No?l, thank you very much for what you do. I know I am looking
>>>>> forward
>>>>> to more bits as I have a new MacBook Pro on the way as of today
>>>>> and I
>>>>> would very much like to start off with a ZFS home. :)
>>>>>
>>>>> I have had some issues with a multi drive pool on my mac pro thou
>>>>> that
>>>>> I'll follow up with later tonight.
>>>>>
>>>>> Sorry for shortness, back to Spanish class, ;)
>>>>>
>>>>> jrm
>>>>> jason at jasonrm.net
>>>>>
>>>>> On Oct 14, 2008, at 6:17 PM, No?l Dellofano <ndellofano at appl 
>>>>> e.com>
>>>>> wrote:
>>>>>
>>>>>> Hey guys!
>>>>>>
>>>>>> Yeah, so the whole schedule ramping up party you allude to is
>>>>>> happening, hence my slowness on the email of late.  Sorry.
>>>>>>
>>>>>> So on to some answers for you.  So I was holding releasing new
>>>>>> bits
>>>>>> until we sync with the new Solaris base, which will have those
>>>>> tasty
>>>>>> case insensitivity options you hunger for.  Problem being, it's
>>>>>> not
>>>>>> stable, and we're still debugging it.  So I didn't want to  
>>>>>> release
>>>>>> shady bits to everyone, hence I haven't released any.  I do have
>>>>> a few
>>>>>> fixes in line for you for some issues.  The trash emptying for
>>>>> example
>>>>>> is fixed, the wacky names of filesystems over AFP and NFS in
>>>>>> Finder
>>>>>> we've got fixes for, and a few others.  Spotlight, I hear you, we
>>>>> are
>>>>>> working on.  Finder issues, also being worked on.  Many issues
>>>>> involve
>>>>>> more then just the ZFS code to change (opting in to preference
>>>>> panes
>>>>>> and such) so fixed bits I get to you still may not do exactly
>>>>> what you
>>>>>> want as other parts of the system has to be adjusted as well.
>>>>>>
>>>>>> and never fear, telling my what drives you nuts is good, it helps
>>>>> me
>>>>>> know what's broken and what you guys use so I can fix it :) So
>>>>> info is
>>>>>> good and I promise to not black list you :)
>>>>>>
>>>>>>
>>>>>> Noel
>>>>>>
>>>>>> On Oct 12, 2008, at 2:03 PM, Boyd Waters wrote:
>>>>>>
>>>>>>>>> hi there @mothership,
>>>>>>>>>
>>>>>>>>> it's sunday afternoon here, so why not use some spare minutes
>>>>> just
>>>>>>>>> to
>>>>>>>>> get on your nerves... :>
>>>>>>>>>
>>>>>>>>> any new bits in sight? especially the problem with the volume
>>>>> names
>>>>>>>>> within Finder and other parts (sharing prefpane, time machine
>>>>>>>>> prefpane, etc...) throughout mac os x is really a show
>>>>> stopper...
>>>>>>>>> (I
>>>>>>>>> try to survive with icons, but they're not used in every
>>>>> place)..
>>>>>>>>> also
>>>>>>>>> the incompatibility (or sort of) with spotlight.
>>>>>>>>> And yes, the fact that ZFS filesystems mounted on any other
>>>>> place
>>>>>>>>> than
>>>>>>>>> the /Volumes/Pool/Fs default do not appear _at all_ in the
>>>>> Finder
>>>>>>>>> (just with cmd-shift-G) is no fun... Oh, and while I'm here,
>>>>> to add
>>>>>>>>> some salt, let me exhume those case-sensitivity flamewars:  
>>>>>>>>> that
>>>>>>>>> filesystem-specific flag is really a must, unfortunately. I
>>>>> keep on
>>>>>>>>> stumbling upon apps that I have to move to my HFS /
>>>>> Applications to
>>>>>>>>> get
>>>>>>>>> them to work... (afaik this issue is likely to be a show
>>>>>>>>> stopper
>>>>>>>>> also
>>>>>>>>> on a fileserver, for example for the network Applications
>>>>>>>>> share...but
>>>>>>>>> then again maybe the afp handles this, haven't tried it yet..)
>>>>>>>>>
>>>>>>>>> ok, now I'm definitively on your blacklist and I better get
>>>>>>>>> some
>>>>>>>>> shelter before you get to Your mailbox ;)
>>>>>>>>>
>>>>>>>>> Sincere regards for all the work done and shared so far,
>>>>>>>>>
>>>>>>>>> Lorenzo
>>>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> On Oct 12, 2008, at 12:08 PM, Byron Servies wrote:
>>>>>>>
>>>>>>>> Having spent the past 3 months doing exactly the same thing, I
>>>>> wish
>>>>>>>> them good luck.  Don't forget sleep and exercise!
>>>>>>>
>>>>>>>
>>>>>>> Oh, man. What you said. I completely agree with that estimate!
>>>>>>>
>>>>>>> And as I understand, you could compile the bits yourself, as the
>>>>>>> source code is posted on the site.
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> zfs-discuss mailing list
>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>>
>>>>>> _______________________________________________
>>>>>> zfs-discuss mailing list
>>>>>> zfs-discuss at lists.macosforge.org
>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From franzschmalzl at spamfreemail.de  Wed Oct 15 13:03:26 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Wed, 15 Oct 2008 22:03:26 +0200
Subject: [zfs-discuss] hungry for new bits...
In-Reply-To: <A9BED72B-EE92-45D5-9AB8-45B9E5ACE912@sogeeky.net>
References: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>
	<FDECDA5D-7A55-4E0F-873E-C0890D413C68@mac.com>
	<59D9F21C-BBB1-4942-9642-A41C23F0FDD4@nrao.edu>
	<819D142E-D01C-47B7-A596-EB0765EB747D@apple.com>
	<3D24322D-9CC8-4B91-8896-8ED11D955F73@mac.com>
	<72feba2a0810150900r5f4d04b4wa2424abbc13a6c0f@mail.gmail.com>
	<56772C58-F3F0-4B36-AAD7-FF2774FCC16A@spamfreemail.de>
	<6382BD79-7816-450F-8602-B48C1E96C49F@spamfreemail.de>
	<1C4728C9-98E9-456E-83F7-7B28C8945C50@apple.com>
	<A9BED72B-EE92-45D5-9AB8-45B9E5ACE912@sogeeky.net>
Message-ID: <BA85668D-D752-4B43-ADAB-D90390766569@spamfreemail.de>

well actually " that i can only subscribe"  :)

ahhhm, what the hell is going on ?

is there a " free german lessons for everyone in america" offer i  
wasn't aware of  ? ;-)




On 15.10.2008, at 21:57, Mr. Zorg wrote:

> Or, more literally "that I can only underscore". Bet you didn't know  
> so many of us spoke German. :)
>
> On Oct 15, 2008, at 11:23 AM, "Jordan K. Hubbard" <jkh at apple.com>  
> wrote:
>
>> Macht nichts.   In american english, we would simply translate that
>> german phrase of yours to the rather more consise:  "What he said!"
>> You may consider this your free Amerikanische Umgangssprache lesson
>> for today. ;-)
>>
>> On Oct 15, 2008, at 9:10 AM, ruebezahl wrote:
>>
>>>
>>> wuuups, sorry for the german, i was doing about 300 things
>>> simultaneously here :)
>>>
>>> i wanted to point out that adrian is completely right :)
>>>
>>>
>>> franz
>>>
>>>
>>>
>>> On 15.10.2008, at 18:07, ruebezahl wrote:
>>>
>>>> das kann ich nur unterschreiben...
>>>>
>>>>
>>>> On 15.10.2008, at 18:00, Adrian Thornton wrote:
>>>>
>>>>> Yes, No?l, I think none of us can thank you enough for all all  
>>>>> your
>>>>> hard work on this project. I know it's been said before, but it
>>>>> deserves saying again. I think OS X and ZFS are about the two most
>>>>> exciting aspects of computing today, and the fact that they're  
>>>>> being
>>>>> put together is all down to the amazing work by you and your team.
>>>>> Thanks so much!
>>>>>
>>>>> - Adrian
>>>>>
>>>>> On Tue, 2008-10-14 at 19:29 -0700, Jason Richard McNeil wrote:
>>>>>>
>>>>>> No?l, thank you very much for what you do. I know I am looking
>>>>>> forward
>>>>>> to more bits as I have a new MacBook Pro on the way as of today
>>>>>> and I
>>>>>> would very much like to start off with a ZFS home. :)
>>>>>>
>>>>>> I have had some issues with a multi drive pool on my mac pro thou
>>>>>> that
>>>>>> I'll follow up with later tonight.
>>>>>>
>>>>>> Sorry for shortness, back to Spanish class, ;)
>>>>>>
>>>>>> jrm
>>>>>> jason at jasonrm.net
>>>>>>
>>>>>> On Oct 14, 2008, at 6:17 PM, No?l Dellofano  
>>>>>> <ndellofano at apple.com>
>>>>>> wrote:
>>>>>>
>>>>>>> Hey guys!
>>>>>>>
>>>>>>> Yeah, so the whole schedule ramping up party you allude to is
>>>>>>> happening, hence my slowness on the email of late.  Sorry.
>>>>>>>
>>>>>>> So on to some answers for you.  So I was holding releasing new
>>>>>>> bits
>>>>>>> until we sync with the new Solaris base, which will have those
>>>>>> tasty
>>>>>>> case insensitivity options you hunger for.  Problem being, it's
>>>>>>> not
>>>>>>> stable, and we're still debugging it.  So I didn't want to  
>>>>>>> release
>>>>>>> shady bits to everyone, hence I haven't released any.  I do have
>>>>>> a few
>>>>>>> fixes in line for you for some issues.  The trash emptying for
>>>>>> example
>>>>>>> is fixed, the wacky names of filesystems over AFP and NFS in
>>>>>>> Finder
>>>>>>> we've got fixes for, and a few others.  Spotlight, I hear you,  
>>>>>>> we
>>>>>> are
>>>>>>> working on.  Finder issues, also being worked on.  Many issues
>>>>>> involve
>>>>>>> more then just the ZFS code to change (opting in to preference
>>>>>> panes
>>>>>>> and such) so fixed bits I get to you still may not do exactly
>>>>>> what you
>>>>>>> want as other parts of the system has to be adjusted as well.
>>>>>>>
>>>>>>> and never fear, telling my what drives you nuts is good, it  
>>>>>>> helps
>>>>>> me
>>>>>>> know what's broken and what you guys use so I can fix it :) So
>>>>>> info is
>>>>>>> good and I promise to not black list you :)
>>>>>>>
>>>>>>>
>>>>>>> Noel
>>>>>>>
>>>>>>> On Oct 12, 2008, at 2:03 PM, Boyd Waters wrote:
>>>>>>>
>>>>>>>>>> hi there @mothership,
>>>>>>>>>>
>>>>>>>>>> it's sunday afternoon here, so why not use some spare minutes
>>>>>> just
>>>>>>>>>> to
>>>>>>>>>> get on your nerves... :>
>>>>>>>>>>
>>>>>>>>>> any new bits in sight? especially the problem with the volume
>>>>>> names
>>>>>>>>>> within Finder and other parts (sharing prefpane, time machine
>>>>>>>>>> prefpane, etc...) throughout mac os x is really a show
>>>>>> stopper...
>>>>>>>>>> (I
>>>>>>>>>> try to survive with icons, but they're not used in every
>>>>>> place)..
>>>>>>>>>> also
>>>>>>>>>> the incompatibility (or sort of) with spotlight.
>>>>>>>>>> And yes, the fact that ZFS filesystems mounted on any other
>>>>>> place
>>>>>>>>>> than
>>>>>>>>>> the /Volumes/Pool/Fs default do not appear _at all_ in the
>>>>>> Finder
>>>>>>>>>> (just with cmd-shift-G) is no fun... Oh, and while I'm here,
>>>>>> to add
>>>>>>>>>> some salt, let me exhume those case-sensitivity flamewars:  
>>>>>>>>>> that
>>>>>>>>>> filesystem-specific flag is really a must, unfortunately. I
>>>>>> keep on
>>>>>>>>>> stumbling upon apps that I have to move to my HFS /
>>>>>> Applications to
>>>>>>>>>> get
>>>>>>>>>> them to work... (afaik this issue is likely to be a show
>>>>>>>>>> stopper
>>>>>>>>>> also
>>>>>>>>>> on a fileserver, for example for the network Applications
>>>>>>>>>> share...but
>>>>>>>>>> then again maybe the afp handles this, haven't tried it  
>>>>>>>>>> yet..)
>>>>>>>>>>
>>>>>>>>>> ok, now I'm definitively on your blacklist and I better get
>>>>>>>>>> some
>>>>>>>>>> shelter before you get to Your mailbox ;)
>>>>>>>>>>
>>>>>>>>>> Sincere regards for all the work done and shared so far,
>>>>>>>>>>
>>>>>>>>>> Lorenzo
>>>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> On Oct 12, 2008, at 12:08 PM, Byron Servies wrote:
>>>>>>>>
>>>>>>>>> Having spent the past 3 months doing exactly the same thing, I
>>>>>> wish
>>>>>>>>> them good luck.  Don't forget sleep and exercise!
>>>>>>>>
>>>>>>>>
>>>>>>>> Oh, man. What you said. I completely agree with that estimate!
>>>>>>>>
>>>>>>>> And as I understand, you could compile the bits yourself, as  
>>>>>>>> the
>>>>>>>> source code is posted on the site.
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> zfs-discuss mailing list
>>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> zfs-discuss mailing list
>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>> _______________________________________________
>>>>>> zfs-discuss mailing list
>>>>>> zfs-discuss at lists.macosforge.org
>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From franzschmalzl at spamfreemail.de  Wed Oct 15 13:04:56 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Wed, 15 Oct 2008 22:04:56 +0200
Subject: [zfs-discuss] hungry for new bits...
In-Reply-To: <2969B25A-CBF9-4ED1-99AF-6F0F8BF009F7@spamfreemail.de>
References: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>
	<FDECDA5D-7A55-4E0F-873E-C0890D413C68@mac.com>
	<59D9F21C-BBB1-4942-9642-A41C23F0FDD4@nrao.edu>
	<819D142E-D01C-47B7-A596-EB0765EB747D@apple.com>
	<3D24322D-9CC8-4B91-8896-8ED11D955F73@mac.com>
	<72feba2a0810150900r5f4d04b4wa2424abbc13a6c0f@mail.gmail.com>
	<56772C58-F3F0-4B36-AAD7-FF2774FCC16A@spamfreemail.de>
	<6382BD79-7816-450F-8602-B48C1E96C49F@spamfreemail.de>
	<1C4728C9-98E9-456E-83F7-7B28C8945C50@apple.com>
	<2969B25A-CBF9-4ED1-99AF-6F0F8BF009F7@spamfreemail.de>
Message-ID: <E9F20067-AA24-4D78-983A-B905ABCF4AFE@spamfreemail.de>




Well, i say danke dir, and have nice abend then :)
>
>
> On 15.10.2008, at 20:23, Jordan K. Hubbard wrote:
>
>> Macht nichts.   In american english, we would simply translate that  
>> german phrase of yours to the rather more consise:  "What he  
>> said!"   You may consider this your free Amerikanische  
>> Umgangssprache lesson for today. ;-)
>>
>> On Oct 15, 2008, at 9:10 AM, ruebezahl wrote:
>>
>>>
>>> wuuups, sorry for the german, i was doing about 300 things
>>> simultaneously here :)
>>>
>>> i wanted to point out that adrian is completely right :)
>>>
>>>
>>> franz
>>>
>>>
>>>
>>> On 15.10.2008, at 18:07, ruebezahl wrote:
>>>
>>>> das kann ich nur unterschreiben...
>>>>
>>>>
>>>> On 15.10.2008, at 18:00, Adrian Thornton wrote:
>>>>
>>>>> Yes, No?l, I think none of us can thank you enough for all all  
>>>>> your
>>>>> hard work on this project. I know it's been said before, but it
>>>>> deserves saying again. I think OS X and ZFS are about the two most
>>>>> exciting aspects of computing today, and the fact that they're  
>>>>> being
>>>>> put together is all down to the amazing work by you and your team.
>>>>> Thanks so much!
>>>>>
>>>>> - Adrian
>>>>>
>>>>> On Tue, 2008-10-14 at 19:29 -0700, Jason Richard McNeil wrote:
>>>>>>
>>>>>> No?l, thank you very much for what you do. I know I am looking
>>>>>> forward
>>>>>> to more bits as I have a new MacBook Pro on the way as of today
>>>>>> and I
>>>>>> would very much like to start off with a ZFS home. :)
>>>>>>
>>>>>> I have had some issues with a multi drive pool on my mac pro thou
>>>>>> that
>>>>>> I'll follow up with later tonight.
>>>>>>
>>>>>> Sorry for shortness, back to Spanish class, ;)
>>>>>>
>>>>>> jrm
>>>>>> jason at jasonrm.net
>>>>>>
>>>>>> On Oct 14, 2008, at 6:17 PM, No?l Dellofano  
>>>>>> <ndellofano at apple.com>
>>>>>> wrote:
>>>>>>
>>>>>>> Hey guys!
>>>>>>>
>>>>>>> Yeah, so the whole schedule ramping up party you allude to is
>>>>>>> happening, hence my slowness on the email of late.  Sorry.
>>>>>>>
>>>>>>> So on to some answers for you.  So I was holding releasing new  
>>>>>>> bits
>>>>>>> until we sync with the new Solaris base, which will have those
>>>>>> tasty
>>>>>>> case insensitivity options you hunger for.  Problem being,  
>>>>>>> it's not
>>>>>>> stable, and we're still debugging it.  So I didn't want to  
>>>>>>> release
>>>>>>> shady bits to everyone, hence I haven't released any.  I do have
>>>>>> a few
>>>>>>> fixes in line for you for some issues.  The trash emptying for
>>>>>> example
>>>>>>> is fixed, the wacky names of filesystems over AFP and NFS in  
>>>>>>> Finder
>>>>>>> we've got fixes for, and a few others.  Spotlight, I hear you,  
>>>>>>> we
>>>>>> are
>>>>>>> working on.  Finder issues, also being worked on.  Many issues
>>>>>> involve
>>>>>>> more then just the ZFS code to change (opting in to preference
>>>>>> panes
>>>>>>> and such) so fixed bits I get to you still may not do exactly
>>>>>> what you
>>>>>>> want as other parts of the system has to be adjusted as well.
>>>>>>>
>>>>>>> and never fear, telling my what drives you nuts is good, it  
>>>>>>> helps
>>>>>> me
>>>>>>> know what's broken and what you guys use so I can fix it :) So
>>>>>> info is
>>>>>>> good and I promise to not black list you :)
>>>>>>>
>>>>>>>
>>>>>>> Noel
>>>>>>>
>>>>>>> On Oct 12, 2008, at 2:03 PM, Boyd Waters wrote:
>>>>>>>
>>>>>>>>>> hi there @mothership,
>>>>>>>>>>
>>>>>>>>>> it's sunday afternoon here, so why not use some spare minutes
>>>>>> just
>>>>>>>>>> to
>>>>>>>>>> get on your nerves... :>
>>>>>>>>>>
>>>>>>>>>> any new bits in sight? especially the problem with the volume
>>>>>> names
>>>>>>>>>> within Finder and other parts (sharing prefpane, time machine
>>>>>>>>>> prefpane, etc...) throughout mac os x is really a show
>>>>>> stopper...
>>>>>>>>>> (I
>>>>>>>>>> try to survive with icons, but they're not used in every
>>>>>> place)..
>>>>>>>>>> also
>>>>>>>>>> the incompatibility (or sort of) with spotlight.
>>>>>>>>>> And yes, the fact that ZFS filesystems mounted on any other
>>>>>> place
>>>>>>>>>> than
>>>>>>>>>> the /Volumes/Pool/Fs default do not appear _at all_ in the
>>>>>> Finder
>>>>>>>>>> (just with cmd-shift-G) is no fun... Oh, and while I'm here,
>>>>>> to add
>>>>>>>>>> some salt, let me exhume those case-sensitivity flamewars:  
>>>>>>>>>> that
>>>>>>>>>> filesystem-specific flag is really a must, unfortunately. I
>>>>>> keep on
>>>>>>>>>> stumbling upon apps that I have to move to my HFS /
>>>>>> Applications to
>>>>>>>>>> get
>>>>>>>>>> them to work... (afaik this issue is likely to be a show  
>>>>>>>>>> stopper
>>>>>>>>>> also
>>>>>>>>>> on a fileserver, for example for the network Applications
>>>>>>>>>> share...but
>>>>>>>>>> then again maybe the afp handles this, haven't tried it  
>>>>>>>>>> yet..)
>>>>>>>>>>
>>>>>>>>>> ok, now I'm definitively on your blacklist and I better get  
>>>>>>>>>> some
>>>>>>>>>> shelter before you get to Your mailbox ;)
>>>>>>>>>>
>>>>>>>>>> Sincere regards for all the work done and shared so far,
>>>>>>>>>>
>>>>>>>>>> Lorenzo
>>>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> On Oct 12, 2008, at 12:08 PM, Byron Servies wrote:
>>>>>>>>
>>>>>>>>> Having spent the past 3 months doing exactly the same thing, I
>>>>>> wish
>>>>>>>>> them good luck.  Don't forget sleep and exercise!
>>>>>>>>
>>>>>>>>
>>>>>>>> Oh, man. What you said. I completely agree with that estimate!
>>>>>>>>
>>>>>>>> And as I understand, you could compile the bits yourself, as  
>>>>>>>> the
>>>>>>>> source code is posted on the site.
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> zfs-discuss mailing list
>>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> zfs-discuss mailing list
>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>> _______________________________________________
>>>>>> zfs-discuss mailing list
>>>>>> zfs-discuss at lists.macosforge.org
>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>


From franzschmalzl at spamfreemail.de  Wed Oct 15 13:05:27 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Wed, 15 Oct 2008 22:05:27 +0200
Subject: [zfs-discuss] hungry for new bits...
In-Reply-To: <BA85668D-D752-4B43-ADAB-D90390766569@spamfreemail.de>
References: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>
	<FDECDA5D-7A55-4E0F-873E-C0890D413C68@mac.com>
	<59D9F21C-BBB1-4942-9642-A41C23F0FDD4@nrao.edu>
	<819D142E-D01C-47B7-A596-EB0765EB747D@apple.com>
	<3D24322D-9CC8-4B91-8896-8ED11D955F73@mac.com>
	<72feba2a0810150900r5f4d04b4wa2424abbc13a6c0f@mail.gmail.com>
	<56772C58-F3F0-4B36-AAD7-FF2774FCC16A@spamfreemail.de>
	<6382BD79-7816-450F-8602-B48C1E96C49F@spamfreemail.de>
	<1C4728C9-98E9-456E-83F7-7B28C8945C50@apple.com>
	<A9BED72B-EE92-45D5-9AB8-45B9E5ACE912@sogeeky.net>
	<BA85668D-D752-4B43-ADAB-D90390766569@spamfreemail.de>
Message-ID: <B9D2CA61-0718-4A5E-A2BF-53792500879E@spamfreemail.de>



well actually " that i can only subscribe"  :)

ahhhm, what the hell is going on ?

is there a " free german lessons for everyone in america" offer i  
wasn't aware of  ? ;-)



>
> On 15.10.2008, at 21:57, Mr. Zorg wrote:
>
>> Or, more literally "that I can only underscore". Bet you didn't  
>> know so many of us spoke German. :)
>>
>> On Oct 15, 2008, at 11:23 AM, "Jordan K. Hubbard" <jkh at apple.com>  
>> wrote:
>>
>>> Macht nichts.   In american english, we would simply translate that
>>> german phrase of yours to the rather more consise:  "What he said!"
>>> You may consider this your free Amerikanische Umgangssprache lesson
>>> for today. ;-)
>>>
>>> On Oct 15, 2008, at 9:10 AM, ruebezahl wrote:
>>>
>>>>
>>>> wuuups, sorry for the german, i was doing about 300 things
>>>> simultaneously here :)
>>>>
>>>> i wanted to point out that adrian is completely right :)
>>>>
>>>>
>>>> franz
>>>>
>>>>
>>>>
>>>> On 15.10.2008, at 18:07, ruebezahl wrote:
>>>>
>>>>> das kann ich nur unterschreiben...
>>>>>
>>>>>
>>>>> On 15.10.2008, at 18:00, Adrian Thornton wrote:
>>>>>
>>>>>> Yes, No?l, I think none of us can thank you enough for all all  
>>>>>> your
>>>>>> hard work on this project. I know it's been said before, but it
>>>>>> deserves saying again. I think OS X and ZFS are about the two  
>>>>>> most
>>>>>> exciting aspects of computing today, and the fact that they're  
>>>>>> being
>>>>>> put together is all down to the amazing work by you and your  
>>>>>> team.
>>>>>> Thanks so much!
>>>>>>
>>>>>> - Adrian
>>>>>>
>>>>>> On Tue, 2008-10-14 at 19:29 -0700, Jason Richard McNeil wrote:
>>>>>>>
>>>>>>> No?l, thank you very much for what you do. I know I am looking
>>>>>>> forward
>>>>>>> to more bits as I have a new MacBook Pro on the way as of today
>>>>>>> and I
>>>>>>> would very much like to start off with a ZFS home. :)
>>>>>>>
>>>>>>> I have had some issues with a multi drive pool on my mac pro  
>>>>>>> thou
>>>>>>> that
>>>>>>> I'll follow up with later tonight.
>>>>>>>
>>>>>>> Sorry for shortness, back to Spanish class, ;)
>>>>>>>
>>>>>>> jrm
>>>>>>> jason at jasonrm.net
>>>>>>>
>>>>>>> On Oct 14, 2008, at 6:17 PM, No?l Dellofano <ndellofano at apple.com 
>>>>>>> >
>>>>>>> wrote:
>>>>>>>
>>>>>>>> Hey guys!
>>>>>>>>
>>>>>>>> Yeah, so the whole schedule ramping up party you allude to is
>>>>>>>> happening, hence my slowness on the email of late.  Sorry.
>>>>>>>>
>>>>>>>> So on to some answers for you.  So I was holding releasing new
>>>>>>>> bits
>>>>>>>> until we sync with the new Solaris base, which will have those
>>>>>>> tasty
>>>>>>>> case insensitivity options you hunger for.  Problem being, it's
>>>>>>>> not
>>>>>>>> stable, and we're still debugging it.  So I didn't want to  
>>>>>>>> release
>>>>>>>> shady bits to everyone, hence I haven't released any.  I do  
>>>>>>>> have
>>>>>>> a few
>>>>>>>> fixes in line for you for some issues.  The trash emptying for
>>>>>>> example
>>>>>>>> is fixed, the wacky names of filesystems over AFP and NFS in
>>>>>>>> Finder
>>>>>>>> we've got fixes for, and a few others.  Spotlight, I hear  
>>>>>>>> you, we
>>>>>>> are
>>>>>>>> working on.  Finder issues, also being worked on.  Many issues
>>>>>>> involve
>>>>>>>> more then just the ZFS code to change (opting in to preference
>>>>>>> panes
>>>>>>>> and such) so fixed bits I get to you still may not do exactly
>>>>>>> what you
>>>>>>>> want as other parts of the system has to be adjusted as well.
>>>>>>>>
>>>>>>>> and never fear, telling my what drives you nuts is good, it  
>>>>>>>> helps
>>>>>>> me
>>>>>>>> know what's broken and what you guys use so I can fix it :) So
>>>>>>> info is
>>>>>>>> good and I promise to not black list you :)
>>>>>>>>
>>>>>>>>
>>>>>>>> Noel
>>>>>>>>
>>>>>>>> On Oct 12, 2008, at 2:03 PM, Boyd Waters wrote:
>>>>>>>>
>>>>>>>>>>> hi there @mothership,
>>>>>>>>>>>
>>>>>>>>>>> it's sunday afternoon here, so why not use some spare  
>>>>>>>>>>> minutes
>>>>>>> just
>>>>>>>>>>> to
>>>>>>>>>>> get on your nerves... :>
>>>>>>>>>>>
>>>>>>>>>>> any new bits in sight? especially the problem with the  
>>>>>>>>>>> volume
>>>>>>> names
>>>>>>>>>>> within Finder and other parts (sharing prefpane, time  
>>>>>>>>>>> machine
>>>>>>>>>>> prefpane, etc...) throughout mac os x is really a show
>>>>>>> stopper...
>>>>>>>>>>> (I
>>>>>>>>>>> try to survive with icons, but they're not used in every
>>>>>>> place)..
>>>>>>>>>>> also
>>>>>>>>>>> the incompatibility (or sort of) with spotlight.
>>>>>>>>>>> And yes, the fact that ZFS filesystems mounted on any other
>>>>>>> place
>>>>>>>>>>> than
>>>>>>>>>>> the /Volumes/Pool/Fs default do not appear _at all_ in the
>>>>>>> Finder
>>>>>>>>>>> (just with cmd-shift-G) is no fun... Oh, and while I'm here,
>>>>>>> to add
>>>>>>>>>>> some salt, let me exhume those case-sensitivity flamewars:  
>>>>>>>>>>> that
>>>>>>>>>>> filesystem-specific flag is really a must, unfortunately. I
>>>>>>> keep on
>>>>>>>>>>> stumbling upon apps that I have to move to my HFS /
>>>>>>> Applications to
>>>>>>>>>>> get
>>>>>>>>>>> them to work... (afaik this issue is likely to be a show
>>>>>>>>>>> stopper
>>>>>>>>>>> also
>>>>>>>>>>> on a fileserver, for example for the network Applications
>>>>>>>>>>> share...but
>>>>>>>>>>> then again maybe the afp handles this, haven't tried it  
>>>>>>>>>>> yet..)
>>>>>>>>>>>
>>>>>>>>>>> ok, now I'm definitively on your blacklist and I better get
>>>>>>>>>>> some
>>>>>>>>>>> shelter before you get to Your mailbox ;)
>>>>>>>>>>>
>>>>>>>>>>> Sincere regards for all the work done and shared so far,
>>>>>>>>>>>
>>>>>>>>>>> Lorenzo
>>>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> On Oct 12, 2008, at 12:08 PM, Byron Servies wrote:
>>>>>>>>>
>>>>>>>>>> Having spent the past 3 months doing exactly the same  
>>>>>>>>>> thing, I
>>>>>>> wish
>>>>>>>>>> them good luck.  Don't forget sleep and exercise!
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> Oh, man. What you said. I completely agree with that estimate!
>>>>>>>>>
>>>>>>>>> And as I understand, you could compile the bits yourself, as  
>>>>>>>>> the
>>>>>>>>> source code is posted on the site.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> zfs-discuss mailing list
>>>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> zfs-discuss mailing list
>>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>>> _______________________________________________
>>>>>>> zfs-discuss mailing list
>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>> _______________________________________________
>>>>>> zfs-discuss mailing list
>>>>>> zfs-discuss at lists.macosforge.org
>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From dirkschelfhout at mac.com  Wed Oct 15 13:14:03 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Wed, 15 Oct 2008 22:14:03 +0200
Subject: [zfs-discuss] hungry for new bits...
In-Reply-To: <A9BED72B-EE92-45D5-9AB8-45B9E5ACE912@sogeeky.net>
References: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>
	<FDECDA5D-7A55-4E0F-873E-C0890D413C68@mac.com>
	<59D9F21C-BBB1-4942-9642-A41C23F0FDD4@nrao.edu>
	<819D142E-D01C-47B7-A596-EB0765EB747D@apple.com>
	<3D24322D-9CC8-4B91-8896-8ED11D955F73@mac.com>
	<72feba2a0810150900r5f4d04b4wa2424abbc13a6c0f@mail.gmail.com>
	<56772C58-F3F0-4B36-AAD7-FF2774FCC16A@spamfreemail.de>
	<6382BD79-7816-450F-8602-B48C1E96C49F@spamfreemail.de>
	<1C4728C9-98E9-456E-83F7-7B28C8945C50@apple.com>
	<A9BED72B-EE92-45D5-9AB8-45B9E5ACE912@sogeeky.net>
Message-ID: <42AF7002-AD06-40C7-85F0-4F0BDEA2C675@mac.com>

better translation is : I agree with this statement
or more loosly : I concur
or : I can emphasize this
On 15 Oct 2008, at 21:57, Mr. Zorg wrote:

> Or, more literally "that I can only underscore". Bet you didn't know
> so many of us spoke German. :)
>
> On Oct 15, 2008, at 11:23 AM, "Jordan K. Hubbard" <jkh at apple.com>  
> wrote:
>
>> Macht nichts.   In american english, we would simply translate that
>> german phrase of yours to the rather more consise:  "What he said!"
>> You may consider this your free Amerikanische Umgangssprache lesson
>> for today. ;-)
>>
>> On Oct 15, 2008, at 9:10 AM, ruebezahl wrote:
>>
>>>
>>> wuuups, sorry for the german, i was doing about 300 things
>>> simultaneously here :)
>>>
>>> i wanted to point out that adrian is completely right :)
>>>
>>>
>>> franz
>>>
>>>
>>>
>>> On 15.10.2008, at 18:07, ruebezahl wrote:
>>>
>>>> das kann ich nur unterschreiben...
>>>>
>>>>
>>>> On 15.10.2008, at 18:00, Adrian Thornton wrote:
>>>>
>>>>> Yes, No?l, I think none of us can thank you enough for all all
>>>>> your
>>>>> hard work on this project. I know it's been said before, but it
>>>>> deserves saying again. I think OS X and ZFS are about the two most
>>>>> exciting aspects of computing today, and the fact that they're
>>>>> being
>>>>> put together is all down to the amazing work by you and your team.
>>>>> Thanks so much!
>>>>>
>>>>> - Adrian
>>>>>
>>>>> On Tue, 2008-10-14 at 19:29 -0700, Jason Richard McNeil wrote:
>>>>>>
>>>>>> No?l, thank you very much for what you do. I know I am looking
>>>>>> forward
>>>>>> to more bits as I have a new MacBook Pro on the way as of today
>>>>>> and I
>>>>>> would very much like to start off with a ZFS home. :)
>>>>>>
>>>>>> I have had some issues with a multi drive pool on my mac pro thou
>>>>>> that
>>>>>> I'll follow up with later tonight.
>>>>>>
>>>>>> Sorry for shortness, back to Spanish class, ;)
>>>>>>
>>>>>> jrm
>>>>>> jason at jasonrm.net
>>>>>>
>>>>>> On Oct 14, 2008, at 6:17 PM, No?l Dellofano <ndellofano at appl
>>>>>> e.com>
>>>>>> wrote:
>>>>>>
>>>>>>> Hey guys!
>>>>>>>
>>>>>>> Yeah, so the whole schedule ramping up party you allude to is
>>>>>>> happening, hence my slowness on the email of late.  Sorry.
>>>>>>>
>>>>>>> So on to some answers for you.  So I was holding releasing new
>>>>>>> bits
>>>>>>> until we sync with the new Solaris base, which will have those
>>>>>> tasty
>>>>>>> case insensitivity options you hunger for.  Problem being, it's
>>>>>>> not
>>>>>>> stable, and we're still debugging it.  So I didn't want to
>>>>>>> release
>>>>>>> shady bits to everyone, hence I haven't released any.  I do have
>>>>>> a few
>>>>>>> fixes in line for you for some issues.  The trash emptying for
>>>>>> example
>>>>>>> is fixed, the wacky names of filesystems over AFP and NFS in
>>>>>>> Finder
>>>>>>> we've got fixes for, and a few others.  Spotlight, I hear you,  
>>>>>>> we
>>>>>> are
>>>>>>> working on.  Finder issues, also being worked on.  Many issues
>>>>>> involve
>>>>>>> more then just the ZFS code to change (opting in to preference
>>>>>> panes
>>>>>>> and such) so fixed bits I get to you still may not do exactly
>>>>>> what you
>>>>>>> want as other parts of the system has to be adjusted as well.
>>>>>>>
>>>>>>> and never fear, telling my what drives you nuts is good, it  
>>>>>>> helps
>>>>>> me
>>>>>>> know what's broken and what you guys use so I can fix it :) So
>>>>>> info is
>>>>>>> good and I promise to not black list you :)
>>>>>>>
>>>>>>>
>>>>>>> Noel
>>>>>>>
>>>>>>> On Oct 12, 2008, at 2:03 PM, Boyd Waters wrote:
>>>>>>>
>>>>>>>>>> hi there @mothership,
>>>>>>>>>>
>>>>>>>>>> it's sunday afternoon here, so why not use some spare minutes
>>>>>> just
>>>>>>>>>> to
>>>>>>>>>> get on your nerves... :>
>>>>>>>>>>
>>>>>>>>>> any new bits in sight? especially the problem with the volume
>>>>>> names
>>>>>>>>>> within Finder and other parts (sharing prefpane, time machine
>>>>>>>>>> prefpane, etc...) throughout mac os x is really a show
>>>>>> stopper...
>>>>>>>>>> (I
>>>>>>>>>> try to survive with icons, but they're not used in every
>>>>>> place)..
>>>>>>>>>> also
>>>>>>>>>> the incompatibility (or sort of) with spotlight.
>>>>>>>>>> And yes, the fact that ZFS filesystems mounted on any other
>>>>>> place
>>>>>>>>>> than
>>>>>>>>>> the /Volumes/Pool/Fs default do not appear _at all_ in the
>>>>>> Finder
>>>>>>>>>> (just with cmd-shift-G) is no fun... Oh, and while I'm here,
>>>>>> to add
>>>>>>>>>> some salt, let me exhume those case-sensitivity flamewars:
>>>>>>>>>> that
>>>>>>>>>> filesystem-specific flag is really a must, unfortunately. I
>>>>>> keep on
>>>>>>>>>> stumbling upon apps that I have to move to my HFS /
>>>>>> Applications to
>>>>>>>>>> get
>>>>>>>>>> them to work... (afaik this issue is likely to be a show
>>>>>>>>>> stopper
>>>>>>>>>> also
>>>>>>>>>> on a fileserver, for example for the network Applications
>>>>>>>>>> share...but
>>>>>>>>>> then again maybe the afp handles this, haven't tried it  
>>>>>>>>>> yet..)
>>>>>>>>>>
>>>>>>>>>> ok, now I'm definitively on your blacklist and I better get
>>>>>>>>>> some
>>>>>>>>>> shelter before you get to Your mailbox ;)
>>>>>>>>>>
>>>>>>>>>> Sincere regards for all the work done and shared so far,
>>>>>>>>>>
>>>>>>>>>> Lorenzo
>>>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> On Oct 12, 2008, at 12:08 PM, Byron Servies wrote:
>>>>>>>>
>>>>>>>>> Having spent the past 3 months doing exactly the same thing, I
>>>>>> wish
>>>>>>>>> them good luck.  Don't forget sleep and exercise!
>>>>>>>>
>>>>>>>>
>>>>>>>> Oh, man. What you said. I completely agree with that estimate!
>>>>>>>>
>>>>>>>> And as I understand, you could compile the bits yourself, as  
>>>>>>>> the
>>>>>>>> source code is posted on the site.
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> zfs-discuss mailing list
>>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> zfs-discuss mailing list
>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>> _______________________________________________
>>>>>> zfs-discuss mailing list
>>>>>> zfs-discuss at lists.macosforge.org
>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From aorchid at mac.com  Wed Oct 15 13:21:21 2008
From: aorchid at mac.com (Aric Gregson)
Date: Wed, 15 Oct 2008 13:21:21 -0700
Subject: [zfs-discuss] late mount
In-Reply-To: <459D3ED1-5BD5-41AE-A445-492C2D482E7E@sogeeky.net>
References: <BCD0E944-CDD6-4404-8F5D-5C90484EB51C@spamfreemail.de>
	<76C2FA33-84B7-49C6-9254-74A6661145B3@re.be>
	<48E2180C.4090102@loveturtle.net>
	<EE40AA30-A795-456F-9183-E184E7DDA0B4@apple.com>
	<46CAEFC3-A3EA-4A7A-B696-3084D92F93FC@durin42.com>
	<20080930150928.f989f8f6.aorchid@mac.com>
	<48E2A79D.9020100@loveturtle.net>
	<20080930155309.71cbf62e.aorchid@mac.com>
	<48E2D622.4010504@loveturtle.net> <48E3B098.50201@mac.com>
	<459D3ED1-5BD5-41AE-A445-492C2D482E7E@sogeeky.net>
Message-ID: <48F650C1.3060205@mac.com>

Mr. Zorg wrote:
> Yep. That's incorrect. Now the good news: since that is a mirror, you 
> can remove one disk at a time, fix it, add it back and let of 
> resilver. Then do the next one.
OK. I finally had a chance to redo my mirror.  It was not this simple of 
course, because once I formatted the drives correctly it had less space 
than before, so I was not able to do a simple 'detach' and 'attach'.

/dev/disk1
    #:                       TYPE NAME                    SIZE       
IDENTIFIER
    0:      GUID_partition_scheme                        *931.5 Gi   disk1
    1:                        EFI                         200.0 Mi   disk1s1
    2:                        ZFS peipoolnew              931.2 Gi   disk1s2
/dev/disk2
    #:                       TYPE NAME                    SIZE       
IDENTIFIER
    0:     FDisk_partition_scheme                        *931.5 Gi   disk2
    1:                                                    931.5 Gi   disk2s1

A very slight difference, but enough for ZFS to balk.  Other than the 
inability to pipe the send/receive commands it went well after that. I 
did not see this fifo thing, so I had to rsync a few filesystems for 
which I received an I/O error on zfs recv.

Looking good now, would love to see the Finder issues and ZFS send/recv 
fixed in the near future.

thanks, aric

From hanche at math.ntnu.no  Wed Oct 15 13:48:13 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Wed, 15 Oct 2008 22:48:13 +0200 (CEST)
Subject: [zfs-discuss] [OT] language lessons
In-Reply-To: <42AF7002-AD06-40C7-85F0-4F0BDEA2C675@mac.com>
References: <1C4728C9-98E9-456E-83F7-7B28C8945C50@apple.com>
	<A9BED72B-EE92-45D5-9AB8-45B9E5ACE912@sogeeky.net>
	<42AF7002-AD06-40C7-85F0-4F0BDEA2C675@mac.com>
Message-ID: <20081015.224813.135902702.hanche@math.ntnu.no>

+ Dirk Schelfhout <dirkschelfhout at mac.com>:

> better translation is : I agree with this statement
> or more loosly : I concur
> or : I can emphasize this

Hmm. My first guess was that the German "unterschreiben" corresponded
directly to the Norwegian word "underskrive", which quite literally
means "write beneath", and usually means "to sign" (a document), but
it also has the figurative sense "to agree" or "concur" as you said.
Is the German word not also used about signing documents?

- Harald

From zorg at sogeeky.net  Wed Oct 15 13:54:29 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Wed, 15 Oct 2008 13:54:29 -0700
Subject: [zfs-discuss] [OT] language lessons
In-Reply-To: <20081015.224813.135902702.hanche@math.ntnu.no>
References: <1C4728C9-98E9-456E-83F7-7B28C8945C50@apple.com>
	<A9BED72B-EE92-45D5-9AB8-45B9E5ACE912@sogeeky.net>
	<42AF7002-AD06-40C7-85F0-4F0BDEA2C675@mac.com>
	<20081015.224813.135902702.hanche@math.ntnu.no>
Message-ID: <E6BFDECB-164F-4EA2-B9C6-AD01E0A6B8ED@sogeeky.net>

Ah, literally "under written". I took that to mean underscore or  
underline. But it also makes sense as "to support", "sign on with", etc.

As for the free German lessons, I spent 7 years there as a kid, but  
I'm very rusty. :)

On Oct 15, 2008, at 1:48 PM, Harald Hanche-Olsen <hanche at math.ntnu.no>  
wrote:

> + Dirk Schelfhout <dirkschelfhout at mac.com>:
>
>> better translation is : I agree with this statement
>> or more loosly : I concur
>> or : I can emphasize this
>
> Hmm. My first guess was that the German "unterschreiben" corresponded
> directly to the Norwegian word "underskrive", which quite literally
> means "write beneath", and usually means "to sign" (a document), but
> it also has the figurative sense "to agree" or "concur" as you said.
> Is the German word not also used about signing documents?
>
> - Harald
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From hanche at math.ntnu.no  Wed Oct 15 14:01:13 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Wed, 15 Oct 2008 23:01:13 +0200 (CEST)
Subject: [zfs-discuss] Non-UTF-8 filenames
Message-ID: <20081015.230113.254759671.hanche@math.ntnu.no>

One of my big reason for using zfs is that I use both Macs and FreeBSD
machines regularly, and I like having a real filesystem (ntfs and fat
need not apply) that will work on both architectures. (Transferring a
hundred gigabytes by moving a disk is so much faster than shipping
them across a network.)

But on the FreeBSD side there is really no special character encoding
used for filenames, which means I typically end up just using
latin-1. And that spells trouble of course, when I access the
filesystem from a mac: ls will display the troublesome filenames, but
any attempt at stat()ing or accessing the file in any way will fail
(including ls -F of course).

Now the number of files involved is not huge in my case, but surely, I
cannot be the only person who runs into this problem? It would be nice
if there were an option to treat a zfs filesystem as having different
encodings of filenames.

Right now what I am thinking of is this: Mount the filesystem on
FreeBSD, run a find job to list all the troublesome pathnames and put
them in a file, transfer that list to the mac, and massage it (using
iconv) into a shell script I can run on the FreeBSD side to rename the
files into something the Mac can understand.

It shouldn't be this hard, IMO. Any thoughts?

- Harald

From lopez.on.the.lists at yellowspace.net  Thu Oct 16 04:28:51 2008
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Thu, 16 Oct 2008 13:28:51 +0200
Subject: [zfs-discuss] hungry for new bits...
In-Reply-To: <1C4728C9-98E9-456E-83F7-7B28C8945C50@apple.com>
References: <EF17B584-9549-49D0-9C8D-5CBF9C8AD908@yellowspace.net>
	<FDECDA5D-7A55-4E0F-873E-C0890D413C68@mac.com>
	<59D9F21C-BBB1-4942-9642-A41C23F0FDD4@nrao.edu>
	<819D142E-D01C-47B7-A596-EB0765EB747D@apple.com>
	<3D24322D-9CC8-4B91-8896-8ED11D955F73@mac.com>
	<72feba2a0810150900r5f4d04b4wa2424abbc13a6c0f@mail.gmail.com>
	<56772C58-F3F0-4B36-AAD7-FF2774FCC16A@spamfreemail.de>
	<6382BD79-7816-450F-8602-B48C1E96C49F@spamfreemail.de>
	<1C4728C9-98E9-456E-83F7-7B28C8945C50@apple.com>
Message-ID: <04A022D5-BCED-4251-8C9F-F1AAAF50066B@yellowspace.net>

Good news I hear, it's nice to see you're listening, working on the  
bugs, and, vor Allem (first of all!), that you're on a good mood. I'll  
love to test the new bits as soon as You deem them stable enough not  
to eat up my holiday pictures (which, I admit, are way too many, but  
yet not worth being all lost!)...

Hut ab und ... Thanx for listening!

Lorenzo



On 15.10.2008, at 20:23, Jordan K. Hubbard wrote:

> Macht nichts.   In american english, we would simply translate that
> german phrase of yours to the rather more consise:  "What he said!"
> You may consider this your free Amerikanische Umgangssprache lesson
> for today. ;-)
>
> On Oct 15, 2008, at 9:10 AM, ruebezahl wrote:
>
>>
>> wuuups, sorry for the german, i was doing about 300 things
>> simultaneously here :)
>>
>> i wanted to point out that adrian is completely right :)
>>
>>
>> franz
>>
>>
>>
>> On 15.10.2008, at 18:07, ruebezahl wrote:
>>
>>> das kann ich nur unterschreiben...
>>>
>>>
>>> On 15.10.2008, at 18:00, Adrian Thornton wrote:
>>>
>>>> Yes, No?l, I think none of us can thank you enough for all all your
>>>> hard work on this project. I know it's been said before, but it
>>>> deserves saying again. I think OS X and ZFS are about the two most
>>>> exciting aspects of computing today, and the fact that they're  
>>>> being
>>>> put together is all down to the amazing work by you and your team.
>>>> Thanks so much!
>>>>
>>>> - Adrian
>>>>
>>>> On Tue, 2008-10-14 at 19:29 -0700, Jason Richard McNeil wrote:
>>>>>
>>>>> No?l, thank you very much for what you do. I know I am looking
>>>>> forward
>>>>> to more bits as I have a new MacBook Pro on the way as of today
>>>>> and I
>>>>> would very much like to start off with a ZFS home. :)
>>>>>
>>>>> I have had some issues with a multi drive pool on my mac pro thou
>>>>> that
>>>>> I'll follow up with later tonight.
>>>>>
>>>>> Sorry for shortness, back to Spanish class, ;)
>>>>>
>>>>> jrm
>>>>> jason at jasonrm.net
>>>>>
>>>>> On Oct 14, 2008, at 6:17 PM, No?l Dellofano <ndellofano at apple.com>
>>>>> wrote:
>>>>>
>>>>>> Hey guys!
>>>>>>
>>>>>> Yeah, so the whole schedule ramping up party you allude to is
>>>>>> happening, hence my slowness on the email of late.  Sorry.
>>>>>>
>>>>>> So on to some answers for you.  So I was holding releasing new
>>>>>> bits
>>>>>> until we sync with the new Solaris base, which will have those
>>>>> tasty
>>>>>> case insensitivity options you hunger for.  Problem being, it's
>>>>>> not
>>>>>> stable, and we're still debugging it.  So I didn't want to  
>>>>>> release
>>>>>> shady bits to everyone, hence I haven't released any.  I do have
>>>>> a few
>>>>>> fixes in line for you for some issues.  The trash emptying for
>>>>> example
>>>>>> is fixed, the wacky names of filesystems over AFP and NFS in
>>>>>> Finder
>>>>>> we've got fixes for, and a few others.  Spotlight, I hear you, we
>>>>> are
>>>>>> working on.  Finder issues, also being worked on.  Many issues
>>>>> involve
>>>>>> more then just the ZFS code to change (opting in to preference
>>>>> panes
>>>>>> and such) so fixed bits I get to you still may not do exactly
>>>>> what you
>>>>>> want as other parts of the system has to be adjusted as well.
>>>>>>
>>>>>> and never fear, telling my what drives you nuts is good, it helps
>>>>> me
>>>>>> know what's broken and what you guys use so I can fix it :) So
>>>>> info is
>>>>>> good and I promise to not black list you :)
>>>>>>
>>>>>>
>>>>>> Noel
>>>>>>
>>>>>> On Oct 12, 2008, at 2:03 PM, Boyd Waters wrote:
>>>>>>
>>>>>>>>> hi there @mothership,
>>>>>>>>>
>>>>>>>>> it's sunday afternoon here, so why not use some spare minutes
>>>>> just
>>>>>>>>> to
>>>>>>>>> get on your nerves... :>
>>>>>>>>>
>>>>>>>>> any new bits in sight? especially the problem with the volume
>>>>> names
>>>>>>>>> within Finder and other parts (sharing prefpane, time machine
>>>>>>>>> prefpane, etc...) throughout mac os x is really a show
>>>>> stopper...
>>>>>>>>> (I
>>>>>>>>> try to survive with icons, but they're not used in every
>>>>> place)..
>>>>>>>>> also
>>>>>>>>> the incompatibility (or sort of) with spotlight.
>>>>>>>>> And yes, the fact that ZFS filesystems mounted on any other
>>>>> place
>>>>>>>>> than
>>>>>>>>> the /Volumes/Pool/Fs default do not appear _at all_ in the
>>>>> Finder
>>>>>>>>> (just with cmd-shift-G) is no fun... Oh, and while I'm here,
>>>>> to add
>>>>>>>>> some salt, let me exhume those case-sensitivity flamewars:  
>>>>>>>>> that
>>>>>>>>> filesystem-specific flag is really a must, unfortunately. I
>>>>> keep on
>>>>>>>>> stumbling upon apps that I have to move to my HFS /
>>>>> Applications to
>>>>>>>>> get
>>>>>>>>> them to work... (afaik this issue is likely to be a show
>>>>>>>>> stopper
>>>>>>>>> also
>>>>>>>>> on a fileserver, for example for the network Applications
>>>>>>>>> share...but
>>>>>>>>> then again maybe the afp handles this, haven't tried it yet..)
>>>>>>>>>
>>>>>>>>> ok, now I'm definitively on your blacklist and I better get
>>>>>>>>> some
>>>>>>>>> shelter before you get to Your mailbox ;)
>>>>>>>>>
>>>>>>>>> Sincere regards for all the work done and shared so far,
>>>>>>>>>
>>>>>>>>> Lorenzo
>>>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> On Oct 12, 2008, at 12:08 PM, Byron Servies wrote:
>>>>>>>
>>>>>>>> Having spent the past 3 months doing exactly the same thing, I
>>>>> wish
>>>>>>>> them good luck.  Don't forget sleep and exercise!
>>>>>>>
>>>>>>>
>>>>>>> Oh, man. What you said. I completely agree with that estimate!
>>>>>>>
>>>>>>> And as I understand, you could compile the bits yourself, as the
>>>>>>> source code is posted on the site.
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> zfs-discuss mailing list
>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>>
>>>>>> _______________________________________________
>>>>>> zfs-discuss mailing list
>>>>>> zfs-discuss at lists.macosforge.org
>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From franzschmalzl at spamfreemail.de  Thu Oct 16 05:52:02 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Thu, 16 Oct 2008 14:52:02 +0200
Subject: [zfs-discuss] [OT] language lessons
In-Reply-To: <20081015.224813.135902702.hanche@math.ntnu.no>
References: <1C4728C9-98E9-456E-83F7-7B28C8945C50@apple.com>
	<A9BED72B-EE92-45D5-9AB8-45B9E5ACE912@sogeeky.net>
	<42AF7002-AD06-40C7-85F0-4F0BDEA2C675@mac.com>
	<20081015.224813.135902702.hanche@math.ntnu.no>
Message-ID: <B0E37414-0361-456A-B9CE-59F602973C65@spamfreemail.de>

> Is the German word not also used about signing documents?

Yes, actually

To sign a contract would be " Einen Vertrag unterschreiben"

But it's also used to agree to some statement as you pointed out :)


-- franz 


On 15.10.2008, at 22:48, Harald Hanche-Olsen wrote:

> + Dirk Schelfhout <dirkschelfhout at mac.com>:
>
>> better translation is : I agree with this statement
>> or more loosly : I concur
>> or : I can emphasize this
>
> Hmm. My first guess was that the German "unterschreiben" corresponded
> directly to the Norwegian word "underskrive", which quite literally
> means "write beneath", and usually means "to sign" (a document), but
> it also has the figurative sense "to agree" or "concur" as you said.
> Is the German word not also used about signing documents?
>
> - Harald
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From franzschmalzl at spamfreemail.de  Thu Oct 16 05:55:45 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Thu, 16 Oct 2008 14:55:45 +0200
Subject: [zfs-discuss] [OT] Language lessons
Message-ID: <55A98398-E6ED-4E1D-8C2E-8822AD803CFD@spamfreemail.de>

To Harald:

I see you are norwegian, aren't you ?

I am really fascinated by your language, and i would love to learn a  
bit from you.

So if you find the time please be so nice and contact me :)

I could offer you some german skills if you find use for them  :)



Best regards,

-- franz 

From Todd.Moore at Sun.COM  Thu Oct 16 22:49:14 2008
From: Todd.Moore at Sun.COM (Todd E. Moore)
Date: Fri, 17 Oct 2008 01:49:14 -0400
Subject: [zfs-discuss] ZFS pool requires root privileges to access
Message-ID: <48F8275A.9020406@sun.com>

An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081017/a7156b8f/attachment.html 

From William.Winnett at Sun.COM  Fri Oct 17 02:50:56 2008
From: William.Winnett at Sun.COM (Bill Winnett)
Date: Fri, 17 Oct 2008 05:50:56 -0400
Subject: [zfs-discuss] error compiling ZFS source
Message-ID: <EAF4BBE5-131D-4CC9-AC2C-369E5EF0F9AE@sun.com>


Fellow ZFS guru's,

My first pass at compiling from sources resulted in these two errors.   
Perhaps someone has already solved this.  Perhaps I am using the  
incorrect gcc version?


Building target ?zfs.kext? of project ?zfs? with configuration  
?Leopard_Release? ? (2 errors)
	    cd /Users/billwinnett/src/zfs-119
     /usr/bin/gcc-3.3 -x c -arch ppc -pipe -Wno-trigraphs -fasm-blocks  
-g -Os -Wmissing-prototypes -Wreturn-type -Wunused-function -Wunused- 
label -Wshadow -DNAMEDSTREAMS=1 -D__APPLE_API_UNSTABLE -D__APPLE__ - 
D_KERNEL -DZFS_LEOPARD_ONLY -fmessage-length=0 -mtune=G5 -Wno- 
deprecated-declarations -Wp,-header-mapfile,/Users/billwinnett/src/ 
zfs-119/build/zfs.build/Leopard_Release/zfs.kext.build/zfs.hmap -Wno- 
unknown-pragmas -Wno-missing-braces -Wno-sign-compare -Wno-parentheses  
-Wno-uninitialized -Wno-implicit-function-declaration -Wno-unused -Wno- 
trigraphs -Wno-char-subscripts -Wno-switch -F/Users/billwinnett/src/ 
zfs-119/build/Leopard_Release -I/Users/billwinnett/src/zfs-119/build/ 
Leopard_Release/include -I/System/Library/Frameworks/Kernel.framework/ 
PrivateHeaders -I/System/Library/Frameworks/Kernel.framework/Headers - 
I/Users/billwinnett/src/zfs-119/build/zfs.build/Leopard_Release/ 
zfs.kext.build/DerivedSources -fno-common -nostdinc -fno-builtin - 
finline -fno-keep-inline-functions -force_cpusubtype_ALL -fno- 
exceptions -msoft-float -static -mlong-branch -DKERNEL - 
DKERNEL_PRIVATE -DDRIVER_PRIVATE -DAPPLE -DNeXT -iquote /Users/ 
billwinnett/src/zfs-119/zfs_common/zfs -isystem /Users/billwinnett/src/ 
zfs-119/zfs_kext/ -isystem /Users/billwinnett/src/zfs-119/zfs_kext/ 
zfs/ -isystem /Users/billwinnett/src/zfs-119/zfs_common/ -isystem / 
Users/billwinnett/src/zfs-119/zfs_common/sys/ -include /Users/ 
billwinnett/src/zfs-119/zfs_common/sys/types.h -include /Users/ 
billwinnett/src/zfs-119/zfs_kext/zfs/sys/zfs_context.h -std=gnu99 -c / 
Users/billwinnett/src/zfs-119/zfs_kext/zfs/zfs_vfsops.c -o /Users/ 
billwinnett/src/zfs-119/build/zfs.build/Leopard_Release/zfs.kext.build/ 
Objects-normal/ppc/zfs_vfsops.o
gcc-3.3: cannot specify -o with -c or -S and multiple compilations
		gcc-3.3: cannot specify -o with -c or -S and multiple compilations
		gcc-3.3: cannot specify -o with -c or -S and multiple compilations
	    cd /Users/billwinnett/src/zfs-119
     /usr/bin/gcc-3.3 -x c -arch ppc -pipe -Wno-trigraphs -fasm-blocks  
-g -Os -Wmissing-prototypes -Wreturn-type -Wunused-function -Wunused- 
label -Wshadow -DNAMEDSTREAMS=1 -D__APPLE_API_UNSTABLE -D__APPLE__ - 
D_KERNEL -DZFS_LEOPARD_ONLY -fmessage-length=0 -mtune=G5 -Wno- 
deprecated-declarations -Wp,-header-mapfile,/Users/billwinnett/src/ 
zfs-119/build/zfs.build/Leopard_Release/zfs.kext.build/zfs.hmap -Wno- 
unknown-pragmas -Wno-missing-braces -Wno-sign-compare -Wno-parentheses  
-Wno-uninitialized -Wno-implicit-function-declaration -Wno-unused -Wno- 
trigraphs -Wno-char-subscripts -Wno-switch -F/Users/billwinnett/src/ 
zfs-119/build/Leopard_Release -I/Users/billwinnett/src/zfs-119/build/ 
Leopard_Release/include -I/System/Library/Frameworks/Kernel.framework/ 
PrivateHeaders -I/System/Library/Frameworks/Kernel.framework/Headers - 
I/Users/billwinnett/src/zfs-119/build/zfs.build/Leopard_Release/ 
zfs.kext.build/DerivedSources -fno-common -nostdinc -fno-builtin - 
finline -fno-keep-inline-functions -force_cpusubtype_ALL -fno- 
exceptions -msoft-float -static -mlong-branch -DKERNEL - 
DKERNEL_PRIVATE -DDRIVER_PRIVATE -DAPPLE -DNeXT -iquote /Users/ 
billwinnett/src/zfs-119/zfs_common/zfs -isystem /Users/billwinnett/src/ 
zfs-119/zfs_kext/ -isystem /Users/billwinnett/src/zfs-119/zfs_kext/ 
zfs/ -isystem /Users/billwinnett/src/zfs-119/zfs_common/ -isystem / 
Users/billwinnett/src/zfs-119/zfs_common/sys/ -include /Users/ 
billwinnett/src/zfs-119/zfs_common/sys/types.h -include /Users/ 
billwinnett/src/zfs-119/zfs_kext/zfs/sys/zfs_context.h -std=gnu99 -c / 
Users/billwinnett/src/zfs-119/zfs_kext/zfs/zfs_vnops.c -o /Users/ 
billwinnett/src/zfs-119/build/zfs.build/Leopard_Release/zfs.kext.build/ 
Objects-normal/ppc/zfs_vnops.o
gcc-3.3: cannot specify -o with -c or -S and multiple compilations
		gcc-3.3: cannot specify -o with -c or -S and multiple compilations
		gcc-3.3: cannot specify -o with -c or -S and multiple compilations
Build failed (2 errors)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081017/44b234a9/attachment.html 

From Todd.Moore at Sun.COM  Fri Oct 17 09:17:51 2008
From: Todd.Moore at Sun.COM (Todd E. Moore)
Date: Fri, 17 Oct 2008 12:17:51 -0400
Subject: [zfs-discuss] ZFS pool requires root privileges to access
In-Reply-To: <48F8275A.9020406@sun.com>
References: <48F8275A.9020406@sun.com>
Message-ID: <48F8BAAF.1030703@sun.com>

An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081017/76b94238/attachment.html 

From andy at aligature.com  Fri Oct 17 09:48:08 2008
From: andy at aligature.com (Andrew Webber)
Date: Fri, 17 Oct 2008 12:48:08 -0400
Subject: [zfs-discuss] ZFS pool requires root privileges to access
In-Reply-To: <48F8BAAF.1030703@sun.com>
References: <48F8275A.9020406@sun.com> <48F8BAAF.1030703@sun.com>
Message-ID: <60b50dc10810170948o3a372cb5u209df019d9306b93@mail.gmail.com>

Why not just chroot -R and be done with it?



On Fri, Oct 17, 2008 at 12:17 PM, Todd E. Moore <Todd.Moore at sun.com> wrote:
> Additional Information after continuing to tinker:
>
> After importing the zpool, if I use "root" to manually 'chmod' the file
> permissions on the zpool's mount point, then non-privilege users can  access
> the pool.  This alone doesn't solve the problem since all files in the pool
> need to be similarly updated.
> If I create the zpool on OpenSolaris (using -o version=6 for compatiblity),
> the Mac OS/X system is able to mount the volume but the default user/group
> is "root:wheel".  Attempts to alter the permissions using "Finder", copy
> files into the pool, and unmount/export the pool result in a 'busy' device
> message from "Finder" during the unmount which necessitates a system reboot
> (system hang/crash if attempt to simply remove the USB device).
> Thinking that the issue may be related to "xattr", I attempted to disable
> "xattr" on the pool using 'zfs set xattr=off'.  This did not alter the
> behavior in any way.
>
> Todd E. Moore wrote:
>
> Issue:  Privileged (root) account required to access zpool imported from Mac
> OS/X.
>
>
> Just installed b119 bits onto my OS/X 10.5.5 system today in an attempt to
> share VirtualBox disk image files between my Mac and my OpenSolaris (2008.11
> b99) laptop.
>
> Install worked well and I was able to create ZFS pools (v6 or v8) on USB
> memory sticks and drives on the Mac without issue.  Unmounted the
> volume/pool, exported the pool, attached the drive to the laptop, and
> performed the zpool import.  I learned that the uid/gid from the user
> account on the Mac gets embedded in the pool and the system attempts to use
> that information when mounting the filesystem.  In this case, I created
> user/group accounts called "leopard" on the OpenSolaris side.
>
> As 'root' user, I can see the pools mounted in /.
> drwxr-xr-x   4 leopard leopard       5 2008-10-17 01:39 a1g_pool
> drwxr-xrwx   5 leopard leopard       8 2008-10-16 17:02 a4g_pool
>
>
> If I run the same 'ls' command as a normal, unprivileged user, the output is
> a lot different -
> ??????????   ? ?    ?          ?                ? a1g_pool
> ??????????   ? ?    ?          ?                ? a4g_pool
>
> A quick Google search turned up similar mentions of this issue over the past
> 6 months with sharing pools with FreeBSD and others, but no resolutions.  I
> tried changing ACL-related properties, but I think the issue is more basic
> and intrinsic to the settings of the base pool.
>
> Below is also a list of the settings reported from a 'zfs get all a1g_pool'
> command -
>
> NAME      PROPERTY              VALUE                  SOURCE
>
> a1g_pool  type                  filesystem             -
>
> a1g_pool  creation              Fri Oct 17  1:39 2008  -
>
> a1g_pool  used                  314K                   -
>
> a1g_pool  available             952M                   -
>
> a1g_pool  referenced            270K                   -
>
> a1g_pool  compressratio         1.00x                  -
>
> a1g_pool  mounted               yes                    -
>
> a1g_pool  quota                 none                   default
>
> a1g_pool  reservation           none                   default
>
> a1g_pool  recordsize            128K                   default
>
> a1g_pool  mountpoint            /a1g_pool              default
>
> a1g_pool  sharenfs              off                    default
>
> a1g_pool  checksum              on                     default
>
> a1g_pool  compression           off                    default
>
> a1g_pool  atime                 on                     default
>
> a1g_pool  devices               on                     default
>
> a1g_pool  exec                  on                     default
>
> a1g_pool  setuid                on                     default
>
> a1g_pool  readonly              off                    default
>
> a1g_pool  zoned                 off                    default
>
> a1g_pool  snapdir               hidden                 default
>
> a1g_pool  aclmode               groupmask              default
>
> a1g_pool  aclinherit            restricted             default
>
> a1g_pool  canmount              on                     default
>
> a1g_pool  shareiscsi            off                    default
>
> a1g_pool  xattr                 on                     default
>
> a1g_pool  copies                1                      default
>
> a1g_pool  version               1                      -
>
> a1g_pool  utf8only              off                    -
>
> a1g_pool  normalization         none                   -
>
> a1g_pool  casesensitivity       sensitive              -
>
> a1g_pool  vscan                 off                    default
>
> a1g_pool  nbmand                off                    default
>
> a1g_pool  sharesmb              off                    default
>
> a1g_pool  refquota              none                   default
>
> a1g_pool  refreservation        none                   default
>
> a1g_pool  primarycache          all                    default
>
> a1g_pool  secondarycache        all                    default
>
> --
> Todd E. Moore
> Sun Microsystems Incorporated
> 443.516.4002
> AIM: toddmoore72462
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>

From mcamou at tecnoguru.com  Sat Oct 18 07:41:25 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Sat, 18 Oct 2008 16:41:25 +0200
Subject: [zfs-discuss] Mixing and matching replication types inside a pool
Message-ID: <762437f0810180741w7ca3c8c5h2ea858b0c2d09aaa@mail.gmail.com>

Hi all,

I currently have a pool that uses 2 mirrored 1TB drives. I want to add 3
RAIDZ'ed 500GB drives to get a total storage of 2TB. However I'd like to
have all my storage in a single volume.

At this point if I want to add a raidz to the pool ZFS tells me that the
pool is mirrored.

Is there any way to accomplish this (i.e., get a single volume with all my
storage, part mirrored and part RAIDZ'ed)? Do I need to recreate the pool
using RAIDZ and add the 2x1TB drives as a 2-volume RAIDZ?

Thanks,
-Mario


-- 
The impossible has, on occasion, let me down
                                                       --R.U. Sirius
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081018/7abacf9e/attachment.html 

From richmc at gmail.com  Sat Oct 18 08:01:01 2008
From: richmc at gmail.com (Richard McClellan)
Date: Sat, 18 Oct 2008 08:01:01 -0700
Subject: [zfs-discuss] Mixing and matching replication types inside a
	pool
In-Reply-To: <762437f0810180741w7ca3c8c5h2ea858b0c2d09aaa@mail.gmail.com>
References: <762437f0810180741w7ca3c8c5h2ea858b0c2d09aaa@mail.gmail.com>
Message-ID: <1066A3ED-7E4E-498B-902B-CDFC381E0209@gmail.com>

You can have multiple RAIDZ vdevs, but ZFS wants the vdevs to be the  
same (I.e., 2 raidz1 vdevs comprised of three disks each). You may be  
able to force it (-f), but with the cheap price  of 500GB disks you're  
better off buying one more and creating a set of 3 by 2 mirrored  
disks. This approach will offer you an easy way to expand your array  
in the future that raidz doesn't offer.

                 Rich

On Oct 18, 2008, at 7:41, "Mario Camou" <mcamou at tecnoguru.com> wrote:

> Hi all,
>
> I currently have a pool that uses 2 mirrored 1TB drives. I want to  
> add 3 RAIDZ'ed 500GB drives to get a total storage of 2TB. However  
> I'd like to have all my storage in a single volume.
>
> At this point if I want to add a raidz to the pool ZFS tells me that  
> the pool is mirrored.
>
> Is there any way to accomplish this (i.e., get a single volume with  
> all my storage, part mirrored and part RAIDZ'ed)? Do I need to  
> recreate the pool using RAIDZ and add the 2x1TB drives as a 2-volume  
> RAIDZ?
>
> Thanks,
> -Mario
>
>
> -- 
> The impossible has, on occasion, let me down
>                                                        --R.U. Sirius
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From mcamou at tecnoguru.com  Sat Oct 18 08:17:23 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Sat, 18 Oct 2008 17:17:23 +0200
Subject: [zfs-discuss] Mixing and matching replication types inside a
	pool
In-Reply-To: <762437f0810180741w7ca3c8c5h2ea858b0c2d09aaa@mail.gmail.com>
References: <762437f0810180741w7ca3c8c5h2ea858b0c2d09aaa@mail.gmail.com>
Message-ID: <762437f0810180817v1dda547co47f7b566b92e96f2@mail.gmail.com>

OK, so I should've googled a bit more. Things are a bit clearer (just a
bit).

It turns out that you *CAN* mix and match but you have to specify the -f
flag to do so. However, in my googling I found several places where it says
that it's not recommended to mix. Can anybody tell me why this would be so?
I realize that mixing redundant and non-redundant (i.e., a mirror with a
plain disk) is just not a good idea, but why would it not be a good idea to
mix a mirror with a raidz?

Thanks,
-Mario.

On Sat, Oct 18, 2008 at 4:41 PM, Mario Camou <mcamou at tecnoguru.com> wrote:

> Hi all,
>
> I currently have a pool that uses 2 mirrored 1TB drives. I want to add 3
> RAIDZ'ed 500GB drives to get a total storage of 2TB. However I'd like to
> have all my storage in a single volume.
>
> At this point if I want to add a raidz to the pool ZFS tells me that the
> pool is mirrored.
>
> Is there any way to accomplish this (i.e., get a single volume with all my
> storage, part mirrored and part RAIDZ'ed)? Do I need to recreate the pool
> using RAIDZ and add the 2x1TB drives as a 2-volume RAIDZ?
>
> Thanks,
> -Mario
>
>
> --
> The impossible has, on occasion, let me down
>                                                        --R.U. Sirius
>



-- 
The impossible has, on occasion, let me down
                                                       --R.U. Sirius
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081018/9c24b12d/attachment.html 

From richmc at gmail.com  Sat Oct 18 08:50:45 2008
From: richmc at gmail.com (Richard McClellan)
Date: Sat, 18 Oct 2008 08:50:45 -0700
Subject: [zfs-discuss] Mixing and matching replication types inside a
	pool
In-Reply-To: <762437f0810180817v1dda547co47f7b566b92e96f2@mail.gmail.com>
References: <762437f0810180741w7ca3c8c5h2ea858b0c2d09aaa@mail.gmail.com>
	<762437f0810180817v1dda547co47f7b566b92e96f2@mail.gmail.com>
Message-ID: <581ABE26-3DD7-4F44-AA95-F9BEF16CD3A3@gmail.com>

The ZFS Best Practices Guide is a good resource: http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide#RAID-Z_Configuration_Requirements_and_Recommendations

		Rich


On Oct 18, 2008, at 08:17 , Mario Camou wrote:

> OK, so I should've googled a bit more. Things are a bit clearer  
> (just a bit).
>
> It turns out that you *CAN* mix and match but you have to specify  
> the -f flag to do so. However, in my googling I found several places  
> where it says that it's not recommended to mix. Can anybody tell me  
> why this would be so? I realize that mixing redundant and non- 
> redundant (i.e., a mirror with a plain disk) is just not a good  
> idea, but why would it not be a good idea to mix a mirror with a  
> raidz?
>
> Thanks,
> -Mario.
>
> On Sat, Oct 18, 2008 at 4:41 PM, Mario Camou <mcamou at tecnoguru.com>  
> wrote:
> Hi all,
>
> I currently have a pool that uses 2 mirrored 1TB drives. I want to  
> add 3 RAIDZ'ed 500GB drives to get a total storage of 2TB. However  
> I'd like to have all my storage in a single volume.
>
> At this point if I want to add a raidz to the pool ZFS tells me that  
> the pool is mirrored.
>
> Is there any way to accomplish this (i.e., get a single volume with  
> all my storage, part mirrored and part RAIDZ'ed)? Do I need to  
> recreate the pool using RAIDZ and add the 2x1TB drives as a 2-volume  
> RAIDZ?
>
> Thanks,
> -Mario
>
>
> -- 
> The impossible has, on occasion, let me down
>                                                        --R.U. Sirius
>
>
>
> -- 
> The impossible has, on occasion, let me down
>                                                        --R.U. Sirius
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081018/be22da41/attachment.html 

From mcamou at tecnoguru.com  Sat Oct 18 10:54:50 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Sat, 18 Oct 2008 19:54:50 +0200
Subject: [zfs-discuss] Mixing and matching replication types inside a
	pool
In-Reply-To: <762437f0810181011x31b0a2a4ke5894c6c9df2a658@mail.gmail.com>
References: <762437f0810180741w7ca3c8c5h2ea858b0c2d09aaa@mail.gmail.com>
	<1066A3ED-7E4E-498B-902B-CDFC381E0209@gmail.com>
	<762437f0810180826p67bd1748y13c2b405d701a464@mail.gmail.com>
	<7BC2A3D3-367B-4D20-BD9A-FB59A66D2F0E@gmail.com>
	<762437f0810180953o34eba121t931a416e6ab463ae@mail.gmail.com>
	<F05A67C2-5A8A-443B-A585-7A04BA2D0DBA@gmail.com>
	<762437f0810181011x31b0a2a4ke5894c6c9df2a658@mail.gmail.com>
Message-ID: <762437f0810181054o3654c495s5b330e1bb6e89940@mail.gmail.com>

I just found that this last question went just to Richard and not the rest
of the list...

On Sat, Oct 18, 2008 at 7:11 PM, Mario Camou <mcamou at tecnoguru.com> wrote:

> Oops....forgot to do the diskutil partitiondisk :( I guess since I haven't
> written anything else to the pool I could somehow detach the raidz, reformat
> it and re-add it? Actually, could there be a problem if I leave them like
> that?
>
> MediaCentral is payware but in some respects is better than Front Row. I
> like that it also supports Live TV and that you don't have to add all your
> media to iTunes (since most of my stuff is on the server). I am
> investigating what I can use for PVR functions (i.e., recording).
>
>
> On Sat, Oct 18, 2008 at 6:59 PM, Richard McClellan <richmc at gmail.com>wrote:
>
>> Did you import/partition them with the
>>
>> diskutil partitiondisk <disk> GPTFormat ZFS %noformat% 100%
>>
>> command?
>>
>> I totally understand the SWMBO.  In my case the disks are for movies of
>> the kids/family so I'm given a little more leeway :-)  I find it works great
>> for doing video editing where the bottleneck is usually disk.  I'll have to
>> check out MediaCentral. (Your USB setup should be more than fine for
>> streaming media.)
>>
>> Let me know how using different vdevs works out for you.
>>
>>                Rich
>>
>>
>> On Oct 18, 2008, at 09:53 , Mario Camou wrote:
>>
>>  . The 3 disks in the RA
>>>
>>
>>
>
>
> --
> The impossible has, on occasion, let me down
>                                                        --R.U. Sirius
>



-- 
The impossible has, on occasion, let me down
                                                       --R.U. Sirius
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081018/d4a8e4b5/attachment.html 

From mcamou at tecnoguru.com  Sat Oct 18 10:59:38 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Sat, 18 Oct 2008 19:59:38 +0200
Subject: [zfs-discuss] Mixing and matching replication types inside a
	pool
In-Reply-To: <762437f0810181054o3654c495s5b330e1bb6e89940@mail.gmail.com>
References: <762437f0810180741w7ca3c8c5h2ea858b0c2d09aaa@mail.gmail.com>
	<1066A3ED-7E4E-498B-902B-CDFC381E0209@gmail.com>
	<762437f0810180826p67bd1748y13c2b405d701a464@mail.gmail.com>
	<7BC2A3D3-367B-4D20-BD9A-FB59A66D2F0E@gmail.com>
	<762437f0810180953o34eba121t931a416e6ab463ae@mail.gmail.com>
	<F05A67C2-5A8A-443B-A585-7A04BA2D0DBA@gmail.com>
	<762437f0810181011x31b0a2a4ke5894c6c9df2a658@mail.gmail.com>
	<762437f0810181054o3654c495s5b330e1bb6e89940@mail.gmail.com>
Message-ID: <762437f0810181059x34b2a10bx186ab1c7f6d6fc55@mail.gmail.com>

One more problem...

I was doing a zpool scrub. I did a zpool status and the host completely
froze after giving the following output:

mario at tumbolia ~/Downloads/incoming/rtorrent 14 % zpool status
  pool: phlogiston
 state: ONLINE
status: One or more devices has experienced an error resulting in data
        corruption.  Applications may be affected.
action: Restore the file in question if possible.  Otherwise restore the
        entire pool from backup.
   see: http://www.sun.com/msg/ZFS-8000-8A
 scrub: scrub stopped with 0 errors on Sat Oct 18 19:45:11 2008
config:

        NAME         STATE     READ WRITE CKSUM
        phlogiston   ONLINE       0     0     4
          mirror     ONLINE       0     0     0
            disk2s2  ONLINE       0     0     0
            disk1s2  ONLINE       0     0     0
          raidz1     ONLINE       0     0     4
            disk5s2  ONLINE       0     0     0
            disk3s2  ONLINE       0     0     0
            disk4s2  ONLINE       0     0     0

Now I have a few questions:

- How can I determine "the file in question"?
- Why does the raidz as a whole give the checksum error? How can I determine
which physical device has the problem?
- How can I recover from this without restoring from backup? I don't
currently have a completely up-to-date backup. If it's just one file that
has a problem I can probably work around it.

Thanks,
-Mario.

On Sat, Oct 18, 2008 at 7:54 PM, Mario Camou <mcamou at tecnoguru.com> wrote:

> I just found that this last question went just to Richard and not the rest
> of the list...
>
> On Sat, Oct 18, 2008 at 7:11 PM, Mario Camou <mcamou at tecnoguru.com> wrote:
>
>> Oops....forgot to do the diskutil partitiondisk :( I guess since I haven't
>> written anything else to the pool I could somehow detach the raidz, reformat
>> it and re-add it? Actually, could there be a problem if I leave them like
>> that?
>>
>>
>> MediaCentral is payware but in some respects is better than Front Row. I
>> like that it also supports Live TV and that you don't have to add all your
>> media to iTunes (since most of my stuff is on the server). I am
>> investigating what I can use for PVR functions (i.e., recording).
>>
>>
>> On Sat, Oct 18, 2008 at 6:59 PM, Richard McClellan <richmc at gmail.com>wrote:
>>
>>> Did you import/partition them with the
>>>
>>> diskutil partitiondisk <disk> GPTFormat ZFS %noformat% 100%
>>>
>>> command?
>>>
>>> I totally understand the SWMBO.  In my case the disks are for movies of
>>> the kids/family so I'm given a little more leeway :-)  I find it works great
>>> for doing video editing where the bottleneck is usually disk.  I'll have to
>>> check out MediaCentral. (Your USB setup should be more than fine for
>>> streaming media.)
>>>
>>> Let me know how using different vdevs works out for you.
>>>
>>>                Rich
>>>
>>>
>>> On Oct 18, 2008, at 09:53 , Mario Camou wrote:
>>>
>>>  . The 3 disks in the RA
>>>>
>>>
>>>
>>
>>
>> --
>> The impossible has, on occasion, let me down
>>                                                        --R.U. Sirius
>>
>
>
>
> --
> The impossible has, on occasion, let me down
>                                                        --R.U. Sirius
>



-- 
The impossible has, on occasion, let me down
                                                       --R.U. Sirius
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081018/178eadd9/attachment-0001.html 

From mcamou at tecnoguru.com  Sat Oct 18 11:29:03 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Sat, 18 Oct 2008 20:29:03 +0200
Subject: [zfs-discuss] Now zpool status is giving me a kernel panic
Message-ID: <762437f0810181129n22f022eu81169f8442a4e4f9@mail.gmail.com>

Another data point.

I booted in single user mode, did the fsck and "mount -uw /" and then did a
"zpool import". When doing a "zpool status -v" I get a kernel panic right
after the config is shown (with all devices showing as ONLINE and with 0
errors) and before where the filenames should be. I still get the "One or
more devices has experienced an error" status message.

If I knew just which device is giving the problem I could just unplug it
(since both groups in the pool have redundancy -- one with mirroring and one
with raidz) but now I'm really leery of trying to do anything that might
result in data corruption.

Any ideas? HELP!!

On Sat, Oct 18, 2008 at 7:59 PM, Mario Camou <mcamou at tecnoguru.com> wrote:

> One more problem...
>
> I was doing a zpool scrub. I did a zpool status and the host completely
> froze after giving the following output:
>
> mario at tumbolia ~/Downloads/incoming/rtorrent 14 % zpool status
>   pool: phlogiston
>  state: ONLINE
> status: One or more devices has experienced an error resulting in data
>         corruption.  Applications may be affected.
> action: Restore the file in question if possible.  Otherwise restore the
>         entire pool from backup.
>    see: http://www.sun.com/msg/ZFS-8000-8A
>  scrub: scrub stopped with 0 errors on Sat Oct 18 19:45:11 2008
> config:
>
>         NAME         STATE     READ WRITE CKSUM
>         phlogiston   ONLINE       0     0     4
>           mirror     ONLINE       0     0     0
>             disk2s2  ONLINE       0     0     0
>             disk1s2  ONLINE       0     0     0
>           raidz1     ONLINE       0     0     4
>             disk5s2  ONLINE       0     0     0
>             disk3s2  ONLINE       0     0     0
>             disk4s2  ONLINE       0     0     0
>
> Now I have a few questions:
>
> - How can I determine "the file in question"?
> - Why does the raidz as a whole give the checksum error? How can I
> determine which physical device has the problem?
> - How can I recover from this without restoring from backup? I don't
> currently have a completely up-to-date backup. If it's just one file that
> has a problem I can probably work around it.
>
> Thanks,
> -Mario.
>
>
> On Sat, Oct 18, 2008 at 7:54 PM, Mario Camou <mcamou at tecnoguru.com> wrote:
>
>> I just found that this last question went just to Richard and not the rest
>> of the list...
>>
>> On Sat, Oct 18, 2008 at 7:11 PM, Mario Camou <mcamou at tecnoguru.com>wrote:
>>
>>> Oops....forgot to do the diskutil partitiondisk :( I guess since I
>>> haven't written anything else to the pool I could somehow detach the raidz,
>>> reformat it and re-add it? Actually, could there be a problem if I leave
>>> them like that?
>>>
>>>
>>> MediaCentral is payware but in some respects is better than Front Row. I
>>> like that it also supports Live TV and that you don't have to add all your
>>> media to iTunes (since most of my stuff is on the server). I am
>>> investigating what I can use for PVR functions (i.e., recording).
>>>
>>>
>>> On Sat, Oct 18, 2008 at 6:59 PM, Richard McClellan <richmc at gmail.com>wrote:
>>>
>>>> Did you import/partition them with the
>>>>
>>>> diskutil partitiondisk <disk> GPTFormat ZFS %noformat% 100%
>>>>
>>>> command?
>>>>
>>>> I totally understand the SWMBO.  In my case the disks are for movies of
>>>> the kids/family so I'm given a little more leeway :-)  I find it works great
>>>> for doing video editing where the bottleneck is usually disk.  I'll have to
>>>> check out MediaCentral. (Your USB setup should be more than fine for
>>>> streaming media.)
>>>>
>>>> Let me know how using different vdevs works out for you.
>>>>
>>>>                Rich
>>>>
>>>>
>>>> On Oct 18, 2008, at 09:53 , Mario Camou wrote:
>>>>
>>>>  . The 3 disks in the RA
>>>>>
>>>>
>>>>
>>>
>>>
>>> --
>>> The impossible has, on occasion, let me down
>>>                                                        --R.U. Sirius
>>>
>>
>>
>>
>> --
>> The impossible has, on occasion, let me down
>>                                                        --R.U. Sirius
>>
>
>
>
> --
> The impossible has, on occasion, let me down
>                                                        --R.U. Sirius
>



-- 
The impossible has, on occasion, let me down
                                                       --R.U. Sirius
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081018/54f308e8/attachment.html 

From riscky at gmail.com  Sat Oct 18 13:00:42 2008
From: riscky at gmail.com (Riscky Abacus)
Date: Sat, 18 Oct 2008 16:00:42 -0400
Subject: [zfs-discuss] Now zpool status is giving me a kernel panic
In-Reply-To: <762437f0810181129n22f022eu81169f8442a4e4f9@mail.gmail.com>
References: <762437f0810181129n22f022eu81169f8442a4e4f9@mail.gmail.com>
Message-ID: <51cfc2260810181300i7de1868fqc8767842fb41a47b@mail.gmail.com>

Mario,
Just based on my experience here... are any of your drives in your
raidz USB devices? or on a USB hub?

On Sat, Oct 18, 2008 at 2:29 PM, Mario Camou <mcamou at tecnoguru.com> wrote:
> Another data point.
>
> I booted in single user mode, did the fsck and "mount -uw /" and then did a
> "zpool import". When doing a "zpool status -v" I get a kernel panic right
> after the config is shown (with all devices showing as ONLINE and with 0
> errors) and before where the filenames should be. I still get the "One or
> more devices has experienced an error" status message.
>
> If I knew just which device is giving the problem I could just unplug it
> (since both groups in the pool have redundancy -- one with mirroring and one
> with raidz) but now I'm really leery of trying to do anything that might
> result in data corruption.
>
> Any ideas? HELP!!
>
> On Sat, Oct 18, 2008 at 7:59 PM, Mario Camou <mcamou at tecnoguru.com> wrote:

From mcamou at tecnoguru.com  Sat Oct 18 13:04:28 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Sat, 18 Oct 2008 22:04:28 +0200
Subject: [zfs-discuss] Now zpool status is giving me a kernel panic
In-Reply-To: <51cfc2260810181300i7de1868fqc8767842fb41a47b@mail.gmail.com>
References: <762437f0810181129n22f022eu81169f8442a4e4f9@mail.gmail.com>
	<51cfc2260810181300i7de1868fqc8767842fb41a47b@mail.gmail.com>
Message-ID: <762437f0810181304p73cb4bd0l51f3bf3c3916905e@mail.gmail.com>

Hi,

Thanks for replying.

All of my drives (5 of them) are on a USB hub. Since the Mac Mini has only 4
USB ports (and one of them is taken up by the keyboard/mouse) I really have
no other option. Why? Could moving some of them off of the hub (or changing
hubs) help?

On Sat, Oct 18, 2008 at 10:00 PM, Riscky Abacus <riscky at gmail.com> wrote:

> Mario,
> Just based on my experience here... are any of your drives in your
> raidz USB devices? or on a USB hub?
>
> On Sat, Oct 18, 2008 at 2:29 PM, Mario Camou <mcamou at tecnoguru.com> wrote:
> > Another data point.
> >
> > I booted in single user mode, did the fsck and "mount -uw /" and then did
> a
> > "zpool import". When doing a "zpool status -v" I get a kernel panic right
> > after the config is shown (with all devices showing as ONLINE and with 0
> > errors) and before where the filenames should be. I still get the "One or
> > more devices has experienced an error" status message.
> >
> > If I knew just which device is giving the problem I could just unplug it
> > (since both groups in the pool have redundancy -- one with mirroring and
> one
> > with raidz) but now I'm really leery of trying to do anything that might
> > result in data corruption.
> >
> > Any ideas? HELP!!
> >
> > On Sat, Oct 18, 2008 at 7:59 PM, Mario Camou <mcamou at tecnoguru.com>
> wrote:
>



-- 
The impossible has, on occasion, let me down
                                                       --R.U. Sirius
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081018/d27a7c51/attachment.html 

From riscky at gmail.com  Sat Oct 18 13:17:05 2008
From: riscky at gmail.com (Riscky Abacus)
Date: Sat, 18 Oct 2008 16:17:05 -0400
Subject: [zfs-discuss] Now zpool status is giving me a kernel panic
In-Reply-To: <762437f0810181304p73cb4bd0l51f3bf3c3916905e@mail.gmail.com>
References: <762437f0810181129n22f022eu81169f8442a4e4f9@mail.gmail.com>
	<51cfc2260810181300i7de1868fqc8767842fb41a47b@mail.gmail.com>
	<762437f0810181304p73cb4bd0l51f3bf3c3916905e@mail.gmail.com>
Message-ID: <51cfc2260810181317v670b42cdxe09278b0e41afa6@mail.gmail.com>

I don't have any knowledge as to why this happens but I've found any
raidz i've created where the drives all sit on the same USB hub
consistently crash when running `zpool status -v`. I have a few ideas
why that might be but nothing solid. I have a feeling this my be to
due to the fact disk3s* numbers my be a different physical volume on
subsequent reboots.

Just take this little test case into account... remove all zfs
volumes... reboot. Now find to flash drives or anyhting out you could
use for this test.... put them on the same usb hub and turn them into
a raidz.... now run `zpool status -v` no crash... reboot and run
`zpool status -v` crash.

I've also noticed that configuration also causes issues shutting down my system.

This is all based on my tinkering with free time... and I haven't
devoted much time to figuring out the root cause... I just don't think
the file system plays well with usb, and has issues with hubs... after
all zfs wasn't exactly designed for desktops but servers IMHO.

On Sat, Oct 18, 2008 at 4:04 PM, Mario Camou <mcamou at tecnoguru.com> wrote:
> Hi,
>
> Thanks for replying.
>
> All of my drives (5 of them) are on a USB hub. Since the Mac Mini has only 4
> USB ports (and one of them is taken up by the keyboard/mouse) I really have
> no other option. Why? Could moving some of them off of the hub (or changing
> hubs) help?
>
> On Sat, Oct 18, 2008 at 10:00 PM, Riscky Abacus <riscky at gmail.com> wrote:
>>
>> Mario,
>> Just based on my experience here... are any of your drives in your
>> raidz USB devices? or on a USB hub?
>>
>> On Sat, Oct 18, 2008 at 2:29 PM, Mario Camou <mcamou at tecnoguru.com> wrote:
>> > Another data point.
>> >
>> > I booted in single user mode, did the fsck and "mount -uw /" and then
>> > did a
>> > "zpool import". When doing a "zpool status -v" I get a kernel panic
>> > right
>> > after the config is shown (with all devices showing as ONLINE and with 0
>> > errors) and before where the filenames should be. I still get the "One
>> > or
>> > more devices has experienced an error" status message.
>> >
>> > If I knew just which device is giving the problem I could just unplug it
>> > (since both groups in the pool have redundancy -- one with mirroring and
>> > one
>> > with raidz) but now I'm really leery of trying to do anything that might
>> > result in data corruption.
>> >
>> > Any ideas? HELP!!
>> >
>> > On Sat, Oct 18, 2008 at 7:59 PM, Mario Camou <mcamou at tecnoguru.com>
>> > wrote:
>
>
>
> --
> The impossible has, on occasion, let me down
>                                                        --R.U. Sirius
>

From mcamou at tecnoguru.com  Sat Oct 18 13:19:12 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Sat, 18 Oct 2008 22:19:12 +0200
Subject: [zfs-discuss] Now zpool status is giving me a kernel panic
In-Reply-To: <762437f0810181304p73cb4bd0l51f3bf3c3916905e@mail.gmail.com>
References: <762437f0810181129n22f022eu81169f8442a4e4f9@mail.gmail.com>
	<51cfc2260810181300i7de1868fqc8767842fb41a47b@mail.gmail.com>
	<762437f0810181304p73cb4bd0l51f3bf3c3916905e@mail.gmail.com>
Message-ID: <762437f0810181319i7b6fb40fw44c1bb7028f47d2c@mail.gmail.com>

Hi again,

Rereading your email I think the operative part was "in the raidz". I
plugged in the 3 raidz drives to ports on the Mac Mini (moving the keyboard
to the same hub with the mirrored drives). Same results. Kernel panic.

I guess I should've said this since the first email. I'm running Leopard
(10.5.5) with the latest updates and zfs-119. It might not be of much use to
anyone but just in case, here's most of the relevant panic data. If you need
the full backtrace or the register values just ask, I can reproduce this
100% of the time.

panic(cpu 1 caller 0x001A8CEC): Kernel trap at 0x003626de, type 14=page
fault
... (register data)...
Error code: 0x00000000

Debugger called: <panic>
Backtrace (CPU 1), Frame: Return Address (4 potential args on stack)
0x35f1bb88 : 0x12b0fa (0x459234 0x35f1bbbc 0x133243 0x0))
... (rest of the backtrace) ...
No mapping exists for frame pointer
Backtrace terminated-invalid frame pointer 0xbfffad88
     Kernel loadable modules in backtrace (with dependencies):
        com.apple.filesystems.zfs(8.0)@0xb6f000->0xc3affff

BSD process name corresponding to current thread: zpool

Mac OS version:
9F33

Kernel version:
Darwin Kernel Version 9.5.0: Wed Sep  3 11:29:43 PDT 2008;
root:xnu-1228.7.58~1/RELEASE_I386
System model name: Macmini1,1 (Mac-F4208EC8)

On Sat, Oct 18, 2008 at 10:04 PM, Mario Camou <mcamou at tecnoguru.com> wrote:

> Hi,
>
> Thanks for replying.
>
> All of my drives (5 of them) are on a USB hub. Since the Mac Mini has only
> 4 USB ports (and one of them is taken up by the keyboard/mouse) I really
> have no other option. Why? Could moving some of them off of the hub (or
> changing hubs) help?
>
>
> On Sat, Oct 18, 2008 at 10:00 PM, Riscky Abacus <riscky at gmail.com> wrote:
>
>> Mario,
>> Just based on my experience here... are any of your drives in your
>> raidz USB devices? or on a USB hub?
>>
>> On Sat, Oct 18, 2008 at 2:29 PM, Mario Camou <mcamou at tecnoguru.com>
>> wrote:
>> > Another data point.
>> >
>> > I booted in single user mode, did the fsck and "mount -uw /" and then
>> did a
>> > "zpool import". When doing a "zpool status -v" I get a kernel panic
>> right
>> > after the config is shown (with all devices showing as ONLINE and with 0
>> > errors) and before where the filenames should be. I still get the "One
>> or
>> > more devices has experienced an error" status message.
>> >
>> > If I knew just which device is giving the problem I could just unplug it
>> > (since both groups in the pool have redundancy -- one with mirroring and
>> one
>> > with raidz) but now I'm really leery of trying to do anything that might
>> > result in data corruption.
>> >
>> > Any ideas? HELP!!
>> >
>> > On Sat, Oct 18, 2008 at 7:59 PM, Mario Camou <mcamou at tecnoguru.com>
>> wrote:
>>
>
>
>
> --
> The impossible has, on occasion, let me down
>                                                        --R.U. Sirius
>



-- 
The impossible has, on occasion, let me down
                                                       --R.U. Sirius
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081018/139a27f9/attachment-0001.html 

From mcamou at tecnoguru.com  Sat Oct 18 13:25:13 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Sat, 18 Oct 2008 22:25:13 +0200
Subject: [zfs-discuss] Now zpool status is giving me a kernel panic
In-Reply-To: <51cfc2260810181317v670b42cdxe09278b0e41afa6@mail.gmail.com>
References: <762437f0810181129n22f022eu81169f8442a4e4f9@mail.gmail.com>
	<51cfc2260810181300i7de1868fqc8767842fb41a47b@mail.gmail.com>
	<762437f0810181304p73cb4bd0l51f3bf3c3916905e@mail.gmail.com>
	<51cfc2260810181317v670b42cdxe09278b0e41afa6@mail.gmail.com>
Message-ID: <762437f0810181325h5a2388a0mfd1b218c8530dbab@mail.gmail.com>

If the problem is with the raidz, is there any way of telling ZFS not to
look for it and just make do with the mirror? The data I have all fits in
the mirror and I haven't (yet) written anything into the disks since adding
the raidz, so I imagine it's possible that the data is still just in the
mirror and the raidz is still empty. One can hope. The problem right now is
that if I do an import without connecting the raidz drives it (obviously)
fails because the raidz isn't present.

AARRRGGGHHHH!!!!

On Sat, Oct 18, 2008 at 10:17 PM, Riscky Abacus <riscky at gmail.com> wrote:

> I don't have any knowledge as to why this happens but I've found any
> raidz i've created where the drives all sit on the same USB hub
> consistently crash when running `zpool status -v`. I have a few ideas
> why that might be but nothing solid. I have a feeling this my be to
> due to the fact disk3s* numbers my be a different physical volume on
> subsequent reboots.
>
> Just take this little test case into account... remove all zfs
> volumes... reboot. Now find to flash drives or anyhting out you could
> use for this test.... put them on the same usb hub and turn them into
> a raidz.... now run `zpool status -v` no crash... reboot and run
> `zpool status -v` crash.
>
> I've also noticed that configuration also causes issues shutting down my
> system.
>
> This is all based on my tinkering with free time... and I haven't
> devoted much time to figuring out the root cause... I just don't think
> the file system plays well with usb, and has issues with hubs... after
> all zfs wasn't exactly designed for desktops but servers IMHO.
>
> On Sat, Oct 18, 2008 at 4:04 PM, Mario Camou <mcamou at tecnoguru.com> wrote:
> > Hi,
> >
> > Thanks for replying.
> >
> > All of my drives (5 of them) are on a USB hub. Since the Mac Mini has
> only 4
> > USB ports (and one of them is taken up by the keyboard/mouse) I really
> have
> > no other option. Why? Could moving some of them off of the hub (or
> changing
> > hubs) help?
> >
> > On Sat, Oct 18, 2008 at 10:00 PM, Riscky Abacus <riscky at gmail.com>
> wrote:
> >>
> >> Mario,
> >> Just based on my experience here... are any of your drives in your
> >> raidz USB devices? or on a USB hub?
> >>
> >> On Sat, Oct 18, 2008 at 2:29 PM, Mario Camou <mcamou at tecnoguru.com>
> wrote:
> >> > Another data point.
> >> >
> >> > I booted in single user mode, did the fsck and "mount -uw /" and then
> >> > did a
> >> > "zpool import". When doing a "zpool status -v" I get a kernel panic
> >> > right
> >> > after the config is shown (with all devices showing as ONLINE and with
> 0
> >> > errors) and before where the filenames should be. I still get the "One
> >> > or
> >> > more devices has experienced an error" status message.
> >> >
> >> > If I knew just which device is giving the problem I could just unplug
> it
> >> > (since both groups in the pool have redundancy -- one with mirroring
> and
> >> > one
> >> > with raidz) but now I'm really leery of trying to do anything that
> might
> >> > result in data corruption.
> >> >
> >> > Any ideas? HELP!!
> >> >
> >> > On Sat, Oct 18, 2008 at 7:59 PM, Mario Camou <mcamou at tecnoguru.com>
> >> > wrote:
> >
> >
> >
> > --
> > The impossible has, on occasion, let me down
> >                                                        --R.U. Sirius
> >
>



-- 
The impossible has, on occasion, let me down
                                                       --R.U. Sirius
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081018/150d378f/attachment.html 

From mcamou at tecnoguru.com  Sat Oct 18 13:40:04 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Sat, 18 Oct 2008 22:40:04 +0200
Subject: [zfs-discuss] Now zpool status is giving me a kernel panic
In-Reply-To: <762437f0810181325h5a2388a0mfd1b218c8530dbab@mail.gmail.com>
References: <762437f0810181129n22f022eu81169f8442a4e4f9@mail.gmail.com>
	<51cfc2260810181300i7de1868fqc8767842fb41a47b@mail.gmail.com>
	<762437f0810181304p73cb4bd0l51f3bf3c3916905e@mail.gmail.com>
	<51cfc2260810181317v670b42cdxe09278b0e41afa6@mail.gmail.com>
	<762437f0810181325h5a2388a0mfd1b218c8530dbab@mail.gmail.com>
Message-ID: <762437f0810181340y244f87a9r46f2a290541ba529@mail.gmail.com>

Hmmmmm....curiouser and curiouser.

I disconnected one of the disks in the raidz, booted into single user and
did a zpool import. This time zpool status did not crash and gave me a
correct pool status (i.e., the disk has a FAULTED state and the raidz1 and
the pool itself are DEGRADED). So now that I have access to the system we
come back to my previous question. How do I completely remove the raidz from
the pool? The mirror is enough to hold the current data so it *SHOULD* be
somehow possible to move all the data into the mirror.

mario at tumbolia ~ 1 % zpool status
  pool: phlogiston
 state: DEGRADED
status: One or more devices could not be used because the label is missing
or
        invalid.  Sufficient replicas exist for the pool to continue
        functioning in a degraded state.
action: Replace the device using 'zpool replace'.
   see: http://www.sun.com/msg/ZFS-8000-4J
 scrub: resilver completed with 0 errors on Sat Oct 18 22:32:59 2008
config:

        NAME                      STATE     READ WRITE CKSUM
        phlogiston                DEGRADED     0     0     0
          mirror                  ONLINE       0     0     0
            disk3s2               ONLINE       0     0     0
            disk4s2               ONLINE       0     0     0
          raidz1                  DEGRADED     0     0     0
            disk1s2               ONLINE       0     0     0
            16293508939705988688  FAULTED      0     0     0  was
/dev/disk1s2
            disk2s2               ONLINE       0     0     0

mario at tumbolia ~ 2 % df -H
Filesystem     Size   Used  Avail Capacity  Mounted on
/dev/disk0s2   120G    80G    39G    68%    /
devfs          117k   117k     0B   100%    /dev
fdesc          1.0k   1.0k     0B   100%    /dev
phlogiston     2.0T   966G   1.0T    50%    /Volumes/phlogiston

According to the df's I did before creating the raidz, I have about 983G in
the mirror.

On Sat, Oct 18, 2008 at 10:25 PM, Mario Camou <mcamou at tecnoguru.com> wrote:

> If the problem is with the raidz, is there any way of telling ZFS not to
> look for it and just make do with the mirror? The data I have all fits in
> the mirror and I haven't (yet) written anything into the disks since adding
> the raidz, so I imagine it's possible that the data is still just in the
> mirror and the raidz is still empty. One can hope. The problem right now is
> that if I do an import without connecting the raidz drives it (obviously)
> fails because the raidz isn't present.
>
> AARRRGGGHHHH!!!!
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081018/ed682bcc/attachment.html 

From richmc at gmail.com  Sat Oct 18 14:21:18 2008
From: richmc at gmail.com (Richard McClellan)
Date: Sat, 18 Oct 2008 14:21:18 -0700
Subject: [zfs-discuss] Now zpool status is giving me a kernel panic
In-Reply-To: <762437f0810181340y244f87a9r46f2a290541ba529@mail.gmail.com>
References: <762437f0810181129n22f022eu81169f8442a4e4f9@mail.gmail.com>
	<51cfc2260810181300i7de1868fqc8767842fb41a47b@mail.gmail.com>
	<762437f0810181304p73cb4bd0l51f3bf3c3916905e@mail.gmail.com>
	<51cfc2260810181317v670b42cdxe09278b0e41afa6@mail.gmail.com>
	<762437f0810181325h5a2388a0mfd1b218c8530dbab@mail.gmail.com>
	<762437f0810181340y244f87a9r46f2a290541ba529@mail.gmail.com>
Message-ID: <DCB07075-3E05-45B2-90F9-1EF17E8D1A24@gmail.com>

On a strictly raidz pool, vdevs cannot be removed. No idea if it's  
possible with heterogenous vdevs.

I've seen disk labels change from one boot to another too. In my case  
in collections of disks via iSCSI.

                 Rich

On Oct 18, 2008, at 13:40, "Mario Camou" <mcamou at tecnoguru.com> wrote:

> Hmmmmm....curiouser and curiouser.
>
> I disconnected one of the disks in the raidz, booted into single  
> user and did a zpool import. This time zpool status did not crash  
> and gave me a correct pool status (i.e., the disk has a FAULTED  
> state and the raidz1 and the pool itself are DEGRADED). So now that  
> I have access to the system we come back to my previous question.  
> How do I completely remove the raidz from the pool? The mirror is  
> enough to hold the current data so it *SHOULD* be somehow possible  
> to move all the data into the mirror.
>
> mario at tumbolia ~ 1 % zpool status
>   pool: phlogiston
>  state: DEGRADED
> status: One or more devices could not be used because the label is  
> missing or
>         invalid.  Sufficient replicas exist for the pool to continue
>         functioning in a degraded state.
> action: Replace the device using 'zpool replace'.
>    see: http://www.sun.com/msg/ZFS-8000-4J
>  scrub: resilver completed with 0 errors on Sat Oct 18 22:32:59 2008
> config:
>
>         NAME                      STATE     READ WRITE CKSUM
>         phlogiston                DEGRADED     0     0     0
>           mirror                  ONLINE       0     0     0
>             disk3s2               ONLINE       0     0     0
>             disk4s2               ONLINE       0     0     0
>           raidz1                  DEGRADED     0     0     0
>             disk1s2               ONLINE       0     0     0
>             16293508939705988688  FAULTED      0     0     0  was / 
> dev/disk1s2
>             disk2s2               ONLINE       0     0     0
>
> mario at tumbolia ~ 2 % df -H
> Filesystem     Size   Used  Avail Capacity  Mounted on
> /dev/disk0s2   120G    80G    39G    68%    /
> devfs          117k   117k     0B   100%    /dev
> fdesc          1.0k   1.0k     0B   100%    /dev
> phlogiston     2.0T   966G   1.0T    50%    /Volumes/phlogiston
>
> According to the df's I did before creating the raidz, I have about  
> 983G in the mirror.
>
> On Sat, Oct 18, 2008 at 10:25 PM, Mario Camou <mcamou at tecnoguru.com>  
> wrote:
> If the problem is with the raidz, is there any way of telling ZFS  
> not to look for it and just make do with the mirror? The data I have  
> all fits in the mirror and I haven't (yet) written anything into the  
> disks since adding the raidz, so I imagine it's possible that the  
> data is still just in the mirror and the raidz is still empty. One  
> can hope. The problem right now is that if I do an import without  
> connecting the raidz drives it (obviously) fails because the raidz  
> isn't present.
>
> AARRRGGGHHHH!!!!
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081018/3c5fd203/attachment-0001.html 

From alex.blewitt at gmail.com  Sun Oct 19 04:46:36 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Sun, 19 Oct 2008 12:46:36 +0100
Subject: [zfs-discuss] Now zpool status is giving me a kernel panic
In-Reply-To: <51cfc2260810181317v670b42cdxe09278b0e41afa6@mail.gmail.com>
References: <762437f0810181129n22f022eu81169f8442a4e4f9@mail.gmail.com>
	<51cfc2260810181300i7de1868fqc8767842fb41a47b@mail.gmail.com>
	<762437f0810181304p73cb4bd0l51f3bf3c3916905e@mail.gmail.com>
	<51cfc2260810181317v670b42cdxe09278b0e41afa6@mail.gmail.com>
Message-ID: <E8E2112A-03C3-4154-8F9A-FB1AE07D69BF@gmail.com>

And despite Apple's recent ditching of firewire on their pseudo-cheap  
range, firewire is still the most sensible and fastest way of using  
external disks.

I did a bunch of timings with hfs and zfs (probably 102) - google  
alblue zfs FireWire for more (yes, there's still no cut n paste on an  
iPhone). Even fw400 beat the pants off of usb2.

I'm in a bit of a quandry - the mini is likely to be refreshed with a  
better graphics card, but if it too loses firewire it's game over for  
me.

Alex

Sent from my (new) iPhone

On 18 Oct 2008, at 21:17, "Riscky Abacus" <riscky at gmail.com> wrote:

> I don't have any knowledge as to why this happens but I've found any
> raidz i've created where the drives all sit on the same USB hub
> consistently crash when running `zpool status -v`. I have a few ideas
> why that might be but nothing solid. I have a feeling this my be to
> due to the fact disk3s* numbers my be a different physical volume on
> subsequent reboots.
>
> Just take this little test case into account... remove all zfs
> volumes... reboot. Now find to flash drives or anyhting out you could
> use for this test.... put them on the same usb hub and turn them into
> a raidz.... now run `zpool status -v` no crash... reboot and run
> `zpool status -v` crash.
>
> I've also noticed that configuration also causes issues shutting  
> down my system.
>
> This is all based on my tinkering with free time... and I haven't
> devoted much time to figuring out the root cause... I just don't think
> the file system plays well with usb, and has issues with hubs... after
> all zfs wasn't exactly designed for desktops but servers IMHO.
>
> On Sat, Oct 18, 2008 at 4:04 PM, Mario Camou <mcamou at tecnoguru.com>  
> wrote:
>> Hi,
>>
>> Thanks for replying.
>>
>> All of my drives (5 of them) are on a USB hub. Since the Mac Mini  
>> has only 4
>> USB ports (and one of them is taken up by the keyboard/mouse) I  
>> really have
>> no other option. Why? Could moving some of them off of the hub (or  
>> changing
>> hubs) help?
>>
>> On Sat, Oct 18, 2008 at 10:00 PM, Riscky Abacus <riscky at gmail.com>  
>> wrote:
>>>
>>> Mario,
>>> Just based on my experience here... are any of your drives in your
>>> raidz USB devices? or on a USB hub?
>>>
>>> On Sat, Oct 18, 2008 at 2:29 PM, Mario Camou  
>>> <mcamou at tecnoguru.com> wrote:
>>>> Another data point.
>>>>
>>>> I booted in single user mode, did the fsck and "mount -uw /" and  
>>>> then
>>>> did a
>>>> "zpool import". When doing a "zpool status -v" I get a kernel panic
>>>> right
>>>> after the config is shown (with all devices showing as ONLINE and  
>>>> with 0
>>>> errors) and before where the filenames should be. I still get the  
>>>> "One
>>>> or
>>>> more devices has experienced an error" status message.
>>>>
>>>> If I knew just which device is giving the problem I could just  
>>>> unplug it
>>>> (since both groups in the pool have redundancy -- one with  
>>>> mirroring and
>>>> one
>>>> with raidz) but now I'm really leery of trying to do anything  
>>>> that might
>>>> result in data corruption.
>>>>
>>>> Any ideas? HELP!!
>>>>
>>>> On Sat, Oct 18, 2008 at 7:59 PM, Mario Camou <mcamou at tecnoguru.com>
>>>> wrote:
>>
>>
>>
>> --
>> The impossible has, on occasion, let me down
>>                                                       --R.U. Sirius
>>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From mcamou at tecnoguru.com  Sun Oct 19 13:55:27 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Sun, 19 Oct 2008 22:55:27 +0200
Subject: [zfs-discuss] HELP PLEASE!!! Re: Now zpool status is giving me a
	kernel panic
Message-ID: <762437f0810191355j53bd826av2e1b683c4545f6cf@mail.gmail.com>

OK, so this shit is getting worse and worse.

I started with the degraded pool. Since I didn't have a 1TB disk on hand, I
removed one of the disks from the mirror and imported the pool with both
sets in degraded status (the raidz with one disk missing -- to get zpool
status to work -- and the mirror also with one disk missing). I created a
new ZFS pool on the disk that I removed from the mirror and duplicated the
data from the old pool to the new one using rsync. So far so good.

Then I destroyed the old pool and added the 1TB disk as a mirror device. It
started resilvering and meanwhile I started using the disk.

At one point "zpool status" started saying it had a large amount of data
errors. I did a "zpool status -v" to see on which files.

And I *AGAIN* got a kernel panic.

Rebooted in single-user mode. Imported the pool. There are 12414 data errors
in it. "zpool status -v"...kernel panic in the exact same address as before.

What's the use of the -v option if all you get from it is a panic? I would
classify this as a critical error.

At this point I just want to get rid of ZFS altogether and return to
something akin to my old setup, using Linux on the server and this time
using Linux's kernel mirroring facilities. Forget about pools, forget about
a single device. I just want to get my data back.

Problem: since these are USB drives, I don't know how to determine which of
the drives currently contains the data and which one is getting filled.

HELP!!!!!!

I'm thinking of getting the Linux ZFS implementation up and running just to
see if I can get my data back. I don't like it because it's a UserFS
implementation and I've previously had problems with UserFS, but if I only
use it to check the mirror status, perhaps resilver and get my data back,
things just MIGHT be OK.

AARRGGHHHHH!!!!

Any ideas as to how I can get my data back?

On Sat, Oct 18, 2008 at 10:40 PM, Mario Camou <mcamou at tecnoguru.com> wrote:

> Hmmmmm....curiouser and curiouser.
>
> I disconnected one of the disks in the raidz, booted into single user and
> did a zpool import. This time zpool status did not crash and gave me a
> correct pool status (i.e., the disk has a FAULTED state and the raidz1 and
> the pool itself are DEGRADED). So now that I have access to the system we
> come back to my previous question. How do I completely remove the raidz from
> the pool? The mirror is enough to hold the current data so it *SHOULD* be
> somehow possible to move all the data into the mirror.
>
> mario at tumbolia ~ 1 % zpool status
>   pool: phlogiston
>  state: DEGRADED
> status: One or more devices could not be used because the label is missing
> or
>         invalid.  Sufficient replicas exist for the pool to continue
>         functioning in a degraded state.
> action: Replace the device using 'zpool replace'.
>    see: http://www.sun.com/msg/ZFS-8000-4J
>  scrub: resilver completed with 0 errors on Sat Oct 18 22:32:59 2008
> config:
>
>         NAME                      STATE     READ WRITE CKSUM
>         phlogiston                DEGRADED     0     0     0
>           mirror                  ONLINE       0     0     0
>             disk3s2               ONLINE       0     0     0
>             disk4s2               ONLINE       0     0     0
>           raidz1                  DEGRADED     0     0     0
>             disk1s2               ONLINE       0     0     0
>             16293508939705988688  FAULTED      0     0     0  was
> /dev/disk1s2
>             disk2s2               ONLINE       0     0     0
>
> mario at tumbolia ~ 2 % df -H
> Filesystem     Size   Used  Avail Capacity  Mounted on
> /dev/disk0s2   120G    80G    39G    68%    /
> devfs          117k   117k     0B   100%    /dev
> fdesc          1.0k   1.0k     0B   100%    /dev
> phlogiston     2.0T   966G   1.0T    50%    /Volumes/phlogiston
>
> According to the df's I did before creating the raidz, I have about 983G in
> the mirror.
>
> On Sat, Oct 18, 2008 at 10:25 PM, Mario Camou <mcamou at tecnoguru.com>wrote:
>
>> If the problem is with the raidz, is there any way of telling ZFS not to
>> look for it and just make do with the mirror? The data I have all fits in
>> the mirror and I haven't (yet) written anything into the disks since adding
>> the raidz, so I imagine it's possible that the data is still just in the
>> mirror and the raidz is still empty. One can hope. The problem right now is
>> that if I do an import without connecting the raidz drives it (obviously)
>> fails because the raidz isn't present.
>>
>> AARRRGGGHHHH!!!!
>>
>>
>>
>


-- 
The impossible has, on occasion, let me down
                                                       --R.U. Sirius
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081019/77228f86/attachment.html 

From hanche at math.ntnu.no  Sun Oct 19 14:46:39 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Sun, 19 Oct 2008 23:46:39 +0200 (CEST)
Subject: [zfs-discuss] HELP PLEASE!!! Re: Now zpool status is giving me
 a kernel panic
In-Reply-To: <762437f0810191355j53bd826av2e1b683c4545f6cf@mail.gmail.com>
References: <762437f0810191355j53bd826av2e1b683c4545f6cf@mail.gmail.com>
Message-ID: <20081019.234639.66639191.hanche@math.ntnu.no>

+ "Mario Camou" <mcamou at tecnoguru.com>:

> I'm thinking of getting the Linux ZFS implementation up and running
> just to see if I can get my data back. I don't like it because it's
> a UserFS implementation and I've previously had problems with
> UserFS, but if I only use it to check the mirror status, perhaps
> resilver and get my data back, things just MIGHT be OK.
> 
> AARRGGHHHHH!!!!
> 
> Any ideas as to how I can get my data back?

If you're desperate, and you seem to be, you might consider trying
opensolaris. Presumably, they have the most mature zfs implementation
around? (Or you could try freebsd, but they're at zfs version 4, and
if you have upgraded yours to version 6 that might not work.)

- Harald

From aorchid at mac.com  Sun Oct 19 14:54:27 2008
From: aorchid at mac.com (Aric Gregson)
Date: Sun, 19 Oct 2008 14:54:27 -0700
Subject: [zfs-discuss] HELP PLEASE!!! Re: Now zpool status is giving me
 a kernel panic
In-Reply-To: <20081019.234639.66639191.hanche@math.ntnu.no>
References: <762437f0810191355j53bd826av2e1b683c4545f6cf@mail.gmail.com>
	<20081019.234639.66639191.hanche@math.ntnu.no>
Message-ID: <20081019145427.3c45fa7f.aorchid@mac.com>

On Sun, 19 Oct 2008 23:46:39 +0200 (CEST)
Harald Hanche-Olsen <hanche at math.ntnu.no> wrote:

> If you're desperate, and you seem to be, you might consider trying
> opensolaris. Presumably, they have the most mature zfs implementation
> around?

If you just want your data back, burn a live CD of an OpenSolaris
distribution and boot it. You can then do whatever you like with your
zpool and the data on it, including rsync it back to a HFS+ partition
that you can read from the Mac OS. Just realize that right now Mac ZFS
is at update 8, so don't upgrade the zpool while you are running the
live OpenSolaris CD.

As far as ZFS usability, Solaris is obviously, the best choice. 

-- 
Aric Gregson

From mcamou at tecnoguru.com  Sun Oct 19 15:03:19 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Mon, 20 Oct 2008 00:03:19 +0200
Subject: [zfs-discuss] HELP PLEASE!!! Re: Now zpool status is giving me
	a kernel panic
In-Reply-To: <20081019.234639.66639191.hanche@math.ntnu.no>
References: <762437f0810191355j53bd826av2e1b683c4545f6cf@mail.gmail.com>
	<20081019.234639.66639191.hanche@math.ntnu.no>
Message-ID: <762437f0810191503i33f4c89flc704fd2af739e3c3@mail.gmail.com>

Hi Harald,

Thanks for replying.

As far as using Solaris on my server, it's not really an option at this
point since I have some hardware on this machine that's not supported. I
might try it just to recover the data. I was thinking of mounting the disks
on another Mac which has the stock ZFS bits, but I've updated to version 6
and if I remember correctly the stock OS X ZFS is version 4.

Can you import a ZFS volume to a host that was created with a different OS?
Is there a way of removing the risk of the data being incorrectly
overwritten?


On Sun, Oct 19, 2008 at 11:46 PM, Harald Hanche-Olsen
<hanche at math.ntnu.no>wrote:

> + "Mario Camou" <mcamou at tecnoguru.com>:
>
> > I'm thinking of getting the Linux ZFS implementation up and running
> > just to see if I can get my data back. I don't like it because it's
> > a UserFS implementation and I've previously had problems with
> > UserFS, but if I only use it to check the mirror status, perhaps
> > resilver and get my data back, things just MIGHT be OK.
> >
> > AARRGGHHHHH!!!!
> >
> > Any ideas as to how I can get my data back?
>
> If you're desperate, and you seem to be, you might consider trying
> opensolaris. Presumably, they have the most mature zfs implementation
> around? (Or you could try freebsd, but they're at zfs version 4, and
> if you have upgraded yours to version 6 that might not work.)
>
> - Harald
>



-- 
The impossible has, on occasion, let me down
                                                       --R.U. Sirius
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081020/9cefd694/attachment.html 

From mcamou at tecnoguru.com  Sun Oct 19 15:05:25 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Mon, 20 Oct 2008 00:05:25 +0200
Subject: [zfs-discuss] HELP PLEASE!!! Re: Now zpool status is giving me
	a kernel panic
In-Reply-To: <20081019145427.3c45fa7f.aorchid@mac.com>
References: <762437f0810191355j53bd826av2e1b683c4545f6cf@mail.gmail.com>
	<20081019.234639.66639191.hanche@math.ntnu.no>
	<20081019145427.3c45fa7f.aorchid@mac.com>
Message-ID: <762437f0810191505v1126a839v7f2a080f5dfa5c6f@mail.gmail.com>

Thanks for replying Aric.

I just might end up doing that. Thanks for the help.

One thinkg I'd like to know is, how can I determine, in a mirrored set that
is resilvering, which of the disks has the "good" data and which is being
copied to?

-Mario.

On Sun, Oct 19, 2008 at 11:54 PM, Aric Gregson <aorchid at mac.com> wrote:

> On Sun, 19 Oct 2008 23:46:39 +0200 (CEST)
> Harald Hanche-Olsen <hanche at math.ntnu.no> wrote:
>
> > If you're desperate, and you seem to be, you might consider trying
> > opensolaris. Presumably, they have the most mature zfs implementation
> > around?
>
> If you just want your data back, burn a live CD of an OpenSolaris
> distribution and boot it. You can then do whatever you like with your
> zpool and the data on it, including rsync it back to a HFS+ partition
> that you can read from the Mac OS. Just realize that right now Mac ZFS
> is at update 8, so don't upgrade the zpool while you are running the
> live OpenSolaris CD.
>
> As far as ZFS usability, Solaris is obviously, the best choice.
>
> --
> Aric Gregson
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>



-- 
The impossible has, on occasion, let me down
                                                       --R.U. Sirius
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081020/75d530fb/attachment-0001.html 

From aorchid at mac.com  Sun Oct 19 15:01:33 2008
From: aorchid at mac.com (Aric Gregson)
Date: Sun, 19 Oct 2008 15:01:33 -0700
Subject: [zfs-discuss] HELP PLEASE!!! Re: Now zpool status is giving me
 a kernel panic
In-Reply-To: <762437f0810191503i33f4c89flc704fd2af739e3c3@mail.gmail.com>
References: <762437f0810191355j53bd826av2e1b683c4545f6cf@mail.gmail.com>
	<20081019.234639.66639191.hanche@math.ntnu.no>
	<762437f0810191503i33f4c89flc704fd2af739e3c3@mail.gmail.com>
Message-ID: <20081019150133.47f9ffde.aorchid@mac.com>

On Mon, 20 Oct 2008 00:03:19 +0200
Mario Camou <mcamou at tecnoguru.com> wrote:

> Can you import a ZFS volume to a host that was created with a
> different OS?

ZFS does not care what OS it uses. You can read the same zpool on
Solaris x64, Solaris Sparc, FreeBSD or Mac.

-- 
Aric Gregson

From mcamou at tecnoguru.com  Sun Oct 19 16:22:34 2008
From: mcamou at tecnoguru.com (Mario Camou)
Date: Mon, 20 Oct 2008 01:22:34 +0200
Subject: [zfs-discuss] HELP PLEASE!!! Re: Now zpool status is giving me
	a kernel panic
In-Reply-To: <20081019151329.3d4e2893.aorchid@mac.com>
References: <762437f0810191355j53bd826av2e1b683c4545f6cf@mail.gmail.com>
	<20081019.234639.66639191.hanche@math.ntnu.no>
	<20081019145427.3c45fa7f.aorchid@mac.com>
	<762437f0810191505v1126a839v7f2a080f5dfa5c6f@mail.gmail.com>
	<20081019150246.26dff495.aorchid@mac.com>
	<762437f0810191514i254991fao7a8c3554d795259b@mail.gmail.com>
	<20081019151329.3d4e2893.aorchid@mac.com>
Message-ID: <762437f0810191622l5442abc3y889e9b6982dadfc6@mail.gmail.com>

Thank you for your help.

In the end I got a Solaris LiveCD and booted it using VMWare. A "zpool
status -v" actually works. I'll let it resilver (it currently says it will
take about 39 hours) and see about recovering the data.

One question. If "zpool status -v" includes a directory name, does that mean
that the whole directory is useless? I.e., that all the files in it have
been lost?

In that case ... ouch....

On Mon, Oct 20, 2008 at 12:13 AM, Aric Gregson <aorchid at mac.com> wrote:

> On Mon, 20 Oct 2008 00:14:03 +0200
> Mario Camou <mcamou at tecnoguru.com> wrote:
>
> > Well, for one thing, the resilvering says it will take about
> > 30-something hours to finish (assuming it doesn't panic at some
> > point). Also, the "12k+ data errors" message doesn't reassure me. I
> > would just like to unplug the "empty" drive, reformat it and copy the
> > data over to it.
>
> Hmm. Not sure about how to do that. I suspect it has been discussed
> on the ZFS discussion list at OpenSolaris, but I did not pay attention
> to it. I have noticed that the resilvering on Mac tends to report a
> longer time than it actually takes, but resilvering can take days
> depending upon the zpool and what happened. I would still probably just
> let it resilver and then deal with it.
>
> You should be able to look at the other list and see what has been
> discussed about the disks. I'm sorry, but I don't remember if your's is
> a mirror or a raidz. If you have a raidz the errors may very well be
> spread out over the different drives, so removing one bad one may not
> be an option.
>
> aric
>



-- 
The impossible has, on occasion, let me down
                                                       --R.U. Sirius
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081020/f6ed110c/attachment.html 

From bplist at thinkpink.com  Sun Oct 19 21:46:14 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Sun, 19 Oct 2008 21:46:14 -0700
Subject: [zfs-discuss] HELP PLEASE!!! Re: Now zpool status is giving me
	a kernel panic
In-Reply-To: <762437f0810191622l5442abc3y889e9b6982dadfc6@mail.gmail.com>
References: <762437f0810191355j53bd826av2e1b683c4545f6cf@mail.gmail.com>
	<20081019.234639.66639191.hanche@math.ntnu.no>
	<20081019145427.3c45fa7f.aorchid@mac.com>
	<762437f0810191505v1126a839v7f2a080f5dfa5c6f@mail.gmail.com>
	<20081019150246.26dff495.aorchid@mac.com>
	<762437f0810191514i254991fao7a8c3554d795259b@mail.gmail.com>
	<20081019151329.3d4e2893.aorchid@mac.com>
	<762437f0810191622l5442abc3y889e9b6982dadfc6@mail.gmail.com>
Message-ID: <90EB0218-A2C1-4035-AFF6-05EF976FFF7E@thinkpink.com>

> In that case ... ouch....

It's probably worth pointing out that even though the Mac OS port of  
ZFS is fairly safe, its apparent reliability is not a replacement for  
a good backup strategy.  I'll trust it after a year of widespread  
release. :)  And even if the filesystem functions completely as  
designed, your data is still susceptible to application-level errors  
or an errant "rm -rf *" (c'mon, who hasn't done that in the wrong  
directory at least once?)

bri


From hanche at math.ntnu.no  Sun Oct 19 21:54:13 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Mon, 20 Oct 2008 06:54:13 +0200 (CEST)
Subject: [zfs-discuss] HELP PLEASE!!! Re: Now zpool status is giving me
 a kernel panic
In-Reply-To: <90EB0218-A2C1-4035-AFF6-05EF976FFF7E@thinkpink.com>
References: <20081019151329.3d4e2893.aorchid@mac.com>
	<762437f0810191622l5442abc3y889e9b6982dadfc6@mail.gmail.com>
	<90EB0218-A2C1-4035-AFF6-05EF976FFF7E@thinkpink.com>
Message-ID: <20081020.065413.26597273.hanche@math.ntnu.no>

+ Brian Pinkerton <bplist at thinkpink.com>:

> It's probably worth pointing out that even though the Mac OS port of
> ZFS is fairly safe, its apparent reliability is not a replacement
> for a good backup strategy.

Absolutely right.

> And even if the filesystem functions completely as designed, your
> data is still susceptible to application-level errors or an errant
> "rm -rf *" (c'mon, who hasn't done that in the wrong directory at
> least once?)

Ah, but that's what zfs snapshots are for. They're so cheap, you can
create snapshots much more frequently than most people can run
backups, giving you a better chance of recovering from such mishaps.

- Harald

From zfs at hessmann.de  Mon Oct 20 03:29:21 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Mon, 20 Oct 2008 12:29:21 +0200
Subject: [zfs-discuss] Now zpool status is giving me a kernel panic
In-Reply-To: <E8E2112A-03C3-4154-8F9A-FB1AE07D69BF@gmail.com>
References: <762437f0810181129n22f022eu81169f8442a4e4f9@mail.gmail.com>
	<51cfc2260810181300i7de1868fqc8767842fb41a47b@mail.gmail.com>
	<762437f0810181304p73cb4bd0l51f3bf3c3916905e@mail.gmail.com>
	<51cfc2260810181317v670b42cdxe09278b0e41afa6@mail.gmail.com>
	<E8E2112A-03C3-4154-8F9A-FB1AE07D69BF@gmail.com>
Message-ID: <1915B68D-42D9-4337-866B-D22B673EDFF0@hessmann.de>

On 19.10.2008, at 13:46, Alex Blewitt wrote:

> I'm in a bit of a quandry - the mini is likely to be refreshed with a
> better graphics card, but if it too loses firewire it's game over for
> me.

I know exactly what you mean. I've got four Minis right now, three of  
them with additional drives (all FW), two with RAID.Z.
Since two are still G4 based, I'm thinking about exchanging them with  
Intel-Minis right now, just to be on the safe side. Would be a shame,  
since the G4s are working fine, but I'm anxious about an upcoming  
update/cancelation.

No?l, you wouldn't have any additional information you'd like to  
share, would you? ;-)


Best regards,

Christian

From lists at loveturtle.net  Mon Oct 20 06:40:07 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Mon, 20 Oct 2008 09:40:07 -0400
Subject: [zfs-discuss] HELP PLEASE!!! Re: Now zpool status is giving me
 a kernel panic
In-Reply-To: <762437f0810191503i33f4c89flc704fd2af739e3c3@mail.gmail.com>
References: <762437f0810191355j53bd826av2e1b683c4545f6cf@mail.gmail.com>	<20081019.234639.66639191.hanche@math.ntnu.no>
	<762437f0810191503i33f4c89flc704fd2af739e3c3@mail.gmail.com>
Message-ID: <48FC8A37.4080303@loveturtle.net>

On 10/19/08 6:03 PM, Mario Camou wrote:
> but I've updated to version 6 and if I remember correctly the stock OS 
> X ZFS is version 4.
>
running zpool upgrade and zfs upgrade with no arguments prints the 
pool/fs version.
FreeBSD is at version 6 not 4. MacOS is version 8 but defaults to 6 on 
creation. :-)

From William.Winnett at Sun.COM  Tue Oct 21 11:38:22 2008
From: William.Winnett at Sun.COM (Bill Winnett)
Date: Tue, 21 Oct 2008 14:38:22 -0400
Subject: [zfs-discuss] Kernel Panic with Zpool Status if a pool has
	corruption
Message-ID: <BAC40FB0-F6A6-49BC-B6CC-36D82BF30EB0@Sun.COM>



It appears that if a pool has some corruption and zfs has detected it,  
then if you use "zpool status" on that pool you will panic.

====
Tue Oct 21 12:12:15 2008
panic(cpu 0 caller 0x001A8CD4): Kernel trap at 0x0036271b, type  
14=page fault, registers:
CR0: 0x80010033, CR2: 0x0000000c, CR3: 0x01287000, CR4: 0x00000660
EAX: 0x00000000, EBX: 0x5dbd7de8, ECX: 0x1f000000, EDX: 0x00000000
CR2: 0x0000000c, EBP: 0x5dbd7d18, ESI: 0x00000200, EDI: 0x09255000
EFL: 0x00010202, EIP: 0x0036271b, CS:  0x00000008, DS:  0x00530010
Error code: 0x00000000

Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x5dbd7b38 : 0x12b0fa (0x4592a4 0x5dbd7b6c 0x133243 0x0)
0x5dbd7b88 : 0x1a8cd4 (0x46280c 0x36271b 0xe 0x461fbc)
0x5dbd7c68 : 0x19ede5 (0x5dbd7c80 0xe07cf20 0x5dbd7d18 0x36271b)
0x5dbd7c78 : 0x36271b (0xe 0x48 0x7e00010 0x190010)
0x5dbd7d18 : 0xc97ea4 (0x0 0x2993e 0x3 0x0)
0x5dbd7d38 : 0xc9f9ba (0x0 0x0 0x5dbd7dc8 0x19d5b1)
0x5dbd7d78 : 0x20300f (0x1f000000 0xcf1c5a20 0x9255000 0x3)
0x5dbd7db8 : 0x1f63a2 (0x5dbd7de8 0x246 0x5dbd7e18 0x1da567)
0x5dbd7e18 : 0x1ec67c (0x76d9ef0 0xcf1c5a20 0x9255000 0x3)
0x5dbd7e78 : 0x3661ff (0x716e7e0 0xcf1c5a20 0x9255000 0x5dbd7f50)
0x5dbd7e98 : 0x38cb8f (0x716e7e0 0xcf1c5a20 0x9255000 0x5dbd7f50)
0x5dbd7f78 : 0x3ddde2 (0x7acd2e0 0x7e1d1e0 0x7e1d224 0x0)
0x5dbd7fc8 : 0x19f2c3 (0x7b99900 0x0 0x10 0x7b99900)
No mapping exists for frame pointer
Backtrace terminated-invalid frame pointer 0xbfffa7d8
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0xc3c000->0xd07fff

BSD process name corresponding to current thread: zpool

Mac OS version:
9E17

Kernel version:
Darwin Kernel Version 9.4.0: Mon Jun  9 19:30:53 PDT 2008;  
root:xnu-1228.5.20~1/RELEASE_I386
System model name: iMac7,1 (Mac-F4238CC8)
====


I loaded virtualbox and installed a Solaris image and attached my  
disks to the VM.  When I ran scrub and status on the pool, Solaris  
correctly indicates corruption.
Now I don't know if the corruption is in the way the file was stored  
by ZFS on the Mac, or if there is a bad sector, but it seems  
suspicious that this happened only
after I added a bunch of pools and disks to my configuration.  I have  
since undone that, but the one pool has problems.  Lucky for me this  
is just junk for testing.

=====
(root at milax)# zpool status -xv
   pool:  bef
  state:  ONLINE
status: One or more devices has experienced an error resulting in data
         corruption.  Applications may be affected.
action: Restore the file in question if possible.  Otherwise restore the
         entire pool from backup.
    see: http://www.sun.com/msg/ZFS-8000-8A
  scrub: scrub in progress for 0h29m, 5.37% done, 8h31m to go
config:

         NAME        STATE     READ WRITE CHKSUM
         bef         ONLINE       0     0      0
           c7t0d0s1  ONLINE       0     0      0
           c6t0d0s1  ONLINE       0     0      0

errors: Permanent errors have been detected in the following files:

         /bef/test/file3
         bef:<0x4845>
         bef:<0x4848>
         /bef/test/file2
         bef:0x193>
         /bef/test/file1
         /bef/test/file5
         bef:<0x4af4>
(root at milax)#
llll


-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081021/8bf51f31/attachment.html 

From lists at loveturtle.net  Thu Oct 23 12:07:23 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Thu, 23 Oct 2008 15:07:23 -0400
Subject: [zfs-discuss] unrelated question....
Message-ID: <4900CB6B.5050007@loveturtle.net>

Hey guys, I was wondering...This doesn't really have anything to do with 
zfs on mac osx but I figured that I would throw this out there since 
some of you are smart :-)

Does anyone know how to get the total snapshot usage for an entire pool?

My desktop has 20 snapshots, I can just look at them and add them up...

but my home fileserver has 206 snapshots...looking at them one by one is 
way less practical.

From werner.donne at re.be  Thu Oct 23 13:34:09 2008
From: werner.donne at re.be (=?ISO-8859-1?Q?Werner_Donn=E9?=)
Date: Thu, 23 Oct 2008 22:34:09 +0200
Subject: [zfs-discuss] unrelated question....
In-Reply-To: <4900CB6B.5050007@loveturtle.net>
References: <4900CB6B.5050007@loveturtle.net>
Message-ID: <DB1172A4-A891-4E72-A6B5-CBECC93BAF13@re.be>

Hi Dillon,

The following command-line will give you the amount of bytes consumed by
all of your snapshots:

zfs list -t snapshot -H -o used | sed 's/K/*1024/' | sed 's/M/ 
*1024*1024/' |
sed 's/G/*1024*1024*1024/' | sed 's/^/a+=/' | awk '{ print $0 } END  
{ print "a"
}' | bc

Best regards,

Werner.

On 23 Oct 2008, at 21:07, Dillon Kass wrote:

> Hey guys, I was wondering...This doesn't really have anything to do  
> with zfs on mac osx but I figured that I would throw this out there  
> since some of you are smart :-)
>
> Does anyone know how to get the total snapshot usage for an entire  
> pool?
>
> My desktop has 20 snapshots, I can just look at them and add them  
> up...
>
> but my home fileserver has 206 snapshots...looking at them one by  
> one is way less practical.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

--
Werner Donn?  --  Re                                     http://www.pincette.biz
Engelbeekstraat 8                                               http://www.re.be
BE-3300 Tienen
tel: (+32) 486 425803	e-mail: werner.donne at re.be






From Jonathan.Edwards at Sun.COM  Thu Oct 23 14:14:18 2008
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Thu, 23 Oct 2008 17:14:18 -0400
Subject: [zfs-discuss] unrelated question....
In-Reply-To: <DB1172A4-A891-4E72-A6B5-CBECC93BAF13@re.be>
References: <4900CB6B.5050007@loveturtle.net>
	<DB1172A4-A891-4E72-A6B5-CBECC93BAF13@re.be>
Message-ID: <DCC90641-F497-4CA2-A1EB-63ABDFEFD7B1@sun.com>


since you can also have KB sized snapshots and might want to limit  
exposure to a pool .. here's another take using the -p option in zfs get

$ cat > snapsum
#!/bin/bash
USAGE="USAGE: $0 pool"
test $# -ne 1 && echo $USAGE && exit 1

sum=0
pool=$1

for snapshot in `zfs list -Hr -t snapshot -o name ${pool}` ; do
   snapsize=`zfs get -Hp -o value used ${snapshot}`
   sum=$((sum + snapsize))
done

echo "Total snapshot usage for ${pool} = ${sum}"
^D
$ chmod 755 snapsum
$ ./snapsum

--
this is still kind of a hack .. there's some functionality you can  
derive in dumping the zdb block usage, but zdb isn't really ported  
here and it's still not that trivial to derive as this sort of  
reporting functionality should really get RFE'd ..

anyhow hth
jonathan



On Oct 23, 2008, at 4:34 PM, Werner Donn? wrote:

> Hi Dillon,
>
> The following command-line will give you the amount of bytes  
> consumed by
> all of your snapshots:
>
> zfs list -t snapshot -H -o used | sed 's/K/*1024/' | sed 's/M/ 
> *1024*1024/' |
> sed 's/G/*1024*1024*1024/' | sed 's/^/a+=/' | awk '{ print $0 } END  
> { print "a"
> }' | bc
>
> Best regards,
>
> Werner.
>
> On 23 Oct 2008, at 21:07, Dillon Kass wrote:
>
>> Hey guys, I was wondering...This doesn't really have anything to do  
>> with zfs on mac osx but I figured that I would throw this out there  
>> since some of you are smart :-)
>>
>> Does anyone know how to get the total snapshot usage for an entire  
>> pool?
>>
>> My desktop has 20 snapshots, I can just look at them and add them  
>> up...
>>
>> but my home fileserver has 206 snapshots...looking at them one by  
>> one is way less practical.
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> --
> Werner Donn?  --  Re                                     http://www.pincette.biz
> Engelbeekstraat 8                                               http://www.re.be
> BE-3300 Tienen
> tel: (+32) 486 425803	e-mail: werner.donne at re.be
>
>
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Thu Oct 23 19:03:04 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu, 23 Oct 2008 19:03:04 -0700
Subject: [zfs-discuss] unrelated question....
In-Reply-To: <DCC90641-F497-4CA2-A1EB-63ABDFEFD7B1@sun.com>
References: <4900CB6B.5050007@loveturtle.net>
	<DB1172A4-A891-4E72-A6B5-CBECC93BAF13@re.be>
	<DCC90641-F497-4CA2-A1EB-63ABDFEFD7B1@sun.com>
Message-ID: <D6575BB0-DC03-46B7-BDCF-A8F937D8B5EA@apple.com>

Hey guys,

So sounds like perhaps this functionality should be added?  Would this  
be a useful feature to have around or no?

thanks!
Noel

On Oct 23, 2008, at 2:14 PM, Jonathan Edwards wrote:

>
> since you can also have KB sized snapshots and might want to limit  
> exposure to a pool .. here's another take using the -p option in zfs  
> get
>
> $ cat > snapsum
> #!/bin/bash
> USAGE="USAGE: $0 pool"
> test $# -ne 1 && echo $USAGE && exit 1
>
> sum=0
> pool=$1
>
> for snapshot in `zfs list -Hr -t snapshot -o name ${pool}` ; do
>  snapsize=`zfs get -Hp -o value used ${snapshot}`
>  sum=$((sum + snapsize))
> done
>
> echo "Total snapshot usage for ${pool} = ${sum}"
> ^D
> $ chmod 755 snapsum
> $ ./snapsum
>
> --
> this is still kind of a hack .. there's some functionality you can  
> derive in dumping the zdb block usage, but zdb isn't really ported  
> here and it's still not that trivial to derive as this sort of  
> reporting functionality should really get RFE'd ..
>
> anyhow hth
> jonathan
>
>
>
> On Oct 23, 2008, at 4:34 PM, Werner Donn? wrote:
>
>> Hi Dillon,
>>
>> The following command-line will give you the amount of bytes  
>> consumed by
>> all of your snapshots:
>>
>> zfs list -t snapshot -H -o used | sed 's/K/*1024/' | sed 's/M/ 
>> *1024*1024/' |
>> sed 's/G/*1024*1024*1024/' | sed 's/^/a+=/' | awk '{ print $0 } END  
>> { print "a"
>> }' | bc
>>
>> Best regards,
>>
>> Werner.
>>
>> On 23 Oct 2008, at 21:07, Dillon Kass wrote:
>>
>>> Hey guys, I was wondering...This doesn't really have anything to  
>>> do with zfs on mac osx but I figured that I would throw this out  
>>> there since some of you are smart :-)
>>>
>>> Does anyone know how to get the total snapshot usage for an entire  
>>> pool?
>>>
>>> My desktop has 20 snapshots, I can just look at them and add them  
>>> up...
>>>
>>> but my home fileserver has 206 snapshots...looking at them one by  
>>> one is way less practical.
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> --
>> Werner Donn?  --  Re                                     http://www.pincette.biz
>> Engelbeekstraat 8                                               http://www.re.be
>> BE-3300 Tienen
>> tel: (+32) 486 425803	e-mail: werner.donne at re.be
>>
>>
>>
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From zorg at sogeeky.net  Thu Oct 23 20:09:17 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Thu, 23 Oct 2008 20:09:17 -0700
Subject: [zfs-discuss] unrelated question....
In-Reply-To: <D6575BB0-DC03-46B7-BDCF-A8F937D8B5EA@apple.com>
References: <4900CB6B.5050007@loveturtle.net>
	<DB1172A4-A891-4E72-A6B5-CBECC93BAF13@re.be>
	<DCC90641-F497-4CA2-A1EB-63ABDFEFD7B1@sun.com>
	<D6575BB0-DC03-46B7-BDCF-A8F937D8B5EA@apple.com>
Message-ID: <C0279BB2-EF67-4789-BA30-B3F5EBBCB853@sogeeky.net>

For what it's worth, I think it would be useful yes. But not nearly as  
much as the myriad of other issues being worked. :)

On Oct 23, 2008, at 7:03 PM, No?l Dellofano <ndellofano at apple.com>  
wrote:

> Hey guys,
>
> So sounds like perhaps this functionality should be added?  Would  
> this be a useful feature to have around or no?
>
> thanks!
> Noel
>
> On Oct 23, 2008, at 2:14 PM, Jonathan Edwards wrote:
>
>>
>> since you can also have KB sized snapshots and might want to limit  
>> exposure to a pool .. here's another take using the -p option in  
>> zfs get
>>
>> $ cat > snapsum
>> #!/bin/bash
>> USAGE="USAGE: $0 pool"
>> test $# -ne 1 && echo $USAGE && exit 1
>>
>> sum=0
>> pool=$1
>>
>> for snapshot in `zfs list -Hr -t snapshot -o name ${pool}` ; do
>> snapsize=`zfs get -Hp -o value used ${snapshot}`
>> sum=$((sum + snapsize))
>> done
>>
>> echo "Total snapshot usage for ${pool} = ${sum}"
>> ^D
>> $ chmod 755 snapsum
>> $ ./snapsum
>>
>> --
>> this is still kind of a hack .. there's some functionality you can  
>> derive in dumping the zdb block usage, but zdb isn't really ported  
>> here and it's still not that trivial to derive as this sort of  
>> reporting functionality should really get RFE'd ..
>>
>> anyhow hth
>> jonathan
>>
>>
>>
>> On Oct 23, 2008, at 4:34 PM, Werner Donn? wrote:
>>
>>> Hi Dillon,
>>>
>>> The following command-line will give you the amount of bytes  
>>> consumed by
>>> all of your snapshots:
>>>
>>> zfs list -t snapshot -H -o used | sed 's/K/*1024/' | sed 's/M/ 
>>> *1024*1024/' |
>>> sed 's/G/*1024*1024*1024/' | sed 's/^/a+=/' | awk '{ print $0 }  
>>> END { print "a"
>>> }' | bc
>>>
>>> Best regards,
>>>
>>> Werner.
>>>
>>> On 23 Oct 2008, at 21:07, Dillon Kass wrote:
>>>
>>>> Hey guys, I was wondering...This doesn't really have anything to  
>>>> do with zfs on mac osx but I figured that I would throw this out  
>>>> there since some of you are smart :-)
>>>>
>>>> Does anyone know how to get the total snapshot usage for an  
>>>> entire pool?
>>>>
>>>> My desktop has 20 snapshots, I can just look at them and add them  
>>>> up...
>>>>
>>>> but my home fileserver has 206 snapshots...looking at them one by  
>>>> one is way less practical.
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> --
>>> Werner Donn?  --  Re                                     http://www.pincette.bi 
>>> z
>>> Engelbeekstraat 8                                               http://www.re.be
>>> BE-3300 Tienen
>>> tel: (+32) 486 425803    e-mail: werner.donne at re.be
>>>
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From lists at loveturtle.net  Fri Oct 24 06:12:54 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Fri, 24 Oct 2008 09:12:54 -0400
Subject: [zfs-discuss] unrelated question....
In-Reply-To: <D6575BB0-DC03-46B7-BDCF-A8F937D8B5EA@apple.com>
References: <4900CB6B.5050007@loveturtle.net>	<DB1172A4-A891-4E72-A6B5-CBECC93BAF13@re.be>	<DCC90641-F497-4CA2-A1EB-63ABDFEFD7B1@sun.com>
	<D6575BB0-DC03-46B7-BDCF-A8F937D8B5EA@apple.com>
Message-ID: <4901C9D6.4090900@loveturtle.net>

It's absolutely a useful feature

When you have many filesystems and automated snapshots it kind of takes 
on a life of it's own and  a general overview of snapshot usage only 
makes sense...for many reasons I'm sure everyone can think of including 
examining the effectiveness of your snapshot schedules.

It would seem others agree since they have scripts ready to go :-)

No?l Dellofano wrote:
> Hey guys,
>
> So sounds like perhaps this functionality should be added?  Would this 
> be a useful feature to have around or no?
>
> thanks!
> Noel
>
> On Oct 23, 2008, at 2:14 PM, Jonathan Edwards wrote:
>
>>
>> since you can also have KB sized snapshots and might want to limit 
>> exposure to a pool .. here's another take using the -p option in zfs get
>>
>> $ cat > snapsum
>> #!/bin/bash
>> USAGE="USAGE: $0 pool"
>> test $# -ne 1 && echo $USAGE && exit 1
>>
>> sum=0
>> pool=$1
>>
>> for snapshot in `zfs list -Hr -t snapshot -o name ${pool}` ; do
>>  snapsize=`zfs get -Hp -o value used ${snapshot}`
>>  sum=$((sum + snapsize))
>> done
>>
>> echo "Total snapshot usage for ${pool} = ${sum}"
>> ^D
>> $ chmod 755 snapsum
>> $ ./snapsum
>>
>> -- 
>> this is still kind of a hack .. there's some functionality you can 
>> derive in dumping the zdb block usage, but zdb isn't really ported 
>> here and it's still not that trivial to derive as this sort of 
>> reporting functionality should really get RFE'd ..
>>
>> anyhow hth
>> jonathan
>>
>>
>>
>> On Oct 23, 2008, at 4:34 PM, Werner Donn? wrote:
>>
>>> Hi Dillon,
>>>
>>> The following command-line will give you the amount of bytes 
>>> consumed by
>>> all of your snapshots:
>>>
>>> zfs list -t snapshot -H -o used | sed 's/K/*1024/' | sed 
>>> 's/M/*1024*1024/' |
>>> sed 's/G/*1024*1024*1024/' | sed 's/^/a+=/' | awk '{ print $0 } END 
>>> { print "a"
>>> }' | bc
>>>
>>> Best regards,
>>>
>>> Werner.
>>>
>>> On 23 Oct 2008, at 21:07, Dillon Kass wrote:
>>>
>>>> Hey guys, I was wondering...This doesn't really have anything to do 
>>>> with zfs on mac osx but I figured that I would throw this out there 
>>>> since some of you are smart :-)
>>>>
>>>> Does anyone know how to get the total snapshot usage for an entire 
>>>> pool?
>>>>
>>>> My desktop has 20 snapshots, I can just look at them and add them 
>>>> up...
>>>>
>>>> but my home fileserver has 206 snapshots...looking at them one by 
>>>> one is way less practical.
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> -- 
>>> Werner Donn?  --  Re                                     
>>> http://www.pincette.biz
>>> Engelbeekstraat 8                                               
>>> http://www.re.be
>>> BE-3300 Tienen
>>> tel: (+32) 486 425803    e-mail: werner.donne at re.be
>>>
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Fri Oct 24 09:39:26 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 24 Oct 2008 09:39:26 -0700
Subject: [zfs-discuss] unrelated question....
In-Reply-To: <4901C9D6.4090900@loveturtle.net>
References: <4900CB6B.5050007@loveturtle.net>
	<DB1172A4-A891-4E72-A6B5-CBECC93BAF13@re.be>
	<DCC90641-F497-4CA2-A1EB-63ABDFEFD7B1@sun.com>
	<D6575BB0-DC03-46B7-BDCF-A8F937D8B5EA@apple.com>
	<4901C9D6.4090900@loveturtle.net>
Message-ID: <7E374BAA-0D07-4721-BC09-B9EECB087C1D@apple.com>

ok, I've opened a new bug to track this issue and added everyones  
comments and workarounds:

<rdar://problem/6317759> Space accounting feature for total snapshot  
usage

It won't be done right away since I've got other bigger things to fix,  
but I'll try and get to it down the road after all the fires are put  
out :)

thanks!
Noel

On Oct 24, 2008, at 6:12 AM, Dillon Kass wrote:

> It's absolutely a useful feature
>
> When you have many filesystems and automated snapshots it kind of  
> takes on a life of it's own and  a general overview of snapshot  
> usage only makes sense...for many reasons I'm sure everyone can  
> think of including examining the effectiveness of your snapshot  
> schedules.
>
> It would seem others agree since they have scripts ready to go :-)
>
> No?l Dellofano wrote:
>> Hey guys,
>>
>> So sounds like perhaps this functionality should be added?  Would  
>> this be a useful feature to have around or no?
>>
>> thanks!
>> Noel
>>
>> On Oct 23, 2008, at 2:14 PM, Jonathan Edwards wrote:
>>
>>>
>>> since you can also have KB sized snapshots and might want to limit  
>>> exposure to a pool .. here's another take using the -p option in  
>>> zfs get
>>>
>>> $ cat > snapsum
>>> #!/bin/bash
>>> USAGE="USAGE: $0 pool"
>>> test $# -ne 1 && echo $USAGE && exit 1
>>>
>>> sum=0
>>> pool=$1
>>>
>>> for snapshot in `zfs list -Hr -t snapshot -o name ${pool}` ; do
>>> snapsize=`zfs get -Hp -o value used ${snapshot}`
>>> sum=$((sum + snapsize))
>>> done
>>>
>>> echo "Total snapshot usage for ${pool} = ${sum}"
>>> ^D
>>> $ chmod 755 snapsum
>>> $ ./snapsum
>>>
>>> -- 
>>> this is still kind of a hack .. there's some functionality you can  
>>> derive in dumping the zdb block usage, but zdb isn't really ported  
>>> here and it's still not that trivial to derive as this sort of  
>>> reporting functionality should really get RFE'd ..
>>>
>>> anyhow hth
>>> jonathan
>>>
>>>
>>>
>>> On Oct 23, 2008, at 4:34 PM, Werner Donn? wrote:
>>>
>>>> Hi Dillon,
>>>>
>>>> The following command-line will give you the amount of bytes  
>>>> consumed by
>>>> all of your snapshots:
>>>>
>>>> zfs list -t snapshot -H -o used | sed 's/K/*1024/' | sed 's/M/ 
>>>> *1024*1024/' |
>>>> sed 's/G/*1024*1024*1024/' | sed 's/^/a+=/' | awk '{ print $0 }  
>>>> END { print "a"
>>>> }' | bc
>>>>
>>>> Best regards,
>>>>
>>>> Werner.
>>>>
>>>> On 23 Oct 2008, at 21:07, Dillon Kass wrote:
>>>>
>>>>> Hey guys, I was wondering...This doesn't really have anything to  
>>>>> do with zfs on mac osx but I figured that I would throw this out  
>>>>> there since some of you are smart :-)
>>>>>
>>>>> Does anyone know how to get the total snapshot usage for an  
>>>>> entire pool?
>>>>>
>>>>> My desktop has 20 snapshots, I can just look at them and add  
>>>>> them up...
>>>>>
>>>>> but my home fileserver has 206 snapshots...looking at them one  
>>>>> by one is way less practical.
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>> -- 
>>>> Werner Donn?  --  Re                                     http://www.pincette.biz
>>>> Engelbeekstraat 8                                               http://www.re.be
>>>> BE-3300 Tienen
>>>> tel: (+32) 486 425803    e-mail: werner.donne at re.be
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From Jonathan.Edwards at Sun.COM  Fri Oct 24 09:53:35 2008
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Fri, 24 Oct 2008 12:53:35 -0400
Subject: [zfs-discuss] unrelated question....
In-Reply-To: <7E374BAA-0D07-4721-BC09-B9EECB087C1D@apple.com>
References: <4900CB6B.5050007@loveturtle.net>
	<DB1172A4-A891-4E72-A6B5-CBECC93BAF13@re.be>
	<DCC90641-F497-4CA2-A1EB-63ABDFEFD7B1@sun.com>
	<D6575BB0-DC03-46B7-BDCF-A8F937D8B5EA@apple.com>
	<4901C9D6.4090900@loveturtle.net>
	<7E374BAA-0D07-4721-BC09-B9EECB087C1D@apple.com>
Message-ID: <AF3D330B-98DF-4371-9A5E-98E1EEA3BD44@Sun.COM>

would it make sense to port zdb and/or perhaps extend it?

On Oct 24, 2008, at 12:39 PM, No?l Dellofano wrote:

> ok, I've opened a new bug to track this issue and added everyones  
> comments and workarounds:
>
> <rdar://problem/6317759> Space accounting feature for total snapshot  
> usage
>
> It won't be done right away since I've got other bigger things to  
> fix, but I'll try and get to it down the road after all the fires  
> are put out :)
>
> thanks!
> Noel
>
> On Oct 24, 2008, at 6:12 AM, Dillon Kass wrote:
>
>> It's absolutely a useful feature
>>
>> When you have many filesystems and automated snapshots it kind of  
>> takes on a life of it's own and  a general overview of snapshot  
>> usage only makes sense...for many reasons I'm sure everyone can  
>> think of including examining the effectiveness of your snapshot  
>> schedules.
>>
>> It would seem others agree since they have scripts ready to go :-)
>>
>> No?l Dellofano wrote:
>>> Hey guys,
>>>
>>> So sounds like perhaps this functionality should be added?  Would  
>>> this be a useful feature to have around or no?
>>>
>>> thanks!
>>> Noel
>>>
>>> On Oct 23, 2008, at 2:14 PM, Jonathan Edwards wrote:
>>>
>>>>
>>>> since you can also have KB sized snapshots and might want to  
>>>> limit exposure to a pool .. here's another take using the -p  
>>>> option in zfs get
>>>>
>>>> $ cat > snapsum
>>>> #!/bin/bash
>>>> USAGE="USAGE: $0 pool"
>>>> test $# -ne 1 && echo $USAGE && exit 1
>>>>
>>>> sum=0
>>>> pool=$1
>>>>
>>>> for snapshot in `zfs list -Hr -t snapshot -o name ${pool}` ; do
>>>> snapsize=`zfs get -Hp -o value used ${snapshot}`
>>>> sum=$((sum + snapsize))
>>>> done
>>>>
>>>> echo "Total snapshot usage for ${pool} = ${sum}"
>>>> ^D
>>>> $ chmod 755 snapsum
>>>> $ ./snapsum
>>>>
>>>> -- 
>>>> this is still kind of a hack .. there's some functionality you  
>>>> can derive in dumping the zdb block usage, but zdb isn't really  
>>>> ported here and it's still not that trivial to derive as this  
>>>> sort of reporting functionality should really get RFE'd ..
>>>>
>>>> anyhow hth
>>>> jonathan
>>>>
>>>>
>>>>
>>>> On Oct 23, 2008, at 4:34 PM, Werner Donn? wrote:
>>>>
>>>>> Hi Dillon,
>>>>>
>>>>> The following command-line will give you the amount of bytes  
>>>>> consumed by
>>>>> all of your snapshots:
>>>>>
>>>>> zfs list -t snapshot -H -o used | sed 's/K/*1024/' | sed 's/M/ 
>>>>> *1024*1024/' |
>>>>> sed 's/G/*1024*1024*1024/' | sed 's/^/a+=/' | awk '{ print $0 }  
>>>>> END { print "a"
>>>>> }' | bc
>>>>>
>>>>> Best regards,
>>>>>
>>>>> Werner.
>>>>>
>>>>> On 23 Oct 2008, at 21:07, Dillon Kass wrote:
>>>>>
>>>>>> Hey guys, I was wondering...This doesn't really have anything  
>>>>>> to do with zfs on mac osx but I figured that I would throw this  
>>>>>> out there since some of you are smart :-)
>>>>>>
>>>>>> Does anyone know how to get the total snapshot usage for an  
>>>>>> entire pool?
>>>>>>
>>>>>> My desktop has 20 snapshots, I can just look at them and add  
>>>>>> them up...
>>>>>>
>>>>>> but my home fileserver has 206 snapshots...looking at them one  
>>>>>> by one is way less practical.
>>>>>> _______________________________________________
>>>>>> zfs-discuss mailing list
>>>>>> zfs-discuss at lists.macosforge.org
>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>
>>>>> -- 
>>>>> Werner Donn?  --  Re                                     http://www.pincette.biz
>>>>> Engelbeekstraat 8                                               http://www.re.be
>>>>> BE-3300 Tienen
>>>>> tel: (+32) 486 425803    e-mail: werner.donne at re.be
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Fri Oct 24 17:06:39 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 24 Oct 2008 17:06:39 -0700
Subject: [zfs-discuss] unrelated question....
In-Reply-To: <AF3D330B-98DF-4371-9A5E-98E1EEA3BD44@Sun.COM>
References: <4900CB6B.5050007@loveturtle.net>
	<DB1172A4-A891-4E72-A6B5-CBECC93BAF13@re.be>
	<DCC90641-F497-4CA2-A1EB-63ABDFEFD7B1@sun.com>
	<D6575BB0-DC03-46B7-BDCF-A8F937D8B5EA@apple.com>
	<4901C9D6.4090900@loveturtle.net>
	<7E374BAA-0D07-4721-BC09-B9EECB087C1D@apple.com>
	<AF3D330B-98DF-4371-9A5E-98E1EEA3BD44@Sun.COM>
Message-ID: <75BA1EB1-C93B-470B-A6D5-E934FC7F848C@apple.com>

we are already in the process of porting zdb.  I think perhaps the  
total snapshot size  though deserves a feature of it's own, as most  
people don't want to go hunting in zdb, and it's really only intended  
as a debugging aid, we really didn't intend it to be any kind of  
status checking mechanism.

Noel

On Oct 24, 2008, at 9:53 AM, Jonathan Edwards wrote:

> would it make sense to port zdb and/or perhaps extend it?
>
> On Oct 24, 2008, at 12:39 PM, No?l Dellofano wrote:
>
>> ok, I've opened a new bug to track this issue and added everyones  
>> comments and workarounds:
>>
>> <rdar://problem/6317759> Space accounting feature for total  
>> snapshot usage
>>
>> It won't be done right away since I've got other bigger things to  
>> fix, but I'll try and get to it down the road after all the fires  
>> are put out :)
>>
>> thanks!
>> Noel
>>
>> On Oct 24, 2008, at 6:12 AM, Dillon Kass wrote:
>>
>>> It's absolutely a useful feature
>>>
>>> When you have many filesystems and automated snapshots it kind of  
>>> takes on a life of it's own and  a general overview of snapshot  
>>> usage only makes sense...for many reasons I'm sure everyone can  
>>> think of including examining the effectiveness of your snapshot  
>>> schedules.
>>>
>>> It would seem others agree since they have scripts ready to go :-)
>>>
>>> No?l Dellofano wrote:
>>>> Hey guys,
>>>>
>>>> So sounds like perhaps this functionality should be added?  Would  
>>>> this be a useful feature to have around or no?
>>>>
>>>> thanks!
>>>> Noel
>>>>
>>>> On Oct 23, 2008, at 2:14 PM, Jonathan Edwards wrote:
>>>>
>>>>>
>>>>> since you can also have KB sized snapshots and might want to  
>>>>> limit exposure to a pool .. here's another take using the -p  
>>>>> option in zfs get
>>>>>
>>>>> $ cat > snapsum
>>>>> #!/bin/bash
>>>>> USAGE="USAGE: $0 pool"
>>>>> test $# -ne 1 && echo $USAGE && exit 1
>>>>>
>>>>> sum=0
>>>>> pool=$1
>>>>>
>>>>> for snapshot in `zfs list -Hr -t snapshot -o name ${pool}` ; do
>>>>> snapsize=`zfs get -Hp -o value used ${snapshot}`
>>>>> sum=$((sum + snapsize))
>>>>> done
>>>>>
>>>>> echo "Total snapshot usage for ${pool} = ${sum}"
>>>>> ^D
>>>>> $ chmod 755 snapsum
>>>>> $ ./snapsum
>>>>>
>>>>> -- 
>>>>> this is still kind of a hack .. there's some functionality you  
>>>>> can derive in dumping the zdb block usage, but zdb isn't really  
>>>>> ported here and it's still not that trivial to derive as this  
>>>>> sort of reporting functionality should really get RFE'd ..
>>>>>
>>>>> anyhow hth
>>>>> jonathan
>>>>>
>>>>>
>>>>>
>>>>> On Oct 23, 2008, at 4:34 PM, Werner Donn? wrote:
>>>>>
>>>>>> Hi Dillon,
>>>>>>
>>>>>> The following command-line will give you the amount of bytes  
>>>>>> consumed by
>>>>>> all of your snapshots:
>>>>>>
>>>>>> zfs list -t snapshot -H -o used | sed 's/K/*1024/' | sed 's/M/ 
>>>>>> *1024*1024/' |
>>>>>> sed 's/G/*1024*1024*1024/' | sed 's/^/a+=/' | awk '{ print $0 }  
>>>>>> END { print "a"
>>>>>> }' | bc
>>>>>>
>>>>>> Best regards,
>>>>>>
>>>>>> Werner.
>>>>>>
>>>>>> On 23 Oct 2008, at 21:07, Dillon Kass wrote:
>>>>>>
>>>>>>> Hey guys, I was wondering...This doesn't really have anything  
>>>>>>> to do with zfs on mac osx but I figured that I would throw  
>>>>>>> this out there since some of you are smart :-)
>>>>>>>
>>>>>>> Does anyone know how to get the total snapshot usage for an  
>>>>>>> entire pool?
>>>>>>>
>>>>>>> My desktop has 20 snapshots, I can just look at them and add  
>>>>>>> them up...
>>>>>>>
>>>>>>> but my home fileserver has 206 snapshots...looking at them one  
>>>>>>> by one is way less practical.
>>>>>>> _______________________________________________
>>>>>>> zfs-discuss mailing list
>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>>
>>>>>> -- 
>>>>>> Werner Donn?  --  Re                                     http://www.pincette.biz
>>>>>> Engelbeekstraat 8                                               http://www.re.be
>>>>>> BE-3300 Tienen
>>>>>> tel: (+32) 486 425803    e-mail: werner.donne at re.be
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> zfs-discuss mailing list
>>>>>> zfs-discuss at lists.macosforge.org
>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From Jonathan.Edwards at Sun.COM  Fri Oct 24 21:07:29 2008
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Sat, 25 Oct 2008 00:07:29 -0400
Subject: [zfs-discuss] unrelated question....
In-Reply-To: <75BA1EB1-C93B-470B-A6D5-E934FC7F848C@apple.com>
References: <4900CB6B.5050007@loveturtle.net>
	<DB1172A4-A891-4E72-A6B5-CBECC93BAF13@re.be>
	<DCC90641-F497-4CA2-A1EB-63ABDFEFD7B1@sun.com>
	<D6575BB0-DC03-46B7-BDCF-A8F937D8B5EA@apple.com>
	<4901C9D6.4090900@loveturtle.net>
	<7E374BAA-0D07-4721-BC09-B9EECB087C1D@apple.com>
	<AF3D330B-98DF-4371-9A5E-98E1EEA3BD44@Sun.COM>
	<75BA1EB1-C93B-470B-A6D5-E934FC7F848C@apple.com>
Message-ID: <60429796-9BA1-4359-8C62-0FB67602831A@Sun.COM>

true true (and good to know on the zdb port) ..

btw on the issue of usability and information visibility - is there a  
better libzfs based GUI than the lockhart one we have in the works? or  
is it just diskutil integration that's being planned?

On Oct 24, 2008, at 8:06 PM, No?l Dellofano wrote:

> we are already in the process of porting zdb.  I think perhaps the  
> total snapshot size  though deserves a feature of it's own, as most  
> people don't want to go hunting in zdb, and it's really only  
> intended as a debugging aid, we really didn't intend it to be any  
> kind of status checking mechanism.
>
> Noel
>
> On Oct 24, 2008, at 9:53 AM, Jonathan Edwards wrote:
>
>> would it make sense to port zdb and/or perhaps extend it?
>>
>> On Oct 24, 2008, at 12:39 PM, No?l Dellofano wrote:
>>
>>> ok, I've opened a new bug to track this issue and added everyones  
>>> comments and workarounds:
>>>
>>> <rdar://problem/6317759> Space accounting feature for total  
>>> snapshot usage
>>>
>>> It won't be done right away since I've got other bigger things to  
>>> fix, but I'll try and get to it down the road after all the fires  
>>> are put out :)
>>>
>>> thanks!
>>> Noel
>>>
>>> On Oct 24, 2008, at 6:12 AM, Dillon Kass wrote:
>>>
>>>> It's absolutely a useful feature
>>>>
>>>> When you have many filesystems and automated snapshots it kind of  
>>>> takes on a life of it's own and  a general overview of snapshot  
>>>> usage only makes sense...for many reasons I'm sure everyone can  
>>>> think of including examining the effectiveness of your snapshot  
>>>> schedules.
>>>>
>>>> It would seem others agree since they have scripts ready to go :-)
>>>>
>>>> No?l Dellofano wrote:
>>>>> Hey guys,
>>>>>
>>>>> So sounds like perhaps this functionality should be added?   
>>>>> Would this be a useful feature to have around or no?
>>>>>
>>>>> thanks!
>>>>> Noel
>>>>>
>>>>> On Oct 23, 2008, at 2:14 PM, Jonathan Edwards wrote:
>>>>>
>>>>>>
>>>>>> since you can also have KB sized snapshots and might want to  
>>>>>> limit exposure to a pool .. here's another take using the -p  
>>>>>> option in zfs get
>>>>>>
>>>>>> $ cat > snapsum
>>>>>> #!/bin/bash
>>>>>> USAGE="USAGE: $0 pool"
>>>>>> test $# -ne 1 && echo $USAGE && exit 1
>>>>>>
>>>>>> sum=0
>>>>>> pool=$1
>>>>>>
>>>>>> for snapshot in `zfs list -Hr -t snapshot -o name ${pool}` ; do
>>>>>> snapsize=`zfs get -Hp -o value used ${snapshot}`
>>>>>> sum=$((sum + snapsize))
>>>>>> done
>>>>>>
>>>>>> echo "Total snapshot usage for ${pool} = ${sum}"
>>>>>> ^D
>>>>>> $ chmod 755 snapsum
>>>>>> $ ./snapsum
>>>>>>
>>>>>> -- 
>>>>>> this is still kind of a hack .. there's some functionality you  
>>>>>> can derive in dumping the zdb block usage, but zdb isn't really  
>>>>>> ported here and it's still not that trivial to derive as this  
>>>>>> sort of reporting functionality should really get RFE'd ..
>>>>>>
>>>>>> anyhow hth
>>>>>> jonathan
>>>>>>
>>>>>>
>>>>>>
>>>>>> On Oct 23, 2008, at 4:34 PM, Werner Donn? wrote:
>>>>>>
>>>>>>> Hi Dillon,
>>>>>>>
>>>>>>> The following command-line will give you the amount of bytes  
>>>>>>> consumed by
>>>>>>> all of your snapshots:
>>>>>>>
>>>>>>> zfs list -t snapshot -H -o used | sed 's/K/*1024/' | sed 's/M/ 
>>>>>>> *1024*1024/' |
>>>>>>> sed 's/G/*1024*1024*1024/' | sed 's/^/a+=/' | awk '{ print  
>>>>>>> $0 } END { print "a"
>>>>>>> }' | bc
>>>>>>>
>>>>>>> Best regards,
>>>>>>>
>>>>>>> Werner.
>>>>>>>
>>>>>>> On 23 Oct 2008, at 21:07, Dillon Kass wrote:
>>>>>>>
>>>>>>>> Hey guys, I was wondering...This doesn't really have anything  
>>>>>>>> to do with zfs on mac osx but I figured that I would throw  
>>>>>>>> this out there since some of you are smart :-)
>>>>>>>>
>>>>>>>> Does anyone know how to get the total snapshot usage for an  
>>>>>>>> entire pool?
>>>>>>>>
>>>>>>>> My desktop has 20 snapshots, I can just look at them and add  
>>>>>>>> them up...
>>>>>>>>
>>>>>>>> but my home fileserver has 206 snapshots...looking at them  
>>>>>>>> one by one is way less practical.
>>>>>>>> _______________________________________________
>>>>>>>> zfs-discuss mailing list
>>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>>>
>>>>>>> -- 
>>>>>>> Werner Donn?  --  Re                                     http://www.pincette.biz
>>>>>>> Engelbeekstraat  
>>>>>>> 8                                               http://www.re.be
>>>>>>> BE-3300 Tienen
>>>>>>> tel: (+32) 486 425803    e-mail: werner.donne at re.be
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> zfs-discuss mailing list
>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>>
>>>>>> _______________________________________________
>>>>>> zfs-discuss mailing list
>>>>>> zfs-discuss at lists.macosforge.org
>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>


From gregor.guncar at gmail.com  Sat Oct 25 02:58:41 2008
From: gregor.guncar at gmail.com (Gregor Guncar)
Date: Sat, 25 Oct 2008 11:58:41 +0200
Subject: [zfs-discuss] zfs on OSX 600 files disappeared
Message-ID: <68B95C93-8F7D-4265-9E79-C427F78BB7E2@gmail.com>

Hi,
I like zfs. Very much. And since I want to use it for some serious  
stuff I need to know what has happened to some 600 files that just  
disappeared from my zfs pool. It is 3 identical disks of 750GB in one  
raidz pool on my MacPro (10.5.5, http://zfs.macosforge.org/downloads/zfs-119bin.tgz) 
. I was processing some data the other day- that resulted in a few  
directories and some 600 files that just simply aren't there anymore.   
I am sure the files were there, because all of them were subsequently  
processed resulting in a single result and log file (containing the  
path of the files) that I have copied on my laptop the same day. Later  
that day my Finder stopped responding when I was searching something  
using spotlight. I couldn't restart  Finder.app (killall Finder) or  
shutdown my mac in any other way (shutdown -h now) than to press the  
on/off button to turn it off.  Next day I couldn't find any of the  
files that I have written the previous day.  So I did zpool scrub,  
export and import and all is reported to be fine, that is no errors. I  
have also turned off continuous spotlight indexing on that disk to  
prevent further hangups.
I know that zfs on OSX is highly experimental and I don't need to  
rescue those files. I just want to know what has happend and what I  
(and developers) can do to prevent it in the future. Zfs being so  
robust should not be prone to such data loss. Is it possible that  
after half an hour my data were still in the RAM (4GB), not yet  
written on the disk? How often does zfs commit changes and what does  
that depend on?
Thank you for your answers!
Cheers,
Gregor



From mailinglists at mailnewsrss.com  Sat Oct 25 10:07:14 2008
From: mailinglists at mailnewsrss.com (Jason Todd Slack-Moehrle)
Date: Sat, 25 Oct 2008 10:07:14 -0700
Subject: [zfs-discuss] ZFS and some questions
Message-ID: <C495BD84-1E6D-4D87-B76B-0EEBF5986464@mailnewsrss.com>

HI All,

There  is a lot going on with ZFS and OS X and I wanted to get a feel  
for what I can do at this point.

1. I know that I cannot boot my new Mac Pro (32gb RAM!) from a ZFS  
partition, but I should be able to partition and create a ZFS  
partition for my dada, correct? I would use VolumeWorks or something  
to do the resize and then Disk Utility to format the unused space ZFS?  
Or does the tar-ball have the utility?

2. How do I move my home directory to the partition? (Can this be done  
in Leopard?, In Tiger I could using the NetInfo Manager, but I dont  
know how in Leopard)

3. I write software, what issues are there with compiling XCode  
projects on ZFS Partitions?

4. Can I still backup to Time Machine?

5. Any issues with other apps, etc?

Thanks!

-Jason

From jlukas at nighttimemidnight.com  Sat Oct 25 10:42:54 2008
From: jlukas at nighttimemidnight.com (Jacob Lukas)
Date: Sat, 25 Oct 2008 10:42:54 -0700
Subject: [zfs-discuss] ZFS and some questions
Message-ID: <4E8C4049-63B0-4CF8-8B10-920653314A42@nighttimemidnight.com>

On Oct 25, 2008, at 10:07, Jason Todd Slack-Moehrle <mailinglists at mailnewsrss.com 
 > wrote:

> HI All,
>
> There  is a lot going on with ZFS and OS X and I wanted to get a  
> feel for what I can do at this point.
>
> 1. I know that I cannot boot my new Mac Pro (32gb RAM!) from a ZFS  
> partition, but I should be able to partition and create a ZFS  
> partition for my dada, correct? I would use VolumeWorks or something  
> to do the resize and then Disk Utility to format the unused space  
> ZFS? Or does the tar-ball have the utility?
>
> 2. How do I move my home directory to the partition? (Can this be  
> done in Leopard?, In Tiger I could using the NetInfo Manager, but I  
> dont know how in Leopard)

You can mount your ZFS volume at /Users/username

> 3. I write software, what issues are there with compiling XCode  
> projects on ZFS Partitions?

I've had no problems compiling code, through XCode or otherwise.

> 4. Can I still backup to Time Machine?

No, but you can use ZFS snapshots.

> 5. Any issues with other apps, etc?

ZFS is case-sensitive, so some apps may fail. The only one I know of  
is DragThing, which fails to start if your home folder is on ZFS.

Hope I've helped,
Jacob

From mailinglists at mailnewsrss.com  Sat Oct 25 11:17:59 2008
From: mailinglists at mailnewsrss.com (Jason Todd Slack-Moehrle)
Date: Sat, 25 Oct 2008 11:17:59 -0700
Subject: [zfs-discuss] ZFS and some questions
In-Reply-To: <4E8C4049-63B0-4CF8-8B10-920653314A42@nighttimemidnight.com>
References: <4E8C4049-63B0-4CF8-8B10-920653314A42@nighttimemidnight.com>
Message-ID: <A9C38B05-0AFD-4412-A355-BCC15025A953@mailnewsrss.com>

Hi,

>> 2. How do I move my home directory to the partition? (Can this be  
>> done in Leopard?, In Tiger I could using the NetInfo Manager, but I  
>> dont know how in Leopard)
>
> You can mount your ZFS volume at /Users/username
>
>> 3. I write software, what issues are there with compiling XCode  
>> projects on ZFS Partitions?
>
> I've had no problems compiling code, through XCode or otherwise.
>
>> 4. Can I still backup to Time Machine?
>
> No, but you can use ZFS snapshots.
>
>> 5. Any issues with other apps, etc?
>
> ZFS is case-sensitive, so some apps may fail. The only one I know of  
> is DragThing, which fails to start if your home folder is on ZFS.

Thanks for the information. I am going to do some playing around and  
see how things are.

-Jason

From hanche at math.ntnu.no  Sat Oct 25 11:20:54 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Sat, 25 Oct 2008 20:20:54 +0200 (CEST)
Subject: [zfs-discuss] ZFS and some questions
In-Reply-To: <4E8C4049-63B0-4CF8-8B10-920653314A42@nighttimemidnight.com>
References: <4E8C4049-63B0-4CF8-8B10-920653314A42@nighttimemidnight.com>
Message-ID: <20081025.202054.148430912.hanche@math.ntnu.no>

+ Jacob Lukas <jlukas at nighttimemidnight.com>:

> > 4. Can I still backup to Time Machine?
> 
> No, but you can use ZFS snapshots.

They're a bit hard to use though: Currently, you have to clone a
snapshot in order to access it. Proper .zfs/snapshot support is
lacking for the time being.

- Harald

From ndellofano at apple.com  Mon Oct 27 15:50:43 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon, 27 Oct 2008 15:50:43 -0700
Subject: [zfs-discuss] unrelated question....
In-Reply-To: <60429796-9BA1-4359-8C62-0FB67602831A@Sun.COM>
References: <4900CB6B.5050007@loveturtle.net>
	<DB1172A4-A891-4E72-A6B5-CBECC93BAF13@re.be>
	<DCC90641-F497-4CA2-A1EB-63ABDFEFD7B1@sun.com>
	<D6575BB0-DC03-46B7-BDCF-A8F937D8B5EA@apple.com>
	<4901C9D6.4090900@loveturtle.net>
	<7E374BAA-0D07-4721-BC09-B9EECB087C1D@apple.com>
	<AF3D330B-98DF-4371-9A5E-98E1EEA3BD44@Sun.COM>
	<75BA1EB1-C93B-470B-A6D5-E934FC7F848C@apple.com>
	<60429796-9BA1-4359-8C62-0FB67602831A@Sun.COM>
Message-ID: <F3106CAC-7768-49B0-BFE0-4BC6C3ABB2BA@apple.com>

right now we've got our hands full doing all the diskutil integration  
stuffs :)

Noel

On Oct 24, 2008, at 9:07 PM, Jonathan Edwards wrote:

> true true (and good to know on the zdb port) ..
>
> btw on the issue of usability and information visibility - is there  
> a better libzfs based GUI than the lockhart one we have in the  
> works? or is it just diskutil integration that's being planned?
>
> On Oct 24, 2008, at 8:06 PM, No?l Dellofano wrote:
>
>> we are already in the process of porting zdb.  I think perhaps the  
>> total snapshot size  though deserves a feature of it's own, as most  
>> people don't want to go hunting in zdb, and it's really only  
>> intended as a debugging aid, we really didn't intend it to be any  
>> kind of status checking mechanism.
>>
>> Noel
>>
>> On Oct 24, 2008, at 9:53 AM, Jonathan Edwards wrote:
>>
>>> would it make sense to port zdb and/or perhaps extend it?
>>>
>>> On Oct 24, 2008, at 12:39 PM, No?l Dellofano wrote:
>>>
>>>> ok, I've opened a new bug to track this issue and added everyones  
>>>> comments and workarounds:
>>>>
>>>> <rdar://problem/6317759> Space accounting feature for total  
>>>> snapshot usage
>>>>
>>>> It won't be done right away since I've got other bigger things to  
>>>> fix, but I'll try and get to it down the road after all the fires  
>>>> are put out :)
>>>>
>>>> thanks!
>>>> Noel
>>>>
>>>> On Oct 24, 2008, at 6:12 AM, Dillon Kass wrote:
>>>>
>>>>> It's absolutely a useful feature
>>>>>
>>>>> When you have many filesystems and automated snapshots it kind  
>>>>> of takes on a life of it's own and  a general overview of  
>>>>> snapshot usage only makes sense...for many reasons I'm sure  
>>>>> everyone can think of including examining the effectiveness of  
>>>>> your snapshot schedules.
>>>>>
>>>>> It would seem others agree since they have scripts ready to go :-)
>>>>>
>>>>> No?l Dellofano wrote:
>>>>>> Hey guys,
>>>>>>
>>>>>> So sounds like perhaps this functionality should be added?   
>>>>>> Would this be a useful feature to have around or no?
>>>>>>
>>>>>> thanks!
>>>>>> Noel
>>>>>>
>>>>>> On Oct 23, 2008, at 2:14 PM, Jonathan Edwards wrote:
>>>>>>
>>>>>>>
>>>>>>> since you can also have KB sized snapshots and might want to  
>>>>>>> limit exposure to a pool .. here's another take using the -p  
>>>>>>> option in zfs get
>>>>>>>
>>>>>>> $ cat > snapsum
>>>>>>> #!/bin/bash
>>>>>>> USAGE="USAGE: $0 pool"
>>>>>>> test $# -ne 1 && echo $USAGE && exit 1
>>>>>>>
>>>>>>> sum=0
>>>>>>> pool=$1
>>>>>>>
>>>>>>> for snapshot in `zfs list -Hr -t snapshot -o name ${pool}` ; do
>>>>>>> snapsize=`zfs get -Hp -o value used ${snapshot}`
>>>>>>> sum=$((sum + snapsize))
>>>>>>> done
>>>>>>>
>>>>>>> echo "Total snapshot usage for ${pool} = ${sum}"
>>>>>>> ^D
>>>>>>> $ chmod 755 snapsum
>>>>>>> $ ./snapsum
>>>>>>>
>>>>>>> -- 
>>>>>>> this is still kind of a hack .. there's some functionality you  
>>>>>>> can derive in dumping the zdb block usage, but zdb isn't  
>>>>>>> really ported here and it's still not that trivial to derive  
>>>>>>> as this sort of reporting functionality should really get  
>>>>>>> RFE'd ..
>>>>>>>
>>>>>>> anyhow hth
>>>>>>> jonathan
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On Oct 23, 2008, at 4:34 PM, Werner Donn? wrote:
>>>>>>>
>>>>>>>> Hi Dillon,
>>>>>>>>
>>>>>>>> The following command-line will give you the amount of bytes  
>>>>>>>> consumed by
>>>>>>>> all of your snapshots:
>>>>>>>>
>>>>>>>> zfs list -t snapshot -H -o used | sed 's/K/*1024/' | sed 's/M/ 
>>>>>>>> *1024*1024/' |
>>>>>>>> sed 's/G/*1024*1024*1024/' | sed 's/^/a+=/' | awk '{ print  
>>>>>>>> $0 } END { print "a"
>>>>>>>> }' | bc
>>>>>>>>
>>>>>>>> Best regards,
>>>>>>>>
>>>>>>>> Werner.
>>>>>>>>
>>>>>>>> On 23 Oct 2008, at 21:07, Dillon Kass wrote:
>>>>>>>>
>>>>>>>>> Hey guys, I was wondering...This doesn't really have  
>>>>>>>>> anything to do with zfs on mac osx but I figured that I  
>>>>>>>>> would throw this out there since some of you are smart :-)
>>>>>>>>>
>>>>>>>>> Does anyone know how to get the total snapshot usage for an  
>>>>>>>>> entire pool?
>>>>>>>>>
>>>>>>>>> My desktop has 20 snapshots, I can just look at them and add  
>>>>>>>>> them up...
>>>>>>>>>
>>>>>>>>> but my home fileserver has 206 snapshots...looking at them  
>>>>>>>>> one by one is way less practical.
>>>>>>>>> _______________________________________________
>>>>>>>>> zfs-discuss mailing list
>>>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>>>>
>>>>>>>> -- 
>>>>>>>> Werner Donn?  --  Re                                     http://www.pincette.biz
>>>>>>>> Engelbeekstraat  
>>>>>>>> 8                                               http://www.re.be
>>>>>>>> BE-3300 Tienen
>>>>>>>> tel: (+32) 486 425803    e-mail: werner.donne at re.be
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> zfs-discuss mailing list
>>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> zfs-discuss mailing list
>>>>>>> zfs-discuss at lists.macosforge.org
>>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>>
>>>>>> _______________________________________________
>>>>>> zfs-discuss mailing list
>>>>>> zfs-discuss at lists.macosforge.org
>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>>
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>
>


From ndellofano at apple.com  Tue Oct 28 20:40:19 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue, 28 Oct 2008 20:40:19 -0700
Subject: [zfs-discuss] Kernel Panic with Zpool Status if a pool
	has	corruption
In-Reply-To: <BAC40FB0-F6A6-49BC-B6CC-36D82BF30EB0@Sun.COM>
References: <BAC40FB0-F6A6-49BC-B6CC-36D82BF30EB0@Sun.COM>
Message-ID: <1D298921-E915-4B9E-B9FC-0AB003EB2320@apple.com>

FYI:

I'm currently working on a fix for this issue.  Was able to get a  
reproduce in house.  Will keep you posted, I believe I know what the  
cause is.

Thanks everyone!
Noel

On Oct 21, 2008, at 11:38 AM, Bill Winnett wrote:

>
>
> It appears that if a pool has some corruption and zfs has detected  
> it, then if you use "zpool status" on that pool you will panic.
>
> ====
> Tue Oct 21 12:12:15 2008
> panic(cpu 0 caller 0x001A8CD4): Kernel trap at 0x0036271b, type  
> 14=page fault, registers:
> CR0: 0x80010033, CR2: 0x0000000c, CR3: 0x01287000, CR4: 0x00000660
> EAX: 0x00000000, EBX: 0x5dbd7de8, ECX: 0x1f000000, EDX: 0x00000000
> CR2: 0x0000000c, EBP: 0x5dbd7d18, ESI: 0x00000200, EDI: 0x09255000
> EFL: 0x00010202, EIP: 0x0036271b, CS:  0x00000008, DS:  0x00530010
> Error code: 0x00000000
>
> Backtrace, Format - Frame : Return Address (4 potential args on stack)
> 0x5dbd7b38 : 0x12b0fa (0x4592a4 0x5dbd7b6c 0x133243 0x0)
> 0x5dbd7b88 : 0x1a8cd4 (0x46280c 0x36271b 0xe 0x461fbc)
> 0x5dbd7c68 : 0x19ede5 (0x5dbd7c80 0xe07cf20 0x5dbd7d18 0x36271b)
> 0x5dbd7c78 : 0x36271b (0xe 0x48 0x7e00010 0x190010)
> 0x5dbd7d18 : 0xc97ea4 (0x0 0x2993e 0x3 0x0)
> 0x5dbd7d38 : 0xc9f9ba (0x0 0x0 0x5dbd7dc8 0x19d5b1)
> 0x5dbd7d78 : 0x20300f (0x1f000000 0xcf1c5a20 0x9255000 0x3)
> 0x5dbd7db8 : 0x1f63a2 (0x5dbd7de8 0x246 0x5dbd7e18 0x1da567)
> 0x5dbd7e18 : 0x1ec67c (0x76d9ef0 0xcf1c5a20 0x9255000 0x3)
> 0x5dbd7e78 : 0x3661ff (0x716e7e0 0xcf1c5a20 0x9255000 0x5dbd7f50)
> 0x5dbd7e98 : 0x38cb8f (0x716e7e0 0xcf1c5a20 0x9255000 0x5dbd7f50)
> 0x5dbd7f78 : 0x3ddde2 (0x7acd2e0 0x7e1d1e0 0x7e1d224 0x0)
> 0x5dbd7fc8 : 0x19f2c3 (0x7b99900 0x0 0x10 0x7b99900)
> No mapping exists for frame pointer
> Backtrace terminated-invalid frame pointer 0xbfffa7d8
>       Kernel loadable modules in backtrace (with dependencies):
>          com.apple.filesystems.zfs(8.0)@0xc3c000->0xd07fff
>
> BSD process name corresponding to current thread: zpool
>
> Mac OS version:
> 9E17
>
> Kernel version:
> Darwin Kernel Version 9.4.0: Mon Jun  9 19:30:53 PDT 2008;  
> root:xnu-1228.5.20~1/RELEASE_I386
> System model name: iMac7,1 (Mac-F4238CC8)
> ====
>
>
> I loaded virtualbox and installed a Solaris image and attached my  
> disks to the VM.  When I ran scrub and status on the pool, Solaris  
> correctly indicates corruption.
> Now I don't know if the corruption is in the way the file was stored  
> by ZFS on the Mac, or if there is a bad sector, but it seems  
> suspicious that this happened only
> after I added a bunch of pools and disks to my configuration.  I  
> have since undone that, but the one pool has problems.  Lucky for me  
> this is just junk for testing.
>
> =====
> (root at milax)# zpool status -xv
>   pool:  bef
>  state:  ONLINE
> status: One or more devices has experienced an error resulting in data
>         corruption.  Applications may be affected.
> action: Restore the file in question if possible.  Otherwise restore  
> the
>         entire pool from backup.
>    see: http://www.sun.com/msg/ZFS-8000-8A
>  scrub: scrub in progress for 0h29m, 5.37% done, 8h31m to go
> config:
>
>         NAME        STATE     READ WRITE CHKSUM
>         bef         ONLINE       0     0      0
>           c7t0d0s1  ONLINE       0     0      0
>           c6t0d0s1  ONLINE       0     0      0
>
> errors: Permanent errors have been detected in the following files:
>
>         /bef/test/file3
>         bef:<0x4845>
>         bef:<0x4848>
>         /bef/test/file2
>         bef:0x193>
>         /bef/test/file1
>         /bef/test/file5
>         bef:<0x4af4>
> (root at milax)#
> llll
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081028/de8d235c/attachment.html>

From m at il.gaw.in  Thu Oct 30 06:03:00 2008
From: m at il.gaw.in (Gawin Dapper)
Date: Thu, 30 Oct 2008 14:03:00 +0100
Subject: [zfs-discuss] ZFS cache devices
Message-ID: <61E98648-B1A3-403F-B069-E479FD843FE5@il.gaw.in>

Hi,

First of all, I really admire the effort that is put into ZFS itself  
and porting ZFS to OS X, great work!

After playing around with ZFS (rev 119) on OS X, I started wondering  
if the current implementation of ZFS on OS X includes 'cache devices'  
to cache storage pool data. I found that in the zpool source  
(zfs_commands/zpool/zpool_vdev.c) the "cache" command is not yet  
implemented.

Are there any plans to implement caching devices, and on what term  
could we expect this?
Thx in advance!

Kind regards,

Gawin

From bplist at thinkpink.com  Fri Oct 31 10:45:35 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Fri, 31 Oct 2008 10:45:35 -0700
Subject: [zfs-discuss] ZFS vs. Seagate 1.5TB drives?
Message-ID: <2AC4FA8F-14D3-49DE-899F-DA11CE8FF02A@thinkpink.com>

Has anyone tried ZFS with the new Seagate 1.5TB drives?  It looks like  
there may be some timeout issues with flushing the write cache on this  
model of the drive that affects HFS with journaling (Mac) and XFS  
(linux).  I'm guessing this will hit ZFS too, but I wanted to ask to  
see if anyone's had direct experience.

tia,
bri


From Todd.Moore at Sun.COM  Fri Oct 17 09:07:19 2008
From: Todd.Moore at Sun.COM (Todd E. Moore)
Date: Fri, 17 Oct 2008 12:07:19 -0400
Subject: [zfs-discuss] Why does ZFS pool require root privileges to
	access?
In-Reply-To: <48F89FE7.2070504@sun.com>
References: <48F89FE7.2070504@sun.com>
Message-ID: <48F8B837.8080506@sun.com>

An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20081017/9be52fe7/attachment.html>

From bp at thinkpink.com  Fri Oct 31 10:43:53 2008
From: bp at thinkpink.com (Brian Pinkerton)
Date: Fri, 31 Oct 2008 10:43:53 -0700
Subject: [zfs-discuss] ZFS vs. Seagate 1.5TB drives?
Message-ID: <C87DBE55-9EC1-4FC5-95B8-B77A14AAF855@thinkpink.com>

Has anyone tried ZFS with the new Seagate 1.5TB drives?  It looks like  
there may be some timeout issues with flushing the write cache on this  
model of the drive that affects HFS with journaling (Mac) and XFS  
(linux).  I'm guessing this will hit ZFS too, but I wanted to ask to  
see if anyone's had direct experience.

tia,
bri


From curtis.schiewek at me.com  Fri Oct 31 10:52:28 2008
From: curtis.schiewek at me.com (Curtis Schiewek)
Date: Fri, 31 Oct 2008 10:52:28 -0700
Subject: [zfs-discuss] ZFS vs. Seagate 1.5TB drives?
Message-ID: <142443242573691909690087752159443073706-Webmail2@me.com>

Hey Brian,

I set up a brand new zpool with 4 of the Seagates in RAIDZ about a week ago (FreeBSD 7.0) and haven't experienced any issues whatever.  I copied ~3TB of data form a prexisting pool to it and had no problems.  I've also done a significant amount of reads from it whithout issue.  I'll check some logs and see if any instances of the disk buffers flushing.

Hope that helps!

Curtis
 
On Friday, October 31, 2008, at 10:45AM, "Brian Pinkerton" <bplist at thinkpink.com> wrote:
>Has anyone tried ZFS with the new Seagate 1.5TB drives?  It looks like  
>there may be some timeout issues with flushing the write cache on this  
>model of the drive that affects HFS with journaling (Mac) and XFS  
>(linux).  I'm guessing this will hit ZFS too, but I wanted to ask to  
>see if anyone's had direct experience.
>
>tia,
>bri
>
>_______________________________________________
>zfs-discuss mailing list
>zfs-discuss at lists.macosforge.org
>http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>

From ndellofano at apple.com  Fri Oct 31 17:25:59 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 31 Oct 2008 17:25:59 -0700
Subject: [zfs-discuss] ZFS cache devices
In-Reply-To: <61E98648-B1A3-403F-B069-E479FD843FE5@il.gaw.in>
References: <61E98648-B1A3-403F-B069-E479FD843FE5@il.gaw.in>
Message-ID: <5F3FD320-40C4-4944-8D37-844673DE03C9@apple.com>

Yeah we haven't really discussed cache device implementation at all.   
So I have no clue if we'll implement it :) Depends on if we decide its  
a win for our customer base and all that goodness.  For now we have  
our hands full with just regular implementation features :)

Noel

On Oct 30, 2008, at 6:03 AM, Gawin Dapper wrote:

> Hi,
>
> First of all, I really admire the effort that is put into ZFS itself  
> and porting ZFS to OS X, great work!
>
> After playing around with ZFS (rev 119) on OS X, I started wondering  
> if the current implementation of ZFS on OS X includes 'cache  
> devices' to cache storage pool data. I found that in the zpool  
> source (zfs_commands/zpool/zpool_vdev.c) the "cache" command is not  
> yet implemented.
>
> Are there any plans to implement caching devices, and on what term  
> could we expect this?
> Thx in advance!
>
> Kind regards,
>
> Gawin
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


