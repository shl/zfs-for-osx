From franzschmalzl at spamfreemail.de  Fri Feb  1 09:47:21 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Fri Feb  1 09:47:33 2008
Subject: [zfs-discuss] TM
Message-ID: <EBC6CB15-C3FB-49B6-8509-30D4CE51EBA2@spamfreemail.de>

I know Time Machine doesn't support zfs volumes yet, but are there any  
people working on this ?

regards,

Franz Schmalzl
From ndellofano at apple.com  Fri Feb  1 11:07:57 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri Feb  1 11:09:18 2008
Subject: [zfs-discuss] TM
In-Reply-To: <EBC6CB15-C3FB-49B6-8509-30D4CE51EBA2@spamfreemail.de>
References: <EBC6CB15-C3FB-49B6-8509-30D4CE51EBA2@spamfreemail.de>
Message-ID: <74272958-6863-4F0F-806D-9FEA6163783C@apple.com>

currently no, there is no one working on this.

Noel

On Feb 1, 2008, at 9:47 AM, Franz Schmalzl wrote:

> I know Time Machine doesn't support zfs volumes yet, but are there  
> any people working on this ?
>
> regards,
>
> Franz Schmalzl
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From William.Winnett at Sun.COM  Sat Feb  2 07:21:16 2008
From: William.Winnett at Sun.COM (Bill Winnett)
Date: Sat Feb  2 07:15:58 2008
Subject: [zfs-discuss] ZFS version ?
Message-ID: <0079DCE7-FA17-425D-B521-262798652165@sun.com>

Hi,

ZFS currently defaults to version 6, but it is based off 8?
If so how can I force ZFS to create volumes as version 8.

thanks,

-bill w.
From gabor at berczi.be  Sat Feb  2 09:26:03 2008
From: gabor at berczi.be (=?ISO-8859-1?Q?B=E9rczi_G=E1bor?=)
Date: Sat Feb  2 09:26:25 2008
Subject: [zfs-discuss] Safari(PubSub)+SQLite bug
Message-ID: <5A92ED88-0C56-41B6-A99F-2D8AC143F454@berczi.be>

Safari can't create a new SQLite DB on ZFS (actually PubSub is  
responsible for this). This is needed for RSS functionality in Safari  
and Mail.

I've created a ticket for this: http://trac.macosforge.org/projects/zfs/ticket/7

-- 
Gabucino

From zorg at sogeeky.net  Sat Feb  2 11:30:21 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Sat Feb  2 11:30:20 2008
Subject: [zfs-discuss] ZFS version ?
In-Reply-To: <0079DCE7-FA17-425D-B521-262798652165@sun.com>
References: <0079DCE7-FA17-425D-B521-262798652165@sun.com>
Message-ID: <42316769-79D5-4C58-AC16-7BBAE64ADFD0@sogeeky.net>

Yes. See the FAQ on the zfs macosforge page for the reason why. I'm  
not sure how to create them in the new version, but issuing the  
upgrade command takes just a second...

On Feb 2, 2008, at 7:21 AM, Bill Winnett <William.Winnett@Sun.COM>  
wrote:

> Hi,
>
> ZFS currently defaults to version 6, but it is based off 8?
> If so how can I force ZFS to create volumes as version 8.
>
> thanks,
>
> -bill w.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
From zorg at sogeeky.net  Sun Feb  3 22:45:44 2008
From: zorg at sogeeky.net (Mr. Zorg ...)
Date: Sun Feb  3 22:45:26 2008
Subject: [zfs-discuss] iWeb bundled movie error on zfs
Message-ID: <fbaadd320802032245i69d5c4efk22cdc5a4b4c8a6a@mail.gmail.com>

Found another issue similar to the iTunes saving issue.  Create an
iWeb domain file on zfs volume then drag and audio or movie clip onto
a sample page.  Save and exit iWeb.  When you come back in, it will
complain that it couldn't find the bundled movie.  Put the domain file
on an HFS+ volume and its fine.

Thanks!
-Mr. Zorg
From ndellofano at apple.com  Sun Feb  3 23:13:07 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Sun Feb  3 23:14:17 2008
Subject: [zfs-discuss] ZFS version ?
In-Reply-To: <42316769-79D5-4C58-AC16-7BBAE64ADFD0@sogeeky.net>
References: <0079DCE7-FA17-425D-B521-262798652165@sun.com>
	<42316769-79D5-4C58-AC16-7BBAE64ADFD0@sogeeky.net>
Message-ID: <35EDAC47-317D-4510-9CB9-349A4A05028C@apple.com>

So as mentioned below, the reason is on the zfs faq for the version  
difference and a full explanation of why the version discrepency  
exists.  Currently, you can't "force" creation of a version 8 pool at  
the time of the zpool create.  After the create you can issue a simple  
'zpool upgrade' in order to upgrade your pool to version 8 if you wish.

Noel

On Feb 2, 2008, at 11:30 AM, Mr. Zorg wrote:

> Yes. See the FAQ on the zfs macosforge page for the reason why. I'm  
> not sure how to create them in the new version, but issuing the  
> upgrade command takes just a second...
>
> On Feb 2, 2008, at 7:21 AM, Bill Winnett <William.Winnett@Sun.COM>  
> wrote:
>
>> Hi,
>>
>> ZFS currently defaults to version 6, but it is based off 8?
>> If so how can I force ZFS to create volumes as version 8.
>>
>> thanks,
>>
>> -bill w.
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From eableson at mac.com  Tue Feb  5 04:03:49 2008
From: eableson at mac.com (Erik Ableson)
Date: Tue Feb  5 04:03:29 2008
Subject: [zfs-discuss] ZFS/TM/OSXS
Message-ID: <06C47470-9126-43DC-A488-B6C0E27243E5@mac.com>

Slightly off topic and I haven't got any spare drives to test this  
idea.  I realise that Time Machine is flaky on a ZFS volume, but I've  
been looking at the behaviour of Time Machine to an AFP network share  
on an OS X Server.  Instead of directly writing to the file system,  
the backups are in a sparseimage (or a series of sparseimage files -  
I'm still waiting for my first backup to complete over the network to  
see what it looks like).

Since the object that would be stored on the ZFS filesystem would be a  
simple file, I was thinking that this should work as a practical  
solution for consolidating Time Machine storage with all the  
advantages of using ZFS.

Has anyone tried this out?

Cheers,

Erik
From david299792 at googlemail.com  Tue Feb  5 05:57:26 2008
From: david299792 at googlemail.com (David Ritchie)
Date: Tue Feb  5 05:57:19 2008
Subject: [zfs-discuss] ZFS/TM/OSXS
In-Reply-To: <06C47470-9126-43DC-A488-B6C0E27243E5@mac.com>
References: <06C47470-9126-43DC-A488-B6C0E27243E5@mac.com>
Message-ID: <B1212313-ED16-41A7-8635-B14180894618@googlemail.com>

TM seems to be rather picky about what kind of network volumes (vis.  
the underlying file system) it will back up to. I've followed several  
hints to try and get it to back up to things other than AFP shares of  
HFS+ file systems from OS X, but haven't got it working. Since it uses  
sparse images it doesn't seem that it should have much trouble  
regardless of the file system.

I'm planning on setting up TM backups to my Mac Pro over the network.  
At the moment, due to my failure to get TM backing up to anything  
else, I'm thinking of having it back up to the HFS+ boot drive, and  
then have cron rsync --inplace the sparse images over to my ZFS volume.

If anyone can suggest how I can make TM happy to back up directly to  
ZFS, I do have spare drives and I'm happy to let you know how it goes.

(It's not the ideal solution for TM on ZFS, as discussed before, but  
it would have many of the advantages).

The TM backups over the network happen to sparse image bundles, by the  
way, which are a new thing in 10.5. It's basically a directory  
containing a bunch of files, and it adds more files as more data are  
added to the sparse image. It's pretty handy.

-- David

On 5 Feb 2008, at 12:03, Erik Ableson wrote:

> Slightly off topic and I haven't got any spare drives to test this  
> idea.  I realise that Time Machine is flaky on a ZFS volume, but  
> I've been looking at the behaviour of Time Machine to an AFP network  
> share on an OS X Server.  Instead of directly writing to the file  
> system, the backups are in a sparseimage (or a series of sparseimage  
> files - I'm still waiting for my first backup to complete over the  
> network to see what it looks like).
>
> Since the object that would be stored on the ZFS filesystem would be  
> a simple file, I was thinking that this should work as a practical  
> solution for consolidating Time Machine storage with all the  
> advantages of using ZFS.
>
> Has anyone tried this out?
>
> Cheers,
>
> Erik
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From sbizna at tejat.net  Tue Feb  5 07:02:06 2008
From: sbizna at tejat.net (Solra Bizna)
Date: Tue Feb  5 07:01:42 2008
Subject: [zfs-discuss] ZFS/TM/OSXS
In-Reply-To: <B1212313-ED16-41A7-8635-B14180894618@googlemail.com>
References: <06C47470-9126-43DC-A488-B6C0E27243E5@mac.com>
	<B1212313-ED16-41A7-8635-B14180894618@googlemail.com>
Message-ID: <b2a3f2d80802050702i31b70c69w107d7fb3b5cecbfa@mail.gmail.com>

On Feb 5, 2008 6:57 AM, David Ritchie <david299792@googlemail.com> wrote:
> If anyone can suggest how I can make TM happy to back up directly to
> ZFS, I do have spare drives and I'm happy to let you know how it goes.
It may possibly work if you touch .com.apple.timemachine.supported on
the root of the zpool (or maybe even one of the filesystems), but my
guess is that it would fail in an odd way if it works at all.
-:sigma.SB
From robert at rehnmark.net  Tue Feb  5 07:35:26 2008
From: robert at rehnmark.net (Robert Rehnmark)
Date: Tue Feb  5 07:35:16 2008
Subject: [zfs-discuss] ZFS/TM/OSXS
In-Reply-To: <B1212313-ED16-41A7-8635-B14180894618@googlemail.com>
References: <06C47470-9126-43DC-A488-B6C0E27243E5@mac.com>
	<B1212313-ED16-41A7-8635-B14180894618@googlemail.com>
Message-ID: <8667C9C3-F67F-4830-9FB3-FD06D82BC6DF@rehnmark.net>

Would it not work if you create an Image file and put it on the pool?
Make the image HFS+ and mount/share it ... then put the sparseimage on  
that.

I'm interrested in doing the same thing but I'm waiting for some parts  
to get my pool up and running.

/Robert



5 feb 2008 kl. 14:57 skrev David Ritchie:

> TM seems to be rather picky about what kind of network volumes (vis.  
> the underlying file system) it will back up to. I've followed  
> several hints to try and get it to back up to things other than AFP  
> shares of HFS+ file systems from OS X, but haven't got it working.  
> Since it uses sparse images it doesn't seem that it should have much  
> trouble regardless of the file system.
>
> I'm planning on setting up TM backups to my Mac Pro over the  
> network. At the moment, due to my failure to get TM backing up to  
> anything else, I'm thinking of having it back up to the HFS+ boot  
> drive, and then have cron rsync --inplace the sparse images over to  
> my ZFS volume.
>
> If anyone can suggest how I can make TM happy to back up directly to  
> ZFS, I do have spare drives and I'm happy to let you know how it goes.
>
> (It's not the ideal solution for TM on ZFS, as discussed before, but  
> it would have many of the advantages).
>
> The TM backups over the network happen to sparse image bundles, by  
> the way, which are a new thing in 10.5. It's basically a directory  
> containing a bunch of files, and it adds more files as more data are  
> added to the sparse image. It's pretty handy.
>
> -- David
>
> On 5 Feb 2008, at 12:03, Erik Ableson wrote:
>
>> Slightly off topic and I haven't got any spare drives to test this  
>> idea.  I realise that Time Machine is flaky on a ZFS volume, but  
>> I've been looking at the behaviour of Time Machine to an AFP  
>> network share on an OS X Server.  Instead of directly writing to  
>> the file system, the backups are in a sparseimage (or a series of  
>> sparseimage files - I'm still waiting for my first backup to  
>> complete over the network to see what it looks like).
>>
>> Since the object that would be stored on the ZFS filesystem would  
>> be a simple file, I was thinking that this should work as a  
>> practical solution for consolidating Time Machine storage with all  
>> the advantages of using ZFS.
>>
>> Has anyone tried this out?
>>
>> Cheers,
>>
>> Erik
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From thompson.chris.lists at gmail.com  Tue Feb  5 08:06:24 2008
From: thompson.chris.lists at gmail.com (Chris Thompson)
Date: Tue Feb  5 08:06:06 2008
Subject: [zfs-discuss] ZFS/TM/OSXS
In-Reply-To: <8667C9C3-F67F-4830-9FB3-FD06D82BC6DF@rehnmark.net>
References: <06C47470-9126-43DC-A488-B6C0E27243E5@mac.com>
	<B1212313-ED16-41A7-8635-B14180894618@googlemail.com>
	<8667C9C3-F67F-4830-9FB3-FD06D82BC6DF@rehnmark.net>
Message-ID: <BC6AD3B2-58F3-455B-992A-5EED8C9DE594@gmail.com>

Time Machine does work on *non-sparse* HFS+ disk images created on top  
of a ZFS volume, provided that you enable  
TMShowUnsupportedNetworkVolumes.  I tried using Time Machine with  
*sparse* disk images on top of ZFS with the v101 seed and it was flaky  
(it seemed to work for a period and then became unreliable); that may  
be fixed in the current seed.

Do be aware that when TMShowUnsupportedNetworkVolumes is enabled, Time  
Machine cannot reclaim deleted space, so you have to manually run  
hdiutil compact every so often or eventually you'll start losing data  
when the drive fills.  See here for more:
http://www.macosxhints.com/article.php?story=20071028173642747

What I eventually settled on was repartitioning my disk to part ZFS  
and part HFS+, the latter used exclusively for Time Machine. This  
doesn't require enabling TMShowUnsupportedNetworkVolumes and seems to  
work without requiring hdiutil compact.

-- Chris
From bplist at thinkpink.com  Tue Feb  5 10:26:23 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Tue Feb  5 10:25:59 2008
Subject: [zfs-discuss] ZFS/TM/OSXS
In-Reply-To: <06C47470-9126-43DC-A488-B6C0E27243E5@mac.com>
References: <06C47470-9126-43DC-A488-B6C0E27243E5@mac.com>
Message-ID: <050958D1-50B8-4ACE-8D67-918D8F454918@thinkpink.com>

While this isn't exactly what you want, I can say I have TM "working"  
across the network via AFP to a ZFS volume.  I say working loosely  
because Time Machine is fairly unpredictable in its behavior --  
sometimes it spends hours and hours in the "Finishing backup" state,  
while other times it completes just fine (funny, considering the  
relatively small amounts of data.)  Needless to say, I'm relying on a  
different backup process until this one is predictable and reliable. :)

bri


On Feb 5, 2008, at 4:03 AM, Erik Ableson wrote:

> Slightly off topic and I haven't got any spare drives to test this  
> idea.  I realise that Time Machine is flaky on a ZFS volume, but  
> I've been looking at the behaviour of Time Machine to an AFP network  
> share on an OS X Server.  Instead of directly writing to the file  
> system, the backups are in a sparseimage (or a series of sparseimage  
> files - I'm still waiting for my first backup to complete over the  
> network to see what it looks like).
>
> Since the object that would be stored on the ZFS filesystem would be  
> a simple file, I was thinking that this should work as a practical  
> solution for consolidating Time Machine storage with all the  
> advantages of using ZFS.
>
> Has anyone tried this out?
>
> Cheers,
>
> Erik
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

From eableson at mac.com  Tue Feb  5 23:24:08 2008
From: eableson at mac.com (Erik Ableson)
Date: Wed Feb  6 00:56:29 2008
Subject: [zfs-discuss] ZFS/TM/OSXS
In-Reply-To: <050958D1-50B8-4ACE-8D67-918D8F454918@thinkpink.com>
References: <06C47470-9126-43DC-A488-B6C0E27243E5@mac.com>
	<050958D1-50B8-4ACE-8D67-918D8F454918@thinkpink.com>
Message-ID: <F7734BAE-D38B-4CF8-B8A6-6F91EB61D3F2@mac.com>

That's actually exactly what I was looking for, but it seems that it's  
still not terribly stable.  I'm guessing that it's the behavior of the  
sparse image bundle file that could be impacting it, although I have  
some of the same erratic issues with "Finishing backup" performance on  
any network TM setup so both of them could be contributing to the issue.

I guess I should wait and see just how Time Machine behaves under  
10.5.2 when it hits.

On the ZFS front, I was just thinking that the perfect place for Apple  
to deploy this would be embedded on the NAS functionality of the  
Airport Extreme.  You have a limited number of cases from a UI  
standpoint as compared to the desktop.   Although what I'd really like  
to see is a Drobo-like implementation that the average user just plugs  
in a drive and it auto configures itself for the maximum available  
space with whatever RAID in can build and then you declare volumes to  
share.

But one thing at a time. :-)

On 5 f?vr. 08, at 19:26, Brian Pinkerton wrote:

> While this isn't exactly what you want, I can say I have TM  
> "working" across the network via AFP to a ZFS volume.  I say working  
> loosely because Time Machine is fairly unpredictable in its behavior  
> -- sometimes it spends hours and hours in the "Finishing backup"  
> state, while other times it completes just fine (funny, considering  
> the relatively small amounts of data.)  Needless to say, I'm relying  
> on a different backup process until this one is predictable and  
> reliable. :)
>
> bri
>
>
> On Feb 5, 2008, at 4:03 AM, Erik Ableson wrote:
>
>> Slightly off topic and I haven't got any spare drives to test this  
>> idea.  I realise that Time Machine is flaky on a ZFS volume, but  
>> I've been looking at the behaviour of Time Machine to an AFP  
>> network share on an OS X Server.  Instead of directly writing to  
>> the file system, the backups are in a sparseimage (or a series of  
>> sparseimage files - I'm still waiting for my first backup to  
>> complete over the network to see what it looks like).
>>
>> Since the object that would be stored on the ZFS filesystem would  
>> be a simple file, I was thinking that this should work as a  
>> practical solution for consolidating Time Machine storage with all  
>> the advantages of using ZFS.

From david299792 at googlemail.com  Thu Feb  7 14:33:07 2008
From: david299792 at googlemail.com (David Ritchie)
Date: Thu Feb  7 14:33:03 2008
Subject: [zfs-discuss] Interesting VMWare issue
Message-ID: <3FC5DC06-6E0A-4306-9247-1896183F52DF@googlemail.com>

Hi,

I've just discovered an interesting issue in VMWare Fusion. I've got a  
VM set up with Windows XP running form a ZFS file system. When I do  
this:-
	1. start the VM in VMWare
	2. suspend
	3. quite vmware
	4. zfs snapshot tank/the-file-system@test
	5. reload the VM, fiddle about a bit, suspend, reload to check it,  
suspend
	6. zfs rollbank tank/the-file-system@test
	7. reload the (now rolled back VM) in VMWare

The VM appears to resume, but just before running again I get a  
message: *** Virtual machine kernel stack fault (hardware reset)

(I can send a screen grab of the message dialog)

The same happens if I suspend, clone, and then try and resume the clone.

I'll try in Parallels and see what happens there...

<<< time passes >>>

No issues in Parallels. It's rather useful to be able to very quickly  
clone a whole, running (suspended) VM in order to experiment with  
something in a matter of seconds.


-- David

(I accidentally sent this from the wrong email address first time)
From zfs-discuss at openhealth.org  Sun Feb 10 16:33:40 2008
From: zfs-discuss at openhealth.org (Jonathan Borden)
Date: Sun Feb 10 16:33:32 2008
Subject: [zfs-discuss] Problem compacting EyeTV program on ZFS
Message-ID: <7a0bc8420802101633h4f1b9e1fr73c19be4541e60ef@mail.gmail.com>

I've set up a zpool filesystem to serve as an EyeTV (3) archive ... in
fact, I have EyeTV running remotely and saving programs over the
network to the zpool (raidz2 fwiw).

The problem occurs when I try to edit and compact a program -- either
locally or remotely. I can play a program and insert the in/out points
but when I try to "Compact" it goes from 0 -> 100% but the program
isn't compacted.

When I move the program onto a non-ZFS drive, it compacts fine...

Strange....

Jonathan
From bwaters at nrao.edu  Sun Feb 10 19:58:56 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Sun Feb 10 19:58:58 2008
Subject: [zfs-discuss] staggered spin-up on external enclosure
Message-ID: <172EBF0D-18F1-40E2-A380-627614332736@nrao.edu>

Howdy all.

A couple of days ago I had to move my big computer to the other side  
of the office, so I turned it off for the first time in a while.

It booted up just fine, and my ZFS raid (hosted on a four-disk  
external SATA enclosure) showed up OK. But during a big file transfer,  
I noticed that the activity light on the fourth disk in the array  
wasn't showing any activity. Uh, oh...

Sure enough, "zpool status" showed that disk as faulted or failed or  
whatever, and suggested that I replace the volume.

But I suspected a timing issue with the drives: the enclosure doesn't  
spin all the drives up at the same time, but does them one at a time,  
in order to manage the current draw on the power supply.

After my (successful) file transfer, I did a snapshot of the file  
system, then rebooted. Since everything was already powered up, I kept  
my fingers crossed.

Sure enough, ZFS saw all four disks upon reboot, and "zpool status"  
showed that it was busy resilvering (in order to get the RAID  
synchronized with the large updated bits that had occurred during the  
impaired state).

Two hours later, and everything is just fine.

I like ZFS!

  - boyd


PS:

This thing
  http://www.addonics.com/products/raid_system/mst4.asp

is better than buttered toast! I got the one with the hardware port  
multiplier, and got the host controller
http://www.addonics.com/products/host_controller/adsa3gpx1-2em.asp
and their funky eSATA cable
(AASA2SA100C)



From info at martin-hauser.net  Mon Feb 11 03:50:44 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Mon Feb 11 04:15:26 2008
Subject: [zfs-discuss] ZFS Binaries 102A fail with a dyld: lay symbol
	binding failed on Os X 10.5 
Message-ID: <7BDE7CDD-7D0C-4C49-83E1-B0284828CD5F@martin-hauser.net>

Hello everyone,

I just wanted to notify you that there is a bug with the current zfs  
binaries (102A) on Mac Os X 10.5. I've filed a bug here: http://trac.macosforge.org/projects/zfs/ticket/6 
  .

It seems the current Xcode has a bug that requires "Use Seperate  
Strip" to be used on the libzfs, otherwise there will be a dyld error  
using the zfs and zpool binaries (more details available on the bug  
report itself).

Just to let you know.

Kind Regards

Martin Hauser
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080211/9f12b0c6/PGP.bin
From info at martin-hauser.net  Mon Feb 11 03:57:37 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Mon Feb 11 05:55:30 2008
Subject: [zfs-discuss] Workaround for send | recv bug
Message-ID: <C78FAA82-9FD5-48E1-8B63-E583F06F1E87@martin-hauser.net>

Hello Everyone,

I've been reading the macosforge page about the known errors to be  
worked upon and wanted to share a workaround which can be used to  
transfer snapshots without wasting extra disk space and even more  
easily transfer via ssh.

First we need two fifo's:

mkfifo zfs_send
ssh user@remotehost mkfifo zfs_recv

Then we need connect those two in reverse order (recv then send)

ssh user@remotehost "zfs receive pool/target < zfs_recv"
cat zfs_send | ssh user@remotehost "cat > zfs_recv"
zfs send snapshot > zfs_send

The order is important but will allow you to send the files across to  
your remote host. The reverse order is important and the fifo's will  
block as long as there is no data.

Kind regards

Martin Hauser



-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080211/03a84dec/PGP.bin
From ndellofano at apple.com  Mon Feb 11 10:56:21 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon Feb 11 10:57:13 2008
Subject: [zfs-discuss] ZFS Binaries 102A fail with a dyld: lay
	symbol	binding failed on Os X 10.5
In-Reply-To: <7BDE7CDD-7D0C-4C49-83E1-B0284828CD5F@martin-hauser.net>
References: <7BDE7CDD-7D0C-4C49-83E1-B0284828CD5F@martin-hauser.net>
Message-ID: <7BABD495-0474-43B4-8C0A-759CAB31CD25@apple.com>

Hey Martin,

So you're seeing this bug trying to build the latest zfs source code  
(102A) in Xcode in Leopard?  Can you tell me what version of Leopard  
and Xcode you're using?

thanks!
Noel

On Feb 11, 2008, at 3:50 AM, Martin Hauser wrote:

> Hello everyone,
>
> I just wanted to notify you that there is a bug with the current zfs  
> binaries (102A) on Mac Os X 10.5. I've filed a bug here: http://trac.macosforge.org/projects/zfs/ticket/6 
>  .
>
> It seems the current Xcode has a bug that requires "Use Seperate  
> Strip" to be used on the libzfs, otherwise there will be a dyld  
> error using the zfs and zpool binaries (more details available on  
> the bug report itself).
>
> Just to let you know.
>
> Kind Regards
>
> Martin Hauser
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From info at martin-hauser.net  Mon Feb 11 13:18:29 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Mon Feb 11 14:15:18 2008
Subject: [zfs-discuss] ZFS Binaries 102A fail with a dyld: lay
	symbol	binding failed on Os X 10.5
In-Reply-To: <7BABD495-0474-43B4-8C0A-759CAB31CD25@apple.com>
References: <7BDE7CDD-7D0C-4C49-83E1-B0284828CD5F@martin-hauser.net>
	<7BABD495-0474-43B4-8C0A-759CAB31CD25@apple.com>
Message-ID: <DBF554DE-BD0D-4B8A-B0EC-1EF9F01D81EF@martin-hauser.net>

Hey Noel,

On Feb 11, 2008, at 19:56 PM, No?l Dellofano wrote:

> So you're seeing this bug trying to build the latest zfs source code  
> (102A) in Xcode in Leopard?  Can you tell me what version of Leopard  
> and Xcode you're using?

Xcode tells me :

Xcode 3.0

Component versions
Xcode IDE: 921.0
Xcode Core: 921.0
ToolSupport: 893.0

Leopard is 10.5.1 with all available updates installed. I have had  
this both on an Imac 20" and a first generation macbook, both being  
intel based machines.

The error itself happens if I download and install the binaries shared  
on macosforge. I'm not quite sure if that happens if I'd be building  
from source without the 'Use Seperate Strip' option enabled on the  
libzfs subproject, cause I haven't tried that.

As I got the error, I found some information on the net about some  
other  project from which i extracted that apparently the 3.0 Version  
of Xcode hat some problems with the internal strip and that would  
result in this error. Using this information, I downloaded the source  
code for zfs, applied the "use seperate strip" option to the libzfs  
subbuild, built, installed and was happy.

I merely posted it here to notion that the default downloadable  
binaries won't run on intel based macintoshes on 10.5.1.

If you need more Information, please let me know.

kind Regards

Martin Hauser
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080211/0b104ead/PGP.bin
From zorg at sogeeky.net  Mon Feb 11 15:59:07 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Mon Feb 11 15:59:07 2008
Subject: [zfs-discuss] ZFS Binaries 102A fail with a dyld: lay symbol
	binding failed on Os X 10.5
In-Reply-To: <DBF554DE-BD0D-4B8A-B0EC-1EF9F01D81EF@martin-hauser.net>
References: <7BDE7CDD-7D0C-4C49-83E1-B0284828CD5F@martin-hauser.net>
	<7BABD495-0474-43B4-8C0A-759CAB31CD25@apple.com>
	<DBF554DE-BD0D-4B8A-B0EC-1EF9F01D81EF@martin-hauser.net>
Message-ID: <D1110F48-0B36-4455-8714-2B1C8FB79931@sogeeky.net>

Not true. Runs just fine for me. There must be something else going on.

On Feb 11, 2008, at 1:18 PM, Martin Hauser <info@martin-hauser.net>  
wrote:

> Hey Noel,
>
> On Feb 11, 2008, at 19:56 PM, No?l Dellofano wrote:
>
>> So you're seeing this bug trying to build the latest zfs source  
>> code (102A) in Xcode in Leopard?  Can you tell me what version of  
>> Leopard and Xcode you're using?
>
> Xcode tells me :
>
> Xcode 3.0
>
> Component versions
> Xcode IDE: 921.0
> Xcode Core: 921.0
> ToolSupport: 893.0
>
> Leopard is 10.5.1 with all available updates installed. I have had  
> this both on an Imac 20" and a first generation macbook, both being  
> intel based machines.
>
> The error itself happens if I download and install the binaries  
> shared on macosforge. I'm not quite sure if that happens if I'd be  
> building from source without the 'Use Seperate Strip' option enabled  
> on the libzfs subproject, cause I haven't tried that.
>
> As I got the error, I found some information on the net about some  
> other  project from which i extracted that apparently the 3.0  
> Version of Xcode hat some problems with the internal strip and that  
> would result in this error. Using this information, I downloaded the  
> source code for zfs, applied the "use seperate strip" option to the  
> libzfs subbuild, built, installed and was happy.
>
> I merely posted it here to notion that the default downloadable  
> binaries won't run on intel based macintoshes on 10.5.1.
>
> If you need more Information, please let me know.
>
> kind Regards
>
> Martin Hauser
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
From William.Winnett at Sun.COM  Mon Feb 11 18:51:56 2008
From: William.Winnett at Sun.COM (Bill Winnett)
Date: Mon Feb 11 18:51:44 2008
Subject: [zfs-discuss] Abort trap
Message-ID: <2252F5C2-7C0F-4AC9-B940-C13BCB0C59B3@sun.com>

How do I root cause this tha "Abort trap"?
/dev/disk2 is a Maxtor 1TB one touch III disk

---
Macintosh-2:~ billwinnett$ diskutil partitiondisk /dev/disk2 GPTFormat  
ZFS %noformat% 100%
Started partitioning on disk disk2
Creating partition map
[ + 0%..10%..20%..30%..40%..50%..60%..70%..80%..90%..100% ]
Finished partitioning on disk disk2
/dev/disk2
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *931.5 Gi    
disk2
    1:                        EFI                         200.0 Mi    
disk2s1
    2:                        ZFS                         931.2 Gi    
disk2s2
Macintosh-2:~ billwinnett$ diskutil list
/dev/disk0
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *298.1 Gi    
disk0
    1:                        EFI                         200.0 Mi    
disk0s1
    2:                  Apple_HFS Machintosh              297.8 Gi    
disk0s2
/dev/disk1
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:     Apple_partition_scheme                        *931.5 Gi    
disk1
    1:        Apple_partition_map                         31.5 Ki     
disk1s1
    2:             Apple_Driver43                         28.0 Ki     
disk1s2
    3:             Apple_Driver43                         28.0 Ki     
disk1s3
    4:           Apple_Driver_ATA                         28.0 Ki     
disk1s4
    5:           Apple_Driver_ATA                         28.0 Ki     
disk1s5
    6:             Apple_FWDriver                         256.0 Ki    
disk1s6
    7:         Apple_Driver_IOKit                         256.0 Ki    
disk1s7
    8:              Apple_Patches                         256.0 Ki    
disk1s8
    9:                  Apple_HFS untitled                931.4 Gi    
disk1s10
/dev/disk2
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *931.5 Gi    
disk2
    1:                        EFI                         200.0 Mi    
disk2s1
    2:                        ZFS                         931.2 Gi    
disk2s2
Macintosh-2:~ billwinnett$ zpool create bfd /dev/disk2s2
Abort trap
Macintosh-2:~ billwinnett$
----
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080211/d56da1fa/attachment.html
From ndellofano at apple.com  Mon Feb 11 19:54:27 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon Feb 11 19:55:46 2008
Subject: [zfs-discuss] Abort trap
In-Reply-To: <2252F5C2-7C0F-4AC9-B940-C13BCB0C59B3@sun.com>
References: <2252F5C2-7C0F-4AC9-B940-C13BCB0C59B3@sun.com>
Message-ID: <9136F32F-11A3-404B-A4AC-DCE91CC9E5AA@apple.com>

Bill,

What version of ZFS are you running?  The abort trap was a problem due  
to a package bug in the ZFS packages that were downloaded from the ADC  
site, which since have been removed, and not from the macosforge  
site.  Are you running ZFS downloaded from zfs.macosforge.org, zfs-102A?

Also, what version of Leopard are you running?

Noel

On Feb 11, 2008, at 6:51 PM, Bill Winnett wrote:

> How do I root cause this tha "Abort trap"?
> /dev/disk2 is a Maxtor 1TB one touch III disk
>
> ---
> Macintosh-2:~ billwinnett$ diskutil partitiondisk /dev/disk2  
> GPTFormat ZFS %noformat% 100%
> Started partitioning on disk disk2
> Creating partition map
> [ + 0%..10%..20%..30%..40%..50%..60%..70%..80%..90%..100% ]
> Finished partitioning on disk disk2
> /dev/disk2
>    #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>    0:      GUID_partition_scheme                        *931.5 Gi    
> disk2
>    1:                        EFI                         200.0 Mi    
> disk2s1
>    2:                        ZFS                         931.2 Gi    
> disk2s2
> Macintosh-2:~ billwinnett$ diskutil list
> /dev/disk0
>    #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>    0:      GUID_partition_scheme                        *298.1 Gi    
> disk0
>    1:                        EFI                         200.0 Mi    
> disk0s1
>    2:                  Apple_HFS Machintosh              297.8 Gi    
> disk0s2
> /dev/disk1
>    #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>    0:     Apple_partition_scheme                        *931.5 Gi    
> disk1
>    1:        Apple_partition_map                         31.5 Ki     
> disk1s1
>    2:             Apple_Driver43                         28.0 Ki     
> disk1s2
>    3:             Apple_Driver43                         28.0 Ki     
> disk1s3
>    4:           Apple_Driver_ATA                         28.0 Ki     
> disk1s4
>    5:           Apple_Driver_ATA                         28.0 Ki     
> disk1s5
>    6:             Apple_FWDriver                         256.0 Ki    
> disk1s6
>    7:         Apple_Driver_IOKit                         256.0 Ki    
> disk1s7
>    8:              Apple_Patches                         256.0 Ki    
> disk1s8
>    9:                  Apple_HFS untitled                931.4 Gi    
> disk1s10
> /dev/disk2
>    #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>    0:      GUID_partition_scheme                        *931.5 Gi    
> disk2
>    1:                        EFI                         200.0 Mi    
> disk2s1
>    2:                        ZFS                         931.2 Gi    
> disk2s2
> Macintosh-2:~ billwinnett$ zpool create bfd /dev/disk2s2
> Abort trap
> Macintosh-2:~ billwinnett$
> ----
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080211/92973108/attachment-0001.html
From info at martin-hauser.net  Tue Feb 12 01:51:50 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Tue Feb 12 01:51:53 2008
Subject: [zfs-discuss] ZFS Binaries 102A fail with a dyld: lay symbol
	binding failed on Os X 10.5
In-Reply-To: <D1110F48-0B36-4455-8714-2B1C8FB79931@sogeeky.net>
References: <7BDE7CDD-7D0C-4C49-83E1-B0284828CD5F@martin-hauser.net>
	<7BABD495-0474-43B4-8C0A-759CAB31CD25@apple.com>
	<DBF554DE-BD0D-4B8A-B0EC-1EF9F01D81EF@martin-hauser.net>
	<D1110F48-0B36-4455-8714-2B1C8FB79931@sogeeky.net>
Message-ID: <9C1B6837-678D-4275-B751-9541E28203C4@martin-hauser.net>

Hello

On Feb 12, 2008, at 00:59 AM, Mr. Zorg wrote:

> Not true. Runs just fine for me. There must be something else going  
> on.
>

Might be, fact is that I have had it on two machines with the binary  
release which were both running 10.5.1. Sure, might be related to  
something completely different, but then, I wonder, what might be the  
problem and what makes it go away after building from source.

Kind Regards

Martin

-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080212/93ae6e6a/PGP.bin
From franzschmalzl at spamfreemail.de  Fri Feb 15 06:27:57 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Fri, 15 Feb 2008 15:27:57 +0100
Subject: [zfs-discuss] Data Integrity
In-Reply-To: <9C1B6837-678D-4275-B751-9541E28203C4@martin-hauser.net>
References: <7BDE7CDD-7D0C-4C49-83E1-B0284828CD5F@martin-hauser.net>
	<7BABD495-0474-43B4-8C0A-759CAB31CD25@apple.com>
	<DBF554DE-BD0D-4B8A-B0EC-1EF9F01D81EF@martin-hauser.net>
	<D1110F48-0B36-4455-8714-2B1C8FB79931@sogeeky.net>
	<9C1B6837-678D-4275-B751-9541E28203C4@martin-hauser.net>
Message-ID: <5934BC43-365B-4026-A1F3-87F0C22D0B86@spamfreemail.de>

Hey guys!

I'm planning to build up one or two external raidz raids to store my  
data on them

just a quick question

is the zfs port stable enough not to kill my data ?
i mean are there any *known* bugs that will corrupt the volume headers  
and/or my data stored on this drives?
is the raidz spanning algorithm working ?
what's about re-silvering ?


thanks
and my best regards

franz schmalzl

From zorg at sogeeky.net  Fri Feb 15 08:53:53 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Fri, 15 Feb 2008 08:53:53 -0800
Subject: [zfs-discuss] Data Integrity
In-Reply-To: <5934BC43-365B-4026-A1F3-87F0C22D0B86@spamfreemail.de>
References: <7BDE7CDD-7D0C-4C49-83E1-B0284828CD5F@martin-hauser.net>
	<7BABD495-0474-43B4-8C0A-759CAB31CD25@apple.com>
	<DBF554DE-BD0D-4B8A-B0EC-1EF9F01D81EF@martin-hauser.net>
	<D1110F48-0B36-4455-8714-2B1C8FB79931@sogeeky.net>
	<9C1B6837-678D-4275-B751-9541E28203C4@martin-hauser.net>
	<5934BC43-365B-4026-A1F3-87F0C22D0B86@spamfreemail.de>
Message-ID: <751CEF63-4E87-45AF-9C5E-5972FC6F3305@sogeeky.net>

 From my experience: yes, no, yes, yes. I've got 6x250gb FireWire  
drives in a raidz1, and despite several stupid mistakes, I've not  
managed to kill it yet.

On Feb 15, 2008, at 6:27 AM, Franz Schmalzl <franzschmalzl at spamfreemail.de 
 > wrote:

> Hey guys!
>
> I'm planning to build up one or two external raidz raids to store my
> data on them
>
> just a quick question
>
> is the zfs port stable enough not to kill my data ?
> i mean are there any *known* bugs that will corrupt the volume headers
> and/or my data stored on this drives?
> is the raidz spanning algorithm working ?
> what's about re-silvering ?
>
>
> thanks
> and my best regards
>
> franz schmalzl
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From bwaters at nrao.edu  Fri Feb 15 11:01:25 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Fri, 15 Feb 2008 12:01:25 -0700
Subject: [zfs-discuss] Data Integrity
In-Reply-To: <751CEF63-4E87-45AF-9C5E-5972FC6F3305@sogeeky.net>
References: <7BDE7CDD-7D0C-4C49-83E1-B0284828CD5F@martin-hauser.net>
	<7BABD495-0474-43B4-8C0A-759CAB31CD25@apple.com>
	<DBF554DE-BD0D-4B8A-B0EC-1EF9F01D81EF@martin-hauser.net>
	<D1110F48-0B36-4455-8714-2B1C8FB79931@sogeeky.net>
	<9C1B6837-678D-4275-B751-9541E28203C4@martin-hauser.net>
	<5934BC43-365B-4026-A1F3-87F0C22D0B86@spamfreemail.de>
	<751CEF63-4E87-45AF-9C5E-5972FC6F3305@sogeeky.net>
Message-ID: <C4CC9C01-6D9D-42FC-A594-6A8750B219E1@nrao.edu>

On Feb 15, 2008, at 9:53 AM, Mr. Zorg wrote:

> From my experience: yes, no, yes, yes. I've got 6x250gb FireWire
> drives in a raidz1, and despite several stupid mistakes, I've not
> managed to kill it yet.


Same here!

I've managed to hammer my 4-element RAIDz1 with crazy use-cases (e.g.  
a directory with millions of files in it) and I trip up on kernel  
panics on occasion. The machine will lock up.

last night I found out what happens when a 2TB raidz runs out of disk  
space. it reports "out of disk space". whee.

I add snapshots, clone file systems, destroy filesystems and snapshots.

But I have not had any data-integrity issues.

Good luck!


  - boyd

scientific programmer
National Radio Astronomy Observatory


From franzschmalzl at spamfreemail.de  Fri Feb 15 11:38:50 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Fri, 15 Feb 2008 20:38:50 +0100
Subject: [zfs-discuss]  Data Integrity
References: <18C140C2-571B-4809-808C-EC8082A3C4CA@spamfreemail.de>
Message-ID: <F3886F22-ECF1-451B-833A-01B55194FAB7@spamfreemail.de>

>
>
> On 15.02.2008, at 17:53, Mr. Zorg wrote:
>
>> yes, no, yes, yes
>>
>
quick and pregnant ;)

thank you
>
>
>> On Feb 15, 2008, at 6:27 AM, Franz Schmalzl <franzschmalzl at spamfreemail.de 
>> > wrote:
>>
>>> Hey guys!
>>>
>>> I'm planning to build up one or two external raidz raids to store my
>>> data on them
>>>
>>> just a quick question
>>>
>>> is the zfs port stable enough not to kill my data ?
>>> i mean are there any *known* bugs that will corrupt the volume  
>>> headers
>>> and/or my data stored on this drives?
>>> is the raidz spanning algorithm working ?
>>> what's about re-silvering ?
>>>
>>>
>>> thanks
>>> and my best regards
>>>
>>> franz schmalzl
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From gabor at berczi.be  Fri Feb 15 13:12:16 2008
From: gabor at berczi.be (=?ISO-8859-1?Q?B=E9rczi_G=E1bor?=)
Date: Fri, 15 Feb 2008 22:12:16 +0100
Subject: [zfs-discuss] ZFS mmap bug with PHP?
Message-ID: <9CFE6352-626A-4E1A-8B5C-ABE418CBE848@berczi.be>

See the attached PHP code. Its functionality:

- unlinks test.cfg
- creates test.cfg with the content "test"
- lists the file with file_get_contents()
- adds an X509 certificate with OpenSSL
- lists the file again
- lists the file again
- lists the file again

The thing is: on HFS+ it works as it should, but on ZFS all subsequent  
file listings contain only "test" - the initial content. DTrace  
notices the opening of the file every time - so this looks like to be  
a very bad low-level problem.

Output:

File contents:testDone.Attaching new CERT into test.cfgFile contents,  
second try:testDone.File contents, third try:testDone.File contents,  
fourth try:testDone.The file should now contain an X509 cert, but PHP  
can't see it :(

What the..?

-- 
Gabucino
-------------- next part --------------
A non-text attachment was scrubbed...
Name: zfsphp.tar
Type: application/x-tar
Size: 10240 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080215/937ecd36/attachment-0001.tar 

From david299792 at googlemail.com  Fri Feb 15 13:20:23 2008
From: david299792 at googlemail.com (David Ritchie)
Date: Fri, 15 Feb 2008 21:20:23 +0000
Subject: [zfs-discuss] 10.5.2 update
Message-ID: <5B4D46CC-0F46-4D18-8405-C57A6F966815@googlemail.com>

Does anyone know if there are any issues with the latest OS X update  
(10.5.2) and ZFS? I'm planning on updating my main machine this  
weekend and just wondered if anyone has come across any problems.

Thanks,

-- David 

From gabor at berczi.be  Fri Feb 15 13:26:26 2008
From: gabor at berczi.be (=?ISO-8859-1?Q?B=E9rczi_G=E1bor?=)
Date: Fri, 15 Feb 2008 22:26:26 +0100
Subject: [zfs-discuss] 10.5.2 update
In-Reply-To: <5B4D46CC-0F46-4D18-8405-C57A6F966815@googlemail.com>
References: <5B4D46CC-0F46-4D18-8405-C57A6F966815@googlemail.com>
Message-ID: <3A62129A-A356-48E9-97FE-C9264FA191D6@berczi.be>

No.

On Feb 15, 2008, at 10:20 PM, David Ritchie wrote:

> Does anyone know if there are any issues with the latest OS X update
> (10.5.2) and ZFS? I'm planning on updating my main machine this
> weekend and just wondered if anyone has come across any problems.
>
> Thanks,
>
> -- David 
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-- 
Gabucino


From q0rban at gmail.com  Fri Feb 15 13:32:28 2008
From: q0rban at gmail.com (james sansbury)
Date: Fri, 15 Feb 2008 16:32:28 -0500
Subject: [zfs-discuss] 10.5.2 update
In-Reply-To: <3A62129A-A356-48E9-97FE-C9264FA191D6@berczi.be>
References: <5B4D46CC-0F46-4D18-8405-C57A6F966815@googlemail.com>
	<3A62129A-A356-48E9-97FE-C9264FA191D6@berczi.be>
Message-ID: <eb3e47ad0802151332w5ea25411p448183b83bd2ff91@mail.gmail.com>

 On Feb 15, 2008, at 10:20 PM, David Ritchie wrote:

Does anyone know if there are any issues with the latest OS X update
>

On Fri, Feb 15, 2008 at 4:26 PM, B?rczi G?bor <gabor at berczi.be> wrote:

No.


'No' you don't know, or 'no' you haven't had any issues?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080215/0a13d39f/attachment.html 

From info at martin-hauser.net  Fri Feb 15 13:50:10 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Fri, 15 Feb 2008 22:50:10 +0100
Subject: [zfs-discuss] 10.5.2 update
In-Reply-To: <eb3e47ad0802151332w5ea25411p448183b83bd2ff91@mail.gmail.com>
References: <5B4D46CC-0F46-4D18-8405-C57A6F966815@googlemail.com>
	<3A62129A-A356-48E9-97FE-C9264FA191D6@berczi.be>
	<eb3e47ad0802151332w5ea25411p448183b83bd2ff91@mail.gmail.com>
Message-ID: <43782E34-5D28-4238-8C8D-7692824DCC13@martin-hauser.net>

I haven't had any issue's and i've seamlessly have updated both my  
Intel Mac Machines to 10.5.2. I had built the latest zfs from source  
though.


On Feb 15, 2008, at 22:32 PM, james sansbury wrote:

> On Feb 15, 2008, at 10:20 PM, David Ritchie wrote:
>
> Does anyone know if there are any issues with the latest OS X update
>
> On Fri, Feb 15, 2008 at 4:26 PM, B?rczi G?bor <gabor at berczi.be> wrote:
>
> No.
>
> 'No' you don't know, or 'no' you haven't had any issues?
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080215/349cee8b/attachment.bin 

From david299792 at googlemail.com  Fri Feb 15 13:56:24 2008
From: david299792 at googlemail.com (David Ritchie)
Date: Fri, 15 Feb 2008 21:56:24 +0000
Subject: [zfs-discuss] 10.5.2 update
In-Reply-To: <43782E34-5D28-4238-8C8D-7692824DCC13@martin-hauser.net>
References: <5B4D46CC-0F46-4D18-8405-C57A6F966815@googlemail.com>
	<3A62129A-A356-48E9-97FE-C9264FA191D6@berczi.be>
	<eb3e47ad0802151332w5ea25411p448183b83bd2ff91@mail.gmail.com>
	<43782E34-5D28-4238-8C8D-7692824DCC13@martin-hauser.net>
Message-ID: <B933ED44-27AA-49B8-82D8-2EB302109C40@googlemail.com>

Cool, thanks. I'm just using the downloaded binaries. I've updated my  
laptop, which has read/write ZFS, but I haven't got a ZFS pool on that.

Is there much difference between the released binaries and the current  
source?

The slightly annoying thing is that I think I'm going to have to  
upgrade my Solaris box too, because when I tried sending snapshots  
from OS X ZFS to solaris ZFS although they seem to send ok, and are  
listed by 'zfs list' they won't mount. The solaris install on that is  
rather out of date though. (probably going to use that box for off  
site backup)


-- David

On 15 Feb 2008, at 21:50, Martin Hauser wrote:

> I haven't had any issue's and i've seamlessly have updated both my  
> Intel Mac Machines to 10.5.2. I had built the latest zfs from source  
> though.
>
>
> On Feb 15, 2008, at 22:32 PM, james sansbury wrote:
>
>> On Feb 15, 2008, at 10:20 PM, David Ritchie wrote:
>>
>> Does anyone know if there are any issues with the latest OS X update
>>
>> On Fri, Feb 15, 2008 at 4:26 PM, B?rczi G?bor <gabor at berczi.be>  
>> wrote:
>>
>> No.
>>
>> 'No' you don't know, or 'no' you haven't had any issues?
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From info at martin-hauser.net  Fri Feb 15 14:08:05 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Fri, 15 Feb 2008 23:08:05 +0100
Subject: [zfs-discuss] 10.5.2 update
In-Reply-To: <B933ED44-27AA-49B8-82D8-2EB302109C40@googlemail.com>
References: <5B4D46CC-0F46-4D18-8405-C57A6F966815@googlemail.com>
	<3A62129A-A356-48E9-97FE-C9264FA191D6@berczi.be>
	<eb3e47ad0802151332w5ea25411p448183b83bd2ff91@mail.gmail.com>
	<43782E34-5D28-4238-8C8D-7692824DCC13@martin-hauser.net>
	<B933ED44-27AA-49B8-82D8-2EB302109C40@googlemail.com>
Message-ID: <02827161-0F16-45EB-9660-38166F84213F@martin-hauser.net>

>
> Cool, thanks. I'm just using the downloaded binaries. I've updated  
> my laptop, which has read/write ZFS, but I haven't got a ZFS pool on  
> that.
>
> Is there much difference between the released binaries and the  
> current source?
>

For me the downloaded binaries just wouldn't work, but I seemed to be  
the only one. That's the exact reason why I mentioned it, because my  
report of the upgrade working is not representative (As far as I know,  
there Is no difference between the binaries and the source).

> The slightly annoying thing is that I think I'm going to have to  
> upgrade my Solaris box too, because when I tried sending snapshots  
> from OS X ZFS to solaris ZFS although they seem to send ok, and are  
> listed by 'zfs list' they won't mount. The solaris install on that  
> is rather out of date though. (probably going to use that box for  
> off site backup)

Have you compared the versions of the Solaris and the Os X zfs  
Versions ? (It seems that zfs is now ready to use pool version 8 and  
normal version 2, might be newer as your older solaris) ?

Martin
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080215/15154028/attachment.bin 

From david299792 at googlemail.com  Fri Feb 15 14:33:11 2008
From: david299792 at googlemail.com (David Ritchie)
Date: Fri, 15 Feb 2008 22:33:11 +0000
Subject: [zfs-discuss] 10.5.2 update
In-Reply-To: <02827161-0F16-45EB-9660-38166F84213F@martin-hauser.net>
References: <5B4D46CC-0F46-4D18-8405-C57A6F966815@googlemail.com>
	<3A62129A-A356-48E9-97FE-C9264FA191D6@berczi.be>
	<eb3e47ad0802151332w5ea25411p448183b83bd2ff91@mail.gmail.com>
	<43782E34-5D28-4238-8C8D-7692824DCC13@martin-hauser.net>
	<B933ED44-27AA-49B8-82D8-2EB302109C40@googlemail.com>
	<02827161-0F16-45EB-9660-38166F84213F@martin-hauser.net>
Message-ID: <17F2CDD4-8B4E-492A-99FC-6EAF63865506@googlemail.com>


On 15 Feb 2008, at 22:08, Martin Hauser wrote:

>>
>> Cool, thanks. I'm just using the downloaded binaries. I've updated  
>> my laptop, which has read/write ZFS, but I haven't got a ZFS pool  
>> on that.
>>
>> Is there much difference between the released binaries and the  
>> current source?
>>
>
> For me the downloaded binaries just wouldn't work, but I seemed to  
> be the only one. That's the exact reason why I mentioned it, because  
> my report of the upgrade working is not representative (As far as I  
> know, there Is no difference between the binaries and the source).
>
Oh, that's rather odd. Thanks for the info anyway.

>> The slightly annoying thing is that I think I'm going to have to  
>> upgrade my Solaris box too, because when I tried sending snapshots  
>> from OS X ZFS to solaris ZFS although they seem to send ok, and are  
>> listed by 'zfs list' they won't mount. The solaris install on that  
>> is rather out of date though. (probably going to use that box for  
>> off site backup)
>
> Have you compared the versions of the Solaris and the Os X zfs  
> Versions ? (It seems that zfs is now ready to use pool version 8 and  
> normal version 2, might be newer as your older solaris) ?
>
Well, I hadn't really looked into it that far after mounting of the  
newly created zfs filesystems on the solaris box failed. I wrote a  
quick script to do a sort of 'zfs send -r' -- ie to send all snapshots  
which don't exist on the destination but do on the source. It worked  
fine, but the fs didn't then mount.

On the solaris box I have:-

	Sun Microsystems Inc. SunOS 5.11 snv_50 October 2007
	...
	This system is currently running ZFS version 3.

I've downloaded the latest solaris on DVD, but haven't got around to  
installing it yet, partly because that machine has no DVD drive. I'm  
hoping it'll boot and install from an external USB one.

On OS X I haven't upgraded the pool, so I'm still getting the message  
about 'zpool upgrade' in 'zpool status'. That's version 6 isn't it?



regards,

-- David

From zorg at sogeeky.net  Fri Feb 15 16:08:07 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Fri, 15 Feb 2008 16:08:07 -0800
Subject: [zfs-discuss] 10.5.2 update
In-Reply-To: <eb3e47ad0802151332w5ea25411p448183b83bd2ff91@mail.gmail.com>
References: <5B4D46CC-0F46-4D18-8405-C57A6F966815@googlemail.com>
	<3A62129A-A356-48E9-97FE-C9264FA191D6@berczi.be>
	<eb3e47ad0802151332w5ea25411p448183b83bd2ff91@mail.gmail.com>
Message-ID: <C31373F1-B1EF-4EFE-8C94-57AC0784D13F@sogeeky.net>

I haven't had any issues. Its been fine so far.

On Feb 15, 2008, at 1:32 PM, "james sansbury" <q0rban at gmail.com> wrote:

> On Feb 15, 2008, at 10:20 PM, David Ritchie wrote:
>
> Does anyone know if there are any issues with the latest OS X update
>
> On Fri, Feb 15, 2008 at 4:26 PM, B?rczi G?bor <gabor at berczi.be> wrot 
> e:
>
> No.
>
> 'No' you don't know, or 'no' you haven't had any issues?
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080215/74dad12b/attachment-0001.html 

From ndellofano at apple.com  Fri Feb 15 17:02:07 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 15 Feb 2008 17:02:07 -0800
Subject: [zfs-discuss] 10.5.2 update
In-Reply-To: <17F2CDD4-8B4E-492A-99FC-6EAF63865506@googlemail.com>
References: <5B4D46CC-0F46-4D18-8405-C57A6F966815@googlemail.com>
	<3A62129A-A356-48E9-97FE-C9264FA191D6@berczi.be>
	<eb3e47ad0802151332w5ea25411p448183b83bd2ff91@mail.gmail.com>
	<43782E34-5D28-4238-8C8D-7692824DCC13@martin-hauser.net>
	<B933ED44-27AA-49B8-82D8-2EB302109C40@googlemail.com>
	<02827161-0F16-45EB-9660-38166F84213F@martin-hauser.net>
	<17F2CDD4-8B4E-492A-99FC-6EAF63865506@googlemail.com>
Message-ID: <51A8BF17-790B-4E7B-979F-1798760680C8@apple.com>

Yeah, your snapshots aren't mounting because your snapshots are a  
newer on disk version then ZFS on your Solaris box.  If you haven't  
upgraded your pools on OSX, then your OSX ZFS is running on disk  
version 6.  So installing the latest Solairs should fix your issue.

Noel


On Feb 15, 2008, at 2:33 PM, David Ritchie wrote:

> On the solaris box I have:-
>
> 	Sun Microsystems Inc. SunOS 5.11 snv_50 October 2007
> 	...
> 	This system is currently running ZFS version 3.
>
> I've downloaded the latest solaris on DVD, but haven't got around to
> installing it yet, partly because that machine has no DVD drive. I'm
> hoping it'll boot and install from an external USB one.
>
> On OS X I haven't upgraded the pool, so I'm still getting the message
> about 'zpool upgrade' in 'zpool status'. That's version 6 isn't it?
>


From ndellofano at apple.com  Fri Feb 15 17:04:15 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 15 Feb 2008 17:04:15 -0800
Subject: [zfs-discuss] ZFS mmap bug with PHP?
In-Reply-To: <9CFE6352-626A-4E1A-8B5C-ABE418CBE848@berczi.be>
References: <9CFE6352-626A-4E1A-8B5C-ABE418CBE848@berczi.be>
Message-ID: <149F9A28-E5D8-4306-BF0D-7F8D12F69A58@apple.com>

the zfs-102A bits are not mmap coherent, hence this is likely causing  
the weirdness you're seeing.  I'm releasing new bits on macosforge  
soon, which have the mmap coherency working so hang tight :)

Noel

On Feb 15, 2008, at 1:12 PM, B?rczi G?bor wrote:

> See the attached PHP code. Its functionality:
>
> - unlinks test.cfg
> - creates test.cfg with the content "test"
> - lists the file with file_get_contents()
> - adds an X509 certificate with OpenSSL
> - lists the file again
> - lists the file again
> - lists the file again
>
> The thing is: on HFS+ it works as it should, but on ZFS all  
> subsequent file listings contain only "test" - the initial content.  
> DTrace notices the opening of the file every time - so this looks  
> like to be a very bad low-level problem.
>
> Output:
>
> File contents:testDone.Attaching new CERT into test.cfgFile  
> contents, second try:testDone.File contents, third try:testDone.File  
> contents, fourth try:testDone.The file should now contain an X509  
> cert, but PHP can't see it :(
>
> What the..?
>
> -- 
> Gabucino
> <zfsphp.tar>_______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dirkschelfhout at mac.com  Fri Feb 15 17:27:02 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Sat, 16 Feb 2008 02:27:02 +0100
Subject: [zfs-discuss] ZFS mmap bug with PHP?
In-Reply-To: <149F9A28-E5D8-4306-BF0D-7F8D12F69A58@apple.com>
References: <9CFE6352-626A-4E1A-8B5C-ABE418CBE848@berczi.be>
	<149F9A28-E5D8-4306-BF0D-7F8D12F69A58@apple.com>
Message-ID: <EEE43AF5-1477-4D5C-955F-E71CA9E0D392@mac.com>

For some time I have been puzzled about zfs on osx.
What makes the file system different on osx than on solaris ?
I mean what has to be changed, added and why ? Isn't it just a  
finished product by sun ?
Does the below statement mean mmap doesn't work on the solaris version ?
Dirk
On 16 Feb 2008, at 02:04, No?l Dellofano wrote:

> the zfs-102A bits are not mmap coherent, hence this is likely causing
> the weirdness you're seeing.  I'm releasing new bits on macosforge
> soon, which have the mmap coherency working so hang tight :)


From dirkschelfhout at mac.com  Fri Feb 15 17:30:54 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Sat, 16 Feb 2008 02:30:54 +0100
Subject: [zfs-discuss] timeline
Message-ID: <7EC0161A-2099-415A-9118-B6568448B6B6@mac.com>

Is there a timeline for zfs to be stable on osx ?

I still had an rsync issue recently.
Eye tv doesn't like it, so there must still be throughput issues.

It's nice to be able to play with it, don't get me wrong :-)

Dirk

From bwaters at nrao.edu  Fri Feb 15 19:17:05 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Fri, 15 Feb 2008 20:17:05 -0700
Subject: [zfs-discuss] ZFS mmap bug with PHP?
In-Reply-To: <EEE43AF5-1477-4D5C-955F-E71CA9E0D392@mac.com>
References: <9CFE6352-626A-4E1A-8B5C-ABE418CBE848@berczi.be>
	<149F9A28-E5D8-4306-BF0D-7F8D12F69A58@apple.com>
	<EEE43AF5-1477-4D5C-955F-E71CA9E0D392@mac.com>
Message-ID: <FB2F1686-303E-4942-B4AA-62EBCA5F2CD3@nrao.edu>


On Feb 15, 2008, at 6:27 PM, Dirk Schelfhout wrote:

> What makes the file system different on osx than on solaris ?

If I had to guess...

OSX and Solaris have some pretty big differences in the way that they  
present the kernel.

The Mac OS X Kernel API is written in a subset of C++, right? I  
suspect that many system calls are different from Solaris, for things  
like memory allocation and the like. But I have no idea.

The layer that abstracts all filesystem implementations to the rest of  
the kernel - the VFS layer - is different.

I haven't memorized Amit Singh's book yet, but it is a fun thing to  
wade through:
http://www.osxbook.com/


The source code for each port is available, has anyone run a diff on  
them?

  - boyd


From zfs-discuss at openhealth.org  Sat Feb 16 20:08:06 2008
From: zfs-discuss at openhealth.org (Jonathan Borden)
Date: Sat, 16 Feb 2008 23:08:06 -0500
Subject: [zfs-discuss] ZFS mmap bug with PHP?
In-Reply-To: <EEE43AF5-1477-4D5C-955F-E71CA9E0D392@mac.com>
References: <9CFE6352-626A-4E1A-8B5C-ABE418CBE848@berczi.be>
	<149F9A28-E5D8-4306-BF0D-7F8D12F69A58@apple.com>
	<EEE43AF5-1477-4D5C-955F-E71CA9E0D392@mac.com>
Message-ID: <7a0bc8420802162008we953867x3a7a4bdf0bb47ecf@mail.gmail.com>

Dirk Schelfhout <dirkschelfhout at mac.com> wrote:
> For some time I have been puzzled about zfs on osx.
> What makes the file system different on osx than on solaris ?
> I mean what has to be changed, added and why ? Isn't it just a
> finished product by sun ?
> Does the below statement mean mmap doesn't work on the solaris version ?
> Dirk

It is fairly clear that ZFS on OS X does not behave like HPFS+ on OS X
from a fairly basic e.g. application level point of view.

For example:

Applications such as EyeTV (see my prior post) do not interact with
ZFS the same way as they do with HPFS even when writing to a network
share (i.e. over AFP).

Similarly the Finder cannot delete files in .Trashes on ZFS.

In prior versions applications such as iTunes have had issues with ZFS.

For these reasons it is apparent that there are application level
incompatibilities. I suppose that mmap is just another one of those.

Jonathan

From gabor at berczi.be  Sun Feb 17 00:37:43 2008
From: gabor at berczi.be (=?ISO-8859-1?Q?B=E9rczi_G=E1bor?=)
Date: Sun, 17 Feb 2008 09:37:43 +0100
Subject: [zfs-discuss] ZFS mmap bug with PHP?
In-Reply-To: <7a0bc8420802162008we953867x3a7a4bdf0bb47ecf@mail.gmail.com>
References: <9CFE6352-626A-4E1A-8B5C-ABE418CBE848@berczi.be>
	<149F9A28-E5D8-4306-BF0D-7F8D12F69A58@apple.com>
	<EEE43AF5-1477-4D5C-955F-E71CA9E0D392@mac.com>
	<7a0bc8420802162008we953867x3a7a4bdf0bb47ecf@mail.gmail.com>
Message-ID: <14AF6F5A-7CD7-485F-B8B8-8FEEF6070ABF@berczi.be>


On Feb 17, 2008, at 5:08 AM, Jonathan Borden wrote:

> In prior versions applications such as iTunes have had issues with  
> ZFS.
>
> For these reasons it is apparent that there are application level
> incompatibilities. I suppose that mmap is just another one of those.

iCal, Safari, PubSub's SQLite failures are another.

-- 
Gabucino


From ndellofano at apple.com  Mon Feb 18 15:20:53 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon, 18 Feb 2008 15:20:53 -0800
Subject: [zfs-discuss] ZFS mmap bug with PHP?
In-Reply-To: <EEE43AF5-1477-4D5C-955F-E71CA9E0D392@mac.com>
References: <9CFE6352-626A-4E1A-8B5C-ABE418CBE848@berczi.be>
	<149F9A28-E5D8-4306-BF0D-7F8D12F69A58@apple.com>
	<EEE43AF5-1477-4D5C-955F-E71CA9E0D392@mac.com>
Message-ID: <40D942B8-E79E-4388-B940-18A091867B3E@apple.com>

> Does the below statement mean mmap doesn't work on the solaris  
> version ?

No, mmap works fine on the Solaris version.  This issue was specific  
to our port on OSX.

> I mean what has to be changed, added and why ? Isn't it just a  
> finished product by sun ?

Yes ZFS is a finished product by Sun and it works well and is in use  
at a number of business in production.  However, in order to port a  
filesystem a lot of changes have to be made since the rest of the  
system isn't Solaris, it's Mac OSX, so things work differently.
As Boyd mentioned, the VFS layer in OS X is quite different from  
Solaris, hence much of the ZFS ZPL code (the upper most layer of the  
filesystem) needed to change in order to accommodate this.  In  
addition, the OS X kernel is much different then the Solaris kernel,  
so there are changes that need to be made in order to accommodate that  
as well.   Disk interfacing and driver layers are also different, and  
the memory allocation schemes are different so adjustments to how ZFS  
works and what it expects in those areas had to be changed as well.   
Basically, a number of things needed to be changed and its still a  
work in progress.  Take a look at the source and compare the two if  
you're interested in more specifics.  But basically, they are two  
different operating systems that each work quite differently, hence  
just picking up the filesystem and dropping it into another operating  
system isn't exactly seamless :)

Noel


On Feb 15, 2008, at 5:27 PM, Dirk Schelfhout wrote:

> For some time I have been puzzled about zfs on osx.
> What makes the file system different on osx than on solaris ?
> I mean what has to be changed, added and why ? Isn't it just a  
> finished product by sun ?
> Does the below statement mean mmap doesn't work on the solaris  
> version ?
> Dirk
> On 16 Feb 2008, at 02:04, No?l Dellofano wrote:
>
>> the zfs-102A bits are not mmap coherent, hence this is likely causing
>> the weirdness you're seeing.  I'm releasing new bits on macosforge
>> soon, which have the mmap coherency working so hang tight :)
>


From zorg at sogeeky.net  Mon Feb 18 18:50:34 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Mon, 18 Feb 2008 18:50:34 -0800
Subject: [zfs-discuss] ZFS mmap bug with PHP?
In-Reply-To: <40D942B8-E79E-4388-B940-18A091867B3E@apple.com>
References: <9CFE6352-626A-4E1A-8B5C-ABE418CBE848@berczi.be>
	<149F9A28-E5D8-4306-BF0D-7F8D12F69A58@apple.com>
	<EEE43AF5-1477-4D5C-955F-E71CA9E0D392@mac.com>
	<40D942B8-E79E-4388-B940-18A091867B3E@apple.com>
Message-ID: <65A57F57-26FE-4CF5-BD97-99041DB2DCA7@sogeeky.net>

At the same time, the core code that understands the fs structure, how  
to create volumes, manage them, etc, I should think is well isolated  
from these lower level issues. And that seems to be the case given all  
the comments here about how solid its actually been with data  
integrity. The only issues seem to be quirky ones due to the  
differences you describe. Is that about right, No?l?

On Feb 18, 2008, at 3:20 PM, No?l Dellofano <ndellofano at apple.com>  
wrote:

>> Does the below statement mean mmap doesn't work on the solaris
>> version ?
>
> No, mmap works fine on the Solaris version.  This issue was specific
> to our port on OSX.
>
>> I mean what has to be changed, added and why ? Isn't it just a
>> finished product by sun ?
>
> Yes ZFS is a finished product by Sun and it works well and is in use
> at a number of business in production.  However, in order to port a
> filesystem a lot of changes have to be made since the rest of the
> system isn't Solaris, it's Mac OSX, so things work differently.
> As Boyd mentioned, the VFS layer in OS X is quite different from
> Solaris, hence much of the ZFS ZPL code (the upper most layer of the
> filesystem) needed to change in order to accommodate this.  In
> addition, the OS X kernel is much different then the Solaris kernel,
> so there are changes that need to be made in order to accommodate that
> as well.   Disk interfacing and driver layers are also different, and
> the memory allocation schemes are different so adjustments to how ZFS
> works and what it expects in those areas had to be changed as well.
> Basically, a number of things needed to be changed and its still a
> work in progress.  Take a look at the source and compare the two if
> you're interested in more specifics.  But basically, they are two
> different operating systems that each work quite differently, hence
> just picking up the filesystem and dropping it into another operating
> system isn't exactly seamless :)
>
> Noel
>
>
> On Feb 15, 2008, at 5:27 PM, Dirk Schelfhout wrote:
>
>> For some time I have been puzzled about zfs on osx.
>> What makes the file system different on osx than on solaris ?
>> I mean what has to be changed, added and why ? Isn't it just a
>> finished product by sun ?
>> Does the below statement mean mmap doesn't work on the solaris
>> version ?
>> Dirk
>> On 16 Feb 2008, at 02:04, No?l Dellofano wrote:
>>
>>> the zfs-102A bits are not mmap coherent, hence this is likely  
>>> causing
>>> the weirdness you're seeing.  I'm releasing new bits on macosforge
>>> soon, which have the mmap coherency working so hang tight :)
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From dirkschelfhout at mac.com  Tue Feb 19 07:24:39 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Tue, 19 Feb 2008 16:24:39 +0100
Subject: [zfs-discuss] raidz and hfsplus striped
Message-ID: <E5697F64-ED45-47A1-8F36-A935E501FA2F@mac.com>

I decided to destroy my raidz pool on 3 500g disks in my macpro and  
format them, so I can use a striped hfsplus raid
and a raidz on the same disks. I get reliability and can still play  
with raidz.

As I unmounted the pool and then did zpool destroy backup ( backup is  
pool name ),
I got the screen where the os asks me to hold down the power button to  
reset.

I later did a zpool destroy while the pool was still mounted and that  
gave no problem.
I am not willing to retry this, but it seems to be a nasty bug. :)

Dirk

From franzschmalzl at spamfreemail.de  Tue Feb 19 10:13:23 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Tue, 19 Feb 2008 19:13:23 +0100
Subject: [zfs-discuss] raidz and hfsplus striped
In-Reply-To: <E5697F64-ED45-47A1-8F36-A935E501FA2F@mac.com>
References: <E5697F64-ED45-47A1-8F36-A935E501FA2F@mac.com>
Message-ID: <C2A806DB-D618-4265-B181-9AFA777E26A7@spamfreemail.de>

I'm facing some linguistic understanding problems here sorry.....

You said you created a sriped HFS and still get reliability?

So you have  created a raidz but formatted the drive HFS+ ?

So you still use the raidz parity algorithms but with a hfs+  
filesystem ?

Afaik just striping hfs+ via apple software raid doesn't bring you any  
safety. i.e. if one drive fails, everything is gone..







On 19.02.2008, at 16:24, Dirk Schelfhout wrote:

> I decided to destroy my raidz pool on 3 500g disks in my macpro and
> format them, so I can use a striped hfsplus raid
> and a raidz on the same disks. I get reliability and can still play
> with raidz.
>
> As I unmounted the pool and then did zpool destroy backup ( backup is
> pool name ),
> I got the screen where the os asks me to hold down the power button to
> reset.
>
> I later did a zpool destroy while the pool was still mounted and that
> gave no problem.
> I am not willing to retry this, but it seems to be a nasty bug. :)
>
> Dirk
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dirkschelfhout at mac.com  Tue Feb 19 10:30:15 2008
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Tue, 19 Feb 2008 19:30:15 +0100
Subject: [zfs-discuss] raidz and hfsplus striped
In-Reply-To: <C2A806DB-D618-4265-B181-9AFA777E26A7@spamfreemail.de>
References: <E5697F64-ED45-47A1-8F36-A935E501FA2F@mac.com>
	<C2A806DB-D618-4265-B181-9AFA777E26A7@spamfreemail.de>
Message-ID: <973D7332-6569-4D30-8B79-B1FCE144C0BD@mac.com>

No, sorry for the misunderstanding.
I formatted the disks in 2 partitions.
I use the 1st partiition of the 3 disks to create an hfsplus stripe.  
( hfsplus is stable, so... reliable )
I use the 2nd partition on each of the 3 disks to create a raidz. ( so  
I can play with it and learn )
rsync really doesn't like syncen to this new raidz ......... :-(

On 19 Feb 2008, at 19:13, Franz Schmalzl wrote:

> I'm facing some linguistic understanding problems here sorry.....
>
> You said you created a sriped HFS and still get reliability?
>
> So you have  created a raidz but formatted the drive HFS+ ?
>
> So you still use the raidz parity algorithms but with a hfs+  
> filesystem ?
>
> Afaik just striping hfs+ via apple software raid doesn't bring you  
> any safety. i.e. if one drive fails, everything is gone..
>
>
>
>
>
>
>
> On 19.02.2008, at 16:24, Dirk Schelfhout wrote:
>
>> I decided to destroy my raidz pool on 3 500g disks in my macpro and
>> format them, so I can use a striped hfsplus raid
>> and a raidz on the same disks. I get reliability and can still play
>> with raidz.
>>
>> As I unmounted the pool and then did zpool destroy backup ( backup is
>> pool name ),
>> I got the screen where the os asks me to hold down the power button  
>> to
>> reset.
>>
>> I later did a zpool destroy while the pool was still mounted and that
>> gave no problem.
>> I am not willing to retry this, but it seems to be a nasty bug. :)
>>
>> Dirk
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From ndellofano at apple.com  Tue Feb 19 15:05:47 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue, 19 Feb 2008 15:05:47 -0800
Subject: [zfs-discuss] raidz and hfsplus striped
In-Reply-To: <973D7332-6569-4D30-8B79-B1FCE144C0BD@mac.com>
References: <E5697F64-ED45-47A1-8F36-A935E501FA2F@mac.com>
	<C2A806DB-D618-4265-B181-9AFA777E26A7@spamfreemail.de>
	<973D7332-6569-4D30-8B79-B1FCE144C0BD@mac.com>
Message-ID: <C1E35C51-A552-4BF7-A7B3-2532FF87A4E1@apple.com>

So I think there is a misunderstanding.   You don't need to unmount a  
pool in order to destroy it :)  It's expected behavior.  ZFS is smart  
enough to do all the tear down under the covers.  (This is a different  
being then how HFS and UFS work so things are a little different)....


Noel

On Feb 19, 2008, at 10:30 AM, Dirk Schelfhout wrote:

> No, sorry for the misunderstanding.
> I formatted the disks in 2 partitions.
> I use the 1st partiition of the 3 disks to create an hfsplus stripe.
> ( hfsplus is stable, so... reliable )
> I use the 2nd partition on each of the 3 disks to create a raidz. ( so
> I can play with it and learn )
> rsync really doesn't like syncen to this new raidz ......... :-(
>
> On 19 Feb 2008, at 19:13, Franz Schmalzl wrote:
>
>> I'm facing some linguistic understanding problems here sorry.....
>>
>> You said you created a sriped HFS and still get reliability?
>>
>> So you have  created a raidz but formatted the drive HFS+ ?
>>
>> So you still use the raidz parity algorithms but with a hfs+
>> filesystem ?
>>
>> Afaik just striping hfs+ via apple software raid doesn't bring you
>> any safety. i.e. if one drive fails, everything is gone..
>>
>>
>>
>>
>>
>>
>>
>> On 19.02.2008, at 16:24, Dirk Schelfhout wrote:
>>
>>> I decided to destroy my raidz pool on 3 500g disks in my macpro and
>>> format them, so I can use a striped hfsplus raid
>>> and a raidz on the same disks. I get reliability and can still play
>>> with raidz.
>>>
>>> As I unmounted the pool and then did zpool destroy backup ( backup  
>>> is
>>> pool name ),
>>> I got the screen where the os asks me to hold down the power button
>>> to
>>> reset.
>>>
>>> I later did a zpool destroy while the pool was still mounted and  
>>> that
>>> gave no problem.
>>> I am not willing to retry this, but it seems to be a nasty bug. :)
>>>
>>> Dirk
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From kane at inius.com  Tue Feb 19 17:49:35 2008
From: kane at inius.com (Kane Dijkman)
Date: Tue, 19 Feb 2008 17:49:35 -0800
Subject: [zfs-discuss] raidz and hfsplus striped
In-Reply-To: <973D7332-6569-4D30-8B79-B1FCE144C0BD@mac.com>
References: <E5697F64-ED45-47A1-8F36-A935E501FA2F@mac.com>
	<C2A806DB-D618-4265-B181-9AFA777E26A7@spamfreemail.de>
	<973D7332-6569-4D30-8B79-B1FCE144C0BD@mac.com>
Message-ID: <791AE8ED-1F88-4512-AA3E-9E37C38B9A73@inius.com>

If you look through the archive for the past month you will find some  
talk about rsync 3 working better with zfs. I downloaded and built the  
latest version of it and all my rsyncing problems on a 1.2TB array  
went away and everything is happy now.

Might solve your issues too.

On Feb 19, 2008, at 10:30 AM, Dirk Schelfhout wrote:

> No, sorry for the misunderstanding.
> I formatted the disks in 2 partitions.
> I use the 1st partiition of the 3 disks to create an hfsplus stripe.
> ( hfsplus is stable, so... reliable )
> I use the 2nd partition on each of the 3 disks to create a raidz. ( so
> I can play with it and learn )
> rsync really doesn't like syncen to this new raidz ......... :-(
>
> On 19 Feb 2008, at 19:13, Franz Schmalzl wrote:
>
>> I'm facing some linguistic understanding problems here sorry.....
>>
>> You said you created a sriped HFS and still get reliability?
>>
>> So you have  created a raidz but formatted the drive HFS+ ?
>>
>> So you still use the raidz parity algorithms but with a hfs+
>> filesystem ?
>>
>> Afaik just striping hfs+ via apple software raid doesn't bring you
>> any safety. i.e. if one drive fails, everything is gone..
>>
>>
>>
>>
>>
>>
>>
>> On 19.02.2008, at 16:24, Dirk Schelfhout wrote:
>>
>>> I decided to destroy my raidz pool on 3 500g disks in my macpro and
>>> format them, so I can use a striped hfsplus raid
>>> and a raidz on the same disks. I get reliability and can still play
>>> with raidz.
>>>
>>> As I unmounted the pool and then did zpool destroy backup ( backup  
>>> is
>>> pool name ),
>>> I got the screen where the os asks me to hold down the power button
>>> to
>>> reset.
>>>
>>> I later did a zpool destroy while the pool was still mounted and  
>>> that
>>> gave no problem.
>>> I am not willing to retry this, but it seems to be a nasty bug. :)
>>>
>>> Dirk
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


----------------------------------------------------------------------------------
Does the movement of the trees make the wind blow?


From zfs-discuss at openhealth.org  Wed Feb 20 16:07:27 2008
From: zfs-discuss at openhealth.org (Jonathan Borden)
Date: Wed, 20 Feb 2008 19:07:27 -0500
Subject: [zfs-discuss] crash on $zpool online gene disk14s2
Message-ID: <7a0bc8420802201607wa5d9f08kcf47ae35d4e79517@mail.gmail.com>

zpool status told me that one of my mirrored disks was unavailable ...
I tried bringing it online:

$ zpool online gene disk14s2

and the machine crashed (BSOD).

rebooted and all was OK...

Jonathan

From bhill at physics.ucsd.edu  Wed Feb 20 20:36:55 2008
From: bhill at physics.ucsd.edu (Bryan Hill)
Date: Wed, 20 Feb 2008 20:36:55 -0800
Subject: [zfs-discuss] Questions from someone new to ZFS
Message-ID: <93B20B70-8A00-4D93-8B3E-489C98275FA1@physics.ucsd.edu>

Hi All:

I've been testing the latest build of ZFS on a 10.5.2 Mac Pro.  It's  
exciting to see this in development.  I'm really looking forward to  
using ZFS someday with all of its great features!

One of the applications I would like to use ZFS for in the future is  
for our IMAP mail stores.  I currently host our mail on and XServe  
with a directly attached Xserve RAID.  The RAID slice is level 5, with  
a hot spare. I have a new Intel Xserve that is slated to become the  
new mail server.  I also have on order one of the newly announced  
Promise VTrak RAID's.  So it got me thinking a little about what could  
be...

Once ZFS is production-ready on OS X, what is the optimal way to  
implement ZFS and a hardware RAID system such as the VTrak?  I've read  
some opinions on the Opensolaris ZFS discussion list, and it seems  
that people recommend using a RAID unit in a JBOD configuration, and  
using ZFS to handle the redundancy.  Is this the way to go?  I would  
think it would be, since you would be creating additional complexity  
to the setup by having redundancy at both the hardware and ZFS level.   
Am I correct or is this wooly thinking?

So if one uses a JBOD configuration, is raidz optimal?  Or would you  
use a pool made up of a collection of mirrored disks?  The Cyrus IMAP  
server on OS X stores each message as a single file, so as you would  
expect, the mail store has millions of small files contained within  
it.  What works best for data of this type?

Any insight is appreciated!

Thanks,
Bryan

---
Bryan D. Hill
UCSD Physics Computing Facility
9500 Gilman Dr.  # 0319
La Jolla, CA 92093
858-534-5538
bhill at ucsd.edu
AIM:  pozvibesd
Web:  http://www.physics.ucsd.edu/pcf




-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080220/f7bcec34/attachment-0001.html 

From bwaters at nrao.edu  Thu Feb 21 01:14:19 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Thu, 21 Feb 2008 02:14:19 -0700
Subject: [zfs-discuss] Questions from someone new to ZFS
In-Reply-To: <93B20B70-8A00-4D93-8B3E-489C98275FA1@physics.ucsd.edu>
References: <93B20B70-8A00-4D93-8B3E-489C98275FA1@physics.ucsd.edu>
Message-ID: <74868149-34C1-4C6F-866A-ECC0AA7D7B10@nrao.edu>


On Feb 20, 2008, at 9:36 PM, Bryan Hill wrote:

> Once ZFS is production-ready on OS X, what is the optimal way to  
> implement ZFS and a hardware RAID system such as the VTrak?  I've  
> read some opinions on the Opensolaris ZFS discussion list, and it  
> seems that people recommend using a RAID unit in a JBOD  
> configuration, and using ZFS to handle the redundancy.

The hardware RAID would give you system-transparent hot-swap. But  
yeah, otherwise it's just a level of complexity that might be better  
incorporated into the ZFS stack. So I'd use it in JBOD mode, but you'd  
have to manage failed drives somehow (not certain about hot-swap). I'm  
not familiar with the VTrak (I mean, I've read about them, but don't  
have one).

> So if one uses a JBOD configuration, is raidz optimal?  Or would you  
> use a pool made up of a collection of mirrored disks?

Read this:
http://blogs.sun.com/roch/entry/when_to_and_not_to

It may help you understand how to best trade off performance and raidz  
overhead when you've got ~12-48 disks to play with.

I'm not sure about "tuning" the file system for performance on lots of  
little files. I've played with such systems using reiserfs and NTFS;  
don't most file systems coalesce small files into the directory tree?  
With ZFS I have no idea...


- boyd

Boyd Waters
Scientific Programmer
National Radio Astronomy Observatory
Socorro, New Mexico, USA, Earth


From bhill at physics.ucsd.edu  Thu Feb 21 09:13:22 2008
From: bhill at physics.ucsd.edu (Bryan Hill)
Date: Thu, 21 Feb 2008 09:13:22 -0800
Subject: [zfs-discuss] Questions from someone new to ZFS
In-Reply-To: <74868149-34C1-4C6F-866A-ECC0AA7D7B10@nrao.edu>
References: <93B20B70-8A00-4D93-8B3E-489C98275FA1@physics.ucsd.edu>
	<74868149-34C1-4C6F-866A-ECC0AA7D7B10@nrao.edu>
Message-ID: <A71D1A75-08C9-42E4-AC9B-9F0F991C76EE@physics.ucsd.edu>


On Feb 21, 2008, at 1:14 AM, Boyd Waters wrote:

>
> On Feb 20, 2008, at 9:36 PM, Bryan Hill wrote:
>
>> Once ZFS is production-ready on OS X, what is the optimal way to  
>> implement ZFS and a hardware RAID system such as the VTrak?  I've  
>> read some opinions on the Opensolaris ZFS discussion list, and it  
>> seems that people recommend using a RAID unit in a JBOD  
>> configuration, and using ZFS to handle the redundancy.
>
> The hardware RAID would give you system-transparent hot-swap. But  
> yeah, otherwise it's just a level of complexity that might be better  
> incorporated into the ZFS stack. So I'd use it in JBOD mode, but  
> you'd have to manage failed drives somehow (not certain about hot- 
> swap). I'm not familiar with the VTrak (I mean, I've read about  
> them, but don't have one).
>
>> So if one uses a JBOD configuration, is raidz optimal?  Or would  
>> you use a pool made up of a collection of mirrored disks?
>
> Read this:
> http://blogs.sun.com/roch/entry/when_to_and_not_to
>
> It may help you understand how to best trade off performance and  
> raidz overhead when you've got ~12-48 disks to play with.

Thanks!  This is great info.

>
>
> I'm not sure about "tuning" the file system for performance on lots  
> of little files. I've played with such systems using reiserfs and  
> NTFS; don't most file systems coalesce small files into the  
> directory tree? With ZFS I have no idea...

I'm using HFS+ for the current mail store in our setup.  Its  
performance isn't so great with millions of tiny files.  I'm hoping  
ZFS deals with this more efficiently.



>
>
>
> - boyd
>
> Boyd Waters
> Scientific Programmer
> National Radio Astronomy Observatory
> Socorro, New Mexico, USA, Earth
>
>


Thanks,
Bryan

---
Bryan D. Hill
UCSD Physics Computing Facilty
9500 Gilman Dr.  # 0319
La Jolla, CA 92093
858-534-5538
bhill at ucsd.edu
AIM:  pozvibesd
Web:  http://www.physics.ucsd.edu/pcf






From buffyg at mac.com  Thu Feb 21 09:49:05 2008
From: buffyg at mac.com (Bayard Bell)
Date: Thu, 21 Feb 2008 17:49:05 +0000
Subject: [zfs-discuss] Questions from someone new to ZFS
In-Reply-To: <A71D1A75-08C9-42E4-AC9B-9F0F991C76EE@physics.ucsd.edu>
References: <93B20B70-8A00-4D93-8B3E-489C98275FA1@physics.ucsd.edu>
	<74868149-34C1-4C6F-866A-ECC0AA7D7B10@nrao.edu>
	<A71D1A75-08C9-42E4-AC9B-9F0F991C76EE@physics.ucsd.edu>
Message-ID: <F18C85B4-FD23-4972-BCA3-D797BF301709@mac.com>

On 21 Feb 2008, at 17:13, Bryan Hill wrote:

>
> On Feb 21, 2008, at 1:14 AM, Boyd Waters wrote:
>
>>
>> On Feb 20, 2008, at 9:36 PM, Bryan Hill wrote:
>>
>>> I'm not sure about "tuning" the file system for performance on lots
>> of little files. I've played with such systems using reiserfs and
>> NTFS; don't most file systems coalesce small files into the
>> directory tree? With ZFS I have no idea...
>
> I'm using HFS+ for the current mail store in our setup.  Its
> performance isn't so great with millions of tiny files.  I'm hoping
> ZFS deals with this more efficiently.

There are a couple of different factors when you're talking about  
filesystem performance in that kind of context, some of which you may  
be able to optimise with a filesystem, others not. Usually having  
large amounts of files in the same directory, irrespective of file  
size, comes down to the problem with how the directory structures are  
maintained on-disk, what the locking structure looks like, and how  
this interacts with your access pattern. I'd offer that in most such  
cases, the answer that's going to "just work", irrespective of access  
pattern, is to decompose the one large directory into several layers  
of m-trees (e.g. instead of my mailbox being named mail/bayard, call  
it mail/b/ba/bayard). This is particularly effective if the file names  
are relatively static, but rename() operations within the same  
filesystem should not penalise distributing those operations against  
multiple directories (because this should give you less lock  
contention by distributing the collision domains) vs. using one  
massive directory. You may yet end up with problems like directory  
fragmentation, but I'd rather have that problem that having all of my  
directory updates serialised against a BFL.

It's always good when you can get optimisations for free from your  
filesystem because it's just that good, but there are usually  
optimisations that you can wring out immediately on whatever you're  
using at the moment if you're willing and able to restructure your data.

Cheers,
Bayard

From franzschmalzl at spamfreemail.de  Thu Feb 21 12:40:54 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Thu, 21 Feb 2008 21:40:54 +0100
Subject: [zfs-discuss] encryption
Message-ID: <A08E672D-D54C-4B26-A23F-CB31A48A50AC@spamfreemail.de>

i know the zfs crypto is in the works...

but is there a possibility to format sparse bundles with a zfs  
filesystem ?

id like to place my valuable data on to a external raidz and i need  
encryption...
so just creating a raidz and laying a sparsebundle with hfs+ inside  
seems pretty pointless to me.

my plan was to create a encrypted sparsebundle with zfs inside, as i  
believe the dmg itself is encrypted ( so it's sort of device level  
encryption, and not filesystem level like with encfs or something )

im a right?

and would it work ?

regards

From bwaters at nrao.edu  Thu Feb 21 12:43:54 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Thu, 21 Feb 2008 13:43:54 -0700
Subject: [zfs-discuss] Questions from someone new to ZFS
In-Reply-To: <A71D1A75-08C9-42E4-AC9B-9F0F991C76EE@physics.ucsd.edu>
References: <93B20B70-8A00-4D93-8B3E-489C98275FA1@physics.ucsd.edu>
	<74868149-34C1-4C6F-866A-ECC0AA7D7B10@nrao.edu>
	<A71D1A75-08C9-42E4-AC9B-9F0F991C76EE@physics.ucsd.edu>
Message-ID: <1CCB868B-26B1-4BD4-ACC7-B3323E2E3207@nrao.edu>


On Feb 21, 2008, at 10:13 AM, Bryan Hill wrote:

> I'm using HFS+ for the current mail store in our setup.  Its  
> performance isn't so great with millions of tiny files.  I'm hoping  
> ZFS deals with this more efficiently.


I have to be careful when I'm talking about filesystem "performance"  
and "millions of tiny files"; I discovered the hard way that most GNU  
user-land tools sort the result of filesystem calls such as readdir()  
using a method that will roll on the floor twitching if the result is  
more than a thousand or so entries.

So for instance doing an "ls bigDir" where bigDir contains 30000 files  
- tiny or not - will hammer the CPU, and not print anything on the  
screen, until after your coffee break is over.

Such nonsense has nothing to do with the filesystem layer. It won't  
matter (much) if that bigDir is on HFS+ or ZFS.

So be careful out there!



From bhill at physics.ucsd.edu  Thu Feb 21 13:55:25 2008
From: bhill at physics.ucsd.edu (Bryan Hill)
Date: Thu, 21 Feb 2008 13:55:25 -0800
Subject: [zfs-discuss] Questions from someone new to ZFS
In-Reply-To: <1CCB868B-26B1-4BD4-ACC7-B3323E2E3207@nrao.edu>
References: <93B20B70-8A00-4D93-8B3E-489C98275FA1@physics.ucsd.edu>
	<74868149-34C1-4C6F-866A-ECC0AA7D7B10@nrao.edu>
	<A71D1A75-08C9-42E4-AC9B-9F0F991C76EE@physics.ucsd.edu>
	<1CCB868B-26B1-4BD4-ACC7-B3323E2E3207@nrao.edu>
Message-ID: <8CBDB97E-1D66-46F8-80A3-55F8224065DD@physics.ucsd.edu>


On Feb 21, 2008, at 12:43 PM, Boyd Waters wrote:

>
> On Feb 21, 2008, at 10:13 AM, Bryan Hill wrote:
>
>> I'm using HFS+ for the current mail store in our setup.  Its
>> performance isn't so great with millions of tiny files.  I'm hoping
>> ZFS deals with this more efficiently.
>
>
> I have to be careful when I'm talking about filesystem "performance"
> and "millions of tiny files"; I discovered the hard way that most GNU
> user-land tools sort the result of filesystem calls such as readdir()
> using a method that will roll on the floor twitching if the result is
> more than a thousand or so entries.
>
> So for instance doing an "ls bigDir" where bigDir contains 30000 files
> - tiny or not - will hammer the CPU, and not print anything on the
> screen, until after your coffee break is over.
>
> Such nonsense has nothing to do with the filesystem layer. It won't
> matter (much) if that bigDir is on HFS+ or ZFS.
>
> So be careful out there!


This is exactly what I see with HFS+ (journaled and case-sensitive).   
The mail store is on its own RAID 5 volume, which I call /Volumes/spool

Under this, I have the directory structure imap/user/<username> for  
each user's mail drop.  A user's folder can contain tens of thousands  
of files and various IMAP subfolders.  Right now, doing an "ls -l" on  
a user who might have, say, 45000 messages in his/her INBOX or  
subfolder can take a while.  One of the things that bug me is that if  
I copy the entire mail store to a freshly created volume, the  
performance is "good" in that  doing an "ls -l" is relatively quick,  
maybe 10-20 seconds.  After a few months or so, doing an ls on the  
same folder with about the same number of files will take possibly a  
minute or two.

Copying the data from one volume to another takes hours.  I currently  
have about 240GB of files that can take over 6 hours to copy off to  
another drive.  After the copy, if reformat the source partition and  
copy the data back, it takes about 3 hours.  Again, the "good"  
performance returns but degrades over time.

Is this typical in what one would experience regardless of filesystem  
type?

Thanks,
Bryan

---
Bryan D. Hill
UCSD Physics Computing Facilty
9500 Gilman Dr.  # 0319
La Jolla, CA 92093
858-534-5538
bhill at ucsd.edu
AIM:  pozvibesd
Web:  http://www.physics.ucsd.edu/pcf






From bwaters at nrao.edu  Thu Feb 21 17:07:04 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Thu, 21 Feb 2008 18:07:04 -0700
Subject: [zfs-discuss] encryption
In-Reply-To: <A08E672D-D54C-4B26-A23F-CB31A48A50AC@spamfreemail.de>
References: <A08E672D-D54C-4B26-A23F-CB31A48A50AC@spamfreemail.de>
Message-ID: <03280C10-127B-4907-8E11-6F9C2D2DD6DB@nrao.edu>


On Feb 21, 2008, at 1:40 PM, Franz Schmalzl wrote:

> so just creating a raidz and laying a sparsebundle with hfs+ inside
> seems pretty pointless to me.

Well FWIW I've done it, but you're right: the sparsebundle is HFS, and  
lacks all the ZFS management goodness.

Of course, you can do ZFS snapshots on the file system hosting the  
sparsebundle, so you can go back to previous states of the hosted  
filesystem, but again not really what you'd like.

It's not completely useless, but the added utility barely compensates  
for the added complexity.


> my plan was to create a encrypted sparsebundle with zfs inside, as i
> believe the dmg itself is encrypted ( so it's sort of device level
> encryption, and not filesystem level like with encfs or something )
>
> im a right?
>
> and would it work ?


Why not try it?

Let's create a raidz with four encrypted disk images, instead of four  
disks:


$ cd /tmp

$ openssl rand -base64 16 > silly-password.txt

$ cat silly-password.txt | hdiutil create -size 500m -encryption - 
stdinpass -type SPARSEBUNDLE -fs HFS+J -layout NONE -volname noname1  
test1
created: /private/tmp/test1.sparsebundle


$ cat silly-password.txt | hdiutil attach -encryption -stdinpass - 
nomount test1.sparsebundle
/dev/disk9


$ diskutil partitiondisk /dev/disk9 GPTFormat ZFS %noformat% 100%
Started partitioning on disk disk9 noname1
Creating partition map
[ + 0%..10%..20%..30%..40%..50%..60%..70%..80%..90%..100% ]
Finished partitioning on disk disk9 noname1
/dev/disk9
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme noname1                *500.0 Mi    
disk9
    1:                        ZFS                         500.0 Mi    
disk9s1



Repeat for sparse images named test2, test3, test4



Now


$ du -sh *.sparsebundle
  31M	test1.sparsebundle
  31M	test2.sparsebundle
  31M	test3.sparsebundle
  31M	test4.sparsebundle


$ zpool create encrypted_images raidz /dev/disk9
disk9    disk9s1


$ zpool create encrypted_images raidz disk{9,10,11,12}s1


$ zpool status
   pool: encrypted_images
  state: ONLINE
status: The pool is formatted using an older on-disk format.  The pool  
can
	still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
	pool will no longer be accessible on older software versions.
  scrub: none requested
config:

	NAME          STATE     READ WRITE CKSUM
	encrypted_images  ONLINE       0     0     0
	  raidz1      ONLINE       0     0     0
	    disk9s1   ONLINE       0     0     0
	    disk10s1  ONLINE       0     0     0
	    disk11s1  ONLINE       0     0     0
	    disk12s1  ONLINE       0     0     0


$ zpool upgrade encrypted_images
This system is currently running ZFS pool version 8.

Successfully upgraded 'encrypted_images' from version 6 to version 8


$ zfs list
NAME                                 USED  AVAIL  REFER  MOUNTPOINT
encrypted_images                     393K  1.41G   294K  /Volumes/ 
encrypted_images



W00t!

Looks like that works!


Now what you want to do, is to make four different disks, each with HFS 
+J on them, create a sparseimage on each disk that is as large as the  
available space on the filesystem. Note that initially the sparseimage  
won't take up all the space. Then create your raidz and have at it!


Be sure to put all your boss' data on there, and tell me how it works  
out.

Seriously, I expect it to be a mite slower than device-backed raidz,  
but it just might work. Or you might run into no end of kernel  
deadlocks and your hair might fall out. But I think it'll work!

Cool!








From bwaters at nrao.edu  Thu Feb 21 17:11:50 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Thu, 21 Feb 2008 18:11:50 -0700
Subject: [zfs-discuss] encryption
In-Reply-To: <03280C10-127B-4907-8E11-6F9C2D2DD6DB@nrao.edu>
References: <A08E672D-D54C-4B26-A23F-CB31A48A50AC@spamfreemail.de>
	<03280C10-127B-4907-8E11-6F9C2D2DD6DB@nrao.edu>
Message-ID: <A2BA872D-C24B-4444-A89C-598013221601@nrao.edu>

Crap, my recipe is broken, please DON'T do that first "zpool create"  
line, I must have pasted that in there in the middle of trying this out.

> $ zpool create encrypted_images raidz /dev/disk9
> disk9    disk9s1
>
>
> $ zpool create encrypted_images raidz disk{9,10,11,12}s1
     ^^^^^^^^^^^^^ do this one ^^^^^^^^^^^^^^^^^^^^^^^^^^^^


That second one is what you want.

From franzschmalzl at spamfreemail.de  Thu Feb 21 18:04:36 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Fri, 22 Feb 2008 03:04:36 +0100
Subject: [zfs-discuss] Fwd:  encryption
References: <913F574E-818C-48F1-B386-94A53848D1E8@spamfreemail.de>
Message-ID: <34D3A50C-5BD9-49CF-BC3D-1EA5A2E94607@spamfreemail.de>



cool thanks !

i wanted to put those images on zfs'ed disks anyway..
so i have twice the zfs goodness ~)
>
>
>>
>> On Feb 21, 2008, at 1:40 PM, Franz Schmalzl wrote:
>>
>>> so just creating a raidz and laying a sparsebundle with hfs+ inside
>>> seems pretty pointless to me.
>>
>> Well FWIW I've done it, but you're right: the sparsebundle is HFS,  
>> and
>> lacks all the ZFS management goodness.
>>
>> Of course, you can do ZFS snapshots on the file system hosting the
>> sparsebundle, so you can go back to previous states of the hosted
>> filesystem, but again not really what you'd like.
>>
>> It's not completely useless, but the added utility barely compensates
>> for the added complexity.
>>
>>
>>> my plan was to create a encrypted sparsebundle with zfs inside, as i
>>> believe the dmg itself is encrypted ( so it's sort of device level
>>> encryption, and not filesystem level like with encfs or something )
>>>
>>> im a right?
>>>
>>> and would it work ?
>>
>>
>> Why not try it?
>>
>> Let's create a raidz with four encrypted disk images, instead of four
>> disks:
>>
>>
>> $ cd /tmp
>>
>> $ openssl rand -base64 16 > silly-password.txt
>>
>> $ cat silly-password.txt | hdiutil create -size 500m -encryption -
>> stdinpass -type SPARSEBUNDLE -fs HFS+J -layout NONE -volname noname1
>> test1
>> created: /private/tmp/test1.sparsebundle
>>
>>
>> $ cat silly-password.txt | hdiutil attach -encryption -stdinpass -
>> nomount test1.sparsebundle
>> /dev/disk9
>>
>>
>> $ diskutil partitiondisk /dev/disk9 GPTFormat ZFS %noformat% 100%
>> Started partitioning on disk disk9 noname1
>> Creating partition map
>> [ + 0%..10%..20%..30%..40%..50%..60%..70%..80%..90%..100% ]
>> Finished partitioning on disk disk9 noname1
>> /dev/disk9
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:      GUID_partition_scheme noname1                *500.0 Mi
>> disk9
>>   1:                        ZFS                         500.0 Mi
>> disk9s1
>>
>>
>>
>> Repeat for sparse images named test2, test3, test4
>>
>>
>>
>> Now
>>
>>
>> $ du -sh *.sparsebundle
>> 31M	test1.sparsebundle
>> 31M	test2.sparsebundle
>> 31M	test3.sparsebundle
>> 31M	test4.sparsebundle
>>
>>
>> $ zpool create encrypted_images raidz /dev/disk9
>> disk9    disk9s1
>>
>>
>> $ zpool create encrypted_images raidz disk{9,10,11,12}s1
>>
>>
>> $ zpool status
>>  pool: encrypted_images
>> state: ONLINE
>> status: The pool is formatted using an older on-disk format.  The  
>> pool
>> can
>> 	still be used, but some features are unavailable.
>> action: Upgrade the pool using 'zpool upgrade'.  Once this is done,  
>> the
>> 	pool will no longer be accessible on older software versions.
>> scrub: none requested
>> config:
>>
>> 	NAME          STATE     READ WRITE CKSUM
>> 	encrypted_images  ONLINE       0     0     0
>> 	  raidz1      ONLINE       0     0     0
>> 	    disk9s1   ONLINE       0     0     0
>> 	    disk10s1  ONLINE       0     0     0
>> 	    disk11s1  ONLINE       0     0     0
>> 	    disk12s1  ONLINE       0     0     0
>>
>>
>> $ zpool upgrade encrypted_images
>> This system is currently running ZFS pool version 8.
>>
>> Successfully upgraded 'encrypted_images' from version 6 to version 8
>>
>>
>> $ zfs list
>> NAME                                 USED  AVAIL  REFER  MOUNTPOINT
>> encrypted_images                     393K  1.41G   294K  /Volumes/
>> encrypted_images
>>
>>
>>
>> W00t!
>>
>> Looks like that works!
>>
>>
>> Now what you want to do, is to make four different disks, each with  
>> HFS
>> +J on them, create a sparseimage on each disk that is as large as the
>> available space on the filesystem. Note that initially the  
>> sparseimage
>> won't take up all the space. Then create your raidz and have at it!
>>
>>
>> Be sure to put all your boss' data on there, and tell me how it works
>> out.
>>
>> Seriously, I expect it to be a mite slower than device-backed raidz,
>> but it just might work. Or you might run into no end of kernel
>> deadlocks and your hair might fall out. But I think it'll work!
>>
>> Cool!
>>
>>
>>
>>
>>
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From zfs-discuss at openhealth.org  Fri Feb 22 09:31:06 2008
From: zfs-discuss at openhealth.org (Jonathan Borden)
Date: Fri, 22 Feb 2008 12:31:06 -0500
Subject: [zfs-discuss] cannot reimport new pool until reboot also crash on
	copy
Message-ID: <7a0bc8420802220931x7e4068bfne9ef62adc297e7d7@mail.gmail.com>

Two issues (possible bug reports):

1) when a new pool is created it seems to act flaky until the machine
is rebooted.
I created a new raidz pool with 5 disks. Copied some stuff. I exported
the pool (using -f) ... I did this because I've noticed that when
doing
$ diskutil list
the pool name is not written to the disk until after a reboot (which
includes export/import)

When i try reimporting e.g.
$ sudo zpool import tank

doesn't work... but when I reboot the machine all is well...

2) While copying ~200 gb into the new pool using finder I started
another copy of ~100 gb... the second copy crashed after ~10gb. The
computer didn't kernel panic, but the directory had a few file in it
and a bunch of blank files that hadn't yet been copies. Perhaps a
threading issue???

Jonathan

From kevinpurcell at pobox.com  Fri Feb 22 10:30:11 2008
From: kevinpurcell at pobox.com (Kevin Purcell)
Date: Fri, 22 Feb 2008 10:30:11 -0800
Subject: [zfs-discuss] Macintouch comments on RAID-Z
Message-ID: <79696DA3-0B5C-44AA-B045-9FAF0F1A21F4@pobox.com>

Comment seen on today's Macintouch:

<http://www.macintouch.com/readerreports/harddrives/ 
index.html#d22feb2008>

> Jeff Baker
> The willingness of people to see ZFS as the solution to any and all  
> problems continues to amaze me. Contrary to the prior comment, RAID- 
> Z in no way obsoletes or replaces the functions of a hardware RAID  
> controller. Typically a user expects high performance from a RAID  
> controller. RAID-Z, by comparison, is slightly slower than a single  
> hard disk. While a normal RAID becomes faster with additional  
> spindles, a RAID-Z exhibits flat to slightly declining performance  
> with more spindles (on random workloads).

I would have thought that RAID-Z performance would scale as the  
number of disks (so long as the controllers can't take multiple  
commands in parallel or for IDE as the number of controllers).

I'd appreciate some comments from those more knowledgeable than I.

Thanks.
--
Kevin Purcell
kevinpurcell at pobox.com



From bplist at thinkpink.com  Fri Feb 22 10:55:09 2008
From: bplist at thinkpink.com (Brian Pinkerton)
Date: Fri, 22 Feb 2008 10:55:09 -0800
Subject: [zfs-discuss] Macintouch comments on RAID-Z
In-Reply-To: <79696DA3-0B5C-44AA-B045-9FAF0F1A21F4@pobox.com>
References: <79696DA3-0B5C-44AA-B045-9FAF0F1A21F4@pobox.com>
Message-ID: <71A35519-A69C-4BF2-8723-B5761ED0EA61@thinkpink.com>

He's right.  Richard says it best:

http://blogs.sun.com/relling/entry/zfs_raid_recommendations_space_performance

Bottom line: for performance use RAID-0, for performance and  
reliability use RAID-1+0.

bri

On Feb 22, 2008, at 10:30 AM, Kevin Purcell wrote:

> Comment seen on today's Macintouch:
>
> <http://www.macintouch.com/readerreports/harddrives/
> index.html#d22feb2008>
>
>> Jeff Baker
>> The willingness of people to see ZFS as the solution to any and all
>> problems continues to amaze me. Contrary to the prior comment, RAID-
>> Z in no way obsoletes or replaces the functions of a hardware RAID
>> controller. Typically a user expects high performance from a RAID
>> controller. RAID-Z, by comparison, is slightly slower than a single
>> hard disk. While a normal RAID becomes faster with additional
>> spindles, a RAID-Z exhibits flat to slightly declining performance
>> with more spindles (on random workloads).
>
> I would have thought that RAID-Z performance would scale as the
> number of disks (so long as the controllers can't take multiple
> commands in parallel or for IDE as the number of controllers).
>
> I'd appreciate some comments from those more knowledgeable than I.
>
> Thanks.
> --
> Kevin Purcell
> kevinpurcell at pobox.com
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From kevinpurcell at pobox.com  Fri Feb 22 11:37:00 2008
From: kevinpurcell at pobox.com (Kevin Purcell)
Date: Fri, 22 Feb 2008 11:37:00 -0800
Subject: [zfs-discuss] Macintouch comments on RAID-Z
In-Reply-To: <71A35519-A69C-4BF2-8723-B5761ED0EA61@thinkpink.com>
References: <79696DA3-0B5C-44AA-B045-9FAF0F1A21F4@pobox.com>
	<71A35519-A69C-4BF2-8723-B5761ED0EA61@thinkpink.com>
Message-ID: <FC12AB4C-A6E6-4DB4-9D43-9C6B9153ED5C@pobox.com>

I also had a blog link that I had found and put aside yesterday and  
read this morning that also addresses this issue in detail. It's  
funny how that happens.

<http://blogs.sun.com/roch/entry/when_to_and_not_to>

The trade offs revolve around bandwidth, IO/s and reliability and the  
trade-offs between those factors.

There is no silver bullet (but there are composite bullets than can  
be selected to be most effective for particular uses).

Thanks for the clarification.

On Feb 22, 2008, at 10:55 AM, Brian Pinkerton wrote:

> He's right.  Richard says it best:
>
> http://blogs.sun.com/relling/entry/ 
> zfs_raid_recommendations_space_performance
>
> Bottom line: for performance use RAID-0, for performance and  
> reliability use RAID-1+0.
>
> bri
>
> On Feb 22, 2008, at 10:30 AM, Kevin Purcell wrote:
>
>> Comment seen on today's Macintouch:
>>
>> <http://www.macintouch.com/readerreports/harddrives/
>> index.html#d22feb2008>
>>
>>> Jeff Baker
>>> The willingness of people to see ZFS as the solution to any and all
>>> problems continues to amaze me. Contrary to the prior comment, RAID-
>>> Z in no way obsoletes or replaces the functions of a hardware RAID
>>> controller. Typically a user expects high performance from a RAID
>>> controller. RAID-Z, by comparison, is slightly slower than a single
>>> hard disk. While a normal RAID becomes faster with additional
>>> spindles, a RAID-Z exhibits flat to slightly declining performance
>>> with more spindles (on random workloads).
>>
>> I would have thought that RAID-Z performance would scale as the
>> number of disks (so long as the controllers can't take multiple
>> commands in parallel or for IDE as the number of controllers).
>>
>> I'd appreciate some comments from those more knowledgeable than I.
>>
>> Thanks.
>> --
>> Kevin Purcell
>> kevinpurcell at pobox.com
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>

--
Kevin Purcell
kevinpurcell at pobox.com



From dpuett at gmail.com  Fri Feb 22 12:16:40 2008
From: dpuett at gmail.com (David Puett)
Date: Fri, 22 Feb 2008 15:16:40 -0500
Subject: [zfs-discuss] Announcing zfsonosx.com
Message-ID: <C3E497D8.281F4%dpuett@gmail.com>

Salutations,

  I'm a big fan of ZFS but I thought there may be a better support option
than a mailing list.  New users may not easily find older information and
ask the same questions over and over.

  I thought I would build a quick site with a forum to make it a little
easier for people to find information.

Introducing zfsonosx.com http://www.zfsonosx.com

  I can promise a few things about the site.  There will never be any
advertising of any type on the site.  I'm hosting this site on my company's
plan and I'm not even mentioning my company.  There will never be a 'please
donate' badge.  There will never be any solicitations of any type.  I don't
care about anyone's email address and will never even look at them nor do
anything else with them.

  Feel free to email me with links or FAQs you want added.  Hopefully there
will files to add as well.

  I don't mean to step on anyone's toes with the site so please let me know
if I am.  Please feel free to send comments, questions, and criticisms.

Cheers,
David



From atl at comp.lancs.ac.uk  Fri Feb 22 12:53:45 2008
From: atl at comp.lancs.ac.uk (Adam Lindsay)
Date: Fri, 22 Feb 2008 20:53:45 +0000
Subject: [zfs-discuss] Macintouch comments on RAID-Z
In-Reply-To: <FC12AB4C-A6E6-4DB4-9D43-9C6B9153ED5C@pobox.com>
References: <79696DA3-0B5C-44AA-B045-9FAF0F1A21F4@pobox.com>	<71A35519-A69C-4BF2-8723-B5761ED0EA61@thinkpink.com>
	<FC12AB4C-A6E6-4DB4-9D43-9C6B9153ED5C@pobox.com>
Message-ID: <47BF3659.9070403@comp.lancs.ac.uk>

Hey all.
I've been working with ZFS as a user (on OpenSolaris) for about a year 
now (but my heart, as always, belongs to Mac).

I would never, ever ascribe "use RAID-0" to Richard Elling. I think he 
would bristle at that, as well.

Since we're plugging blog entries, I'll throw my experimental results 
into the ring as well:
  <http://lindsay.at/blog/archive/tag/zfs-performance/>
(The system I benchmark has the same number of SATA-II disks as those 
newly Apple-endorsed Promise RAID subsystems.)

My take-aways about ideal performance configurations are this:
If you care enough about your data to use ZFS, don't bother with RAID-0, 
  except in very special circumstances.

If you care about random I/O reads, then mirror (RAID-1-like) 
configurations are the way to go.

If you care about streaming I/O performance, then RAIDZ (RAID-5 and 
-6-like) configurations are the way to go.

Real systems are contrained by I/O bus throughput and vibrations and the 
like, so performance seldom scales as linearly as you would expect.

The real take-away is have an idea what you expect your workflows to be, 
and configure for that. Keep in mind ZFS's flexibility, and know that a 
system can contain a varied mix of zpools appropriate to multiple use cases.

adam


Kevin Purcell wrote:
> I also had a blog link that I had found and put aside yesterday and  
> read this morning that also addresses this issue in detail. It's  
> funny how that happens.
> 
> <http://blogs.sun.com/roch/entry/when_to_and_not_to>
> 
> The trade offs revolve around bandwidth, IO/s and reliability and the  
> trade-offs between those factors.
> 
> There is no silver bullet (but there are composite bullets than can  
> be selected to be most effective for particular uses).
> 
> Thanks for the clarification.
> 
> On Feb 22, 2008, at 10:55 AM, Brian Pinkerton wrote:
> 
>> He's right.  Richard says it best:
>>
>> http://blogs.sun.com/relling/entry/ 
>> zfs_raid_recommendations_space_performance
>>
>> Bottom line: for performance use RAID-0, for performance and  
>> reliability use RAID-1+0.
>>
>> bri
>>
>> On Feb 22, 2008, at 10:30 AM, Kevin Purcell wrote:
>>
>>> Comment seen on today's Macintouch:
>>>
>>> <http://www.macintouch.com/readerreports/harddrives/
>>> index.html#d22feb2008>
>>>
>>>> Jeff Baker
>>>> The willingness of people to see ZFS as the solution to any and all
>>>> problems continues to amaze me. Contrary to the prior comment, RAID-
>>>> Z in no way obsoletes or replaces the functions of a hardware RAID
>>>> controller. Typically a user expects high performance from a RAID
>>>> controller. RAID-Z, by comparison, is slightly slower than a single
>>>> hard disk. While a normal RAID becomes faster with additional
>>>> spindles, a RAID-Z exhibits flat to slightly declining performance
>>>> with more spindles (on random workloads).
>>> I would have thought that RAID-Z performance would scale as the
>>> number of disks (so long as the controllers can't take multiple
>>> commands in parallel or for IDE as the number of controllers).
>>>
>>> I'd appreciate some comments from those more knowledgeable than I.
>>>
>>> Thanks.
>>> --
>>> Kevin Purcell
>>> kevinpurcell at pobox.com
>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
> 
> --
> Kevin Purcell
> kevinpurcell at pobox.com
> 
> 
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From karl.gyllstrom at gmail.com  Fri Feb 22 12:59:26 2008
From: karl.gyllstrom at gmail.com (Karl Gyllstrom)
Date: Fri, 22 Feb 2008 15:59:26 -0500
Subject: [zfs-discuss] Spotlight(?)
Message-ID: <47E278CB-3745-4C7E-BF0F-60B8B4854A01@gmail.com>

I assume Spotlight is still not working correctly on ZFS drives.  Is  
there any thoughts on when (if ever) this will be supported in the mac  
port?

Google Desktop also fails to work correctly.  While it used to crash  
my computer when crawling my zfs filesystem, it no longer does.   
However, it doesn't seem to be correctly indexing files either.

From karl.gyllstrom at gmail.com  Fri Feb 22 13:01:27 2008
From: karl.gyllstrom at gmail.com (Karl Gyllstrom)
Date: Fri, 22 Feb 2008 16:01:27 -0500
Subject: [zfs-discuss] ~/Library in zfs
Message-ID: <6C77A0AB-2C1B-48C4-9C50-EF50DFE3B7A7@gmail.com>

I've been using zfs since the first seed.  At that point, I had to  
move my ~/Library off the ZFS drive and symlink it because there were  
just too many crashes/poorly functioning apps (i.e., mail search,  
presumably because of spotlight).  I'm wondering whether anyone has  
recent, positive experience with using the Library on the zfs space.

Thanks!

From info at martin-hauser.net  Fri Feb 22 13:23:48 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Fri, 22 Feb 2008 22:23:48 +0100
Subject: [zfs-discuss] ~/Library in zfs
In-Reply-To: <6C77A0AB-2C1B-48C4-9C50-EF50DFE3B7A7@gmail.com>
References: <6C77A0AB-2C1B-48C4-9C50-EF50DFE3B7A7@gmail.com>
Message-ID: <0B271966-CB5B-44EC-BC8A-DE498B72FAEE@martin-hauser.net>

Hello,

On Feb 22, 2008, at 22:01 PM, Karl Gyllstrom wrote:

> I've been using zfs since the first seed.  At that point, I had to
> move my ~/Library off the ZFS drive and symlink it because there were
> just too many crashes/poorly functioning apps (i.e., mail search,
> presumably because of spotlight).  I'm wondering whether anyone has
> recent, positive experience with using the Library on the zfs space.


As far as I can tell and as far as the support seems to be going, is  
that any file that is a normal file, like something that works under  
Linux or Sun (or can be copied from there and back to Os X and still  
work) is good to be stored on ZFS. So for example you .plist files (as  
far as I know they are just some sort of binary xml files) can be  
safely hosted on a zfs volume. The files in the Caches directory  
probably can't. Anything that has Icons will maybe loose them as well.  
In any case everything that relies on resource forks will have a  
problem because, as far as I know, zfs does not yet support them.

On the other Hand, some software seems to fix that kinds of problems  
(recent Itunes update is said to be working good on zfs, haven't tried  
yet... , EyeTV 3.0.1 seems now to work correctly from zfs volumes as  
well... )

Probably it would be a good idea to compose a list of files and  
programms that are known to work with zfs and which don't.... would be  
nice to collect them in some kind of wiki, wouldn't it?

Martin
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080222/bb5d8540/attachment.bin 

From bwaters at nrao.edu  Fri Feb 22 13:44:47 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Fri, 22 Feb 2008 14:44:47 -0700
Subject: [zfs-discuss] ~/Library in zfs
In-Reply-To: <0B271966-CB5B-44EC-BC8A-DE498B72FAEE@martin-hauser.net>
References: <6C77A0AB-2C1B-48C4-9C50-EF50DFE3B7A7@gmail.com>
	<0B271966-CB5B-44EC-BC8A-DE498B72FAEE@martin-hauser.net>
Message-ID: <4195369C-194E-4608-95D6-B8C8A4034C91@nrao.edu>


On Feb 22, 2008, at 2:23 PM, Martin Hauser wrote:

>> (i.e., mail search,
>> presumably because of spotlight)

Yep, that's probably it: Spotlight.



From jim at netgate.com  Fri Feb 22 13:50:45 2008
From: jim at netgate.com (Jim Thompson)
Date: Fri, 22 Feb 2008 11:50:45 -1000
Subject: [zfs-discuss] Macintouch comments on RAID-Z
In-Reply-To: <47BF3659.9070403@comp.lancs.ac.uk>
References: <79696DA3-0B5C-44AA-B045-9FAF0F1A21F4@pobox.com>	<71A35519-A69C-4BF2-8723-B5761ED0EA61@thinkpink.com>
	<FC12AB4C-A6E6-4DB4-9D43-9C6B9153ED5C@pobox.com>
	<47BF3659.9070403@comp.lancs.ac.uk>
Message-ID: <EFD7A5CB-4B72-4847-9962-315645861CBA@netgate.com>


> My take-aways about ideal performance configurations are this:
> If you care enough about your data to use ZFS, don't bother with  
> RAID-0,
>  except in very special circumstances.

Traditionally, the worry is because losing one spindle takes down the  
entire filesystem.

As drives (and arrays) become larger, a whole new issue comes into  
play, the drives are not returning your data, but a statistical model  
of it.

<http://www.c0t0d0s0.org/uploads/endofraid5.pdf>

> If you care about random I/O reads, then mirror (RAID-1-like)
> configurations are the way to go.

If you care about random I/O reads, use a SSD.

> If you care about streaming I/O performance, then RAIDZ (RAID-5 and
> -6-like) configurations are the way to go.


Is anyone @ Sun (or elsewhere) looking at using SSDs for the parity  
drive in a RAID-Z setup?

jim


From hhdave at blueyonder.co.uk  Thu Feb  7 14:22:26 2008
From: hhdave at blueyonder.co.uk (David Ritchie)
Date: Thu, 07 Feb 2008 22:22:26 -0000
Subject: [zfs-discuss] Interesting VMWare issue
Message-ID: <4E009B52-B7AB-444B-83AC-EB99A5E6C59F@blueyonder.co.uk>

Hi,

I've just discovered an interesting issue in VMWare Fusion. I've got a  
VM set up with Windows XP running form a ZFS file system. When I do  
this:-
	1. start the VM in VMWare
	2. suspend
	3. quite vmware
	4. zfs snapshot tank/the-file-system at test
	5. reload the VM, fiddle about a bit, suspend, reload to check it,  
suspend
	6. zfs rollbank tank/the-file-system at test
	7. reload the (now rolled back VM) in VMWare

The VM appears to resume, but just before running again I get a  
message: *** Virtual machine kernel stack fault (hardware reset)

The same happens if I suspend, clone, and then try and resume the clone.

I'll try in Parallels and see what happens there...


-- David


From eableson at gmail.com  Tue Feb  5 07:31:43 2008
From: eableson at gmail.com (Erik ABLESON)
Date: Tue, 05 Feb 2008 15:31:43 -0000
Subject: [zfs-discuss] ZFS/TM/OSXS
In-Reply-To: <b2a3f2d80802050702i31b70c69w107d7fb3b5cecbfa@mail.gmail.com>
References: <06C47470-9126-43DC-A488-B6C0E27243E5@mac.com>
	<B1212313-ED16-41A7-8635-B14180894618@googlemail.com>
	<b2a3f2d80802050702i31b70c69w107d7fb3b5cecbfa@mail.gmail.com>
Message-ID: <87DD00B0-2B7E-4D0E-AD31-9A80FDC0FCB4@gmail.com>

That shouldn't make any difference since the client will be seeing a  
time machine approved share from the server.

The question more about the behaviour of the new sparse image bundles  
instead of a regular sparse image. I did manage to test a SuperDuper  
backup to an image on ZFS which worked perfectly since the filesystem  
only saw one big file. But from what I understand, bundles are just a  
Finder abstraction of a directory so that might not work as well on a  
ZFS backed share.


Erik Ableson
+33 6 80 83 58 28

Le 5 f?vr. 08 ? 16:02, Solra Bizna <sbizna at tejat.net> a ?crit :

> On Feb 5, 2008 6:57 AM, David Ritchie <david299792 at googlemail.com>  
> wrote:
>> If anyone can suggest how I can make TM happy to back up directly to
>> ZFS, I do have spare drives and I'm happy to let you know how it  
>> goes.
> It may possibly work if you touch .com.apple.timemachine.supported on
> the root of the zpool (or maybe even one of the filesystems), but my
> guess is that it would fail in an odd way if it works at all.
> -:sigma.SB
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From eableson at gmail.com  Tue Feb  5 07:50:28 2008
From: eableson at gmail.com (Erik ABLESON)
Date: Tue, 05 Feb 2008 15:50:28 -0000
Subject: [zfs-discuss] ZFS/TM/OSXS
In-Reply-To: <8667C9C3-F67F-4830-9FB3-FD06D82BC6DF@rehnmark.net>
References: <06C47470-9126-43DC-A488-B6C0E27243E5@mac.com>
	<B1212313-ED16-41A7-8635-B14180894618@googlemail.com>
	<8667C9C3-F67F-4830-9FB3-FD06D82BC6DF@rehnmark.net>
Message-ID: <B3A1A32C-B7C5-48EB-9F54-35CEF1DCC54A@gmail.com>

I seem to recall trying that a while ago on an NFS share. Even with  
the unsupported volume hack activated, the Time Machine UI doesn't  
display volumes mounted from disk images (whether they're stored  
locally or remotely) as potential destinations.

Erik Ableson
+33 6 80 83 58 28

Le 5 f?vr. 08 ? 16:35, Robert Rehnmark <robert at rehnmark.net> a  
?crit :

> Would it not work if you create an Image file and put it on the pool?
> Make the image HFS+ and mount/share it ... then put the sparseimage  
> on that.
>
> I'm interrested in doing the same thing but I'm waiting for some  
> parts to get my pool up and running.
>
> /Robert
>
>
>
> 5 feb 2008 kl. 14:57 skrev David Ritchie:
>
>> TM seems to be rather picky about what kind of network volumes  
>> (vis. the underlying file system) it will back up to. I've followed  
>> several hints to try and get it to back up to things other than AFP  
>> shares of HFS+ file systems from OS X, but haven't got it working.  
>> Since it uses sparse images it doesn't seem that it should have  
>> much trouble regardless of the file system.
>>
>> I'm planning on setting up TM backups to my Mac Pro over the  
>> network. At the moment, due to my failure to get TM backing up to  
>> anything else, I'm thinking of having it back up to the HFS+ boot  
>> drive, and then have cron rsync --inplace the sparse images over to  
>> my ZFS volume.
>>
>> If anyone can suggest how I can make TM happy to back up directly  
>> to ZFS, I do have spare drives and I'm happy to let you know how it  
>> goes.
>>
>> (It's not the ideal solution for TM on ZFS, as discussed before,  
>> but it would have many of the advantages).
>>
>> The TM backups over the network happen to sparse image bundles, by  
>> the way, which are a new thing in 10.5. It's basically a directory  
>> containing a bunch of files, and it adds more files as more data  
>> are added to the sparse image. It's pretty handy.
>>
>> -- David
>>
>> On 5 Feb 2008, at 12:03, Erik Ableson wrote:
>>
>>> Slightly off topic and I haven't got any spare drives to test this  
>>> idea.  I realise that Time Machine is flaky on a ZFS volume, but  
>>> I've been looking at the behaviour of Time Machine to an AFP  
>>> network share on an OS X Server.  Instead of directly writing to  
>>> the file system, the backups are in a sparseimage (or a series of  
>>> sparseimage files - I'm still waiting for my first backup to  
>>> complete over the network to see what it looks like).
>>>
>>> Since the object that would be stored on the ZFS filesystem would  
>>> be a simple file, I was thinking that this should work as a  
>>> practical solution for consolidating Time Machine storage with all  
>>> the advantages of using ZFS.
>>>
>>> Has anyone tried this out?
>>>
>>> Cheers,
>>>
>>> Erik
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From btm at pobox.com  Fri Feb 22 09:05:34 2008
From: btm at pobox.com (Brett Ault-McCoy)
Date: Fri, 22 Feb 2008 17:05:34 +0000
Subject: [zfs-discuss] Fwd: encryption
In-Reply-To: <34D3A50C-5BD9-49CF-BC3D-1EA5A2E94607@spamfreemail.de>
References: <913F574E-818C-48F1-B386-94A53848D1E8@spamfreemail.de>
	<34D3A50C-5BD9-49CF-BC3D-1EA5A2E94607@spamfreemail.de>
Message-ID: <7bccd8dc0802220905lcb5b24od8776f9a8607cd41@mail.gmail.com>

You can use a sparseimage to host a zfs pool.  Create the sparseimage in
normal fashion then use diskutil eraseVolume to reformat it as ZFS.  I
believe it'll even create the pool and mount it for you automatically.  A
sparseimage acts like a disk so you can do anything on it that you can do on
a real disk.  I did this before I realized that zfs could just use files
directly.
++Brett;


On Fri, Feb 22, 2008 at 2:04 AM, Franz Schmalzl <
franzschmalzl at spamfreemail.de> wrote:

>
>
> cool thanks !
>
> i wanted to put those images on zfs'ed disks anyway..
> so i have twice the zfs goodness ~)
> >
> >
> >>
> >> On Feb 21, 2008, at 1:40 PM, Franz Schmalzl wrote:
> >>
> >>> so just creating a raidz and laying a sparsebundle with hfs+ inside
> >>> seems pretty pointless to me.
> >>
> >> Well FWIW I've done it, but you're right: the sparsebundle is HFS,
> >> and
> >> lacks all the ZFS management goodness.
> >>
> >> Of course, you can do ZFS snapshots on the file system hosting the
> >> sparsebundle, so you can go back to previous states of the hosted
> >> filesystem, but again not really what you'd like.
> >>
> >> It's not completely useless, but the added utility barely compensates
> >> for the added complexity.
> >>
> >>
> >>> my plan was to create a encrypted sparsebundle with zfs inside, as i
> >>> believe the dmg itself is encrypted ( so it's sort of device level
> >>> encryption, and not filesystem level like with encfs or something )
> >>>
> >>> im a right?
> >>>
> >>> and would it work ?
> >>
> >>
> >> Why not try it?
> >>
> >> Let's create a raidz with four encrypted disk images, instead of four
> >> disks:
> >>
> >>
> >> $ cd /tmp
> >>
> >> $ openssl rand -base64 16 > silly-password.txt
> >>
> >> $ cat silly-password.txt | hdiutil create -size 500m -encryption -
> >> stdinpass -type SPARSEBUNDLE -fs HFS+J -layout NONE -volname noname1
> >> test1
> >> created: /private/tmp/test1.sparsebundle
> >>
> >>
> >> $ cat silly-password.txt | hdiutil attach -encryption -stdinpass -
> >> nomount test1.sparsebundle
> >> /dev/disk9
> >>
> >>
> >> $ diskutil partitiondisk /dev/disk9 GPTFormat ZFS %noformat% 100%
> >> Started partitioning on disk disk9 noname1
> >> Creating partition map
> >> [ + 0%..10%..20%..30%..40%..50%..60%..70%..80%..90%..100% ]
> >> Finished partitioning on disk disk9 noname1
> >> /dev/disk9
> >>   #:                       TYPE NAME                    SIZE
> >> IDENTIFIER
> >>   0:      GUID_partition_scheme noname1                *500.0 Mi
> >> disk9
> >>   1:                        ZFS                         500.0 Mi
> >> disk9s1
> >>
> >>
> >>
> >> Repeat for sparse images named test2, test3, test4
> >>
> >>
> >>
> >> Now
> >>
> >>
> >> $ du -sh *.sparsebundle
> >> 31M  test1.sparsebundle
> >> 31M  test2.sparsebundle
> >> 31M  test3.sparsebundle
> >> 31M  test4.sparsebundle
> >>
> >>
> >> $ zpool create encrypted_images raidz /dev/disk9
> >> disk9    disk9s1
> >>
> >>
> >> $ zpool create encrypted_images raidz disk{9,10,11,12}s1
> >>
> >>
> >> $ zpool status
> >>  pool: encrypted_images
> >> state: ONLINE
> >> status: The pool is formatted using an older on-disk format.  The
> >> pool
> >> can
> >>      still be used, but some features are unavailable.
> >> action: Upgrade the pool using 'zpool upgrade'.  Once this is done,
> >> the
> >>      pool will no longer be accessible on older software versions.
> >> scrub: none requested
> >> config:
> >>
> >>      NAME          STATE     READ WRITE CKSUM
> >>      encrypted_images  ONLINE       0     0     0
> >>        raidz1      ONLINE       0     0     0
> >>          disk9s1   ONLINE       0     0     0
> >>          disk10s1  ONLINE       0     0     0
> >>          disk11s1  ONLINE       0     0     0
> >>          disk12s1  ONLINE       0     0     0
> >>
> >>
> >> $ zpool upgrade encrypted_images
> >> This system is currently running ZFS pool version 8.
> >>
> >> Successfully upgraded 'encrypted_images' from version 6 to version 8
> >>
> >>
> >> $ zfs list
> >> NAME                                 USED  AVAIL  REFER  MOUNTPOINT
> >> encrypted_images                     393K  1.41G   294K  /Volumes/
> >> encrypted_images
> >>
> >>
> >>
> >> W00t!
> >>
> >> Looks like that works!
> >>
> >>
> >> Now what you want to do, is to make four different disks, each with
> >> HFS
> >> +J on them, create a sparseimage on each disk that is as large as the
> >> available space on the filesystem. Note that initially the
> >> sparseimage
> >> won't take up all the space. Then create your raidz and have at it!
> >>
> >>
> >> Be sure to put all your boss' data on there, and tell me how it works
> >> out.
> >>
> >> Seriously, I expect it to be a mite slower than device-backed raidz,
> >> but it just might work. Or you might run into no end of kernel
> >> deadlocks and your hair might fall out. But I think it'll work!
> >>
> >> Cool!
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >> _______________________________________________
> >> zfs-discuss mailing list
> >> zfs-discuss at lists.macosforge.org
> >> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080222/f5384c75/attachment-0001.html 

From btm at pobox.com  Fri Feb 22 16:14:34 2008
From: btm at pobox.com (Brett Ault-McCoy)
Date: Sat, 23 Feb 2008 00:14:34 +0000
Subject: [zfs-discuss] Macintouch comments on RAID-Z
In-Reply-To: <EFD7A5CB-4B72-4847-9962-315645861CBA@netgate.com>
References: <79696DA3-0B5C-44AA-B045-9FAF0F1A21F4@pobox.com>
	<71A35519-A69C-4BF2-8723-B5761ED0EA61@thinkpink.com>
	<FC12AB4C-A6E6-4DB4-9D43-9C6B9153ED5C@pobox.com>
	<47BF3659.9070403@comp.lancs.ac.uk>
	<EFD7A5CB-4B72-4847-9962-315645861CBA@netgate.com>
Message-ID: <7bccd8dc0802221614n197b907fua41178486f1fcf19@mail.gmail.com>

On Fri, Feb 22, 2008 at 9:50 PM, Jim Thompson <jim at netgate.com> wrote:

> Is anyone @ Sun (or elsewhere) looking at using SSDs for the parity
> drive in a RAID-Z setup?


RAID-Z does not have a parity disk.  The parity is striped across the disks.
 Each drive contains 1/N of the parity, where N is the drives in the RAID-Z
set.  This is the same as with RAID-5.

RAID-4 has disks dedicated to parity.  SSD might help there since it would
be multiple times faster than a normal disk so would be able to handle
parity updates for changes to multiple drives.

++Brett;
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080223/1b7c96e1/attachment.html 

From btm at pobox.com  Fri Feb 22 16:21:16 2008
From: btm at pobox.com (Brett Ault-McCoy)
Date: Sat, 23 Feb 2008 00:21:16 +0000
Subject: [zfs-discuss] Macintouch comments on RAID-Z
In-Reply-To: <79696DA3-0B5C-44AA-B045-9FAF0F1A21F4@pobox.com>
References: <79696DA3-0B5C-44AA-B045-9FAF0F1A21F4@pobox.com>
Message-ID: <7bccd8dc0802221621p53c0501ai9c7945de752ec0bb@mail.gmail.com>

The only reason hardware raid controllers ever give better performance is
because they put a huge amount of battery backed RAM on them.  Since they
are basically just CPU's like in any computer any real benefit comes from
offloading the CPU.  In something like a modern Mac Pro with 8 cores
offloading to an external CPU isn't necessarily a gain (unless your workload
can peg all 8 cores in your machine).
The problem with most RAID controllers is that there is no guarantee of on
disk data consistancy.  Writes go to RAM and are then written to disk at
some later time in the most efficient order.  A failure of the controller
can render all of the disks connected to it corrupt.  A failure of the
battery can do the same thing in a power outage.  Batteries can seem
perfectly fine right up to the point you put a load on them.

ZFS RAID-Z claims to ensure on-disk consistancy at all times.  You don't get
this for free.  Anyone who tells you otherwise is lieing.  If your data is
really important, use mirrors (with N>2 if it's really, really important).
 RAID was a good idea when inexpensive disks were still really expensive.
 Now with 1T disks costing a few hundred dollars it just doesn't seem
worthwhile to me anymore.

++Brett;


On Fri, Feb 22, 2008 at 6:30 PM, Kevin Purcell <kevinpurcell at pobox.com>
wrote:

> Comment seen on today's Macintouch:
>
> <http://www.macintouch.com/readerreports/harddrives/
> index.html#d22feb2008>
>
> > Jeff Baker
> > The willingness of people to see ZFS as the solution to any and all
> > problems continues to amaze me. Contrary to the prior comment, RAID-
> > Z in no way obsoletes or replaces the functions of a hardware RAID
> > controller. Typically a user expects high performance from a RAID
> > controller. RAID-Z, by comparison, is slightly slower than a single
> > hard disk. While a normal RAID becomes faster with additional
> > spindles, a RAID-Z exhibits flat to slightly declining performance
> > with more spindles (on random workloads).
>
> I would have thought that RAID-Z performance would scale as the
> number of disks (so long as the controllers can't take multiple
> commands in parallel or for IDE as the number of controllers).
>
> I'd appreciate some comments from those more knowledgeable than I.
>
> Thanks.
> --
> Kevin Purcell
> kevinpurcell at pobox.com
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080223/a6ad6a14/attachment.html 

From btm at pobox.com  Fri Feb 22 16:22:13 2008
From: btm at pobox.com (Brett Ault-McCoy)
Date: Sat, 23 Feb 2008 00:22:13 +0000
Subject: [zfs-discuss] Spotlight(?)
In-Reply-To: <47E278CB-3745-4C7E-BF0F-60B8B4854A01@gmail.com>
References: <47E278CB-3745-4C7E-BF0F-60B8B4854A01@gmail.com>
Message-ID: <7bccd8dc0802221622s3b3ad566hadf69440db44cf9e@mail.gmail.com>

Google Desktop uses the data from Spotlight.  If Spotlight doesn't work,
Google Desktop doesn't work.  If Spotlight works, Google Desktop should
work.
++Brett;


On Fri, Feb 22, 2008 at 8:59 PM, Karl Gyllstrom <karl.gyllstrom at gmail.com>
wrote:

> I assume Spotlight is still not working correctly on ZFS drives.  Is
> there any thoughts on when (if ever) this will be supported in the mac
> port?
>
> Google Desktop also fails to work correctly.  While it used to crash
> my computer when crawling my zfs filesystem, it no longer does.
> However, it doesn't seem to be correctly indexing files either.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080223/b68eb1b9/attachment.html 

From zfs-discuss at openhealth.org  Sat Feb 23 06:49:40 2008
From: zfs-discuss at openhealth.org (Jonathan Borden)
Date: Sat, 23 Feb 2008 09:49:40 -0500
Subject: [zfs-discuss] ~/Library in zfs
In-Reply-To: <0B271966-CB5B-44EC-BC8A-DE498B72FAEE@martin-hauser.net>
References: <6C77A0AB-2C1B-48C4-9C50-EF50DFE3B7A7@gmail.com>
	<0B271966-CB5B-44EC-BC8A-DE498B72FAEE@martin-hauser.net>
Message-ID: <7a0bc8420802230649i5621f82bo62fe391e9438ecb4@mail.gmail.com>

Martin Hauser <info at martin-hauser.net> wrote:

>
>  On the other Hand, some software seems to fix that kinds of problems
>  (recent Itunes update is said to be working good on zfs, haven't tried
>  yet... , EyeTV 3.0.1 seems now to work correctly from zfs volumes as
>  well... )
>

I am not sure what problems you are having with EyeTV and ZFS; however
the issue I have, not being able to compress and EyeTV file located on
a zpool, persists in 3.0.1. Otherwise EyeTV has generally worked for
me with ZFS in 3.0.

Jonathan

From oliver.otway at gmail.com  Sat Feb 23 07:45:36 2008
From: oliver.otway at gmail.com (Oliver Otway)
Date: Sun, 24 Feb 2008 01:15:36 +0930
Subject: [zfs-discuss] emptying the Trash
In-Reply-To: <mailman.952.1203778131.8166.zfs-discuss@lists.macosforge.org>
References: <mailman.952.1203778131.8166.zfs-discuss@lists.macosforge.org>
Message-ID: <09C0CAF7-AF57-4A2F-A251-D5C4FAD72ED5@gmail.com>

Hey, if you got bored of the trash issue
edit and copy the following into your .profile



Ztrash()
	{
	cd /Volumes/?Your zpool?/.Trashes/503/
	sudo rm -fr *

	cd /Volumes/?Your zpool?/?ZFS_Volume01?/.Trashes/503/
	sudo rm -fr *

#	For more ZFS FS uncomment and edit below
#	cd /Volumes/?Your zpool?/?ZFS_Volume02?/.Trashes/503/
#	sudo rm -fr *
#	cd /Volumes/?Your zpool?/?ZFS_Volume03?/.Trashes/503/
#	sudo rm -fr *
#	cd /Volumes/?Your zpool?/?ZFS_Volume04?/.Trashes/503/
#	sudo rm -fr *
	}



where
	?Your zpool? is the name of your pool
	 ?ZFS_Volume01? is the name of your ZFS Volume

then next time you login to bash, type Ztrash to empty the trash on
the zpool and any FS in the pool

From ndellofano at apple.com  Sat Feb 23 18:54:11 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Sat Feb 23 18:55:45 2008
Subject: [zfs-discuss] Fwd: encryption
In-Reply-To: <7bccd8dc0802220905lcb5b24od8776f9a8607cd41@mail.gmail.com>
References: <913F574E-818C-48F1-B386-94A53848D1E8@spamfreemail.de>
	<34D3A50C-5BD9-49CF-BC3D-1EA5A2E94607@spamfreemail.de>
	<7bccd8dc0802220905lcb5b24od8776f9a8607cd41@mail.gmail.com>
Message-ID: <75B86980-D671-4E7B-8556-79249CB026B9@apple.com>

yeah as Brett mentioned below, the simplest thing to do, especially if  
you're lazy (like me :)  )  :

#mkfile 128M /var/tmp/filevdev
#sudo zpool create whirl /var/tmp/filevdev

This will create a zfs pool that will use the file as a vdev/backing  
store.  One thing to mention (on the FAQ) is that you should do a  
'zpool export -f whirl'  before you reboot.  Otherwise OSX still sees  
that file as being open and in use and hence will stall the shutdown  
process.  We're working to streamline this.

Noel

On Feb 22, 2008, at 9:05 AM, Brett Ault-McCoy wrote:

> You can use a sparseimage to host a zfs pool.  Create the  
> sparseimage in normal fashion then use diskutil eraseVolume to  
> reformat it as ZFS.  I believe it'll even create the pool and mount  
> it for you automatically.  A sparseimage acts like a disk so you can  
> do anything on it that you can do on a real disk.  I did this before  
> I realized that zfs could just use files directly.
>
> ++Brett;
>
>
> On Fri, Feb 22, 2008 at 2:04 AM, Franz Schmalzl <franzschmalzl@spamfreemail.de 
> > wrote:
>
>
> cool thanks !
>
> i wanted to put those images on zfs'ed disks anyway..
> so i have twice the zfs goodness ~)
> >
> >
> >>
> >> On Feb 21, 2008, at 1:40 PM, Franz Schmalzl wrote:
> >>
> >>> so just creating a raidz and laying a sparsebundle with hfs+  
> inside
> >>> seems pretty pointless to me.
> >>
> >> Well FWIW I've done it, but you're right: the sparsebundle is HFS,
> >> and
> >> lacks all the ZFS management goodness.
> >>
> >> Of course, you can do ZFS snapshots on the file system hosting the
> >> sparsebundle, so you can go back to previous states of the hosted
> >> filesystem, but again not really what you'd like.
> >>
> >> It's not completely useless, but the added utility barely  
> compensates
> >> for the added complexity.
> >>
> >>
> >>> my plan was to create a encrypted sparsebundle with zfs inside,  
> as i
> >>> believe the dmg itself is encrypted ( so it's sort of device level
> >>> encryption, and not filesystem level like with encfs or  
> something )
> >>>
> >>> im a right?
> >>>
> >>> and would it work ?
> >>
> >>
> >> Why not try it?
> >>
> >> Let's create a raidz with four encrypted disk images, instead of  
> four
> >> disks:
> >>
> >>
> >> $ cd /tmp
> >>
> >> $ openssl rand -base64 16 > silly-password.txt
> >>
> >> $ cat silly-password.txt | hdiutil create -size 500m -encryption -
> >> stdinpass -type SPARSEBUNDLE -fs HFS+J -layout NONE -volname  
> noname1
> >> test1
> >> created: /private/tmp/test1.sparsebundle
> >>
> >>
> >> $ cat silly-password.txt | hdiutil attach -encryption -stdinpass -
> >> nomount test1.sparsebundle
> >> /dev/disk9
> >>
> >>
> >> $ diskutil partitiondisk /dev/disk9 GPTFormat ZFS %noformat% 100%
> >> Started partitioning on disk disk9 noname1
> >> Creating partition map
> >> [ + 0%..10%..20%..30%..40%..50%..60%..70%..80%..90%..100% ]
> >> Finished partitioning on disk disk9 noname1
> >> /dev/disk9
> >>   #:                       TYPE NAME                    SIZE
> >> IDENTIFIER
> >>   0:      GUID_partition_scheme noname1                *500.0 Mi
> >> disk9
> >>   1:                        ZFS                         500.0 Mi
> >> disk9s1
> >>
> >>
> >>
> >> Repeat for sparse images named test2, test3, test4
> >>
> >>
> >>
> >> Now
> >>
> >>
> >> $ du -sh *.sparsebundle
> >> 31M  test1.sparsebundle
> >> 31M  test2.sparsebundle
> >> 31M  test3.sparsebundle
> >> 31M  test4.sparsebundle
> >>
> >>
> >> $ zpool create encrypted_images raidz /dev/disk9
> >> disk9    disk9s1
> >>
> >>
> >> $ zpool create encrypted_images raidz disk{9,10,11,12}s1
> >>
> >>
> >> $ zpool status
> >>  pool: encrypted_images
> >> state: ONLINE
> >> status: The pool is formatted using an older on-disk format.  The
> >> pool
> >> can
> >>      still be used, but some features are unavailable.
> >> action: Upgrade the pool using 'zpool upgrade'.  Once this is done,
> >> the
> >>      pool will no longer be accessible on older software versions.
> >> scrub: none requested
> >> config:
> >>
> >>      NAME          STATE     READ WRITE CKSUM
> >>      encrypted_images  ONLINE       0     0     0
> >>        raidz1      ONLINE       0     0     0
> >>          disk9s1   ONLINE       0     0     0
> >>          disk10s1  ONLINE       0     0     0
> >>          disk11s1  ONLINE       0     0     0
> >>          disk12s1  ONLINE       0     0     0
> >>
> >>
> >> $ zpool upgrade encrypted_images
> >> This system is currently running ZFS pool version 8.
> >>
> >> Successfully upgraded 'encrypted_images' from version 6 to  
> version 8
> >>
> >>
> >> $ zfs list
> >> NAME                                 USED  AVAIL  REFER  MOUNTPOINT
> >> encrypted_images                     393K  1.41G   294K  /Volumes/
> >> encrypted_images
> >>
> >>
> >>
> >> W00t!
> >>
> >> Looks like that works!
> >>
> >>
> >> Now what you want to do, is to make four different disks, each with
> >> HFS
> >> +J on them, create a sparseimage on each disk that is as large as  
> the
> >> available space on the filesystem. Note that initially the
> >> sparseimage
> >> won't take up all the space. Then create your raidz and have at it!
> >>
> >>
> >> Be sure to put all your boss' data on there, and tell me how it  
> works
> >> out.
> >>
> >> Seriously, I expect it to be a mite slower than device-backed  
> raidz,
> >> but it just might work. Or you might run into no end of kernel
> >> deadlocks and your hair might fall out. But I think it'll work!
> >>
> >> Cool!
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >> _______________________________________________
> >> zfs-discuss mailing list
> >> zfs-discuss@lists.macosforge.org
> >> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080223/711637ea/attachment.html
From ndellofano at apple.com  Sat Feb 23 18:56:32 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Sat Feb 23 18:58:05 2008
Subject: [zfs-discuss] Spotlight(?)
In-Reply-To: <7bccd8dc0802221622s3b3ad566hadf69440db44cf9e@mail.gmail.com>
References: <47E278CB-3745-4C7E-BF0F-60B8B4854A01@gmail.com>
	<7bccd8dc0802221622s3b3ad566hadf69440db44cf9e@mail.gmail.com>
Message-ID: <86FE745A-A4AC-4CA6-B269-55973146DC19@apple.com>

Spotlight still doesn't work correctly on ZFS (see the ZFS FAQ).   
However, we are working on fixing this currently and we are aiming to  
have it  be supported in the future.

Noel

On Feb 22, 2008, at 4:22 PM, Brett Ault-McCoy wrote:

> Google Desktop uses the data from Spotlight.  If Spotlight doesn't  
> work, Google Desktop doesn't work.  If Spotlight works, Google  
> Desktop should work.
>
> ++Brett;
>
>
> On Fri, Feb 22, 2008 at 8:59 PM, Karl Gyllstrom <karl.gyllstrom@gmail.com 
> > wrote:
> I assume Spotlight is still not working correctly on ZFS drives.  Is
> there any thoughts on when (if ever) this will be supported in the mac
> port?
>
> Google Desktop also fails to work correctly.  While it used to crash
> my computer when crawling my zfs filesystem, it no longer does.
> However, it doesn't seem to be correctly indexing files either.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080223/5954cc15/attachment.html
From ndellofano at apple.com  Sat Feb 23 19:04:42 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Sat Feb 23 19:06:16 2008
Subject: [zfs-discuss] ~/Library in zfs
In-Reply-To: <0B271966-CB5B-44EC-BC8A-DE498B72FAEE@martin-hauser.net>
References: <6C77A0AB-2C1B-48C4-9C50-EF50DFE3B7A7@gmail.com>
	<0B271966-CB5B-44EC-BC8A-DE498B72FAEE@martin-hauser.net>
Message-ID: <C2DD3B0D-DDC6-4705-AF57-5556CE998397@apple.com>

So ZFS supports all extended attributes and handles them correctly  
currently.  I run my home directory off ZFS (so *all* my files are  
hosted off of a ZFS pool and this works fine).  Spotlight  
functionality however is a different matter all together and something  
we are working on.
But currently, you can store any file on ZFS that has extended  
attributes just fine(this includes anything with a resource fork),  it  
has full extended attribute support.

Noel


On Feb 22, 2008, at 1:23 PM, Martin Hauser wrote:

> Hello,
>
> On Feb 22, 2008, at 22:01 PM, Karl Gyllstrom wrote:
>
>> I've been using zfs since the first seed.  At that point, I had to
>> move my ~/Library off the ZFS drive and symlink it because there were
>> just too many crashes/poorly functioning apps (i.e., mail search,
>> presumably because of spotlight).  I'm wondering whether anyone has
>> recent, positive experience with using the Library on the zfs space.
>
>
> As far as I can tell and as far as the support seems to be going, is  
> that any file that is a normal file, like something that works under  
> Linux or Sun (or can be copied from there and back to Os X and still  
> work) is good to be stored on ZFS. So for example you .plist files  
> (as far as I know they are just some sort of binary xml files) can  
> be safely hosted on a zfs volume. The files in the Caches directory  
> probably can't. Anything that has Icons will maybe loose them as  
> well. In any case everything that relies on resource forks will have  
> a problem because, as far as I know, zfs does not yet support them.
>
> On the other Hand, some software seems to fix that kinds of problems  
> (recent Itunes update is said to be working good on zfs, haven't  
> tried yet... , EyeTV 3.0.1 seems now to work correctly from zfs  
> volumes as well... )
>
> Probably it would be a good idea to compose a list of files and  
> programms that are known to work with zfs and which don't.... would  
> be nice to collect them in some kind of wiki, wouldn't it?
>
> Martin
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From zfs-discuss at openhealth.org  Sat Feb 23 20:41:55 2008
From: zfs-discuss at openhealth.org (Jonathan Borden)
Date: Sat Feb 23 20:41:57 2008
Subject: [zfs-discuss] Can a subdirectory be converted into a filesystem?
Message-ID: <7a0bc8420802232041h2cd712e4x89781de6cf770a05@mail.gmail.com>

I decided that I wanted to take a snapshot of a particular
subdirectory that I have which is a subdirectory of a filesystem on a
zpool. No such luck. Duh! I realize that it needs to be a filesystem
rather than a subdirectory. Is there a way to convert a subdirectory
into a filesystem without employing the brute force technique of
creating  a new filesytem, copying the subdir into the filesystem and
then deleting the subdir?

Jonathan
From danchr at daimi.au.dk  Sun Feb 24 03:55:26 2008
From: danchr at daimi.au.dk (Dan Villiom Podlaski Christiansen)
Date: Sun, 24 Feb 2008 12:55:26 +0100
Subject: [zfs-discuss] emptying the Trash
In-Reply-To: <09C0CAF7-AF57-4A2F-A251-D5C4FAD72ED5@gmail.com>
References: <mailman.952.1203778131.8166.zfs-discuss@lists.macosforge.org>
	<09C0CAF7-AF57-4A2F-A251-D5C4FAD72ED5@gmail.com>
Message-ID: <8DC4F6B9-E689-41EC-ADE8-F00D5A180A6C@daimi.au.dk>

On 23 Feb 2008, at 16:45, Oliver Otway wrote:

> Hey, if you got bored of the trash issue
> edit and copy the following into your .profile

?

Here's another version which will get your filesystems from the  
excellent ZFS command line utilities:
> function ztrash () {
>   for dir in $(zfs list -H -t filesystem -o mountpoint); do
>     if [[ $dir = /* && -d $dir/.Trashes/$UID ]]; then
>       find $dir/.Trashes/$UID -mindepth 1 -delete -print
>     fi
>   done
> }

Written for the Z shell, but also works in Bash and the Korn shell.  
Also, it doesn't use capital letters; they're evil.

--

Dan Villiom Podlaski Christiansen
stud.scient., danchr at daimi.au.dk

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 1945 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080224/e1014fc8/attachment.bin 

From franzschmalzl at spamfreemail.de  Sun Feb 24 08:59:08 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sun, 24 Feb 2008 17:59:08 +0100
Subject: [zfs-discuss] zpool export
Message-ID: <E837B12D-404E-482E-9B07-B86E8D02D46C@spamfreemail.de>

i have to do a zpool export after unmounting a pool, i know
(kernel panic when replugging the underlaying device if i don't)

are there plans to change this behaviour ?
(maybe an automatic export )

regards 

From jbsnyder at gmail.com  Sun Feb 24 14:00:59 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Sun, 24 Feb 2008 16:00:59 -0600
Subject: [zfs-discuss] Using Mirror for Automatic Backup?
Message-ID: <33644d3c0802241400x2c1b8cfn6b262d4b9e916f69@mail.gmail.com>

So, I'm interested in playing around with ZFS a little, since I've got
2 macs running leopard.  My question is related to doing automatic
backups?

When reading the FAQ, I noticed the "Unplugging my USB drive with my
ZFS pool on it causes a panic" question, and saw the comment about
this being the case unless a volume is mirror or raidz.  Now, I
understand that this is mainly designed for dealing with swapping
drives out under failure conditions, but it go me thinking about
another possibility.  Could one take say take a directory on a laptop
and have that be a zfs volume that is based off of a partition on the
internal drive and a partition on an external USB drive and basically
plug into that drive whenever it is physically available to do a
backup, and then most other times when untethered only use the
internal partition?  With a typical raid situation this, would require
a full sync to take place on re-attaching the drive, but I was
wondering if ZFS might handle this differently?  Also, I assume that
the external drive could be used by itself as a recovery source if the
internal drive blows up and requires replacement.

Is this a bad idea, should I just use zfs send if I want backup?  If
the proposed idea isn't a bad idea, is there a command which should be
executed prior to disconnecting the external drive to keep the state
on there slightly more sane or known?

-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From jbsnyder at gmail.com  Tue Feb 26 06:00:12 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Tue, 26 Feb 2008 08:00:12 -0600
Subject: [zfs-discuss] Using Mirror for Automatic Backup?
In-Reply-To: <33644d3c0802241400x2c1b8cfn6b262d4b9e916f69@mail.gmail.com>
References: <33644d3c0802241400x2c1b8cfn6b262d4b9e916f69@mail.gmail.com>
Message-ID: <33644d3c0802260600m67b668fh235e4d42cdae8ed4@mail.gmail.com>

I see that someone has done this with Solaris at least, I wonder if
the resilvering/re-mirroring will take long with a hard disk instead
of a stick?
http://solarisdesktop.blogspot.com/2007/02/stick-to-zfs-or-laptop-with-mirrored.html

Anyone tried this with OS X?  I think I may try this today.

On Sun, Feb 24, 2008 at 4:00 PM, James Snyder <jbsnyder at gmail.com> wrote:
> So, I'm interested in playing around with ZFS a little, since I've got
>  2 macs running leopard.  My question is related to doing automatic
>  backups?
>
>  When reading the FAQ, I noticed the "Unplugging my USB drive with my
>  ZFS pool on it causes a panic" question, and saw the comment about
>  this being the case unless a volume is mirror or raidz.  Now, I
>  understand that this is mainly designed for dealing with swapping
>  drives out under failure conditions, but it go me thinking about
>  another possibility.  Could one take say take a directory on a laptop
>  and have that be a zfs volume that is based off of a partition on the
>  internal drive and a partition on an external USB drive and basically
>  plug into that drive whenever it is physically available to do a
>  backup, and then most other times when untethered only use the
>  internal partition?  With a typical raid situation this, would require
>  a full sync to take place on re-attaching the drive, but I was
>  wondering if ZFS might handle this differently?  Also, I assume that
>  the external drive could be used by itself as a recovery source if the
>  internal drive blows up and requires replacement.
>
>  Is this a bad idea, should I just use zfs send if I want backup?  If
>  the proposed idea isn't a bad idea, is there a command which should be
>  executed prior to disconnecting the external drive to keep the state
>  on there slightly more sane or known?
>
>  --
>  James Snyder
>  Biomedical Engineering
>  Northwestern University
>  jbsnyder at gmail.com
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From William.Winnett at Sun.COM  Tue Feb 26 11:22:52 2008
From: William.Winnett at Sun.COM (Bill Winnett)
Date: Tue, 26 Feb 2008 14:22:52 -0500
Subject: [zfs-discuss] ZFS Case Sensitivity and file system
Message-ID: <DCAA5007-56E7-404B-B1F5-850AE34D40BF@sun.com>


Hi,

Are there plans to add a case insensitive flag to ZFS for OS X?

-bill winnett

From gagix at mac.com  Tue Feb 26 11:47:07 2008
From: gagix at mac.com (Gautam Godse)
Date: Tue, 26 Feb 2008 11:47:07 -0800
Subject: [zfs-discuss] My ZFS experiments
Message-ID: <47C46CBB.3080102@mac.com>

hi folks,
I am a filesystem maven and I really enjoy experimenting and evaluating 
new filesystems. So when ZFS came out in Leopard I immediately read most 
of the available literature/blogs/mailing lists on ZFS and then started 
experimenting with it.

Here is my current setup:
I got a 8 drive cage from Fry's 
(http://www.datoptic.com/cgi-bin/web.cgi?product=eBOX-U&detail=yes) 
which has a USB 2.0 port to connect it to my mac. [ I know, single 
controller, single point of failure, but I think good enough for my 
purpose outlined below]

My current zpool is as follows:

GTank - 4 x 500 GB - raidz disks

I have 2 x 300gb disks that I want to add to the raidz GTank pool above. 
But as I understand from previous postings in the list I would need at 
least 2 more 300 GB disks to setup a second raidz array and then I would 
be able to add it to GTank. Is that right ?

Now, if instead i had the following configuration:

GTank - 4 x 500GB - mirror disks

then would I be able to do the following:

# zpool add GTank mirror disk4 disk 5

where disk4 &5 are the 300GB disks..

My primary purpose of creating the zpool is to store Media files 
(movies, pictures, songs). These would not change a lot. 90% of the time 
there would be only READ activity on the disks and occasionally when I 
have new media I would WRITE to the disks.

In this scenario what is better ? Raidz or Mirror ??

--
Gautam Godse

From gagix at mac.com  Tue Feb 26 15:12:00 2008
From: gagix at mac.com (Gautam Godse)
Date: Tue, 26 Feb 2008 15:12:00 -0800
Subject: [zfs-discuss] My ZFS experiments
In-Reply-To: <92a044b0802261452n102ecff5jcd8064a01fed8295@mail.gmail.com>
References: <47C46CBB.3080102@mac.com>
	<92a044b0802261452n102ecff5jcd8064a01fed8295@mail.gmail.com>
Message-ID: <47C49CC0.8060404@mac.com>

Hmm. That is interesting. So how is data written across all 6 disks in 
the pool ?
Would zfs break up the file and write it partially on the raidz disk and 
partially on the mirror disks ? What happens wehn both the mirrors fail 
? Would zfs recover the file/selfheal from the data written on the raidz 
disks ?
I would be curious to know..

Paul Lenhardt wrote:
> Actually, you can combine Raidz and Mirror together.  You could do the
> following;
> Setup your GTank with 4 x 500GB as Raidz.  Then add the 300GB disks as
> a mirror with your command:
>
> # zpool add GTank mirror disk4 disk5
>
> Where disk4 & 5 are 300GB disks.
>
> I am not sure if, for performance, it is better to have them all Raidz
> or Mirrored.
>
> -Paul Lenhardt
>
> On Tue, Feb 26, 2008 at 1:47 PM, Gautam Godse <gagix at mac.com> wrote:
>   
>> hi folks,
>>  I am a filesystem maven and I really enjoy experimenting and evaluating
>>  new filesystems. So when ZFS came out in Leopard I immediately read most
>>  of the available literature/blogs/mailing lists on ZFS and then started
>>  experimenting with it.
>>
>>  Here is my current setup:
>>  I got a 8 drive cage from Fry's
>>  (http://www.datoptic.com/cgi-bin/web.cgi?product=eBOX-U&detail=yes)
>>  which has a USB 2.0 port to connect it to my mac. [ I know, single
>>  controller, single point of failure, but I think good enough for my
>>  purpose outlined below]
>>
>>  My current zpool is as follows:
>>
>>  GTank - 4 x 500 GB - raidz disks
>>
>>  I have 2 x 300gb disks that I want to add to the raidz GTank pool above.
>>  But as I understand from previous postings in the list I would need at
>>  least 2 more 300 GB disks to setup a second raidz array and then I would
>>  be able to add it to GTank. Is that right ?
>>
>>  Now, if instead i had the following configuration:
>>
>>  GTank - 4 x 500GB - mirror disks
>>
>>  then would I be able to do the following:
>>
>>  # zpool add GTank mirror disk4 disk 5
>>
>>  where disk4 &5 are the 300GB disks..
>>
>>  My primary purpose of creating the zpool is to store Media files
>>  (movies, pictures, songs). These would not change a lot. 90% of the time
>>  there would be only READ activity on the disks and occasionally when I
>>  have new media I would WRITE to the disks.
>>
>>  In this scenario what is better ? Raidz or Mirror ??
>>
>>  --
>>  Gautam Godse
>>  _______________________________________________
>>  zfs-discuss mailing list
>>  zfs-discuss at lists.macosforge.org
>>  http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>>     

From sbizna at tejat.net  Wed Feb 27 03:07:41 2008
From: sbizna at tejat.net (Solra Bizna)
Date: Wed, 27 Feb 2008 04:07:41 -0700
Subject: [zfs-discuss] ZFS Case Sensitivity and file system
In-Reply-To: <6EA5A7F4-C390-422A-9B69-CB267B2BF122@Sun.COM>
References: <DCAA5007-56E7-404B-B1F5-850AE34D40BF@sun.com>
	<b2a3f2d80802262049n4a232f10q9c5053b82b20037e@mail.gmail.com>
	<6EA5A7F4-C390-422A-9B69-CB267B2BF122@Sun.COM>
Message-ID: <b2a3f2d80802270307ue5fb3ddw33b62907e5306ed5@mail.gmail.com>

Ack, forgot to be cc'ing the list.

On Wed, Feb 27, 2008 at 3:55 AM, Bill Winnett <William.Winnett at sun.com> wrote:
> On Feb 26, 2008, at 11:49 PM, Solra Bizna wrote:
> On Tue, Feb 26, 2008 at 12:22 PM, Bill Winnett <William.Winnett at sun.com>
> wrote:
>  Are there plans to add a case insensitive flag to ZFS for OS X?
> Case insensitivity (in filesystems and in general) is actually very
> complicated. Support for insensitivity is a significant source of
> headaches for outside implementors of HFS+, for instance. As such, I
> strongly doubt it would be added (though it's not impossible).
> Simple ASCII case folding is easy enough but that would break on any
> non-English system.
> -:sigma.SB
> Mac OS file systems (HFS Standard and HFS Plus, aka HFS Extended) have
> traditionally been "case-insensitive" file systems.
> Also, the windows world is case insensitive.  Currently, a OS X Server can
> serve up filesystems that Windows/NT can use without
> any problems.  Without a case-insensitive flag for ZFS , a truly transparent
> ZFS filesystem for serving Windows/NT won't exist.
> It is also worth noting that OS X itself recommends the use of
> case-insensitive HFS+ for the startup/boot partition filesystem, up
> until 10.3 OS X did not have the ability to even make a case sensitive HFS
> filesystem.
This is all true. I'd forgotten about Windows's case insensitivity,
which is much more deeply ingrained than that of Mac OS. In light of
that, it's not so improbable but still very complicated.
-:sigma.SB

From William.Winnett at Sun.COM  Wed Feb 27 03:28:02 2008
From: William.Winnett at Sun.COM (Bill Winnett)
Date: Wed, 27 Feb 2008 06:28:02 -0500
Subject: [zfs-discuss] Fwd:  ZFS Case Sensitivity and file system
References: <6EA5A7F4-C390-422A-9B69-CB267B2BF122@Sun.COM>
Message-ID: <D777D5A6-3B18-4365-AE52-9F40E4AFB4E4@sun.com>



Begin forwarded message:

> From: Bill Winnett <William.Winnett at Sun.COM>
> Date: February 27, 2008 5:55:38 AM EST
> To: Solra Bizna <sbizna at tejat.net>
> Subject: Re: [zfs-discuss] ZFS Case Sensitivity and file system
>
>
> On Feb 26, 2008, at 11:49 PM, Solra Bizna wrote:
>
>> On Tue, Feb 26, 2008 at 12:22 PM, Bill Winnett <William.Winnett at sun.com 
>> > wrote:
>>> Are there plans to add a case insensitive flag to ZFS for OS X?
>> Case insensitivity (in filesystems and in general) is actually very
>> complicated. Support for insensitivity is a significant source of
>> headaches for outside implementors of HFS+, for instance. As such, I
>> strongly doubt it would be added (though it's not impossible).
>> Simple ASCII case folding is easy enough but that would break on any
>> non-English system.
>> -:sigma.SB
>
>
> Mac OS file systems (HFS Standard and HFS Plus, aka HFS Extended)  
> have traditionally been "case-insensitive" file systems.
> Also, the windows world is case insensitive.  Currently, a OS X  
> Server can serve up filesystems that Windows/NT can use without
> any problems.  Without a case-insensitive flag for ZFS , a truly  
> transparent  ZFS filesystem for serving Windows/NT won't exist.
> It is also worth noting that OS X itself recommends the use of case- 
> insensitive HFS+ for the startup/boot partition filesystem, up
> until 10.3 OS X did not have the ability to even make a case  
> sensitive HFS filesystem.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080227/fb51dcc0/attachment.html 

From simon.crosland at googlemail.com  Wed Feb 27 09:06:23 2008
From: simon.crosland at googlemail.com (Simon Crosland)
Date: Wed, 27 Feb 2008 17:06:23 +0000
Subject: [zfs-discuss] ZFS Case Sensitivity and file system
In-Reply-To: <DCAA5007-56E7-404B-B1F5-850AE34D40BF@sun.com>
References: <DCAA5007-56E7-404B-B1F5-850AE34D40BF@sun.com>
Message-ID: <4d87168d0802270906i2f88d6aembb53a2f70562c7b9@mail.gmail.com>

On 26/02/2008, Bill Winnett <William.Winnett at sun.com> wrote:
>
>  Are there plans to add a case insensitive flag to ZFS for OS X?

Since case insensitivity is being added to ZFS on OpenSolaris as part
of the CIFS server project, I guess it is possible that it could be
forward ported to OS X. However, if you look at the amount of change
that was necessary for that on OpenSolaris, running through not just
ZFS but the VFS and other parts of the kernel as well, you will see
that it is a non-trivial task. I guess that some of this support must
exist in OS X for the existing case insensitive filesystems like HFS+,
but I wonder whether it would be suitable for use with ZFS as well?

Cheers,
Simon

From ndellofano at apple.com  Wed Feb 27 14:48:19 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed, 27 Feb 2008 14:48:19 -0800
Subject: [zfs-discuss] ZFS Case Sensitivity and file system
In-Reply-To: <4d87168d0802270906i2f88d6aembb53a2f70562c7b9@mail.gmail.com>
References: <DCAA5007-56E7-404B-B1F5-850AE34D40BF@sun.com>
	<4d87168d0802270906i2f88d6aembb53a2f70562c7b9@mail.gmail.com>
Message-ID: <E90FA6F4-E765-4D9E-B688-E4518B34EC29@apple.com>

Yes, Simon is correct.  ZFS in OpenSolaris now has a case insensitive  
option that can be set on ZFS filesystems through the 'zfs set'  
interface.  It is planned to bring this code into ZFS on OSX, however  
as noted, it requires a very large amount of change to the code base  
and also surrounding layers.  Hence we'll eventually sync up with this  
but we're working first on stabilizing everything else as much as  
possible before adding the case insensitivity  option.

Noel

On Feb 27, 2008, at 9:06 AM, Simon Crosland wrote:

> On 26/02/2008, Bill Winnett <William.Winnett at sun.com> wrote:
>>
>> Are there plans to add a case insensitive flag to ZFS for OS X?
>
> Since case insensitivity is being added to ZFS on OpenSolaris as part
> of the CIFS server project, I guess it is possible that it could be
> forward ported to OS X. However, if you look at the amount of change
> that was necessary for that on OpenSolaris, running through not just
> ZFS but the VFS and other parts of the kernel as well, you will see
> that it is a non-trivial task. I guess that some of this support must
> exist in OS X for the existing case insensitive filesystems like HFS+,
> but I wonder whether it would be suitable for use with ZFS as well?
>
> Cheers,
> Simon
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From jbsnyder at gmail.com  Wed Feb 27 17:04:22 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Wed, 27 Feb 2008 19:04:22 -0600
Subject: [zfs-discuss] Kernel Panics After IO Hang
Message-ID: <33644d3c0802271704p4f5da849i58fa000764710254@mail.gmail.com>

Hi -

I've been playing with the ZFS build and just ran into an issue that I
thought might be good to note.  I don't have ADC access so I can't
check if this is in rdar or not.

I have a zfs volume on /Volumes/Aquarius that I am using as the home
for my user.  I fired up Parallels on a recent boot and I think I
ended up in a situation where things were swapping like crazy or
waiting a lot on IO from something since it seemed like every app
attempting to hit disk was beachballing.  After a little while I gave
up and held down the power button and rebooted.  On reboot, things are
stable at the login screen, but when I login as the user on ZFS (other
users login fine), I get a kernel panic.  I've attached a series of 3
ones from separate reboots to give a flavor for where the hang
occurred.  I'm snapshotting the state of the drive, and I'm going to
try and get things back to a state with no panic on login.

Zfs list:
NAME                            USED  AVAIL  REFER  MOUNTPOINT
Aquarium                       33.4G  53.7G  32.9G  /Volumes/Aquarium
Aquarium at ediaback-feb25         108M      -  29.8G  -
Aquarium at erlangback-20080225   8.47M      -  30.4G  -
Aquarium at __zb_full_1204057064  22.1M      -  32.4G  -
Aquarium at __zb_incr               42K      -  32.9G  -

zfs get Aqarium:

NAME                           PROPERTY       VALUE                  SOURCE
Aquarium                       type           filesystem             -
Aquarium                       creation       Sun Feb 24 18:49 2008  -
Aquarium                       used           33.4G                  -
Aquarium                       available      53.7G                  -
Aquarium                       referenced     32.9G                  -
Aquarium                       compressratio  1.21x                  -
Aquarium                       mounted        yes                    -
Aquarium                       quota          none                   default
Aquarium                       reservation    none                   default
Aquarium                       recordsize     128K                   default
Aquarium                       mountpoint     /Volumes/Aquarium      default
Aquarium                       sharenfs       off                    default
Aquarium                       checksum       on                     default
Aquarium                       compression    on                     local
Aquarium                       atime          off                    local
Aquarium                       devices        on                     default
Aquarium                       exec           on                     default
Aquarium                       setuid         on                     default
Aquarium                       readonly       off                    default
Aquarium                       zoned          off                    default
Aquarium                       snapdir        hidden                 default
Aquarium                       aclmode        groupmask              default
Aquarium                       aclinherit     secure                 default
Aquarium                       canmount       on                     default
Aquarium                       shareiscsi     off                    default
Aquarium                       xattr          on                     default
Aquarium                       copies         1                      default
Aquarium                       version        1                      -


zpool list:
NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
Aquarium               88.5G   33.4G   55.1G    37%  ONLINE     -

zpool status:
  pool: Aquarium
 state: ONLINE
status: The pool is formatted using an older on-disk format.  The pool can
	still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
	pool will no longer be accessible on older software versions.
 scrub: scrub in progress, 77.33% done, 0h7m to go
config:

	NAME        STATE     READ WRITE CKSUM
	Aquarium    ONLINE       0     0     0
	  disk0s4   ONLINE       0     0     0
	  disk0s3   ONLINE       0     0     0

And the panics:

Wed Feb 27 18:19:56 2008
panic(cpu 1 caller 0x001A83A6): Double fault at 0x00c98d23,
thread:0x656e048, trapno:0x8, err:0x0),registers:
CR0: 0x80010033, CR2: 0x549eff88, CR3: 0x0118a000, CR4: 0x00000660
EAX: 0x0501b000, EBX: 0x50539c00, ECX: 0x00000000, EDX: 0x0007a867
ESP: 0x549eff90, EBP: 0x549f0038, ESI: 0x5044b5a0, EDI: 0x0501b0b8
EFL: 0x00010206, EIP: 0x00c98d23

Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x2e6a5fb8 : 0x12b0e1 (0x457024 0x2e6a5fec 0x13321a 0x0)
0x2e6a6008 : 0x1a83a6 (0x45ffd4 0xc98d23 0x656e048 0x8)
0x2e6a60e8 : 0x19fbc3 (0x2e6a6100 0xeeeeeeee 0xeeeeeeee 0xeeeeeeee)
0x549f0038 : 0xca1598 (0x549f01ec 0x0 0x0 0x549f01ec)
0x549f0208 : 0xc97fa1 (0x5044b588 0x206 0x549f0238 0x51ad8c0)
0x549f0248 : 0xc41e5b (0x5044b588 0x1 0x549f02d8 0x51ad8c0)
0x549f0288 : 0x1f2eb1 (0x549f02a4 0x8 0x549f02ac 0x0)
0x549f02c8 : 0x1da45f (0x502b870 0x54af144 0x0 0x2)
0x549f0318 : 0x1da67b (0x502b870 0x1 0x549f0348 0x19d311)
0x549f0368 : 0x1dc2e6 (0x0 0x4f33c0 0x549f0408 0x37c76738)
0x549f03e8 : 0xc98ca5 (0x0 0x34 0x549f041c 0x37c766c4)
0x549f0468 : 0xc990ba (0x501b248 0x0 0x200 0x0)
0x549f0518 : 0xca1598 (0x549f06cc 0x0 0x0 0x549f06cc)
0x549f06e8 : 0xc97fa1 (0x5044b670 0x206 0x549f0718 0x51ad94c)
0x549f0728 : 0xc41e5b (0x5044b670 0x1 0x549f07b8 0x51ad94c)
0x549f0768 : 0x1f2eb1 (0x549f0784 0x8 0x549f078c 0x0)
	Backtrace continues...
      Kernel loadable modules in backtrace (with dependencies):
         com.apple.filesystems.zfs(8.0)@0xc3a000->0xd05fff

BSD process name corresponding to current thread: Xquartz

Mac OS version:
9C31

Kernel version:
Darwin Kernel Version 9.2.0: Tue Feb  5 16:13:22 PST 2008;
root:xnu-1228.3.13~1/RELEASE_I386
System model name: MacBook2,1 (Mac-F4208CAA)

Wed Feb 27 18:22:10 2008
panic(cpu 0 caller 0x001A83A6): Double fault at 0x00c6211c,
thread:0x5065d60, trapno:0x8, err:0x0),registers:
CR0: 0x80010033, CR2: 0x53427fc8, CR3: 0x0118a000, CR4: 0x00000660
EAX: 0x534280ac, EBX: 0x5050ed64, ECX: 0x0516568c, EDX: 0x00004000
ESP: 0x53427fd0, EBP: 0x53428118, ESI: 0x00000000, EDI: 0x00000000
EFL: 0x00010246, EIP: 0x00c6211c

Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x4e9e28 : 0x12b0e1 (0x457024 0x4e9e5c 0x13321a 0x0)
0x4e9e78 : 0x1a83a6 (0x45ffd4 0xc6211c 0x5065d60 0x8)
0x4e9f58 : 0x19fbc3 (0x4e9f70 0x0 0x0 0x0)
0x53428118 : 0xc59f26 (0x516568c 0xf510000 0x0 0x4000)
0x534281a8 : 0xc676be (0x5050ed10 0x0 0x2 0xcd2d98)
0x534281f8 : 0xc6794e (0x4d1af00 0x7a894 0x0 0x1)
0x53428228 : 0xc62fde (0x4d1af00 0x7a894 0x0 0xccf9a4)
0x53428288 : 0xc98d92 (0x4f43770 0x7a894 0x0 0x0)
0x53428338 : 0xca1598 (0x534284ec 0x0 0x0 0x534284ec)
0x53428508 : 0xc97fa1 (0x503c7670 0x216 0x53428538 0x5192d20)
0x53428548 : 0xc41e5b (0x503c7670 0x1 0x53428598 0x5192d20)
0x53428588 : 0x1f2eb1 (0x534285a4 0x8 0x534285ac 0x0)
0x534285c8 : 0x1da45f (0x518ac70 0x5062024 0x0 0x2)
0x53428618 : 0x1da67b (0x518ac70 0x1 0x53428648 0x19d311)
0x53428668 : 0x1dc2e6 (0x0 0x4f33c0 0x53428708 0x38904690)
0x534286e8 : 0xc98ca5 (0x0 0x34 0x5342871c 0x3890461c)
	Backtrace continues...
      Kernel loadable modules in backtrace (with dependencies):
         com.apple.filesystems.zfs(8.0)@0xc3a000->0xd05fff

BSD process name corresponding to current thread: pbs

Mac OS version:
9C31

Kernel version:
Darwin Kernel Version 9.2.0: Tue Feb  5 16:13:22 PST 2008;
root:xnu-1228.3.13~1/RELEASE_I386
System model name: MacBook2,1 (Mac-F4208CAA)

Wed Feb 27 18:24:17 2008
panic(cpu 1 caller 0x001A83A6): Double fault at 0x00c62fd9,
thread:0x50f2f20, trapno:0x8, err:0x0),registers:
CR0: 0x80010033, CR2: 0x546cfff8, CR3: 0x0118a000, CR4: 0x00000660
EAX: 0x04f52800, EBX: 0x05153a48, ECX: 0x05153800, EDX: 0x00000000
ESP: 0x546d0000, EBP: 0x546d0058, ESI: 0x516575a0, EDI: 0x051538b8
EFL: 0x00010206, EIP: 0x00c62fd9

Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x2e6a5fb8 : 0x12b0e1 (0x457024 0x2e6a5fec 0x13321a 0x0)
0x2e6a6008 : 0x1a83a6 (0x45ffd4 0xc62fd9 0x50f2f20 0x8)
0x2e6a60e8 : 0x19fbc3 (0x2e6a6100 0xeeeeeeee 0xeeeeeeee 0xeeeeeeee)
0x546d0058 : 0xc98d92 (0x4974290 0x7a894 0x0 0x0)
0x546d0108 : 0xca1598 (0x546d02bc 0x0 0x0 0x546d02bc)
0x546d02d8 : 0xc97fa1 (0x51657588 0x212 0x546d0308 0x5176118)
0x546d0318 : 0xc41e5b (0x51657588 0x1 0x546d0368 0x5176118)
0x546d0358 : 0x1f2eb1 (0x546d0374 0x8 0x546d037c 0x0)
0x546d0398 : 0x1da45f (0x5172760 0x50f0324 0x0 0x2)
0x546d03e8 : 0x1da67b (0x5172760 0x1 0x546d0418 0x19d311)
0x546d0438 : 0x1dc2e6 (0x0 0x4f33c0 0x546d04d8 0x377906f0)
0x546d04b8 : 0xc98ca5 (0x0 0x34 0x546d04ec 0x3779067c)
0x546d0538 : 0xc990ba (0x5153b9c 0x0 0x200 0x0)
0x546d05e8 : 0xca1598 (0x546d079c 0x0 0x0 0x546d079c)
0x546d07b8 : 0xc97fa1 (0x51657670 0x212 0x546d07e8 0x5176460)
0x546d07f8 : 0xc41e5b (0x51657670 0x1 0x546d0848 0x5176460)
	Backtrace continues...
      Kernel loadable modules in backtrace (with dependencies):
         com.apple.filesystems.zfs(8.0)@0xc3a000->0xd05fff

BSD process name corresponding to current thread: Unison

Mac OS version:
9C31

Kernel version:
Darwin Kernel Version 9.2.0: Tue Feb  5 16:13:22 PST 2008;
root:xnu-1228.3.13~1/RELEASE_I386
System model name: MacBook2,1 (Mac-F4208CAA)


Not sure what else might be helpful :-)

-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From jbsnyder at gmail.com  Thu Feb 28 12:51:54 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Thu, 28 Feb 2008 14:51:54 -0600
Subject: [zfs-discuss] mounting fs from one pool under another
Message-ID: <33644d3c0802281251k446b80f5tfafccfa53d6d2859@mail.gmail.com>

I have a quick question regarding mounting one pool within another.  I
have made my home directory a subset of a pool called Aquarium
(Aquarium/jsnyder), and I've also made another pool that I'm mirroring
with an external drive called Puddle, within which I have my Documents
folder, which I mount at /Users/jsnyder/Documents.  This worked OK the
first time I attempted it, but during subsequent situations, I seem to
be getting maybe the Documents fs mounted first and then the jsnyder
fs mounted later leaving me with an empty documents dir and no access
to what's on Puddle/Documents.  Is there some way to set the order in
which the filesystems are mounted or to handle this differently?  I
want my Documents dir mirrored, but not the rest, which I just backup.

Any Ideas?

Thanks!

-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From ndellofano at apple.com  Thu Feb 28 14:28:17 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu, 28 Feb 2008 14:28:17 -0800
Subject: [zfs-discuss] mounting fs from one pool under another
In-Reply-To: <33644d3c0802281251k446b80f5tfafccfa53d6d2859@mail.gmail.com>
References: <33644d3c0802281251k446b80f5tfafccfa53d6d2859@mail.gmail.com>
Message-ID: <99BEF6B5-41AB-4CF4-A039-08A85A2D837E@apple.com>

currently I'm afraid there isn't any way to order which filesystems  
get mounted first.  The order is dynamic and completely relies on  
which disks diskutil finds first and in which order it attempts to  
bring them online.  Hence sometimes the Puddle pool comes online first  
and is mounted and sometimes your Aquarium pool gets discovered first.
I think this is a good issue to bring to light though and will keep it  
in mind as we're working on getting things streamlined in the system.

thanks!
Noel

On Feb 28, 2008, at 12:51 PM, James Snyder wrote:

> I have a quick question regarding mounting one pool within another.  I
> have made my home directory a subset of a pool called Aquarium
> (Aquarium/jsnyder), and I've also made another pool that I'm mirroring
> with an external drive called Puddle, within which I have my Documents
> folder, which I mount at /Users/jsnyder/Documents.  This worked OK the
> first time I attempted it, but during subsequent situations, I seem to
> be getting maybe the Documents fs mounted first and then the jsnyder
> fs mounted later leaving me with an empty documents dir and no access
> to what's on Puddle/Documents.  Is there some way to set the order in
> which the filesystems are mounted or to handle this differently?  I
> want my Documents dir mirrored, but not the rest, which I just backup.
>
> Any Ideas?
>
> Thanks!
>
> -- 
> James Snyder
> Biomedical Engineering
> Northwestern University
> jbsnyder at gmail.com
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Thu Feb 28 17:36:42 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu, 28 Feb 2008 17:36:42 -0800
Subject: [zfs-discuss] Kernel Panics After IO Hang
In-Reply-To: <33644d3c0802271704p4f5da849i58fa000764710254@mail.gmail.com>
References: <33644d3c0802271704p4f5da849i58fa000764710254@mail.gmail.com>
Message-ID: <717695B1-B4CF-4FD9-B07C-20D67D9B9B15@apple.com>

There have been a few "issues" with using Parallels on Leopard  
supported, and I've never tested ZFS with it either so something evil  
could be happening.  I'll hunt around and see if this is ZFS specific  
or some weird Parallels issue.

thanks!
Noel

On Feb 27, 2008, at 5:04 PM, James Snyder wrote:

> Hi -
>
> I've been playing with the ZFS build and just ran into an issue that I
> thought might be good to note.  I don't have ADC access so I can't
> check if this is in rdar or not.
>
> I have a zfs volume on /Volumes/Aquarius that I am using as the home
> for my user.  I fired up Parallels on a recent boot and I think I
> ended up in a situation where things were swapping like crazy or
> waiting a lot on IO from something since it seemed like every app
> attempting to hit disk was beachballing.  After a little while I gave
> up and held down the power button and rebooted.  On reboot, things are
> stable at the login screen, but when I login as the user on ZFS (other
> users login fine), I get a kernel panic.  I've attached a series of 3
> ones from separate reboots to give a flavor for where the hang
> occurred.  I'm snapshotting the state of the drive, and I'm going to
> try and get things back to a state with no panic on login.
>
> Zfs list:
> NAME                            USED  AVAIL  REFER  MOUNTPOINT
> Aquarium                       33.4G  53.7G  32.9G  /Volumes/Aquarium
> Aquarium at ediaback-feb25         108M      -  29.8G  -
> Aquarium at erlangback-20080225   8.47M      -  30.4G  -
> Aquarium at __zb_full_1204057064  22.1M      -  32.4G  -
> Aquarium at __zb_incr               42K      -  32.9G  -
>
> zfs get Aqarium:
>
> NAME                           PROPERTY       VALUE                   
> SOURCE
> Aquarium                       type           filesystem             -
> Aquarium                       creation       Sun Feb 24 18:49 2008  -
> Aquarium                       used           33.4G                  -
> Aquarium                       available      53.7G                  -
> Aquarium                       referenced     32.9G                  -
> Aquarium                       compressratio  1.21x                  -
> Aquarium                       mounted        yes                    -
> Aquarium                       quota          none                    
> default
> Aquarium                       reservation    none                    
> default
> Aquarium                       recordsize     128K                    
> default
> Aquarium                       mountpoint     /Volumes/Aquarium       
> default
> Aquarium                       sharenfs       off                     
> default
> Aquarium                       checksum       on                      
> default
> Aquarium                       compression    on                      
> local
> Aquarium                       atime          off                     
> local
> Aquarium                       devices        on                      
> default
> Aquarium                       exec           on                      
> default
> Aquarium                       setuid         on                      
> default
> Aquarium                       readonly       off                     
> default
> Aquarium                       zoned          off                     
> default
> Aquarium                       snapdir        hidden                  
> default
> Aquarium                       aclmode        groupmask               
> default
> Aquarium                       aclinherit     secure                  
> default
> Aquarium                       canmount       on                      
> default
> Aquarium                       shareiscsi     off                     
> default
> Aquarium                       xattr          on                      
> default
> Aquarium                       copies         1                       
> default
> Aquarium                       version        1                      -
>
>
> zpool list:
> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
> ALTROOT
> Aquarium               88.5G   33.4G   55.1G    37%  ONLINE     -
>
> zpool status:
>  pool: Aquarium
> state: ONLINE
> status: The pool is formatted using an older on-disk format.  The  
> pool can
> 	still be used, but some features are unavailable.
> action: Upgrade the pool using 'zpool upgrade'.  Once this is done,  
> the
> 	pool will no longer be accessible on older software versions.
> scrub: scrub in progress, 77.33% done, 0h7m to go
> config:
>
> 	NAME        STATE     READ WRITE CKSUM
> 	Aquarium    ONLINE       0     0     0
> 	  disk0s4   ONLINE       0     0     0
> 	  disk0s3   ONLINE       0     0     0
>
> And the panics:
>
> Wed Feb 27 18:19:56 2008
> panic(cpu 1 caller 0x001A83A6): Double fault at 0x00c98d23,
> thread:0x656e048, trapno:0x8, err:0x0),registers:
> CR0: 0x80010033, CR2: 0x549eff88, CR3: 0x0118a000, CR4: 0x00000660
> EAX: 0x0501b000, EBX: 0x50539c00, ECX: 0x00000000, EDX: 0x0007a867
> ESP: 0x549eff90, EBP: 0x549f0038, ESI: 0x5044b5a0, EDI: 0x0501b0b8
> EFL: 0x00010206, EIP: 0x00c98d23
>
> Backtrace, Format - Frame : Return Address (4 potential args on stack)
> 0x2e6a5fb8 : 0x12b0e1 (0x457024 0x2e6a5fec 0x13321a 0x0)
> 0x2e6a6008 : 0x1a83a6 (0x45ffd4 0xc98d23 0x656e048 0x8)
> 0x2e6a60e8 : 0x19fbc3 (0x2e6a6100 0xeeeeeeee 0xeeeeeeee 0xeeeeeeee)
> 0x549f0038 : 0xca1598 (0x549f01ec 0x0 0x0 0x549f01ec)
> 0x549f0208 : 0xc97fa1 (0x5044b588 0x206 0x549f0238 0x51ad8c0)
> 0x549f0248 : 0xc41e5b (0x5044b588 0x1 0x549f02d8 0x51ad8c0)
> 0x549f0288 : 0x1f2eb1 (0x549f02a4 0x8 0x549f02ac 0x0)
> 0x549f02c8 : 0x1da45f (0x502b870 0x54af144 0x0 0x2)
> 0x549f0318 : 0x1da67b (0x502b870 0x1 0x549f0348 0x19d311)
> 0x549f0368 : 0x1dc2e6 (0x0 0x4f33c0 0x549f0408 0x37c76738)
> 0x549f03e8 : 0xc98ca5 (0x0 0x34 0x549f041c 0x37c766c4)
> 0x549f0468 : 0xc990ba (0x501b248 0x0 0x200 0x0)
> 0x549f0518 : 0xca1598 (0x549f06cc 0x0 0x0 0x549f06cc)
> 0x549f06e8 : 0xc97fa1 (0x5044b670 0x206 0x549f0718 0x51ad94c)
> 0x549f0728 : 0xc41e5b (0x5044b670 0x1 0x549f07b8 0x51ad94c)
> 0x549f0768 : 0x1f2eb1 (0x549f0784 0x8 0x549f078c 0x0)
> 	Backtrace continues...
>      Kernel loadable modules in backtrace (with dependencies):
>         com.apple.filesystems.zfs(8.0)@0xc3a000->0xd05fff
>
> BSD process name corresponding to current thread: Xquartz
>
> Mac OS version:
> 9C31
>
> Kernel version:
> Darwin Kernel Version 9.2.0: Tue Feb  5 16:13:22 PST 2008;
> root:xnu-1228.3.13~1/RELEASE_I386
> System model name: MacBook2,1 (Mac-F4208CAA)
>
> Wed Feb 27 18:22:10 2008
> panic(cpu 0 caller 0x001A83A6): Double fault at 0x00c6211c,
> thread:0x5065d60, trapno:0x8, err:0x0),registers:
> CR0: 0x80010033, CR2: 0x53427fc8, CR3: 0x0118a000, CR4: 0x00000660
> EAX: 0x534280ac, EBX: 0x5050ed64, ECX: 0x0516568c, EDX: 0x00004000
> ESP: 0x53427fd0, EBP: 0x53428118, ESI: 0x00000000, EDI: 0x00000000
> EFL: 0x00010246, EIP: 0x00c6211c
>
> Backtrace, Format - Frame : Return Address (4 potential args on stack)
> 0x4e9e28 : 0x12b0e1 (0x457024 0x4e9e5c 0x13321a 0x0)
> 0x4e9e78 : 0x1a83a6 (0x45ffd4 0xc6211c 0x5065d60 0x8)
> 0x4e9f58 : 0x19fbc3 (0x4e9f70 0x0 0x0 0x0)
> 0x53428118 : 0xc59f26 (0x516568c 0xf510000 0x0 0x4000)
> 0x534281a8 : 0xc676be (0x5050ed10 0x0 0x2 0xcd2d98)
> 0x534281f8 : 0xc6794e (0x4d1af00 0x7a894 0x0 0x1)
> 0x53428228 : 0xc62fde (0x4d1af00 0x7a894 0x0 0xccf9a4)
> 0x53428288 : 0xc98d92 (0x4f43770 0x7a894 0x0 0x0)
> 0x53428338 : 0xca1598 (0x534284ec 0x0 0x0 0x534284ec)
> 0x53428508 : 0xc97fa1 (0x503c7670 0x216 0x53428538 0x5192d20)
> 0x53428548 : 0xc41e5b (0x503c7670 0x1 0x53428598 0x5192d20)
> 0x53428588 : 0x1f2eb1 (0x534285a4 0x8 0x534285ac 0x0)
> 0x534285c8 : 0x1da45f (0x518ac70 0x5062024 0x0 0x2)
> 0x53428618 : 0x1da67b (0x518ac70 0x1 0x53428648 0x19d311)
> 0x53428668 : 0x1dc2e6 (0x0 0x4f33c0 0x53428708 0x38904690)
> 0x534286e8 : 0xc98ca5 (0x0 0x34 0x5342871c 0x3890461c)
> 	Backtrace continues...
>      Kernel loadable modules in backtrace (with dependencies):
>         com.apple.filesystems.zfs(8.0)@0xc3a000->0xd05fff
>
> BSD process name corresponding to current thread: pbs
>
> Mac OS version:
> 9C31
>
> Kernel version:
> Darwin Kernel Version 9.2.0: Tue Feb  5 16:13:22 PST 2008;
> root:xnu-1228.3.13~1/RELEASE_I386
> System model name: MacBook2,1 (Mac-F4208CAA)
>
> Wed Feb 27 18:24:17 2008
> panic(cpu 1 caller 0x001A83A6): Double fault at 0x00c62fd9,
> thread:0x50f2f20, trapno:0x8, err:0x0),registers:
> CR0: 0x80010033, CR2: 0x546cfff8, CR3: 0x0118a000, CR4: 0x00000660
> EAX: 0x04f52800, EBX: 0x05153a48, ECX: 0x05153800, EDX: 0x00000000
> ESP: 0x546d0000, EBP: 0x546d0058, ESI: 0x516575a0, EDI: 0x051538b8
> EFL: 0x00010206, EIP: 0x00c62fd9
>
> Backtrace, Format - Frame : Return Address (4 potential args on stack)
> 0x2e6a5fb8 : 0x12b0e1 (0x457024 0x2e6a5fec 0x13321a 0x0)
> 0x2e6a6008 : 0x1a83a6 (0x45ffd4 0xc62fd9 0x50f2f20 0x8)
> 0x2e6a60e8 : 0x19fbc3 (0x2e6a6100 0xeeeeeeee 0xeeeeeeee 0xeeeeeeee)
> 0x546d0058 : 0xc98d92 (0x4974290 0x7a894 0x0 0x0)
> 0x546d0108 : 0xca1598 (0x546d02bc 0x0 0x0 0x546d02bc)
> 0x546d02d8 : 0xc97fa1 (0x51657588 0x212 0x546d0308 0x5176118)
> 0x546d0318 : 0xc41e5b (0x51657588 0x1 0x546d0368 0x5176118)
> 0x546d0358 : 0x1f2eb1 (0x546d0374 0x8 0x546d037c 0x0)
> 0x546d0398 : 0x1da45f (0x5172760 0x50f0324 0x0 0x2)
> 0x546d03e8 : 0x1da67b (0x5172760 0x1 0x546d0418 0x19d311)
> 0x546d0438 : 0x1dc2e6 (0x0 0x4f33c0 0x546d04d8 0x377906f0)
> 0x546d04b8 : 0xc98ca5 (0x0 0x34 0x546d04ec 0x3779067c)
> 0x546d0538 : 0xc990ba (0x5153b9c 0x0 0x200 0x0)
> 0x546d05e8 : 0xca1598 (0x546d079c 0x0 0x0 0x546d079c)
> 0x546d07b8 : 0xc97fa1 (0x51657670 0x212 0x546d07e8 0x5176460)
> 0x546d07f8 : 0xc41e5b (0x51657670 0x1 0x546d0848 0x5176460)
> 	Backtrace continues...
>      Kernel loadable modules in backtrace (with dependencies):
>         com.apple.filesystems.zfs(8.0)@0xc3a000->0xd05fff
>
> BSD process name corresponding to current thread: Unison
>
> Mac OS version:
> 9C31
>
> Kernel version:
> Darwin Kernel Version 9.2.0: Tue Feb  5 16:13:22 PST 2008;
> root:xnu-1228.3.13~1/RELEASE_I386
> System model name: MacBook2,1 (Mac-F4208CAA)
>
>
> Not sure what else might be helpful :-)
>
> -- 
> James Snyder
> Biomedical Engineering
> Northwestern University
> jbsnyder at gmail.com
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Thu Feb 28 17:47:47 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu, 28 Feb 2008 17:47:47 -0800
Subject: [zfs-discuss] zpool export
In-Reply-To: <E837B12D-404E-482E-9B07-B86E8D02D46C@spamfreemail.de>
References: <E837B12D-404E-482E-9B07-B86E8D02D46C@spamfreemail.de>
Message-ID: <F83178A0-83FB-48CD-85C3-ED664C01FBCF@apple.com>

We are working on trying to change the system so things like this are  
streamlined more.  And you can just do a 'zpool export' instead of  
unmounting and then exporting.  The unmount happens automatically as  
part of the export.  But yes, this is a bit confusing for external  
drives especially since you tend to want to remove them easily or they  
may get bumped out.  We're working on improving this behavior.

Noel

On Feb 24, 2008, at 8:59 AM, Franz Schmalzl wrote:

> i have to do a zpool export after unmounting a pool, i know
> (kernel panic when replugging the underlaying device if i don't)
>
> are there plans to change this behaviour ?
> (maybe an automatic export )
>
> regards
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From jbsnyder at gmail.com  Thu Feb 28 17:57:17 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Thu, 28 Feb 2008 19:57:17 -0600
Subject: [zfs-discuss] Kernel Panics After IO Hang
In-Reply-To: <717695B1-B4CF-4FD9-B07C-20D67D9B9B15@apple.com>
References: <33644d3c0802271704p4f5da849i58fa000764710254@mail.gmail.com>
	<717695B1-B4CF-4FD9-B07C-20D67D9B9B15@apple.com>
Message-ID: <33644d3c0802281757q2e358334k6a945879b1b20777@mail.gmail.com>

As a followup on this, I made a snapshot form another account after
this from another account and had zetaback back that up, I then
restored to a fresh pool.  I previously had those two
partitions/slices being in the pool on the same disk, I removed those
and made one larger partition for the pool and then received the full
and incremental backups to that.  After that, no crashing on login.
The only other change I made during the restore process was to change
the mountpoint for zfs to my normal user directory location because it
seemed like some applications were expecting things to still be in
their original /Users/jsnyder location.

So, lots of moving parts/variables there, hard to say what may or may
not have fixed the problem :-)

On Thu, Feb 28, 2008 at 7:36 PM, No?l Dellofano <ndellofano at apple.com> wrote:
> There have been a few "issues" with using Parallels on Leopard
>  supported, and I've never tested ZFS with it either so something evil
>  could be happening.  I'll hunt around and see if this is ZFS specific
>  or some weird Parallels issue.
>
>  thanks!
>  Noel
>
>
>
>  On Feb 27, 2008, at 5:04 PM, James Snyder wrote:
>
>  > Hi -
>  >
>  > I've been playing with the ZFS build and just ran into an issue that I
>  > thought might be good to note.  I don't have ADC access so I can't
>  > check if this is in rdar or not.
>  >
>  > I have a zfs volume on /Volumes/Aquarius that I am using as the home
>  > for my user.  I fired up Parallels on a recent boot and I think I
>  > ended up in a situation where things were swapping like crazy or
>  > waiting a lot on IO from something since it seemed like every app
>  > attempting to hit disk was beachballing.  After a little while I gave
>  > up and held down the power button and rebooted.  On reboot, things are
>  > stable at the login screen, but when I login as the user on ZFS (other
>  > users login fine), I get a kernel panic.  I've attached a series of 3
>  > ones from separate reboots to give a flavor for where the hang
>  > occurred.  I'm snapshotting the state of the drive, and I'm going to
>  > try and get things back to a state with no panic on login.
>  >
>  > Zfs list:
>  > NAME                            USED  AVAIL  REFER  MOUNTPOINT
>  > Aquarium                       33.4G  53.7G  32.9G  /Volumes/Aquarium
>  > Aquarium at ediaback-feb25         108M      -  29.8G  -
>  > Aquarium at erlangback-20080225   8.47M      -  30.4G  -
>  > Aquarium at __zb_full_1204057064  22.1M      -  32.4G  -
>  > Aquarium at __zb_incr               42K      -  32.9G  -
>  >
>  > zfs get Aqarium:
>  >
>  > NAME                           PROPERTY       VALUE
>  > SOURCE
>  > Aquarium                       type           filesystem             -
>  > Aquarium                       creation       Sun Feb 24 18:49 2008  -
>  > Aquarium                       used           33.4G                  -
>  > Aquarium                       available      53.7G                  -
>  > Aquarium                       referenced     32.9G                  -
>  > Aquarium                       compressratio  1.21x                  -
>  > Aquarium                       mounted        yes                    -
>  > Aquarium                       quota          none
>  > default
>  > Aquarium                       reservation    none
>  > default
>  > Aquarium                       recordsize     128K
>  > default
>  > Aquarium                       mountpoint     /Volumes/Aquarium
>  > default
>  > Aquarium                       sharenfs       off
>  > default
>  > Aquarium                       checksum       on
>  > default
>  > Aquarium                       compression    on
>  > local
>  > Aquarium                       atime          off
>  > local
>  > Aquarium                       devices        on
>  > default
>  > Aquarium                       exec           on
>  > default
>  > Aquarium                       setuid         on
>  > default
>  > Aquarium                       readonly       off
>  > default
>  > Aquarium                       zoned          off
>  > default
>  > Aquarium                       snapdir        hidden
>  > default
>  > Aquarium                       aclmode        groupmask
>  > default
>  > Aquarium                       aclinherit     secure
>  > default
>  > Aquarium                       canmount       on
>  > default
>  > Aquarium                       shareiscsi     off
>  > default
>  > Aquarium                       xattr          on
>  > default
>  > Aquarium                       copies         1
>  > default
>  > Aquarium                       version        1                      -
>  >
>  >
>  > zpool list:
>  > NAME                    SIZE    USED   AVAIL    CAP  HEALTH
>  > ALTROOT
>  > Aquarium               88.5G   33.4G   55.1G    37%  ONLINE     -
>  >
>  > zpool status:
>  >  pool: Aquarium
>  > state: ONLINE
>  > status: The pool is formatted using an older on-disk format.  The
>  > pool can
>  >       still be used, but some features are unavailable.
>  > action: Upgrade the pool using 'zpool upgrade'.  Once this is done,
>  > the
>  >       pool will no longer be accessible on older software versions.
>  > scrub: scrub in progress, 77.33% done, 0h7m to go
>  > config:
>  >
>  >       NAME        STATE     READ WRITE CKSUM
>  >       Aquarium    ONLINE       0     0     0
>  >         disk0s4   ONLINE       0     0     0
>  >         disk0s3   ONLINE       0     0     0
>  >
>  > And the panics:
>  >
>  > Wed Feb 27 18:19:56 2008
>  > panic(cpu 1 caller 0x001A83A6): Double fault at 0x00c98d23,
>  > thread:0x656e048, trapno:0x8, err:0x0),registers:
>  > CR0: 0x80010033, CR2: 0x549eff88, CR3: 0x0118a000, CR4: 0x00000660
>  > EAX: 0x0501b000, EBX: 0x50539c00, ECX: 0x00000000, EDX: 0x0007a867
>  > ESP: 0x549eff90, EBP: 0x549f0038, ESI: 0x5044b5a0, EDI: 0x0501b0b8
>  > EFL: 0x00010206, EIP: 0x00c98d23
>  >
>  > Backtrace, Format - Frame : Return Address (4 potential args on stack)
>  > 0x2e6a5fb8 : 0x12b0e1 (0x457024 0x2e6a5fec 0x13321a 0x0)
>  > 0x2e6a6008 : 0x1a83a6 (0x45ffd4 0xc98d23 0x656e048 0x8)
>  > 0x2e6a60e8 : 0x19fbc3 (0x2e6a6100 0xeeeeeeee 0xeeeeeeee 0xeeeeeeee)
>  > 0x549f0038 : 0xca1598 (0x549f01ec 0x0 0x0 0x549f01ec)
>  > 0x549f0208 : 0xc97fa1 (0x5044b588 0x206 0x549f0238 0x51ad8c0)
>  > 0x549f0248 : 0xc41e5b (0x5044b588 0x1 0x549f02d8 0x51ad8c0)
>  > 0x549f0288 : 0x1f2eb1 (0x549f02a4 0x8 0x549f02ac 0x0)
>  > 0x549f02c8 : 0x1da45f (0x502b870 0x54af144 0x0 0x2)
>  > 0x549f0318 : 0x1da67b (0x502b870 0x1 0x549f0348 0x19d311)
>  > 0x549f0368 : 0x1dc2e6 (0x0 0x4f33c0 0x549f0408 0x37c76738)
>  > 0x549f03e8 : 0xc98ca5 (0x0 0x34 0x549f041c 0x37c766c4)
>  > 0x549f0468 : 0xc990ba (0x501b248 0x0 0x200 0x0)
>  > 0x549f0518 : 0xca1598 (0x549f06cc 0x0 0x0 0x549f06cc)
>  > 0x549f06e8 : 0xc97fa1 (0x5044b670 0x206 0x549f0718 0x51ad94c)
>  > 0x549f0728 : 0xc41e5b (0x5044b670 0x1 0x549f07b8 0x51ad94c)
>  > 0x549f0768 : 0x1f2eb1 (0x549f0784 0x8 0x549f078c 0x0)
>  >       Backtrace continues...
>  >      Kernel loadable modules in backtrace (with dependencies):
>  >         com.apple.filesystems.zfs(8.0)@0xc3a000->0xd05fff
>  >
>  > BSD process name corresponding to current thread: Xquartz
>  >
>  > Mac OS version:
>  > 9C31
>  >
>  > Kernel version:
>  > Darwin Kernel Version 9.2.0: Tue Feb  5 16:13:22 PST 2008;
>  > root:xnu-1228.3.13~1/RELEASE_I386
>  > System model name: MacBook2,1 (Mac-F4208CAA)
>  >
>  > Wed Feb 27 18:22:10 2008
>  > panic(cpu 0 caller 0x001A83A6): Double fault at 0x00c6211c,
>  > thread:0x5065d60, trapno:0x8, err:0x0),registers:
>  > CR0: 0x80010033, CR2: 0x53427fc8, CR3: 0x0118a000, CR4: 0x00000660
>  > EAX: 0x534280ac, EBX: 0x5050ed64, ECX: 0x0516568c, EDX: 0x00004000
>  > ESP: 0x53427fd0, EBP: 0x53428118, ESI: 0x00000000, EDI: 0x00000000
>  > EFL: 0x00010246, EIP: 0x00c6211c
>  >
>  > Backtrace, Format - Frame : Return Address (4 potential args on stack)
>  > 0x4e9e28 : 0x12b0e1 (0x457024 0x4e9e5c 0x13321a 0x0)
>  > 0x4e9e78 : 0x1a83a6 (0x45ffd4 0xc6211c 0x5065d60 0x8)
>  > 0x4e9f58 : 0x19fbc3 (0x4e9f70 0x0 0x0 0x0)
>  > 0x53428118 : 0xc59f26 (0x516568c 0xf510000 0x0 0x4000)
>  > 0x534281a8 : 0xc676be (0x5050ed10 0x0 0x2 0xcd2d98)
>  > 0x534281f8 : 0xc6794e (0x4d1af00 0x7a894 0x0 0x1)
>  > 0x53428228 : 0xc62fde (0x4d1af00 0x7a894 0x0 0xccf9a4)
>  > 0x53428288 : 0xc98d92 (0x4f43770 0x7a894 0x0 0x0)
>  > 0x53428338 : 0xca1598 (0x534284ec 0x0 0x0 0x534284ec)
>  > 0x53428508 : 0xc97fa1 (0x503c7670 0x216 0x53428538 0x5192d20)
>  > 0x53428548 : 0xc41e5b (0x503c7670 0x1 0x53428598 0x5192d20)
>  > 0x53428588 : 0x1f2eb1 (0x534285a4 0x8 0x534285ac 0x0)
>  > 0x534285c8 : 0x1da45f (0x518ac70 0x5062024 0x0 0x2)
>  > 0x53428618 : 0x1da67b (0x518ac70 0x1 0x53428648 0x19d311)
>  > 0x53428668 : 0x1dc2e6 (0x0 0x4f33c0 0x53428708 0x38904690)
>  > 0x534286e8 : 0xc98ca5 (0x0 0x34 0x5342871c 0x3890461c)
>  >       Backtrace continues...
>  >      Kernel loadable modules in backtrace (with dependencies):
>  >         com.apple.filesystems.zfs(8.0)@0xc3a000->0xd05fff
>  >
>  > BSD process name corresponding to current thread: pbs
>  >
>  > Mac OS version:
>  > 9C31
>  >
>  > Kernel version:
>  > Darwin Kernel Version 9.2.0: Tue Feb  5 16:13:22 PST 2008;
>  > root:xnu-1228.3.13~1/RELEASE_I386
>  > System model name: MacBook2,1 (Mac-F4208CAA)
>  >
>  > Wed Feb 27 18:24:17 2008
>  > panic(cpu 1 caller 0x001A83A6): Double fault at 0x00c62fd9,
>  > thread:0x50f2f20, trapno:0x8, err:0x0),registers:
>  > CR0: 0x80010033, CR2: 0x546cfff8, CR3: 0x0118a000, CR4: 0x00000660
>  > EAX: 0x04f52800, EBX: 0x05153a48, ECX: 0x05153800, EDX: 0x00000000
>  > ESP: 0x546d0000, EBP: 0x546d0058, ESI: 0x516575a0, EDI: 0x051538b8
>  > EFL: 0x00010206, EIP: 0x00c62fd9
>  >
>  > Backtrace, Format - Frame : Return Address (4 potential args on stack)
>  > 0x2e6a5fb8 : 0x12b0e1 (0x457024 0x2e6a5fec 0x13321a 0x0)
>  > 0x2e6a6008 : 0x1a83a6 (0x45ffd4 0xc62fd9 0x50f2f20 0x8)
>  > 0x2e6a60e8 : 0x19fbc3 (0x2e6a6100 0xeeeeeeee 0xeeeeeeee 0xeeeeeeee)
>  > 0x546d0058 : 0xc98d92 (0x4974290 0x7a894 0x0 0x0)
>  > 0x546d0108 : 0xca1598 (0x546d02bc 0x0 0x0 0x546d02bc)
>  > 0x546d02d8 : 0xc97fa1 (0x51657588 0x212 0x546d0308 0x5176118)
>  > 0x546d0318 : 0xc41e5b (0x51657588 0x1 0x546d0368 0x5176118)
>  > 0x546d0358 : 0x1f2eb1 (0x546d0374 0x8 0x546d037c 0x0)
>  > 0x546d0398 : 0x1da45f (0x5172760 0x50f0324 0x0 0x2)
>  > 0x546d03e8 : 0x1da67b (0x5172760 0x1 0x546d0418 0x19d311)
>  > 0x546d0438 : 0x1dc2e6 (0x0 0x4f33c0 0x546d04d8 0x377906f0)
>  > 0x546d04b8 : 0xc98ca5 (0x0 0x34 0x546d04ec 0x3779067c)
>  > 0x546d0538 : 0xc990ba (0x5153b9c 0x0 0x200 0x0)
>  > 0x546d05e8 : 0xca1598 (0x546d079c 0x0 0x0 0x546d079c)
>  > 0x546d07b8 : 0xc97fa1 (0x51657670 0x212 0x546d07e8 0x5176460)
>  > 0x546d07f8 : 0xc41e5b (0x51657670 0x1 0x546d0848 0x5176460)
>  >       Backtrace continues...
>  >      Kernel loadable modules in backtrace (with dependencies):
>  >         com.apple.filesystems.zfs(8.0)@0xc3a000->0xd05fff
>  >
>  > BSD process name corresponding to current thread: Unison
>  >
>  > Mac OS version:
>  > 9C31
>  >
>  > Kernel version:
>  > Darwin Kernel Version 9.2.0: Tue Feb  5 16:13:22 PST 2008;
>  > root:xnu-1228.3.13~1/RELEASE_I386
>  > System model name: MacBook2,1 (Mac-F4208CAA)
>  >
>  >
>  > Not sure what else might be helpful :-)
>  >
>  > --
>  > James Snyder
>  > Biomedical Engineering
>  > Northwestern University
>  > jbsnyder at gmail.com
>  > _______________________________________________
>  > zfs-discuss mailing list
>  > zfs-discuss at lists.macosforge.org
>  > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From jbsnyder at gmail.com  Thu Feb 28 18:09:04 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Thu, 28 Feb 2008 20:09:04 -0600
Subject: [zfs-discuss] Kernel Panics After IO Hang
In-Reply-To: <33644d3c0802281757q2e358334k6a945879b1b20777@mail.gmail.com>
References: <33644d3c0802271704p4f5da849i58fa000764710254@mail.gmail.com>
	<717695B1-B4CF-4FD9-B07C-20D67D9B9B15@apple.com>
	<33644d3c0802281757q2e358334k6a945879b1b20777@mail.gmail.com>
Message-ID: <33644d3c0802281809nd1735bfkbeb93ceeea9b84a9@mail.gmail.com>

One other very quick note on this matter is that I'm definitely
hitting the filesystem with more than the usual array of launching
apps on login (about 10 of those in login items).  I keep forgetting
to change this to delay this particular operation, but I have launchd
items that make an attempt at automatically syncing my home directory
to a backup server on login using Unison.  That said, I'm still doing
the same after the afore-mentioned restore, so perhaps that just
confounds the situation a little further and is unrelated.

On Thu, Feb 28, 2008 at 7:57 PM, James Snyder <jbsnyder at gmail.com> wrote:
> As a followup on this, I made a snapshot form another account after
>  this from another account and had zetaback back that up, I then
>  restored to a fresh pool.  I previously had those two
>  partitions/slices being in the pool on the same disk, I removed those
>  and made one larger partition for the pool and then received the full
>  and incremental backups to that.  After that, no crashing on login.
>  The only other change I made during the restore process was to change
>  the mountpoint for zfs to my normal user directory location because it
>  seemed like some applications were expecting things to still be in
>  their original /Users/jsnyder location.
>
>  So, lots of moving parts/variables there, hard to say what may or may
>  not have fixed the problem :-)
>
>
>
>  On Thu, Feb 28, 2008 at 7:36 PM, No?l Dellofano <ndellofano at apple.com> wrote:
>  > There have been a few "issues" with using Parallels on Leopard
>  >  supported, and I've never tested ZFS with it either so something evil
>  >  could be happening.  I'll hunt around and see if this is ZFS specific
>  >  or some weird Parallels issue.
>  >
>  >  thanks!
>  >  Noel
>  >
>  >
>  >
>  >  On Feb 27, 2008, at 5:04 PM, James Snyder wrote:
>  >
>  >  > Hi -
>  >  >
>  >  > I've been playing with the ZFS build and just ran into an issue that I
>  >  > thought might be good to note.  I don't have ADC access so I can't
>  >  > check if this is in rdar or not.
>  >  >
>  >  > I have a zfs volume on /Volumes/Aquarius that I am using as the home
>  >  > for my user.  I fired up Parallels on a recent boot and I think I
>  >  > ended up in a situation where things were swapping like crazy or
>  >  > waiting a lot on IO from something since it seemed like every app
>  >  > attempting to hit disk was beachballing.  After a little while I gave
>  >  > up and held down the power button and rebooted.  On reboot, things are
>  >  > stable at the login screen, but when I login as the user on ZFS (other
>  >  > users login fine), I get a kernel panic.  I've attached a series of 3
>  >  > ones from separate reboots to give a flavor for where the hang
>  >  > occurred.  I'm snapshotting the state of the drive, and I'm going to
>  >  > try and get things back to a state with no panic on login.
>  >  >
>  >  > Zfs list:
>  >  > NAME                            USED  AVAIL  REFER  MOUNTPOINT
>  >  > Aquarium                       33.4G  53.7G  32.9G  /Volumes/Aquarium
>  >  > Aquarium at ediaback-feb25         108M      -  29.8G  -
>  >  > Aquarium at erlangback-20080225   8.47M      -  30.4G  -
>  >  > Aquarium at __zb_full_1204057064  22.1M      -  32.4G  -
>  >  > Aquarium at __zb_incr               42K      -  32.9G  -
>  >  >
>  >  > zfs get Aqarium:
>  >  >
>  >  > NAME                           PROPERTY       VALUE
>  >  > SOURCE
>  >  > Aquarium                       type           filesystem             -
>  >  > Aquarium                       creation       Sun Feb 24 18:49 2008  -
>  >  > Aquarium                       used           33.4G                  -
>  >  > Aquarium                       available      53.7G                  -
>  >  > Aquarium                       referenced     32.9G                  -
>  >  > Aquarium                       compressratio  1.21x                  -
>  >  > Aquarium                       mounted        yes                    -
>  >  > Aquarium                       quota          none
>  >  > default
>  >  > Aquarium                       reservation    none
>  >  > default
>  >  > Aquarium                       recordsize     128K
>  >  > default
>  >  > Aquarium                       mountpoint     /Volumes/Aquarium
>  >  > default
>  >  > Aquarium                       sharenfs       off
>  >  > default
>  >  > Aquarium                       checksum       on
>  >  > default
>  >  > Aquarium                       compression    on
>  >  > local
>  >  > Aquarium                       atime          off
>  >  > local
>  >  > Aquarium                       devices        on
>  >  > default
>  >  > Aquarium                       exec           on
>  >  > default
>  >  > Aquarium                       setuid         on
>  >  > default
>  >  > Aquarium                       readonly       off
>  >  > default
>  >  > Aquarium                       zoned          off
>  >  > default
>  >  > Aquarium                       snapdir        hidden
>  >  > default
>  >  > Aquarium                       aclmode        groupmask
>  >  > default
>  >  > Aquarium                       aclinherit     secure
>  >  > default
>  >  > Aquarium                       canmount       on
>  >  > default
>  >  > Aquarium                       shareiscsi     off
>  >  > default
>  >  > Aquarium                       xattr          on
>  >  > default
>  >  > Aquarium                       copies         1
>  >  > default
>  >  > Aquarium                       version        1                      -
>  >  >
>  >  >
>  >  > zpool list:
>  >  > NAME                    SIZE    USED   AVAIL    CAP  HEALTH
>  >  > ALTROOT
>  >  > Aquarium               88.5G   33.4G   55.1G    37%  ONLINE     -
>  >  >
>  >  > zpool status:
>  >  >  pool: Aquarium
>  >  > state: ONLINE
>  >  > status: The pool is formatted using an older on-disk format.  The
>  >  > pool can
>  >  >       still be used, but some features are unavailable.
>  >  > action: Upgrade the pool using 'zpool upgrade'.  Once this is done,
>  >  > the
>  >  >       pool will no longer be accessible on older software versions.
>  >  > scrub: scrub in progress, 77.33% done, 0h7m to go
>  >  > config:
>  >  >
>  >  >       NAME        STATE     READ WRITE CKSUM
>  >  >       Aquarium    ONLINE       0     0     0
>  >  >         disk0s4   ONLINE       0     0     0
>  >  >         disk0s3   ONLINE       0     0     0
>  >  >
>  >  > And the panics:
>  >  >
>  >  > Wed Feb 27 18:19:56 2008
>  >  > panic(cpu 1 caller 0x001A83A6): Double fault at 0x00c98d23,
>  >  > thread:0x656e048, trapno:0x8, err:0x0),registers:
>  >  > CR0: 0x80010033, CR2: 0x549eff88, CR3: 0x0118a000, CR4: 0x00000660
>  >  > EAX: 0x0501b000, EBX: 0x50539c00, ECX: 0x00000000, EDX: 0x0007a867
>  >  > ESP: 0x549eff90, EBP: 0x549f0038, ESI: 0x5044b5a0, EDI: 0x0501b0b8
>  >  > EFL: 0x00010206, EIP: 0x00c98d23
>  >  >
>  >  > Backtrace, Format - Frame : Return Address (4 potential args on stack)
>  >  > 0x2e6a5fb8 : 0x12b0e1 (0x457024 0x2e6a5fec 0x13321a 0x0)
>  >  > 0x2e6a6008 : 0x1a83a6 (0x45ffd4 0xc98d23 0x656e048 0x8)
>  >  > 0x2e6a60e8 : 0x19fbc3 (0x2e6a6100 0xeeeeeeee 0xeeeeeeee 0xeeeeeeee)
>  >  > 0x549f0038 : 0xca1598 (0x549f01ec 0x0 0x0 0x549f01ec)
>  >  > 0x549f0208 : 0xc97fa1 (0x5044b588 0x206 0x549f0238 0x51ad8c0)
>  >  > 0x549f0248 : 0xc41e5b (0x5044b588 0x1 0x549f02d8 0x51ad8c0)
>  >  > 0x549f0288 : 0x1f2eb1 (0x549f02a4 0x8 0x549f02ac 0x0)
>  >  > 0x549f02c8 : 0x1da45f (0x502b870 0x54af144 0x0 0x2)
>  >  > 0x549f0318 : 0x1da67b (0x502b870 0x1 0x549f0348 0x19d311)
>  >  > 0x549f0368 : 0x1dc2e6 (0x0 0x4f33c0 0x549f0408 0x37c76738)
>  >  > 0x549f03e8 : 0xc98ca5 (0x0 0x34 0x549f041c 0x37c766c4)
>  >  > 0x549f0468 : 0xc990ba (0x501b248 0x0 0x200 0x0)
>  >  > 0x549f0518 : 0xca1598 (0x549f06cc 0x0 0x0 0x549f06cc)
>  >  > 0x549f06e8 : 0xc97fa1 (0x5044b670 0x206 0x549f0718 0x51ad94c)
>  >  > 0x549f0728 : 0xc41e5b (0x5044b670 0x1 0x549f07b8 0x51ad94c)
>  >  > 0x549f0768 : 0x1f2eb1 (0x549f0784 0x8 0x549f078c 0x0)
>  >  >       Backtrace continues...
>  >  >      Kernel loadable modules in backtrace (with dependencies):
>  >  >         com.apple.filesystems.zfs(8.0)@0xc3a000->0xd05fff
>  >  >
>  >  > BSD process name corresponding to current thread: Xquartz
>  >  >
>  >  > Mac OS version:
>  >  > 9C31
>  >  >
>  >  > Kernel version:
>  >  > Darwin Kernel Version 9.2.0: Tue Feb  5 16:13:22 PST 2008;
>  >  > root:xnu-1228.3.13~1/RELEASE_I386
>  >  > System model name: MacBook2,1 (Mac-F4208CAA)
>  >  >
>  >  > Wed Feb 27 18:22:10 2008
>  >  > panic(cpu 0 caller 0x001A83A6): Double fault at 0x00c6211c,
>  >  > thread:0x5065d60, trapno:0x8, err:0x0),registers:
>  >  > CR0: 0x80010033, CR2: 0x53427fc8, CR3: 0x0118a000, CR4: 0x00000660
>  >  > EAX: 0x534280ac, EBX: 0x5050ed64, ECX: 0x0516568c, EDX: 0x00004000
>  >  > ESP: 0x53427fd0, EBP: 0x53428118, ESI: 0x00000000, EDI: 0x00000000
>  >  > EFL: 0x00010246, EIP: 0x00c6211c
>  >  >
>  >  > Backtrace, Format - Frame : Return Address (4 potential args on stack)
>  >  > 0x4e9e28 : 0x12b0e1 (0x457024 0x4e9e5c 0x13321a 0x0)
>  >  > 0x4e9e78 : 0x1a83a6 (0x45ffd4 0xc6211c 0x5065d60 0x8)
>  >  > 0x4e9f58 : 0x19fbc3 (0x4e9f70 0x0 0x0 0x0)
>  >  > 0x53428118 : 0xc59f26 (0x516568c 0xf510000 0x0 0x4000)
>  >  > 0x534281a8 : 0xc676be (0x5050ed10 0x0 0x2 0xcd2d98)
>  >  > 0x534281f8 : 0xc6794e (0x4d1af00 0x7a894 0x0 0x1)
>  >  > 0x53428228 : 0xc62fde (0x4d1af00 0x7a894 0x0 0xccf9a4)
>  >  > 0x53428288 : 0xc98d92 (0x4f43770 0x7a894 0x0 0x0)
>  >  > 0x53428338 : 0xca1598 (0x534284ec 0x0 0x0 0x534284ec)
>  >  > 0x53428508 : 0xc97fa1 (0x503c7670 0x216 0x53428538 0x5192d20)
>  >  > 0x53428548 : 0xc41e5b (0x503c7670 0x1 0x53428598 0x5192d20)
>  >  > 0x53428588 : 0x1f2eb1 (0x534285a4 0x8 0x534285ac 0x0)
>  >  > 0x534285c8 : 0x1da45f (0x518ac70 0x5062024 0x0 0x2)
>  >  > 0x53428618 : 0x1da67b (0x518ac70 0x1 0x53428648 0x19d311)
>  >  > 0x53428668 : 0x1dc2e6 (0x0 0x4f33c0 0x53428708 0x38904690)
>  >  > 0x534286e8 : 0xc98ca5 (0x0 0x34 0x5342871c 0x3890461c)
>  >  >       Backtrace continues...
>  >  >      Kernel loadable modules in backtrace (with dependencies):
>  >  >         com.apple.filesystems.zfs(8.0)@0xc3a000->0xd05fff
>  >  >
>  >  > BSD process name corresponding to current thread: pbs
>  >  >
>  >  > Mac OS version:
>  >  > 9C31
>  >  >
>  >  > Kernel version:
>  >  > Darwin Kernel Version 9.2.0: Tue Feb  5 16:13:22 PST 2008;
>  >  > root:xnu-1228.3.13~1/RELEASE_I386
>  >  > System model name: MacBook2,1 (Mac-F4208CAA)
>  >  >
>  >  > Wed Feb 27 18:24:17 2008
>  >  > panic(cpu 1 caller 0x001A83A6): Double fault at 0x00c62fd9,
>  >  > thread:0x50f2f20, trapno:0x8, err:0x0),registers:
>  >  > CR0: 0x80010033, CR2: 0x546cfff8, CR3: 0x0118a000, CR4: 0x00000660
>  >  > EAX: 0x04f52800, EBX: 0x05153a48, ECX: 0x05153800, EDX: 0x00000000
>  >  > ESP: 0x546d0000, EBP: 0x546d0058, ESI: 0x516575a0, EDI: 0x051538b8
>  >  > EFL: 0x00010206, EIP: 0x00c62fd9
>  >  >
>  >  > Backtrace, Format - Frame : Return Address (4 potential args on stack)
>  >  > 0x2e6a5fb8 : 0x12b0e1 (0x457024 0x2e6a5fec 0x13321a 0x0)
>  >  > 0x2e6a6008 : 0x1a83a6 (0x45ffd4 0xc62fd9 0x50f2f20 0x8)
>  >  > 0x2e6a60e8 : 0x19fbc3 (0x2e6a6100 0xeeeeeeee 0xeeeeeeee 0xeeeeeeee)
>  >  > 0x546d0058 : 0xc98d92 (0x4974290 0x7a894 0x0 0x0)
>  >  > 0x546d0108 : 0xca1598 (0x546d02bc 0x0 0x0 0x546d02bc)
>  >  > 0x546d02d8 : 0xc97fa1 (0x51657588 0x212 0x546d0308 0x5176118)
>  >  > 0x546d0318 : 0xc41e5b (0x51657588 0x1 0x546d0368 0x5176118)
>  >  > 0x546d0358 : 0x1f2eb1 (0x546d0374 0x8 0x546d037c 0x0)
>  >  > 0x546d0398 : 0x1da45f (0x5172760 0x50f0324 0x0 0x2)
>  >  > 0x546d03e8 : 0x1da67b (0x5172760 0x1 0x546d0418 0x19d311)
>  >  > 0x546d0438 : 0x1dc2e6 (0x0 0x4f33c0 0x546d04d8 0x377906f0)
>  >  > 0x546d04b8 : 0xc98ca5 (0x0 0x34 0x546d04ec 0x3779067c)
>  >  > 0x546d0538 : 0xc990ba (0x5153b9c 0x0 0x200 0x0)
>  >  > 0x546d05e8 : 0xca1598 (0x546d079c 0x0 0x0 0x546d079c)
>  >  > 0x546d07b8 : 0xc97fa1 (0x51657670 0x212 0x546d07e8 0x5176460)
>  >  > 0x546d07f8 : 0xc41e5b (0x51657670 0x1 0x546d0848 0x5176460)
>  >  >       Backtrace continues...
>  >  >      Kernel loadable modules in backtrace (with dependencies):
>  >  >         com.apple.filesystems.zfs(8.0)@0xc3a000->0xd05fff
>  >  >
>  >  > BSD process name corresponding to current thread: Unison
>  >  >
>  >  > Mac OS version:
>  >  > 9C31
>  >  >
>  >  > Kernel version:
>  >  > Darwin Kernel Version 9.2.0: Tue Feb  5 16:13:22 PST 2008;
>  >  > root:xnu-1228.3.13~1/RELEASE_I386
>  >  > System model name: MacBook2,1 (Mac-F4208CAA)
>  >  >
>  >  >
>  >  > Not sure what else might be helpful :-)
>  >  >
>  >  > --
>  >  > James Snyder
>  >  > Biomedical Engineering
>  >  > Northwestern University
>  >  > jbsnyder at gmail.com
>  >  > _______________________________________________
>  >  > zfs-discuss mailing list
>  >  > zfs-discuss at lists.macosforge.org
>  >  > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>  >
>  >
>
>
>
>  --
>  James Snyder
>  Biomedical Engineering
>  Northwestern University
>  jbsnyder at gmail.com
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder at gmail.com

From gagix at mac.com  Thu Feb 28 20:37:42 2008
From: gagix at mac.com (Gautam Godse)
Date: Thu, 28 Feb 2008 20:37:42 -0800
Subject: [zfs-discuss] Recurring warning message - "Disk Insertion"
Message-ID: <311F5338-6EC0-4599-9D2A-4BA9BEA7362F@mac.com>

hi
I don't know if this is a feature or a bug, but since I created a  
zpool, whenever I power on the disk array, i get the following  
recurring message as follows:

Message: "Disk Insertion - The disk you inserted was not readable by  
this computer" with buttons for Initialize, Ignore, Eject

1. I get this message once upon power on. I get 3 dialog boxes with  
the same message even though I have 4 physical hard disks in the array.
2. I get this message every time i import the zpool
3. And again everytime i copy files onto the zpool.

If i do not take action on the message, it goes away after a few  
minutes..

Is this expected ? If not, how do i stop this message from showing up  
everytime I try to write/access the zpool.

--
Gautam

From gagix at mac.com  Thu Feb 28 20:48:41 2008
From: gagix at mac.com (Gautam Godse)
Date: Thu, 28 Feb 2008 20:48:41 -0800
Subject: [zfs-discuss] Recurring warning message - "Disk Insertion"
In-Reply-To: <311F5338-6EC0-4599-9D2A-4BA9BEA7362F@mac.com>
References: <311F5338-6EC0-4599-9D2A-4BA9BEA7362F@mac.com>
Message-ID: <A3DF00BC-E664-4614-BD6F-78B94B583D93@mac.com>

Ahh. I think I found the problem. None of my disks have the GPT label..
------
/dev/disk2
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:                                                   *465.8 Gi    
disk2
/dev/disk3
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:     Apple_partition_scheme                        *465.8 Gi    
disk3
/dev/disk4
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:     FDisk_partition_scheme                        *465.8 Gi    
disk4
/dev/disk5
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:                                                   *465.8 Gi    
disk5
-----------

The ZFS Mac docs state that the GPT label is required for ZFS to work,  
but how has it been working all this while for me ???

On Feb 28, 2008, at 8:37 PM, Gautam Godse wrote:

> hi
> I don't know if this is a feature or a bug, but since I created a
> zpool, whenever I power on the disk array, i get the following
> recurring message as follows:
>
> Message: "Disk Insertion - The disk you inserted was not readable by
> this computer" with buttons for Initialize, Ignore, Eject
>
> 1. I get this message once upon power on. I get 3 dialog boxes with
> the same message even though I have 4 physical hard disks in the  
> array.
> 2. I get this message every time i import the zpool
> 3. And again everytime i copy files onto the zpool.
>
> If i do not take action on the message, it goes away after a few
> minutes..
>
> Is this expected ? If not, how do i stop this message from showing up
> everytime I try to write/access the zpool.
>
> --
> Gautam
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From Phil.Harman at Sun.COM  Fri Feb 29 06:15:15 2008
From: Phil.Harman at Sun.COM (Phil Harman)
Date: Fri, 29 Feb 2008 14:15:15 +0000
Subject: [zfs-discuss] panic with VMware during zfs rollback
Message-ID: <47C81373.7050701@sun.com>

Hi,

This is my second day with ZFS on MacOS (10.5.2), but I have been using 
ZFS for a couple of years in Solaris.

I've been experimenting with using ZFS (on an external firewire drive) 
to host VMware Fusion virtual machines (snapshots and clones are 
particularly useful for this).

Yesterday, I installed a Solaris instance in a VM hosted on my internal 
drive (formatted with HFS+).

This morning I copied the VM to my external drive (formatted with ZFS), 
took a snapshot, booted it, took another snapshot, and then tried some 
customisation.

Then I decided to discard my changes, so I stopped the VM and tried ...

# zfs rollback ext0/fs/tmp

However, this failed with a warning that the filesystem was busy.

I then tried ...

# zfs rollback -f ext0/fs/tmp

which produced this panic ...

Fri Feb 29 08:29:59 2008
panic(cpu 1 caller 0x001A8C8A): Kernel trap at 0x0019a250, type 14=page 
fault, registers:
CR0: 0x8001003b, CR2: 0xdeadbf31, CR3: 0x00d64000, CR4: 0x00000660
EAX: 0xdeadbf31, EBX: 0x00000001, ECX: 0x00000001, EDX: 0xdeadbf31
CR2: 0xdeadbf31, EBP: 0x665637b8, ESI: 0xdeadbf2f, EDI: 0xdeadbeef
EFL: 0x00010002, EIP: 0x0019a250, CS:  0x00000008, DS:  0x00000010
Error code: 0x00000000

Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x665635b8 : 0x12b0e1 (0x457024 0x665635ec 0x13321a 0x0)
0x66563608 : 0x1a8c8a (0x460550 0x19a250 0xe 0x45fd00)
0x665636e8 : 0x19eb67 (0x66563700 0x552c6020 0x665637b8 0x19a250)
0x665636f8 : 0x19a250 (0xe 0x12e70048 0x66560010 0x120010)
0x665637b8 : 0x19d87a (0x67198c0 0x552a0000 0x66563808 0x19d311)
0x665637f8 : 0x5caf0382 (0xdeadbf2f 0x1 0x66563818 0x1a236f)
0x66563818 : 0x5ca9c8f5 (0xdeadbf2f 0x1 0x66563848 0x19d569)
0x665638b8 : 0x1f3137 (0x665638dc 0x0 0x66563998 0x17ae3c)
0x66563918 : 0x3bd960 (0xa8d3240 0x83bab00 0x0 0x2e432000)
0x66563998 : 0x185de0 (0xa8d3240 0x83bab00 0x0 0x2e432000)
0x66563a08 : 0x186139 (0xbb58b40 0x2e432000 0x0 0x4000)
0x66563a38 : 0x15e6d3 (0xbb58b40 0x2e432000 0x0 0x4000)
0x66563c78 : 0x15ec83 (0xb5018c0 0x0 0x0 0x40000000)
0x66563cd8 : 0x397405 (0xbaeb3f0 0x0 0x0 0x40000000)
0x66563d38 : 0x1da437 (0x40000000 0x0 0x0 0x2)
0x66563d88 : 0x1da67b (0xa8d3240 0x1 0x66563dd8 0x1f2e30)
   Backtrace continues...
     Kernel loadable modules in backtrace (with dependencies):
        com.apple.filesystems.zfs(8.0)@0x5ca95000->0x5cb60fff

BSD process name corresponding to current thread: vmware-vmx

Mac OS version:
9C31

Kernel version:
Darwin Kernel Version 9.2.0: Tue Feb  5 16:13:22 PST 2008; 
root:xnu-1228.3.13~1/RELEASE_I386
System model name: MacBookPro3,1 (Mac-F42388C8)



So, I guess the responsibility is lies with ZFS, VMware or both.

The next time I tried the same thing, I didn't just stop the VM, but I 
quit Fusion too, and this worked just fine.

Phil
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/x-pkcs7-signature
Size: 3383 bytes
Desc: S/MIME Cryptographic Signature
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080229/e8530b5a/attachment-0001.bin 

From Phil.Harman at Sun.COM  Fri Feb 29 06:24:08 2008
From: Phil.Harman at Sun.COM (Phil Harman)
Date: Fri, 29 Feb 2008 14:24:08 +0000
Subject: [zfs-discuss] ZFS,
	VMware Fusion and poor Solaris install I/O performance
Message-ID: <47C81588.3080701@sun.com>

I have VMware Fusion on MacOS 10.5.2 with the latest ZFS binaries.

As per my panic email, I'm attempting to install Solaris on an external 
firewire drive formatted with ZFS.

I've done several installs on my internal drive (formatted with HFS+) 
and they are pretty quick.

Installs on ZFS are very slow, and I've been intrigued by the following 
zpool iostats ...

               capacity     operations    bandwidth
pool         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
ext0        65.8G  82.2G    116     22  14.6M  1.52M
  disk1s2   65.8G  82.2G    116     22  14.6M  1.52M
----------  -----  -----  -----  -----  -----  -----

               capacity     operations    bandwidth
pool         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
ext0        65.8G  82.2G     98     26  12.2M  3.07M
  disk1s2   65.8G  82.2G     98     26  12.2M  3.07M
----------  -----  -----  -----  -----  -----  -----

               capacity     operations    bandwidth
pool         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
ext0        65.8G  82.2G     81     46  10.1M  3.73M
  disk1s2   65.8G  82.2G     81     46  10.1M  3.73M
----------  -----  -----  -----  -----  -----  -----

Basically, it's doing a lot more reading than writing. Yet this should 
me mostly write (it's an OS install).

I'm a big DTrace user in the Solaris domain, and I'd love to use DTrace 
to investigate this, but I don't see any probes with the string 'zfs' 
anywhere when I type ...

# dtrace -l | grep -i zfs

I'd also like to know what configuration and monitoring options I have 
on MacOS for the ARC etc.

Phil
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/x-pkcs7-signature
Size: 3383 bytes
Desc: S/MIME Cryptographic Signature
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080229/dd04457b/attachment.bin 

From franzschmalzl at spamfreemail.de  Fri Feb 29 08:36:50 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Fri, 29 Feb 2008 17:36:50 +0100
Subject: [zfs-discuss] zpool export
In-Reply-To: <F83178A0-83FB-48CD-85C3-ED664C01FBCF@apple.com>
References: <E837B12D-404E-482E-9B07-B86E8D02D46C@spamfreemail.de>
	<F83178A0-83FB-48CD-85C3-ED664C01FBCF@apple.com>
Message-ID: <D088A790-F024-41FC-A448-C076E49BA1EA@spamfreemail.de>


On 29.02.2008, at 02:47, No?l Dellofano wrote:

> We are working on trying to change the system so things like this  
> are streamlined more.  And you can just do a 'zpool export' instead  
> of unmounting and then exporting.  The unmount happens automatically  
> as part of the export.  But yes, this is a bit confusing for  
> external drives especially since you tend to want to remove them  
> easily or they may get bumped out.  We're working on improving this  
> behavior.
>
> Noel


thanks for the info!

another question

export as normal user strangely does not work for me, sudo is needed
altough the mountpoint belongs to the normal user
(unmounting the volume via the gui also needs an admin password)
mounting as normal user works tough


thanks for your answer


>
>
> On Feb 24, 2008, at 8:59 AM, Franz Schmalzl wrote:
>
>> i have to do a zpool export after unmounting a pool, i know
>> (kernel panic when replugging the underlaying device if i don't)
>>
>> are there plans to change this behaviour ?
>> (maybe an automatic export )
>>
>> regards
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From cshuman at nordam.com  Fri Feb 29 09:10:33 2008
From: cshuman at nordam.com (Shuman, Chris)
Date: Fri, 29 Feb 2008 11:10:33 -0600
Subject: [zfs-discuss] Finder copy fails with "File In Use"
Message-ID: <4AC87E46E30CE84EA59AD3E5DE7D3F3501EF96D9@PCINSEX1.nordam.com>

I've been having problems while trying to do simple finder copies from,
(not limited to)  ~/Music to filesystem ( zdata/home/chris/Music ). I
get an error indicating that the file is in use. The files have not even
been recently opened and I've tried to insure that the system isn't
generating a preview pane (column view), but I still see the error quite
often. Copy gets aborted at that time. Any ideas?

 

1.6Ghz G5, 10.5.2, zfs vs 6, mirrored raid via USB

 

Chris



This message is intended only for the named recipient(s). If you are not one of the named recipients, you have received this message in error. Please notify the sender immediately and delete the message (along with its attachments) in a way so that no image of it is retrievable or accessible from the computer where this message has been displayed or from any network to which such computer is connected. Any use or disclosure of this message or its contents/attachments by anyone other than the intended recipient(s) is expressly prohibited and could result in civil or criminal liability.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080229/6de350ed/attachment.html 

From zorg at sogeeky.net  Fri Feb 29 12:33:47 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Fri, 29 Feb 2008 12:33:47 -0800
Subject: [zfs-discuss] Finder copy fails with "File In Use"
In-Reply-To: <4AC87E46E30CE84EA59AD3E5DE7D3F3501EF96D9@PCINSEX1.nordam.com>
References: <4AC87E46E30CE84EA59AD3E5DE7D3F3501EF96D9@PCINSEX1.nordam.com>
Message-ID: <904B2CE1-0364-45BF-8415-300D2EB4785E@sogeeky.net>

Are you by chance accessing these over afp?  I've seen that behavior  
on zfs shared over afp a lot, particularly with folders.

On Feb 29, 2008, at 9:10 AM, "Shuman, Chris" <cshuman at nordam.com> wrote:

> I?ve been having problems while trying to do simple finder copies fr 
> om, (not limited to)  ~/Music to filesystem ( zdata/home/chris/Music 
>  ). I get an error indicating that the file is in use. The files hav 
> e not even been recently opened and I?ve tried to insure that the sy 
> stem isn?t generating a preview pane (column view), but I still see  
> the error quite often. Copy gets aborted at that time. Any ideas?
>
>
>
> 1.6Ghz G5, 10.5.2, zfs vs 6, mirrored raid via USB
>
>
>
> Chris
>
>
> This message is intended only for the named recipient(s). If you are  
> not one of the named recipients, you have received this message in  
> error. Please notify the sender immediately and delete the message  
> (along with its attachments) in a way so that no image of it is  
> retrievable or accessible from the computer where this message has  
> been displayed or from any network to which such computer is  
> connected. Any use or disclosure of this message or its contents/ 
> attachments by anyone other than the intended recipient(s) is
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080229/c81865c4/attachment.html 

From cshuman at nordam.com  Fri Feb 29 12:41:03 2008
From: cshuman at nordam.com (Shuman, Chris)
Date: Fri, 29 Feb 2008 14:41:03 -0600
Subject: [zfs-discuss] Finder copy fails with "File In Use"
In-Reply-To: <904B2CE1-0364-45BF-8415-300D2EB4785E@sogeeky.net>
References: <4AC87E46E30CE84EA59AD3E5DE7D3F3501EF96D9@PCINSEX1.nordam.com>
	<904B2CE1-0364-45BF-8415-300D2EB4785E@sogeeky.net>
Message-ID: <4AC87E46E30CE84EA59AD3E5DE7D3F3501EF9726@PCINSEX1.nordam.com>

No. The copy from and to are both local. Copying from internal HFS+
formatted SATA drive to an externally connected USB zpool. Both mounted
in the finder. Really, just drag and drop. Can't get much simpler,
right?  ; )

-Chris

________________________________

From: Mr. Zorg [mailto:zorg at sogeeky.net] 
Sent: Friday, February 29, 2008 2:34 PM
To: Shuman, Chris
Cc: <zfs-discuss at lists.macosforge.org>
Subject: Re: [zfs-discuss] Finder copy fails with "File In Use"

 

Are you by chance accessing these over afp?  I've seen that behavior on
zfs shared over afp a lot, particularly with folders. 

On Feb 29, 2008, at 9:10 AM, "Shuman, Chris" <cshuman at nordam.com> wrote:

	I've been having problems while trying to do simple finder
copies from, (not limited to)  ~/Music to filesystem (
zdata/home/chris/Music ). I get an error indicating that the file is in
use. The files have not even been recently opened and I've tried to
insure that the system isn't generating a preview pane (column view),
but I still see the error quite often. Copy gets aborted at that time.
Any ideas?

	 

	1.6Ghz G5, 10.5.2, zfs vs 6, mirrored raid via USB

	 

	Chris

	
	This message is intended only for the named recipient(s). If you
are not one of the named recipients, you have received this message in
error. Please notify the sender immediately and delete the message
(along with its attachments) in a way so that no image of it is
retrievable or accessible from the computer where this message has been
displayed or from any network to which such computer is connected. Any
use or disclosure of this message or its contents/attachments by anyone
other than the intended recipient(s) is

	_______________________________________________
	zfs-discuss mailing list
	zfs-discuss at lists.macosforge.org
	http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss



This message is intended only for the named recipient(s). If you are not one of the named recipients, you have received this message in error. Please notify the sender immediately and delete the message (along with its attachments) in a way so that no image of it is retrievable or accessible from the computer where this message has been displayed or from any network to which such computer is connected. Any use or disclosure of this message or its contents/attachments by anyone other than the intended recipient(s) is expressly prohibited and could result in civil or criminal liability.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080229/6caf4fb6/attachment-0001.html 

