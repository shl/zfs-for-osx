<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [zfs-discuss] Re: RAIDZ continuous panic after zpool replace
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=%5Bzfs-discuss%5D%20Re%3A%20RAIDZ%20continuous%20panic%20after%20zpool%20replace&In-Reply-To=B2B4DD52-E3A7-4C3F-9CC8-0F6CD55E00A9%40electricteaparty.net">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000485.html">
   <LINK REL="Next"  HREF="000486.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[zfs-discuss] Re: RAIDZ continuous panic after zpool replace</H1>
    <B>Jonathan Edwards</B> 
    <A HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=%5Bzfs-discuss%5D%20Re%3A%20RAIDZ%20continuous%20panic%20after%20zpool%20replace&In-Reply-To=B2B4DD52-E3A7-4C3F-9CC8-0F6CD55E00A9%40electricteaparty.net"
       TITLE="[zfs-discuss] Re: RAIDZ continuous panic after zpool replace">Jonathan.Edwards at Sun.COM
       </A><BR>
    <I>Wed Apr 16 03:55:41 PDT 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="000485.html">[zfs-discuss] Re: RAIDZ continuous panic after zpool replace
</A></li>
        <LI>Next message: <A HREF="000486.html">[zfs-discuss] ram
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#487">[ date ]</a>
              <a href="thread.html#487">[ thread ]</a>
              <a href="subject.html#487">[ subject ]</a>
              <a href="author.html#487">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>in this case what you're seeing is the inability to get the config  
information off one of the disks (it's telling you the device  
identifier it can't find) .. possibly a corrupt uberblock .. if you  
were simply replacing disks i'm thinking you should have seen 9  
devices during the replace - so while i'm not sure what devices you're  
replacing with other devices i'm guessing there's another device  
somewhere that might be missing from the config as you may not have  
enough valid data to reliably bring the pool around .. you're in a  
sticky situation, and i've noticed that zdb isn't distributed with the  
macosforge port (which can be helpful for diagnosing label problems)

on a side note - there's been known problems with the Si3114  
controller and data corruption - you may want to consider a different  
controller and get your data off this hardware (see: <A HREF="http://opensolaris.org/jive/thread.jspa?messageID=196275">http://opensolaris.org/jive/thread.jspa?messageID=196275</A>) 
  if you can at this point ..

on another side note .. the SXDE should get you newer bits than  
whatever solaris build you were trying to import the pool into ..  
belenix i believe is around nv72 (8/2007) .. the last SXDE is going to  
be nv79b (01/2008), and the current nevada build should be nv86 or  
nv87 (04/2008)

On Apr 16, 2008, at 2:42 AM, Thornton Adrian wrote:

&gt;<i> Turns out a Belenix LiveCD recognizes my SATA card and sees my pool.  
</I>&gt;<i> But it's not all good news. For some reason, though Belenix shows  
</I>&gt;<i> seven out of eight devices online and happy, it still claims it  
</I>&gt;<i> can't import the pool. Here's the zpool import:
</I>&gt;<i>
</I>&gt;<i> pool: Archives
</I>&gt;<i> id: 16750878087681595004
</I>&gt;<i> state: UNAVAIL
</I>&gt;<i> status: One or more devices are missing from the system.
</I>&gt;<i> action: The pool cannot be imported. Attach the missing devices and  
</I>&gt;<i> try again.
</I>&gt;<i> see: <A HREF="http://www.sun.com/msg/ZFS-8000-3C">http://www.sun.com/msg/ZFS-8000-3C</A>
</I>&gt;<i> config:
</I>&gt;<i>
</I>&gt;<i> 	Archives		UNAVAIL	insufficient replicas
</I>&gt;<i> 		raidz1	UNAVAIL	corrupted data
</I>&gt;<i> 		c8d1p0	ONLINE
</I>&gt;<i> 		c2t1d0p0	ONLINE
</I>&gt;<i> 	`	c7d1p0	ONLINE
</I>&gt;<i> 		c2t0d0p0	ONLINE
</I>&gt;<i> 		c8d0s1	ONLINE
</I>&gt;<i> 		c4d0s1	ONLINE
</I>&gt;<i> 		636893339691769212	UNAVAIL	cannot open
</I>&gt;<i> 		c5d0s1	ONLINE
</I>&gt;<i>
</I>&gt;<i> Looks fine, right? (Well, aside from the fact is should say DEGRADED  
</I>&gt;<i> not UNAVAIL.) One missing disk in an 8-disk raidz1 pool. But zpool  
</I>&gt;<i> import -f Archives says:
</I>&gt;<i>
</I>&gt;<i> cannot import 'Archives': invalid vdev configuration
</I>&gt;<i>
</I>&gt;<i> So... what's going on then? Any clues?
</I>&gt;<i>
</I>&gt;<i> Cheers,
</I>&gt;<i> Adrian
</I>&gt;<i>
</I>&gt;<i> On 15-Apr-08, at 8:37 PM, Thornton Adrian wrote:
</I>&gt;&gt;<i> I managed to boot in single-user mode and a zpool import showed the  
</I>&gt;&gt;<i> pool with one faulted device, just as it should. I thought maybe I  
</I>&gt;&gt;<i> could do a zpool import -f poolname followed rapidly by a zpool  
</I>&gt;&gt;<i> destroy poolname to clear any tasks, and then import -f again, but  
</I>&gt;&gt;<i> I got a panic. Luckily it wrote the details to the screen this  
</I>&gt;&gt;<i> time, so it's time for me to type...
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> panic(cpu 1 caller 0x37046B71): &quot;[ZFS]: assertion failed in / 
</I>&gt;&gt;<i> Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/zfs_kext/ 
</I>&gt;&gt;<i> zfs/dmu.c line 437: 0 == dmu_buf_hold_array(os, object, offset,  
</I>&gt;&gt;<i> size, FALSE, FTAG, &amp;numbufs, &amp;dbp)&quot;@Volumes/pixie_d
</I>&gt;&gt;<i> Debugger called: &lt;panic&gt;
</I>&gt;&gt;<i> Backtrace, Format - Frame : Return Address (4 potential args on  
</I>&gt;&gt;<i> stack)
</I>&gt;&gt;<i> 0x391b7b78 : 0x12b0f7 (0x4581f4 0x391b7bac 0x133230 0x0)
</I>&gt;&gt;<i> 0x391b7bc8 : 0x37046b71 (0x370b2e0c 0x370b29e4 0x1b5 0x370b2dbc)
</I>&gt;&gt;<i> 0x391b7c58 : 0x3702efe9 (0x511fd18 0x1ec 0x0 0xe938)
</I>&gt;&gt;<i> 0x391b7d28 : 0x3702b9c9 (0x515ce20 0x1 0c515cc2c 0c511fd18)
</I>&gt;&gt;<i> 0x391b7db8 : 0x370730b7 (0x515cc00 0x25ca1f 0x0 0x0)
</I>&gt;&gt;<i> 0x391b7e08 : 0x3706508f (0x487f800 0x25ca1f 0x0 0x0)
</I>&gt;&gt;<i> 0x391b7f58 : 0x3704e22a (0x4a3d800 0x25ca1f 0x0 0x1369fb)
</I>&gt;&gt;<i> 0x391b7fc8 : 0x19eadc (0x511bc00 0x0 0x1a20b5 0x51bcba0)
</I>&gt;&gt;<i> Backtrace terminate-invalid frame pointer 0
</I>&gt;&gt;<i> 	Kernel loadable modules in backtrace (with dependencies):
</I>&gt;&gt;<i> 		com.apple.filesystems.zfs(8.0)@0x3701d000-&gt;0x370e8fff
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> BSD process name corresponding to current thread: kernel_task
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Mac OS version:
</I>&gt;&gt;<i> 9C7010
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> On 15-Apr-08, at 7:35 PM, Thornton Adrian wrote:
</I>&gt;&gt;&gt;<i> I'm having a big problem here. I was replacing the drives in my  
</I>&gt;&gt;&gt;<i> RAIDZ with bigger ones, and in the midst of a resilver the machine  
</I>&gt;&gt;&gt;<i> panicked. I rebooted, and now it panics shortly after the login  
</I>&gt;&gt;&gt;<i> screen every time I reboot the machine. &quot;Screw it,&quot; I thought, and  
</I>&gt;&gt;&gt;<i> moved the disks over to a Solaris 10 box to repair the array.  
</I>&gt;&gt;&gt;<i> Unfortunately, no matter what I do the Solaris box won't recognize  
</I>&gt;&gt;&gt;<i> my SiI 3114 card so I can't get it to see all the drives. The only  
</I>&gt;&gt;&gt;<i> way to get OS X to boot properly is in safe mode or by detaching  
</I>&gt;&gt;&gt;<i> some of the drives. In safe mode, zfs doesn't function.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Is there any way I can STOP it from doing whatever it's trying to  
</I>&gt;&gt;&gt;<i> do (resilver? scrub?) and figure out what the problem is? Or at  
</I>&gt;&gt;&gt;<i> least how to I find a log of the kernel panic so I can post it for  
</I>&gt;&gt;&gt;<i> your perusal? This is utterly aggravating because 7 of the 8  
</I>&gt;&gt;&gt;<i> drives in the RAIDZ should be perfectly fine, and my data should  
</I>&gt;&gt;&gt;<i> be intact. I just can't get the bloody thing to stop panicking if  
</I>&gt;&gt;&gt;<i> I have all the drives attached.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Thanks,
</I>&gt;&gt;&gt;<i> Adrian
</I>&gt;&gt;<i>
</I>&gt;<i>
</I>&gt;<i> _______________________________________________
</I>&gt;<i> zfs-discuss mailing list
</I>&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">zfs-discuss at lists.macosforge.org</A>
</I>&gt;<i> <A HREF="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">http://lists.macosforge.org/mailman/listinfo/zfs-discuss</A>
</I>
</PRE>



<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000485.html">[zfs-discuss] Re: RAIDZ continuous panic after zpool replace
</A></li>
	<LI>Next message: <A HREF="000486.html">[zfs-discuss] ram
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#487">[ date ]</a>
              <a href="thread.html#487">[ thread ]</a>
              <a href="subject.html#487">[ subject ]</a>
              <a href="author.html#487">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.macosforge.org/mailman/listinfo/zfs-discuss">More information about the zfs-discuss
mailing list</a><br>
</body></html>
