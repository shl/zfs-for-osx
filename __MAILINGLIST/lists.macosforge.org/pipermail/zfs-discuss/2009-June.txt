From dmz+lists at tffenterprises.com  Mon Jun  8 13:20:47 2009
From: dmz+lists at tffenterprises.com (Daniel M. Zimmerman)
Date: Mon, 08 Jun 2009 13:20:47 -0700
Subject: [zfs-discuss] ZFS in Snow Leopard
Message-ID: <366B67791694FCC702D6BE55@D-128-208-244-210.dhcp4.washington.edu>

So... now that ZFS has been officially removed from the feature list for 
Snow Leopard Server (see <http://www.apple.com/server/macosx/specs.html>, 
under "File Systems"), and presumably will also not appear in Snow Leopard 
non-Server, can anybody from Apple comment on the future of ZFS on Mac OS 
X?

I hope it will be supported as a plugin available from macosforge.org, at 
least... It would be sad to see it die entirely.

-Dan

------------------------------------------------------------------
Daniel M. Zimmerman                                TFF Enterprises
1900 Commerce St. Box 358426   http://www.tffenterprises.com/~dmz/
Tacoma, WA  98402  USA                      dmz at tffenterprises.com

From magnu213 at umn.edu  Mon Jun  8 13:31:22 2009
From: magnu213 at umn.edu (Carl Magnuson)
Date: Mon, 8 Jun 2009 15:31:22 -0500
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>	<A923A88C-9816-4E06-BD28-69296A50AB62@umn.edu>	<EC271224-1098-4A38-B4EB-B3716821EA35@umn.edu>	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
Message-ID: <6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>

I am writing after receiving a fixed external enclosure which has  
solved my previous problem (quoted email below).  My pool now starts  
up correctly however it looks like it is operating without two disks:

g5:~ bobcat$ zpool status -v
   pool: cpool
  state: DEGRADED
status: One or more devices could not be used because the label is  
missing or
	invalid.  Sufficient replicas exist for the pool to continue
	functioning in a degraded state.
action: Replace the device using 'zpool replace'.
    see: http://www.sun.com/msg/ZFS-8000-4J
  scrub: resilver completed with 0 errors on Mon Jun  8 15:15:31 2009
config:

	NAME                      STATE     READ WRITE CKSUM
	cpool                     DEGRADED     0     0     0
	  raidz2                  DEGRADED     0     0     0
	    disk2s2               ONLINE       0     0     0
	    disk3s2               ONLINE       0     0     0
	    disk6s2               ONLINE       0     0     0
	    disk5s2               ONLINE       0     0     0
	    disk4s2               ONLINE       0     0     0
	    1751454142452109774   FAULTED      0     0     0  was /dev/disk8s2
	    13962824586312699565  UNAVAIL      0     0     0  was /dev/disk9s2

errors: No known data errors

Looking in Disk Utility I see all seven disks in the array are  
recognized by OSX.  Is it possible that swapping drives around when  
switching to the new enclosure has caused some to not be found by  
ZFS?  Or maybe more likely that one or two died in the meantime?  Is  
there a way to identify which the two drives that are FAULTED and  
UNAVAIL are to easily swap them out or try readding them to the pool?

I realize this isn't strictly a OSX ZFS issue, but this seems like the  
best place to ask.

Thanks Much,

Carl



On Apr 21, 2009, at 11:01 PM, Carl Magnuson wrote:

> You are right on all counts.  I am using two 5 disk enclosures with  
> an eSATA port multiplier.  After playing around I see that the  
> indicator lights on one of the enclosures aren't indicating as they  
> should... none of the 5 drives from that enclosure are showing up,  
> but I can pull one and put it elsewhere and it will indicating a  
> problem with the enclosure.
>
> Thanks for the help, sorry to bother everyone with a non ZFS  
> problem, it is appreciated.
>
> Carl
>
>
>
>
> On Apr 21, 2009, at 4:53 PM, James R. Van Artsdalen wrote:
>
>> Carl Magnuson wrote:
>>> diskutil list shows two of my pool disks as disk3 and disk4. There  
>>> is no
>>> disk7 listed (though it is shown in the "zpool import" error), and  
>>> it
>>> looks like the pool disks 5 and 6 are missing and two of my other  
>>> disks
>>> took their place, though it is odd that no disk moved to disk2.  The
>>> complete output is shown below.
>>
>> Ignore the disk numbers output by zpool status: those are not  
>> meaningful
>> here.
>>
>> This isn't a ZFS error (yet).  Some disks in the pool simply aren't
>> being seen by OSX for whatever reason.  If diskutil can't see them  
>> then
>> neither can higher layers of OSX such as ZFS.  Fix this first - make
>> sure all pool members are visible to diskutil - then worry about ZFS.
>>
>> The number of disks missing is 5 - is there a 5-drive enclosure in  
>> use?
>> Check power & data cables, maybe cycle its power, etc.
>>
>> disk7 & disk8 look odd: are there a couple of eSATA port multiplier
>> enclosures involved?
>>
>


From bryanhenry at mac.com  Mon Jun  8 13:32:46 2009
From: bryanhenry at mac.com (Bryan Henry)
Date: Mon, 08 Jun 2009 13:32:46 -0700
Subject: [zfs-discuss] ZFS in Snow Leopard
In-Reply-To: <366B67791694FCC702D6BE55@D-128-208-244-210.dhcp4.washington.edu>
References: <366B67791694FCC702D6BE55@D-128-208-244-210.dhcp4.washington.edu>
Message-ID: <7B4BF05F-7E6D-4AC1-A8B7-82B13079DB49@mac.com>

Just a note: While it would be nice for someone working on the project  
to comment, keep in mind that most of Apple's engineers are going to  
be at WWDC all week and probably not as responsive as usual.

- Bryan

On Jun 8, 2009, at 1:20 PM, Daniel M. Zimmerman wrote:

> So... now that ZFS has been officially removed from the feature list  
> for Snow Leopard Server (see <http://www.apple.com/server/macosx/specs.html 
> >, under "File Systems"), and presumably will also not appear in  
> Snow Leopard non-Server, can anybody from Apple comment on the  
> future of ZFS on Mac OS X?
>
> I hope it will be supported as a plugin available from  
> macosforge.org, at least... It would be sad to see it die entirely.
>
> -Dan
>
> ------------------------------------------------------------------
> Daniel M. Zimmerman                                TFF Enterprises
> 1900 Commerce St. Box 358426   http://www.tffenterprises.com/~dmz/
> Tacoma, WA  98402  USA                      dmz at tffenterprises.com
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From sebastian at pixelmilk.com  Mon Jun  8 14:29:09 2009
From: sebastian at pixelmilk.com (=?ISO-8859-1?Q?Sebastian_D=F6ll?=)
Date: Mon, 8 Jun 2009 23:29:09 +0200
Subject: [zfs-discuss] ZFS in Snow Leopard
In-Reply-To: <366B67791694FCC702D6BE55@D-128-208-244-210.dhcp4.washington.edu>
References: <366B67791694FCC702D6BE55@D-128-208-244-210.dhcp4.washington.edu>
Message-ID: <900DC79C-ED35-41A5-B78A-8064C1DF54F2@pixelmilk.com>

ZFS is a feature of Snow Leopard Server, but not supported for booting  
or key services. Have a look under key technologies.
But nothing else was promised for the release. I think they will maybe  
work on ZFS and Xsan.

Am 08.06.2009 um 22:20 schrieb Daniel M. Zimmerman:

> So... now that ZFS has been officially removed from the feature list  
> for Snow Leopard Server (see <http://www.apple.com/server/macosx/specs.html 
> >, under "File Systems"), and presumably will also not appear in  
> Snow Leopard non-Server, can anybody from Apple comment on the  
> future of ZFS on Mac OS X?
>
> I hope it will be supported as a plugin available from  
> macosforge.org, at least... It would be sad to see it die entirely.
>
> -Dan
>
> ------------------------------------------------------------------
> Daniel M. Zimmerman                                TFF Enterprises
> 1900 Commerce St. Box 358426   http://www.tffenterprises.com/~dmz/
> Tacoma, WA  98402  USA                      dmz at tffenterprises.com
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4818 bytes
Desc: not available
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090608/4c828352/attachment.bin>

From james at themacplace.co.uk  Mon Jun  8 14:33:18 2009
From: james at themacplace.co.uk (James Relph)
Date: Mon, 8 Jun 2009 22:33:18 +0100
Subject: [zfs-discuss] ZFS in Snow Leopard
In-Reply-To: <900DC79C-ED35-41A5-B78A-8064C1DF54F2@pixelmilk.com>
References: <366B67791694FCC702D6BE55@D-128-208-244-210.dhcp4.washington.edu>
	<900DC79C-ED35-41A5-B78A-8064C1DF54F2@pixelmilk.com>
Message-ID: <08C6A108-18F8-4BD6-BD2A-CD43166B2141@themacplace.co.uk>

> ZFS is a feature of Snow Leopard Server, but not supported for  
> booting or key services. Have a look under key technologies.
> But nothing else was promised for the release. I think they will  
> maybe work on ZFS and Xsan.

I can't see anything listed on the current Snow Leopard Server site, http://www.apple.com/server/macosx/ 
  , on any of the pages, that mentions ZFS at all.  Am I just missing  
the section?

James Relph
ACSA 10.5

www.themacplace.co.uk

From sebastian at pixelmilk.com  Mon Jun  8 14:34:27 2009
From: sebastian at pixelmilk.com (=?ISO-8859-1?Q?Sebastian_D=F6ll?=)
Date: Mon, 8 Jun 2009 23:34:27 +0200
Subject: [zfs-discuss] ZFS in Snow Leopard
In-Reply-To: <08C6A108-18F8-4BD6-BD2A-CD43166B2141@themacplace.co.uk>
References: <366B67791694FCC702D6BE55@D-128-208-244-210.dhcp4.washington.edu>
	<900DC79C-ED35-41A5-B78A-8064C1DF54F2@pixelmilk.com>
	<08C6A108-18F8-4BD6-BD2A-CD43166B2141@themacplace.co.uk>
Message-ID: <DB529072-9C16-4D13-B999-F21C85B1780D@pixelmilk.com>

http://www.apple.com/server/macosx/specs.html

right at the bottom at key technologies



Am 08.06.2009 um 23:33 schrieb James Relph:

>> ZFS is a feature of Snow Leopard Server, but not supported for  
>> booting or key services. Have a look under key technologies.
>> But nothing else was promised for the release. I think they will  
>> maybe work on ZFS and Xsan.
>
> I can't see anything listed on the current Snow Leopard Server site, http://www.apple.com/server/macosx/ 
>  , on any of the pages, that mentions ZFS at all.  Am I just missing  
> the section?
>
> James Relph
> ACSA 10.5
>
> www.themacplace.co.uk
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4818 bytes
Desc: not available
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090608/3e3bab30/attachment-0001.bin>

From sebastian at pixelmilk.com  Mon Jun  8 14:35:42 2009
From: sebastian at pixelmilk.com (=?ISO-8859-1?Q?Sebastian_D=F6ll?=)
Date: Mon, 8 Jun 2009 23:35:42 +0200
Subject: [zfs-discuss] ZFS in Snow Leopard
In-Reply-To: <08C6A108-18F8-4BD6-BD2A-CD43166B2141@themacplace.co.uk>
References: <366B67791694FCC702D6BE55@D-128-208-244-210.dhcp4.washington.edu>
	<900DC79C-ED35-41A5-B78A-8064C1DF54F2@pixelmilk.com>
	<08C6A108-18F8-4BD6-BD2A-CD43166B2141@themacplace.co.uk>
Message-ID: <2ED77189-D47B-4B32-B92C-F3B2F9C797C7@pixelmilk.com>

sorry, server specs http://www.apple.com/de/server/macosx/specs.html

and there at the bottom


Am 08.06.2009 um 23:33 schrieb James Relph:

>> ZFS is a feature of Snow Leopard Server, but not supported for  
>> booting or key services. Have a look under key technologies.
>> But nothing else was promised for the release. I think they will  
>> maybe work on ZFS and Xsan.
>
> I can't see anything listed on the current Snow Leopard Server site, http://www.apple.com/server/macosx/ 
>  , on any of the pages, that mentions ZFS at all.  Am I just missing  
> the section?
>
> James Relph
> ACSA 10.5
>
> www.themacplace.co.uk
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4818 bytes
Desc: not available
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090608/c2bb4caa/attachment.bin>

From sebastian at pixelmilk.com  Mon Jun  8 14:42:00 2009
From: sebastian at pixelmilk.com (=?ISO-8859-1?Q?Sebastian_D=F6ll?=)
Date: Mon, 8 Jun 2009 23:42:00 +0200
Subject: [zfs-discuss] ZFS in Snow Leopard
In-Reply-To: <D891C2E2-3387-4706-80BA-343F89A2B8BF@mac.com>
References: <366B67791694FCC702D6BE55@D-128-208-244-210.dhcp4.washington.edu>
	<900DC79C-ED35-41A5-B78A-8064C1DF54F2@pixelmilk.com>
	<08C6A108-18F8-4BD6-BD2A-CD43166B2141@themacplace.co.uk>
	<2ED77189-D47B-4B32-B92C-F3B2F9C797C7@pixelmilk.com>
	<D891C2E2-3387-4706-80BA-343F89A2B8BF@mac.com>
Message-ID: <9B6D2689-ACE8-4115-B9D4-CD8ECA5CCB2F@pixelmilk.com>

That's maybe true, but I think it maybe could be available,
because it has been around since early beta versions of 10.5,
so it should be ready, but not for most services. We'll see, because
in FreeBSD it has reached a real good level.


Am 08.06.2009 um 23:39 schrieb Bryan Henry:

> That German page is probably out of date. The US site doesn't list  
> ZFS anymore.
>
> - Bryan
>
> Sent from my iPhone
>
> On Jun 8, 2009, at 2:35 PM, Sebastian D?ll <sebastian at pixelmilk.com>  
> wrote:
>
>> sorry, server specs http://www.apple.com/de/server/macosx/specs.html
>>
>> and there at the bottom
>>
>>
>> Am 08.06.2009 um 23:33 schrieb James Relph:
>>
>>>> ZFS is a feature of Snow Leopard Server, but not supported for  
>>>> booting or key services. Have a look under key technologies.
>>>> But nothing else was promised for the release. I think they will  
>>>> maybe work on ZFS and Xsan.
>>>
>>> I can't see anything listed on the current Snow Leopard Server  
>>> site, http://www.apple.com/server/macosx/ , on any of the pages,  
>>> that mentions ZFS at all.  Am I just missing the section?
>>>
>>> James Relph
>>> ACSA 10.5
>>>
>>> www.themacplace.co.uk
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4818 bytes
Desc: not available
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090608/127c4206/attachment.bin>

From bryanhenry at mac.com  Mon Jun  8 14:41:20 2009
From: bryanhenry at mac.com (Bryan Henry)
Date: Mon, 08 Jun 2009 14:41:20 -0700
Subject: [zfs-discuss] ZFS in Snow Leopard
In-Reply-To: <2ED77189-D47B-4B32-B92C-F3B2F9C797C7@pixelmilk.com>
References: <366B67791694FCC702D6BE55@D-128-208-244-210.dhcp4.washington.edu>
	<900DC79C-ED35-41A5-B78A-8064C1DF54F2@pixelmilk.com>
	<08C6A108-18F8-4BD6-BD2A-CD43166B2141@themacplace.co.uk>
	<2ED77189-D47B-4B32-B92C-F3B2F9C797C7@pixelmilk.com>
Message-ID: <47B17E65-C4DB-41E1-BF2C-25D11B20ADF4@mac.com>

That German page is probbly out of date. The US site doesn't list ZFS  
anymore.

- Bryan

Sent from my iPhone

On Jun 8, 2009, at 2:35 PM, Sebastian D?ll <sebastian at pixelmilk.com>  
wrote:

> sorry, server specs http://www.apple.com/de/server/macosx/specs.html
>
> and there at the bottom
>
>
> Am 08.06.2009 um 23:33 schrieb James Relph:
>
>>> ZFS is a feature of Snow Leopard Server, but not supported for  
>>> booting or key services. Have a look under key technologies.
>>> But nothing else was promised for the release. I think they will  
>>> maybe work on ZFS and Xsan.
>>
>> I can't see anything listed on the current Snow Leopard Server  
>> site, http://www.apple.com/server/macosx/ , on any of the pages,  
>> that mentions ZFS at all.  Am I just missing the section?
>>
>> James Relph
>> ACSA 10.5
>>
>> www.themacplace.co.uk
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From james at themacplace.co.uk  Mon Jun  8 14:43:10 2009
From: james at themacplace.co.uk (James Relph)
Date: Mon, 8 Jun 2009 22:43:10 +0100
Subject: [zfs-discuss] ZFS in Snow Leopard
In-Reply-To: <2ED77189-D47B-4B32-B92C-F3B2F9C797C7@pixelmilk.com>
References: <366B67791694FCC702D6BE55@D-128-208-244-210.dhcp4.washington.edu>
	<900DC79C-ED35-41A5-B78A-8064C1DF54F2@pixelmilk.com>
	<08C6A108-18F8-4BD6-BD2A-CD43166B2141@themacplace.co.uk>
	<2ED77189-D47B-4B32-B92C-F3B2F9C797C7@pixelmilk.com>
Message-ID: <F4CE12E8-6AB4-48A5-9BD6-19C98299D7C7@themacplace.co.uk>

On 8 Jun 2009, at 22:35, Sebastian D?ll wrote:

> sorry, server specs http://www.apple.com/de/server/macosx/specs.html
>
> and there at the bottom

Strange, that's not there on the US site:

http://www.apple.com/server/macosx/specs.html

bus is also there on the UK site:

http://www.apple.com/uk/server/macosx/specs.html

Typo?  On which site though?

Those are all new pages for Snow Leopard Server (ie different than  
yesterday), so seems odd if one is just out of date.

James Relph
ACSA 10.5

www.themacplace.co.uk

From bryanhenry at mac.com  Mon Jun  8 14:49:00 2009
From: bryanhenry at mac.com (Bryan Henry)
Date: Mon, 08 Jun 2009 14:49:00 -0700
Subject: [zfs-discuss] ZFS in Snow Leopard
In-Reply-To: <F4CE12E8-6AB4-48A5-9BD6-19C98299D7C7@themacplace.co.uk>
References: <366B67791694FCC702D6BE55@D-128-208-244-210.dhcp4.washington.edu>
	<900DC79C-ED35-41A5-B78A-8064C1DF54F2@pixelmilk.com>
	<08C6A108-18F8-4BD6-BD2A-CD43166B2141@themacplace.co.uk>
	<2ED77189-D47B-4B32-B92C-F3B2F9C797C7@pixelmilk.com>
	<F4CE12E8-6AB4-48A5-9BD6-19C98299D7C7@themacplace.co.uk>
Message-ID: <5086D48A-FEDD-47F5-91AE-4276013E305F@mac.com>

I certainly hope it's the US site that's out of date, but it's usually  
the non-US sites that are out of date for a while after keynotes and  
press events.

- Bryan

Sent from my iPhone

On Jun 8, 2009, at 2:43 PM, James Relph <james at themacplace.co.uk> wrote:

> On 8 Jun 2009, at 22:35, Sebastian D?ll wrote:
>
>> sorry, server specs http://www.apple.com/de/server/macosx/specs.html
>>
>> and there at the bottom
>
> Strange, that's not there on the US site:
>
> http://www.apple.com/server/macosx/specs.html
>
> bus is also there on the UK site:
>
> http://www.apple.com/uk/server/macosx/specs.html
>
> Typo?  On which site though?
>
> Those are all new pages for Snow Leopard Server (ie different than  
> yesterday), so seems odd if one is just out of date.
>
> James Relph
> ACSA 10.5
>
> www.themacplace.co.uk
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From mattreichling at gmail.com  Mon Jun  8 15:04:18 2009
From: mattreichling at gmail.com (Matt Reichling)
Date: Mon, 8 Jun 2009 15:04:18 -0700
Subject: [zfs-discuss] ZFS in Snow Leopard
In-Reply-To: <5086D48A-FEDD-47F5-91AE-4276013E305F@mac.com>
References: <366B67791694FCC702D6BE55@D-128-208-244-210.dhcp4.washington.edu>
	<900DC79C-ED35-41A5-B78A-8064C1DF54F2@pixelmilk.com>
	<08C6A108-18F8-4BD6-BD2A-CD43166B2141@themacplace.co.uk>
	<2ED77189-D47B-4B32-B92C-F3B2F9C797C7@pixelmilk.com>
	<F4CE12E8-6AB4-48A5-9BD6-19C98299D7C7@themacplace.co.uk>
	<5086D48A-FEDD-47F5-91AE-4276013E305F@mac.com>
Message-ID: <74651C90-57DF-4A65-8904-EED5987C4172@gmail.com>

The US site used to list it and it has recently been removed. It is  
extremely doubtful that it is the one that is out of date.

-Matt Reichling

On Jun 8, 2009, at 2:49 PM, Bryan Henry wrote:

> I certainly hope it's the US site that's out of date, but it's  
> usually the non-US sites that are out of date for a while after  
> keynotes and press events.
>
> - Bryan
>
> Sent from my iPhone
>
> On Jun 8, 2009, at 2:43 PM, James Relph <james at themacplace.co.uk>  
> wrote:
>
>> On 8 Jun 2009, at 22:35, Sebastian D?ll wrote:
>>
>>> sorry, server specs http://www.apple.com/de/server/macosx/specs.html
>>>
>>> and there at the bottom
>>
>> Strange, that's not there on the US site:
>>
>> http://www.apple.com/server/macosx/specs.html
>>
>> bus is also there on the UK site:
>>
>> http://www.apple.com/uk/server/macosx/specs.html
>>
>> Typo?  On which site though?
>>
>> Those are all new pages for Snow Leopard Server (ie different than  
>> yesterday), so seems odd if one is just out of date.
>>
>> James Relph
>> ACSA 10.5
>>
>> www.themacplace.co.uk
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From lists at loveturtle.net  Mon Jun  8 18:14:31 2009
From: lists at loveturtle.net (Dillon Kass)
Date: Mon, 08 Jun 2009 21:14:31 -0400
Subject: [zfs-discuss] ZFS in Snow Leopard
In-Reply-To: <366B67791694FCC702D6BE55@D-128-208-244-210.dhcp4.washington.edu>
References: <366B67791694FCC702D6BE55@D-128-208-244-210.dhcp4.washington.edu>
Message-ID: <4A2DB777.5020807@loveturtle.net>

On 6/8/09 4:20 PM, Daniel M. Zimmerman wrote:
> So... now that ZFS has been officially removed from the feature list 
> for Snow Leopard Server (see 
> <http://www.apple.com/server/macosx/specs.html>, under "File 
> Systems"), and presumably will also not appear in Snow Leopard 
> non-Server, can anybody from Apple comment on the future of ZFS on Mac 
> OS X?
Have you lost your mind? It's impossible to get a straight answer out of 
Apple. When 10.6 is released you will simply find out.

How did we find out that 10.5 development had stopped and moved to 
in-house 10.6? We found out because there was a small hint in a post 
once sometime and then everyone quit responding and there haven't been 
any more releases.

How will we find out what's going on with 10.6? When 10.6 comes out! 
Maybe if we're lucky we'll just wake up the day 10.6 is released and 
zfs.macosforge.org will be updated! or maybe the list will just quit 
working and it will disappear!

From lists at nerdbynature.de  Mon Jun  8 18:40:38 2009
From: lists at nerdbynature.de (Christian Kujau)
Date: Mon, 8 Jun 2009 18:40:38 -0700 (PDT)
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<A923A88C-9816-4E06-BD28-69296A50AB62@umn.edu>
	<EC271224-1098-4A38-B4EB-B3716821EA35@umn.edu>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
Message-ID: <alpine.DEB.2.01.0906081835030.6439@bogon>

On Mon, 8 Jun 2009, Carl Magnuson wrote:
> 	NAME                      STATE     READ WRITE CKSUM
> 	cpool                     DEGRADED     0     0     0
> 	  raidz2                  DEGRADED     0     0     0
> 	    disk2s2               ONLINE       0     0     0
> 	    disk3s2               ONLINE       0     0     0
> 	    disk6s2               ONLINE       0     0     0
> 	    disk5s2               ONLINE       0     0     0
> 	    disk4s2               ONLINE       0     0     0
> 	    1751454142452109774   FAULTED      0     0     0  was /dev/disk8s2
> 	    13962824586312699565  UNAVAIL      0     0     0  was /dev/disk9s2
[...] 
> one or two died in the meantime?  Is there a way to identify which the two
> drives that are FAULTED and UNAVAIL are to easily swap them out or try
> readding them to the pool?

Well, the "new" disk names are rather strange, but: what about 
/dev/disk[89], are they available at all? Can you access these disks and 
verify if they can be read from? (e.g. use dd(1) or something). Are they 
recognized by diskutil as well? If so, you should be able to integrate 
them into your raidz as advised by ZFS.

Christian.
-- 
BOFH excuse #446:

Mailer-daemon is busy burning your message in hell.


From magnu213 at umn.edu  Tue Jun  9 12:00:29 2009
From: magnu213 at umn.edu (Carl Magnuson)
Date: Tue, 9 Jun 2009 14:00:29 -0500
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <alpine.DEB.2.01.0906081835030.6439@bogon>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<A923A88C-9816-4E06-BD28-69296A50AB62@umn.edu>
	<EC271224-1098-4A38-B4EB-B3716821EA35@umn.edu>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
Message-ID: <6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>

I did not try writing to disk 8 or 9 as I'm not sure what might be on  
them, but doing a "dd if=/dev/disk9" or "dd if/dev/disk9s1" prints a  
bunch of junk to the terminal for both disks 8 and 9.  Doing it on / 
dev/disk8s2 or /dev/disk9s2 returns with "Resource busy"

They do both show up diskutil.  I guess my concern is that I'm not  
sure if disks are renumbered if they are attached to a different  
physical interface - maybe two of my other disks are labeled disk[8/9]  
now and I wouldn't want to accidentally overwrite them. Do you know if  
there is any way to confirm this - which disks are actually associated  
with which volumes? I'm also a little unsure how I wold reintegrate  
them in my raid (format them like I was creating a pool from scratch,  
and add them to the existing pool?).

Thanks,

Carl




On Jun 8, 2009, at 8:40 PM, Christian Kujau wrote:

> On Mon, 8 Jun 2009, Carl Magnuson wrote:
>> 	NAME                      STATE     READ WRITE CKSUM
>> 	cpool                     DEGRADED     0     0     0
>> 	  raidz2                  DEGRADED     0     0     0
>> 	    disk2s2               ONLINE       0     0     0
>> 	    disk3s2               ONLINE       0     0     0
>> 	    disk6s2               ONLINE       0     0     0
>> 	    disk5s2               ONLINE       0     0     0
>> 	    disk4s2               ONLINE       0     0     0
>> 	    1751454142452109774   FAULTED      0     0     0  was /dev/ 
>> disk8s2
>> 	    13962824586312699565  UNAVAIL      0     0     0  was /dev/ 
>> disk9s2
> [...]
>> one or two died in the meantime?  Is there a way to identify which  
>> the two
>> drives that are FAULTED and UNAVAIL are to easily swap them out or  
>> try
>> readding them to the pool?
>
> Well, the "new" disk names are rather strange, but: what about
> /dev/disk[89], are they available at all? Can you access these disks  
> and
> verify if they can be read from? (e.g. use dd(1) or something). Are  
> they
> recognized by diskutil as well? If so, you should be able to integrate
> them into your raidz as advised by ZFS.
>
> Christian.
> -- 
> BOFH excuse #446:
>
> Mailer-daemon is busy burning your message in hell.
>


From lists at nerdbynature.de  Tue Jun  9 12:56:51 2009
From: lists at nerdbynature.de (Christian Kujau)
Date: Tue, 9 Jun 2009 12:56:51 -0700 (PDT)
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<A923A88C-9816-4E06-BD28-69296A50AB62@umn.edu>
	<EC271224-1098-4A38-B4EB-B3716821EA35@umn.edu>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
Message-ID: <alpine.DEB.2.01.0906091242590.6439@bogon>

On Tue, 9 Jun 2009, Carl Magnuson wrote:
> I did not try writing to disk 8 or 9 as I'm not sure what might be on them,

Sure, I did not want you _write_ to them :)

> but doing a "dd if=/dev/disk9" or "dd if/dev/disk9s1" prints a bunch of junk
> to the terminal for both disks 8 and 9.  Doing it on /dev/disk8s2 or
> /dev/disk9s2 returns with "Resource busy"

s2 is usually the "whole disk", not sure why OSX returns "Resource busy" 
sometimes (it does this here too for some disks).

> They do both show up diskutil.  I guess my concern is that I'm not sure if
> disks are renumbered if they are attached to a different physical interface -

Well, they show up in diskutil. Is their layout not different from the 
other disks? What about their contents? Try "file -s /dev/disk8*" and it 
will guess what's on the partitions, maybe that will make you recognize 
them as "the right disks".

> maybe two of my other disks are labeled disk[8/9] now and I wouldn't want to
> accidentally overwrite them. Do you know if there is any way to confirm this -
> which disks are actually associated with which volumes?

system_profiler lists the hardware in/attached to your system, you could 
look up the disk model or the serial# on your disk and then search for it:

--------------------
$ system_profiler | less
[...]
          Capacity: 55.89 GB
          Model: Hitachi HTS541060G9AT00
          Revision: MB3AA5AJ
          Serial Number: MPB350X5G98HVC
[...]
  File System: Journaled HFS+
              BSD Name: disk0s3
              Mount Point: /
---------------------

> I'm also a little
> unsure how I wold reintegrate them in my raid (format them like I was creating
> a pool from scratch, and add them to the existing pool?).

Well, ZFS told you to:

---------------------
g5:~ bobcat$ zpool status -v
 pool: cpool
state: DEGRADED
status: One or more devices could not be used because the label is missing
        or invalid.  Sufficient replicas exist for the pool to continue
        functioning in a degraded state.
action: Replace the device using 'zpool replace'.
  see: http://www.sun.com/msg/ZFS-8000-4J
---------------------

I think this is the way to go here. If thing go wrong, you still have your 
backups, right? :-)

Christian.
-- 
BOFH excuse #194:

We only support a 1200 bps connection.

From magnu213 at umn.edu  Tue Jun  9 13:54:49 2009
From: magnu213 at umn.edu (Carl Magnuson)
Date: Tue, 9 Jun 2009 15:54:49 -0500
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <alpine.DEB.2.01.0906091242590.6439@bogon>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<A923A88C-9816-4E06-BD28-69296A50AB62@umn.edu>
	<EC271224-1098-4A38-B4EB-B3716821EA35@umn.edu>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
Message-ID: <587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>


On Jun 9, 2009, at 2:56 PM, Christian Kujau wrote:
>> maybe two of my other disks are labeled disk[8/9] now and I  
>> wouldn't want to
>> accidentally overwrite them. Do you know if there is any way to  
>> confirm this -
>> which disks are actually associated with which volumes?
>
> system_profiler lists the hardware in/attached to your system, you  
> could
> look up the disk model or the serial# on your disk and then search  
> for it:

After looking at the system profiler results, I see that disk8 is  
supposed to be part of the pool, and 9 is a different disk which is  
not supposed to be part of the pool (or any pool).


> ---------------------
>
>> I'm also a little
>> unsure how I wold reintegrate them in my raid (format them like I  
>> was creating
>> a pool from scratch, and add them to the existing pool?).
>
> Well, ZFS told you to:
>
> ---------------------
> g5:~ bobcat$ zpool status -v
> pool: cpool
> state: DEGRADED
> status: One or more devices could not be used because the label is  
> missing
>        or invalid.  Sufficient replicas exist for the pool to continue
>        functioning in a degraded state.
> action: Replace the device using 'zpool replace'.
>  see: http://www.sun.com/msg/ZFS-8000-4J
> ---------------------
>
> I think this is the way to go here. If thing go wrong, you still  
> have your
> backups, right? :-)

I attempted:
g5:~ bobcat$ zpool replace 1751454142452109774 /dev/disk8s2
cannot open '1751454142452109774': name must begin with a letter

and

g5:~ bobcat$ zpool replace cpool /dev/disk8s2
cannot replace /dev/disk8s2 with /dev/disk8s2: /dev/disk8s2 is busy

Is there some other way to refer to the disk which is marked as  
faulted?  I can try a -f to force adding disk8, but I am not sure if  
it will remove the disk marked as 1751454142452109774.

Thanks,

Carl

From caronni at gmail.com  Tue Jun  9 14:38:39 2009
From: caronni at gmail.com (Germano Caronni)
Date: Tue, 9 Jun 2009 23:38:39 +0200
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org> 
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com> 
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org> 
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu> 
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu> 
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
Message-ID: <327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>

remember the previous posters statement about backups ;-)


> I attempted:
> g5:~ bobcat$ zpool replace 1751454142452109774 /dev/disk8s2
> cannot open '1751454142452109774': name must begin with a letter
>

zpool replace [-f] pool device [new_device]

you missed the pool name.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090609/e66ef824/attachment.html>

From magnu213 at umn.edu  Tue Jun  9 14:47:42 2009
From: magnu213 at umn.edu (Carl Magnuson)
Date: Tue, 9 Jun 2009 16:47:42 -0500
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
Message-ID: <7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>

Doh!  After adding pool name I still get the device is busy error:
cannot replace 1751454142452109774 with /dev/disk8s2: /dev/disk8s2 is  
busy

Even with the -f flag and running as root it will not let me replace  
it.  Is there some way to get the drive in a state that isn't busy?   
Or will disk utility let me reformat it and then I can try adding it  
to the pool?

Thanks again,

Carl



On Jun 9, 2009, at 4:38 PM, Germano Caronni wrote:

> remember the previous posters statement about backups ;-)
>
> I attempted:
> g5:~ bobcat$ zpool replace 1751454142452109774 /dev/disk8s2
> cannot open '1751454142452109774': name must begin with a letter
>
>
> zpool replace [-f] pool device [new_device]
> you missed the pool name.
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090609/af7213c7/attachment.html>

From lists at nerdbynature.de  Tue Jun  9 15:47:18 2009
From: lists at nerdbynature.de (Christian Kujau)
Date: Tue, 9 Jun 2009 15:47:18 -0700 (PDT)
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
Message-ID: <alpine.DEB.2.01.0906091538300.6439@bogon>

On Tue, 9 Jun 2009, Carl Magnuson wrote:
> Doh!  After adding pool name I still get the device is busy error:
> cannot replace 1751454142452109774 with /dev/disk8s2: /dev/disk8s2 is busy

Is /dev/disk8s* perhaps somewhere mounted alread? (diskutil list). 
Unfortunately I'm not too OSX-savvy to know why it returns -EBUSY even for 
read() operations.

> Even with the -f flag and running as root it will not let me replace it.  Is
> there some way to get the drive in a state that isn't busy?  Or will disk
> utility let me reformat it and then I can try adding it to the pool?

If you've correctly identified disk8 to be the disk you think it is, you 
could try this, of course. I wonder if "diskutil" has some magic to make 
disk8 go from "busy" to "not busy"...

Christian.
-- 
BOFH excuse #227:

Fatal error right in front of screen


From Jonathan.Edwards at Sun.COM  Tue Jun  9 17:42:22 2009
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Tue, 09 Jun 2009 20:42:22 -0400
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
Message-ID: <D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>

what's your 'diskutil list' and 'zpool status -v' look like now?

from your previous output it appears that the vdev label got corrupted  
somehow on the disk, and there's another disk that you appear to be  
using for backup, but it's hard to tell which is which right now

btw - the /dev/disk value shouldn't matter as the zfs pool information  
is stored on the disks themselves (not tied to the /dev entry) .. we  
used to always demonstrate this by building a pool, exporting it,  
moving the disks all around and re-importing it.

---
.je

On Jun 9, 2009, at 5:47 PM, Carl Magnuson wrote:

> Doh!  After adding pool name I still get the device is busy error:
> cannot replace 1751454142452109774 with /dev/disk8s2: /dev/disk8s2  
> is busy
>
> Even with the -f flag and running as root it will not let me replace  
> it.  Is there some way to get the drive in a state that isn't busy?   
> Or will disk utility let me reformat it and then I can try adding it  
> to the pool?
>
> Thanks again,
>
> Carl
>
>
>
> On Jun 9, 2009, at 4:38 PM, Germano Caronni wrote:
>
>> remember the previous posters statement about backups ;-)
>>
>> I attempted:
>> g5:~ bobcat$ zpool replace 1751454142452109774 /dev/disk8s2
>> cannot open '1751454142452109774': name must begin with a letter
>>
>> zpool replace [-f] pool device [new_device]
>> you missed the pool name.
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From magnu213 at umn.edu  Tue Jun  9 17:51:31 2009
From: magnu213 at umn.edu (Carl Magnuson)
Date: Tue, 9 Jun 2009 19:51:31 -0500
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>
Message-ID: <1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>

Here is the output John:

g5:~ bobcat$ diskutil list
/dev/disk0
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *298.1 Gi    
disk0
    1:                        EFI                         200.0 Mi    
disk0s1
    2:                  Apple_HFS Boot                    297.8 Gi    
disk0s2
/dev/disk1
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:     Apple_partition_scheme                        *232.9 Gi    
disk1
    1:        Apple_partition_map                         31.5 Ki     
disk1s1
    2:                  Apple_HFS Users                   232.8 Gi    
disk1s3
/dev/disk2
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *465.8 Gi    
disk2
    1:                        EFI                         200.0 Mi    
disk2s1
    2:                        ZFS cpool                   465.4 Gi    
disk2s2
/dev/disk3
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *465.8 Gi    
disk3
    1:                        EFI                         200.0 Mi    
disk3s1
    2:                        ZFS cpool                   465.4 Gi    
disk3s2
/dev/disk4
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *465.8 Gi    
disk4
    1:                        EFI                         200.0 Mi    
disk4s1
    2:                        ZFS cpool                   465.4 Gi    
disk4s2
/dev/disk5
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *465.8 Gi    
disk5
    1:                        EFI                         200.0 Mi    
disk5s1
    2:                        ZFS cpool                   465.4 Gi    
disk5s2
/dev/disk6
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *465.8 Gi    
disk6
    1:                        EFI                         200.0 Mi    
disk6s1
    2:                        ZFS cpool                   465.4 Gi    
disk6s2
/dev/disk7
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *465.8 Gi    
disk7
    1:                        EFI                         200.0 Mi    
disk7s1
    2:                        ZFS cpool                   465.4 Gi    
disk7s2
/dev/disk8
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *465.8 Gi    
disk8
    1:                        EFI                         200.0 Mi    
disk8s1
    2:                        ZFS cpool                   465.4 Gi    
disk8s2
/dev/disk9
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *465.8 Gi    
disk9
    1:                        EFI                         200.0 Mi    
disk9s1
    2:                  Apple_HFS Backup                  465.4 Gi    
disk9s2
/dev/disk10
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:      GUID_partition_scheme                        *931.5 Gi    
disk10
    1:                        EFI                         200.0 Mi    
disk10s1
    2:                  Apple_HFS Backup-G5               931.2 Gi    
disk10s2
/dev/disk11
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:                                                   *320.0 Ki    
disk11
/dev/disk12
    #:                       TYPE NAME                    SIZE        
IDENTIFIER
    0:                                                   *320.0 Ki    
disk12


g5:~ bobcat$ zpool status -v
   pool: cpool
  state: DEGRADED
status: One or more devices could not be used because the label is  
missing or
	invalid.  Sufficient replicas exist for the pool to continue
	functioning in a degraded state.
action: Replace the device using 'zpool replace'.
    see: http://www.sun.com/msg/ZFS-8000-4J
  scrub: resilver completed with 0 errors on Mon Jun  8 15:15:31 2009
config:

	NAME                      STATE     READ WRITE CKSUM
	cpool                     DEGRADED     0     0     0
	  raidz2                  DEGRADED     0     0     0
	    disk2s2               ONLINE       0     0     0
	    disk3s2               ONLINE       0     0     0
	    disk6s2               ONLINE       0     0     0
	    disk5s2               ONLINE       0     0     0
	    disk4s2               ONLINE       0     0     0
	    1751454142452109774   FAULTED      0     0     0  was /dev/disk8s2
	    13962824586312699565  UNAVAIL      0     0     0  was /dev/disk9s2

errors: No known data errors

Thanks,

Carl

On Jun 9, 2009, at 7:42 PM, Jonathan Edwards wrote:

> what's your 'diskutil list' and 'zpool status -v' look like now?
>
> from your previous output it appears that the vdev label got  
> corrupted somehow on the disk, and there's another disk that you  
> appear to be using for backup, but it's hard to tell which is which  
> right now
>
> btw - the /dev/disk value shouldn't matter as the zfs pool  
> information is stored on the disks themselves (not tied to the /dev  
> entry) .. we used to always demonstrate this by building a pool,  
> exporting it, moving the disks all around and re-importing it.
>
> ---
> .je


From Jonathan.Edwards at Sun.COM  Tue Jun  9 19:43:53 2009
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Tue, 09 Jun 2009 22:43:53 -0400
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>
	<1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>
Message-ID: <C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM>

okay .. so it looks like disk2 - disk8 should be your 5+2 raidz2 pool  
named cpool, but now it can't find the vdev with guid  
13962824586312699565 and disk8s2 is showing faulted (the guid looks a  
little funky too)

zdb isn't ported to the mac so you can't exactly walk the headers, and  
that might be a little too deep for where you want to go - so it looks  
like you want to get disk7s2 and disk8s2 back into the pool .. since  
disk8 is kind of recognized as being part of the pool (only faulted  
now) .. what might work is:

# zpool replace cpool disk8s2 disk8s2

see if this completes .. (or perhaps zpool replace 1751454142452109774  
disk8s2) .. this should just do an in place upgrade .. then:

# zpool replace cpool 13962824586312699565 disk7s2

it's a little odd to me that the vdev guids are being displayed there  
instead of the last known devices .. but perhaps that's just because  
the previous device (disk9s2) was changed and it couldn't find this  
particular guid

btw - out of curiousity - when you built the pool initially did you do  
a:
# zpool create cpool raidz2 disk2s2 disk3s2 disk4s2 .. <etc>
or a
# zpool create cpool raidz2 disk2 disk3 disk4 .. <etc>

oh, and if you have spare disks .. it might pay to put in a hot spare  
here .. haven't tested this too much with the mac port, but this  
should just work to replace degraded or missing devices .. (good  
choice though on raidz2!!)

On Jun 9, 2009, at 8:51 PM, Carl Magnuson wrote:

> Here is the output John:
>
> g5:~ bobcat$ diskutil list
> /dev/disk0
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:      GUID_partition_scheme                        *298.1 Gi    
> disk0
>   1:                        EFI                         200.0 Mi    
> disk0s1
>   2:                  Apple_HFS Boot                    297.8 Gi    
> disk0s2
> /dev/disk1
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:     Apple_partition_scheme                        *232.9 Gi    
> disk1
>   1:        Apple_partition_map                         31.5 Ki     
> disk1s1
>   2:                  Apple_HFS Users                   232.8 Gi    
> disk1s3
> /dev/disk2
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:      GUID_partition_scheme                        *465.8 Gi    
> disk2
>   1:                        EFI                         200.0 Mi    
> disk2s1
>   2:                        ZFS cpool                   465.4 Gi    
> disk2s2
> /dev/disk3
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:      GUID_partition_scheme                        *465.8 Gi    
> disk3
>   1:                        EFI                         200.0 Mi    
> disk3s1
>   2:                        ZFS cpool                   465.4 Gi    
> disk3s2
> /dev/disk4
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:      GUID_partition_scheme                        *465.8 Gi    
> disk4
>   1:                        EFI                         200.0 Mi    
> disk4s1
>   2:                        ZFS cpool                   465.4 Gi    
> disk4s2
> /dev/disk5
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:      GUID_partition_scheme                        *465.8 Gi    
> disk5
>   1:                        EFI                         200.0 Mi    
> disk5s1
>   2:                        ZFS cpool                   465.4 Gi    
> disk5s2
> /dev/disk6
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:      GUID_partition_scheme                        *465.8 Gi    
> disk6
>   1:                        EFI                         200.0 Mi    
> disk6s1
>   2:                        ZFS cpool                   465.4 Gi    
> disk6s2
> /dev/disk7
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:      GUID_partition_scheme                        *465.8 Gi    
> disk7
>   1:                        EFI                         200.0 Mi    
> disk7s1
>   2:                        ZFS cpool                   465.4 Gi    
> disk7s2
> /dev/disk8
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:      GUID_partition_scheme                        *465.8 Gi    
> disk8
>   1:                        EFI                         200.0 Mi    
> disk8s1
>   2:                        ZFS cpool                   465.4 Gi    
> disk8s2
> /dev/disk9
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:      GUID_partition_scheme                        *465.8 Gi    
> disk9
>   1:                        EFI                         200.0 Mi    
> disk9s1
>   2:                  Apple_HFS Backup                  465.4 Gi    
> disk9s2
> /dev/disk10
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:      GUID_partition_scheme                        *931.5 Gi    
> disk10
>   1:                        EFI                         200.0 Mi    
> disk10s1
>   2:                  Apple_HFS Backup-G5               931.2 Gi    
> disk10s2
> /dev/disk11
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:                                                   *320.0 Ki    
> disk11
> /dev/disk12
>   #:                       TYPE NAME                    SIZE        
> IDENTIFIER
>   0:                                                   *320.0 Ki    
> disk12
>
>
> g5:~ bobcat$ zpool status -v
>  pool: cpool
> state: DEGRADED
> status: One or more devices could not be used because the label is  
> missing or
> 	invalid.  Sufficient replicas exist for the pool to continue
> 	functioning in a degraded state.
> action: Replace the device using 'zpool replace'.
>   see: http://www.sun.com/msg/ZFS-8000-4J
> scrub: resilver completed with 0 errors on Mon Jun  8 15:15:31 2009
> config:
>
> 	NAME                      STATE     READ WRITE CKSUM
> 	cpool                     DEGRADED     0     0     0
> 	  raidz2                  DEGRADED     0     0     0
> 	    disk2s2               ONLINE       0     0     0
> 	    disk3s2               ONLINE       0     0     0
> 	    disk6s2               ONLINE       0     0     0
> 	    disk5s2               ONLINE       0     0     0
> 	    disk4s2               ONLINE       0     0     0
> 	    1751454142452109774   FAULTED      0     0     0  was /dev/ 
> disk8s2
> 	    13962824586312699565  UNAVAIL      0     0     0  was /dev/ 
> disk9s2
>
> errors: No known data errors
>
> Thanks,
>
> Carl


From magnu213 at umn.edu  Tue Jun  9 23:34:25 2009
From: magnu213 at umn.edu (Carl Magnuson)
Date: Wed, 10 Jun 2009 01:34:25 -0500
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>
	<1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>
	<C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM>
Message-ID: <00B2033E-D08B-4BE3-BB09-4614F1F4A8B5@umn.edu>


On Jun 9, 2009, at 9:43 PM, Jonathan Edwards wrote:

> .. what might work is:
>
> # zpool replace cpool disk8s2 disk8s2
>
> see if this completes .. (or perhaps zpool replace  
> 1751454142452109774 disk8s2) .. this should just do an in place  
> upgrade .. then:

I still run into the same issue with disk8 being busy:

g5:~ bobcat$ zpool replace cpool disk8s2 disk8s2
cannot replace disk8s2 with disk8s2: disk8s2 is busy
g5:~ bobcat$ zpool replace cpool 1751454142452109774 disk8s2
cannot replace 1751454142452109774 with disk8s2: disk8s2 is busy

Any ideas why the drive is busy if it supposedly isn't in use, and how  
to get around this?

> btw - out of curiousity - when you built the pool initially did you  
> do a:
> # zpool create cpool raidz2 disk2s2 disk3s2 disk4s2 .. <etc>
> or a
> # zpool create cpool raidz2 disk2 disk3 disk4 .. <etc>

I believe I did the first one which is the way the example did on the  
zfs macosforge page.

> oh, and if you have spare disks .. it might pay to put in a hot  
> spare here .. haven't tested this too much with the mac port, but  
> this should just work to replace degraded or missing devices ..  
> (good choice though on raidz2!!)

I don't have any spares sitting around, but am thinking of getting  
another because I am getting pretty worried about the pool dying...

Thanks,

Carl

From Jonathan.Edwards at Sun.COM  Wed Jun 10 07:11:18 2009
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Wed, 10 Jun 2009 10:11:18 -0400
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <00B2033E-D08B-4BE3-BB09-4614F1F4A8B5@umn.edu>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>
	<1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>
	<C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM>
	<00B2033E-D08B-4BE3-BB09-4614F1F4A8B5@umn.edu>
Message-ID: <80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM>


On Jun 10, 2009, at 2:34 AM, Carl Magnuson wrote:

> On Jun 9, 2009, at 9:43 PM, Jonathan Edwards wrote:
>
>> .. what might work is:
>>
>> # zpool replace cpool disk8s2 disk8s2
>>
>> see if this completes .. (or perhaps zpool replace  
>> 1751454142452109774 disk8s2) .. this should just do an in place  
>> upgrade .. then:
>
> I still run into the same issue with disk8 being busy:
>
> g5:~ bobcat$ zpool replace cpool disk8s2 disk8s2
> cannot replace disk8s2 with disk8s2: disk8s2 is busy
> g5:~ bobcat$ zpool replace cpool 1751454142452109774 disk8s2
> cannot replace 1751454142452109774 with disk8s2: disk8s2 is busy
>
> Any ideas why the drive is busy if it supposedly isn't in use, and  
> how to get around this?

my guess is that disk8 is the faulted device which is still being held  
by the pool or something else .. you can try doing a diskutil eject / 
dev/disk8s2, but i'm guessing you'll have a problem here as well ..

what about the other one?  can you do either a
# zpool replace cpool disk8s2 disk7s2
or
# zpool replace cpool 13962824586312699565 disk7s2
(for the one that was previously disk9s2)

---
.je

From magnu213 at umn.edu  Wed Jun 10 08:31:33 2009
From: magnu213 at umn.edu (Carl Magnuson)
Date: Wed, 10 Jun 2009 10:31:33 -0500
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>
	<1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>
	<C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM>
	<00B2033E-D08B-4BE3-BB09-4614F1F4A8B5@umn.edu>
	<80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM>
Message-ID: <F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>


On Jun 10, 2009, at 9:11 AM, Jonathan Edwards wrote:
>> my guess is that disk8 is the faulted device which is still being  
>> held by the pool or something else .. you can try doing a diskutil  
>> eject /dev/disk8s2, but i'm guessing you'll have a problem here as  
>> well ..

It looks like it is ejecting the disks:

g5:~ bobcat$ diskutil eject /dev/disk8s2
Disk /dev/disk8s2 ejected
g5:~ bobcat$ diskutil eject /dev/disk8
Disk /dev/disk8 ejected
g5:~ bobcat$ diskutil eject /dev/disk7
Disk /dev/disk7 ejected
g5:~ bobcat$ diskutil eject /dev/disk7s2
Disk /dev/disk7s2 ejected


> what about the other one?  can you do either a
> # zpool replace cpool disk8s2 disk7s2
> or
> # zpool replace cpool 13962824586312699565 disk7s2
> (for the one that was previously disk9s2)

However when I try and replace them:

g5:~ bobcat$ zpool replace cpool 13962824586312699565 disk7s2
cannot replace 13962824586312699565 with disk7s2: disk7s2 is busy
g5:~ bobcat$ zpool replace cpool 1751454142452109774 disk8s2
cannot replace 1751454142452109774 with disk8s2: disk8s2 is busy

Seems like I am having the same issue with both drives.

Carl

From webmaster at thm-online.org  Wed Jun 10 08:38:00 2009
From: webmaster at thm-online.org (Thomas Maier)
Date: Wed, 10 Jun 2009 17:38:00 +0200
Subject: [zfs-discuss]  Apple's ZFS implementation
In-Reply-To: <F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>
	<1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>
	<C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM>
	<00B2033E-D08B-4BE3-BB09-4614F1F4A8B5@umn.edu>
	<80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM>
	<F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>
Message-ID: <6EAAC539-2A39-4E42-B5E5-BF3B6DDF7009@thm-online.org>

Does anyone know where Apple lost its implementation of ZFS in Mac OS  
X 10.6 Server?



Am 10.06.2009 um 17:31 schrieb Carl Magnuson:

>
> On Jun 10, 2009, at 9:11 AM, Jonathan Edwards wrote:
>>> my guess is that disk8 is the faulted device which is still being  
>>> held by the pool or something else .. you can try doing a diskutil  
>>> eject /dev/disk8s2, but i'm guessing you'll have a problem here as  
>>> well ..
>
> It looks like it is ejecting the disks:
>
> g5:~ bobcat$ diskutil eject /dev/disk8s2
> Disk /dev/disk8s2 ejected
> g5:~ bobcat$ diskutil eject /dev/disk8
> Disk /dev/disk8 ejected
> g5:~ bobcat$ diskutil eject /dev/disk7
> Disk /dev/disk7 ejected
> g5:~ bobcat$ diskutil eject /dev/disk7s2
> Disk /dev/disk7s2 ejected
>
>
>> what about the other one?  can you do either a
>> # zpool replace cpool disk8s2 disk7s2
>> or
>> # zpool replace cpool 13962824586312699565 disk7s2
>> (for the one that was previously disk9s2)
>
> However when I try and replace them:
>
> g5:~ bobcat$ zpool replace cpool 13962824586312699565 disk7s2
> cannot replace 13962824586312699565 with disk7s2: disk7s2 is busy
> g5:~ bobcat$ zpool replace cpool 1751454142452109774 disk8s2
> cannot replace 1751454142452109774 with disk8s2: disk8s2 is busy
>
> Seems like I am having the same issue with both drives.
>
> Carl
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>


From Jonathan.Edwards at Sun.COM  Wed Jun 10 09:18:20 2009
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Wed, 10 Jun 2009 12:18:20 -0400
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>
	<1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>
	<C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM>
	<00B2033E-D08B-4BE3-BB09-4614F1F4A8B5@umn.edu>
	<80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM>
	<F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>
Message-ID: <0502E3E2-5AE3-452A-A40D-7C8C0F2CB5B7@Sun.COM>


On Jun 10, 2009, at 11:31 AM, Carl Magnuson wrote:

>
> On Jun 10, 2009, at 9:11 AM, Jonathan Edwards wrote:
>>> my guess is that disk8 is the faulted device which is still being  
>>> held by the pool or something else .. you can try doing a diskutil  
>>> eject /dev/disk8s2, but i'm guessing you'll have a problem here as  
>>> well ..
>
> It looks like it is ejecting the disks:
>
> g5:~ bobcat$ diskutil eject /dev/disk8s2
> Disk /dev/disk8s2 ejected
> g5:~ bobcat$ diskutil eject /dev/disk8
> Disk /dev/disk8 ejected
> g5:~ bobcat$ diskutil eject /dev/disk7
> Disk /dev/disk7 ejected
> g5:~ bobcat$ diskutil eject /dev/disk7s2
> Disk /dev/disk7s2 ejected
>
>
>> what about the other one?  can you do either a
>> # zpool replace cpool disk8s2 disk7s2
>> or
>> # zpool replace cpool 13962824586312699565 disk7s2
>> (for the one that was previously disk9s2)
>
> However when I try and replace them:
>
> g5:~ bobcat$ zpool replace cpool 13962824586312699565 disk7s2
> cannot replace 13962824586312699565 with disk7s2: disk7s2 is busy
> g5:~ bobcat$ zpool replace cpool 1751454142452109774 disk8s2
> cannot replace 1751454142452109774 with disk8s2: disk8s2 is busy
>
> Seems like I am having the same issue with both drives.

with the eject, if the drive is still attached .. i've found if you  
try to do any query it will attempt to reattach - so make sure you're  
not doing a diskutil list in between .. (also make sure there's no  
scrub running)

i guess you could also try exporting the pool and attempting to  
reimport:
# zpool export cpool
# zpool import -d /dev cpool

or boot the opensolaris 2009.06 LiveCD and try to recover with this ..  
(just don't upgrade the pool or the filesystem)

of course you could also reformat the drive for EFI blank and then try  
to replace .. just make sure you've got the right devices in place and  
not some cached /dev/disk path

---
.je

From hanche at math.ntnu.no  Wed Jun 10 14:35:55 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Wed, 10 Jun 2009 23:35:55 +0200 (CEST)
Subject: [zfs-discuss] Apple's ZFS implementation
In-Reply-To: <6EAAC539-2A39-4E42-B5E5-BF3B6DDF7009@thm-online.org>
References: <80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM>
	<F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>
	<6EAAC539-2A39-4E42-B5E5-BF3B6DDF7009@thm-online.org>
Message-ID: <20090610.233555.182076238.hanche@math.ntnu.no>

+ Thomas Maier <webmaster at thm-online.org>:

> Does anyone know where Apple lost its implementation of ZFS in Mac OS
> X 10.6 Server?

I don't think anyone on the list knows anything, except for the Apple
employees, and they are not talking about it, apparently. We'll just
have to wait and see.

- Harald

From alex.blewitt at gmail.com  Wed Jun 10 14:59:40 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Wed, 10 Jun 2009 22:59:40 +0100
Subject: [zfs-discuss] Apple's ZFS implementation
In-Reply-To: <20090610.233555.182076238.hanche@math.ntnu.no>
References: <80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM>
	<F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>
	<6EAAC539-2A39-4E42-B5E5-BF3B6DDF7009@thm-online.org>
	<20090610.233555.182076238.hanche@math.ntnu.no>
Message-ID: <636fd28e0906101459pb5f2dc9o98de2947baa9f7d5@mail.gmail.com>

On Wed, Jun 10, 2009 at 10:35 PM, Harald
Hanche-Olsen<hanche at math.ntnu.no> wrote:
> + Thomas Maier <webmaster at thm-online.org>:
>
>> Does anyone know where Apple lost its implementation of ZFS in Mac OS
>> X 10.6 Server?
>
> I don't think anyone on the list knows anything, except for the Apple
> employees, and they are not talking about it, apparently. We'll just
> have to wait and see.

Sad but true. Bear in mind there were a few kernel-ish changes needed
to get a decent ZFS implementation together; it may be that they
didn't have time to incorporate some of those. I live in hope that
Noel or similar is reading this, and wants to say "when 10.6 is
released, we'll update the bits on zfs.macosforge.org again" but given
that I'm typing this on a G5 computer (sob) I'm not setting
expectations too high.

I would expect that there would be *some* ZFS bits, like we've had
already, but not necessarily in a formal release. Bear in mind that
ZFS was only ever advertised on the server stuff anyway (though it was
RO in client before) probably because the GUI was going to be
server-side specific. It may be that the .kext works OK but that the
GUI wasn't up to scratch so they've taken it off the feature list.

I'd really be surprised if Apple were walking away from ZFS
permanently though, and I guess if it doesn't make it then the ZFS mac
forge stuff is still available to hack ...

Alex

From lists at nerdbynature.de  Wed Jun 10 20:36:40 2009
From: lists at nerdbynature.de (Christian Kujau)
Date: Wed, 10 Jun 2009 20:36:40 -0700 (PDT)
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>
	<1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>
	<C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM>
	<00B2033E-D08B-4BE3-BB09-4614F1F4A8B5@umn.edu>
	<80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM>
	<F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>
Message-ID: <alpine.DEB.2.01.0906102034050.6439@bogon>

On Wed, 10 Jun 2009, Carl Magnuson wrote:
> g5:~ bobcat$ zpool replace cpool 13962824586312699565 disk7s2
> cannot replace 13962824586312699565 with disk7s2: disk7s2 is busy
> g5:~ bobcat$ zpool replace cpool 1751454142452109774 disk8s2
> cannot replace 1751454142452109774 with disk8s2: disk8s2 is busy

Hm, you ejected the drves, yet they ae still returning busy...weird. I 
don't want to sound like a Windoze-helpdesk, but: is rebooting an option?

Christian.
-- 
BOFH excuse #92:

Stale file handle (next time use Tupperware(tm)!)

From magnu213 at umn.edu  Thu Jun 11 07:58:39 2009
From: magnu213 at umn.edu (Carl Magnuson)
Date: Thu, 11 Jun 2009 09:58:39 -0500
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <alpine.DEB.2.01.0906102034050.6439@bogon>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>
	<1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>
	<C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM>
	<00B2033E-D08B-4BE3-BB09-4614F1F4A8B5@umn.edu>
	<80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM>
	<F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>
	<alpine.DEB.2.01.0906102034050.6439@bogon>
Message-ID: <987DA95C-3D41-485D-8FDE-5412D1697518@umn.edu>

Christian,

After a reboot I have the following zpool status:

g5:~ bobcat$ zpool status
   pool: cpool
  state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
	attempt was made to correct the error.  Applications are unaffected.
action: Determine if the device needs to be replaced, and clear the  
errors
	using 'zpool clear' or replace the device with 'zpool replace'.
    see: http://www.sun.com/msg/ZFS-8000-9P
  scrub: resilver completed with 0 errors on Thu Jun 11 09:50:58 2009
config:

	NAME         STATE     READ WRITE CKSUM
	cpool        ONLINE       0     0     0
	  raidz2     ONLINE       0     0     0
	    disk2s2  ONLINE       0     0     0
	    disk3s2  ONLINE       0     0     0
	    disk4s2  ONLINE       0     0     0
	    disk6s2  ONLINE       0     0     0
	    disk5s2  ONLINE       0     0     0
	    disk7s2  ONLINE       0     0   102
	    disk8s2  ONLINE       0     0   104

errors: No known data errors


Which seems very odd to me that it seem to come back together.  Is  
there cause for concern - after another reboot the drives might not  
all come back into the pool like they did last time?  Or should that  
be an isolated circumstance?

Thanks so much,

Carl



On Jun 10, 2009, at 10:36 PM, Christian Kujau wrote:

> On Wed, 10 Jun 2009, Carl Magnuson wrote:
>> g5:~ bobcat$ zpool replace cpool 13962824586312699565 disk7s2
>> cannot replace 13962824586312699565 with disk7s2: disk7s2 is busy
>> g5:~ bobcat$ zpool replace cpool 1751454142452109774 disk8s2
>> cannot replace 1751454142452109774 with disk8s2: disk8s2 is busy
>
> Hm, you ejected the drves, yet they ae still returning busy...weird. I
> don't want to sound like a Windoze-helpdesk, but: is rebooting an  
> option?
>
> Christian.
> -- 
> BOFH excuse #92:
>
> Stale file handle (next time use Tupperware(tm)!)


From caronni at gmail.com  Thu Jun 11 08:24:56 2009
From: caronni at gmail.com (Germano Caronni)
Date: Thu, 11 Jun 2009 17:24:56 +0200
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <987DA95C-3D41-485D-8FDE-5412D1697518@umn.edu>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org> 
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com> 
	<1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>
	<C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM> 
	<00B2033E-D08B-4BE3-BB09-4614F1F4A8B5@umn.edu>
	<80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM> 
	<F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>
	<alpine.DEB.2.01.0906102034050.6439@bogon> 
	<987DA95C-3D41-485D-8FDE-5412D1697518@umn.edu>
Message-ID: <327b821f0906110824n4da7939di7d2a57c68577ff77@mail.gmail.com>

My take:
Depending on physical circumstances (ie what kind of drives, what kind of
adapter, port multiplier, etc.) I have seen frequent timeouts on disks upon
reboot, leading to faulted zpools under Leopard. So, yes, this might happen
again, depending on why it happened this time.

Germano


On Thu, Jun 11, 2009 at 16:58, Carl Magnuson <magnu213 at umn.edu> wrote:

> Christian,
>
> After a reboot I have the following zpool status:
>
> g5:~ bobcat$ zpool status
>  pool: cpool
>  state: ONLINE
> status: One or more devices has experienced an unrecoverable error.  An
>        attempt was made to correct the error.  Applications are unaffected.
> action: Determine if the device needs to be replaced, and clear the errors
>        using 'zpool clear' or replace the device with 'zpool replace'.
>   see: http://www.sun.com/msg/ZFS-8000-9P
>  scrub: resilver completed with 0 errors on Thu Jun 11 09:50:58 2009
> config:
>
>        NAME         STATE     READ WRITE CKSUM
>        cpool        ONLINE       0     0     0
>          raidz2     ONLINE       0     0     0
>            disk2s2  ONLINE       0     0     0
>            disk3s2  ONLINE       0     0     0
>            disk4s2  ONLINE       0     0     0
>            disk6s2  ONLINE       0     0     0
>            disk5s2  ONLINE       0     0     0
>            disk7s2  ONLINE       0     0   102
>            disk8s2  ONLINE       0     0   104
>
> errors: No known data errors
>
>
> Which seems very odd to me that it seem to come back together.  Is there
> cause for concern - after another reboot the drives might not all come back
> into the pool like they did last time?  Or should that be an isolated
> circumstance?
>
> Thanks so much,
>
> Carl
>
>
>
>
> On Jun 10, 2009, at 10:36 PM, Christian Kujau wrote:
>
>  On Wed, 10 Jun 2009, Carl Magnuson wrote:
>>
>>> g5:~ bobcat$ zpool replace cpool 13962824586312699565 disk7s2
>>> cannot replace 13962824586312699565 with disk7s2: disk7s2 is busy
>>> g5:~ bobcat$ zpool replace cpool 1751454142452109774 disk8s2
>>> cannot replace 1751454142452109774 with disk8s2: disk8s2 is busy
>>>
>>
>> Hm, you ejected the drves, yet they ae still returning busy...weird. I
>> don't want to sound like a Windoze-helpdesk, but: is rebooting an option?
>>
>> Christian.
>> --
>> BOFH excuse #92:
>>
>> Stale file handle (next time use Tupperware(tm)!)
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090611/7cdaf91d/attachment.html>

From lists at nerdbynature.de  Thu Jun 11 10:59:46 2009
From: lists at nerdbynature.de (Christian Kujau)
Date: Thu, 11 Jun 2009 10:59:46 -0700 (PDT)
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <987DA95C-3D41-485D-8FDE-5412D1697518@umn.edu>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>
	<1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>
	<C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM>
	<00B2033E-D08B-4BE3-BB09-4614F1F4A8B5@umn.edu>
	<80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM>
	<F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>
	<alpine.DEB.2.01.0906102034050.6439@bogon>
	<987DA95C-3D41-485D-8FDE-5412D1697518@umn.edu>
Message-ID: <alpine.DEB.2.01.0906111053270.6439@bogon>

On Thu, 11 Jun 2009, Carl Magnuson wrote:
> 	    disk7s2  ONLINE       0     0   102
> 	    disk8s2  ONLINE       0     0   104

I'm glad the disks are back in the pool, but these errors: are they 
increasing when you're writing to the pool? Did "zpool scrub" find 
something odd? I wouldn't trust those disks yet. Maybe you should "dd 
if=/dev/disk7 of=/dev/null bs=1024k" and see if this runs through without 
any errors (see dmesg). If so, you could go ahead and "zpool clear" those 
errors and hope they won't come back.

Christian.
-- 
BOFH excuse #343:

The ATM board has run out of 10 pound notes.  We are having a whip round to refill it, care to contribute ?

From magnu213 at umn.edu  Thu Jun 11 16:00:44 2009
From: magnu213 at umn.edu (Carl Magnuson)
Date: Thu, 11 Jun 2009 18:00:44 -0500
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <alpine.DEB.2.01.0906111053270.6439@bogon>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>
	<1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>
	<C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM>
	<00B2033E-D08B-4BE3-BB09-4614F1F4A8B5@umn.edu>
	<80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM>
	<F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>
	<alpine.DEB.2.01.0906102034050.6439@bogon>
	<987DA95C-3D41-485D-8FDE-5412D1697518@umn.edu>
	<alpine.DEB.2.01.0906111053270.6439@bogon>
Message-ID: <59029BB5-5EB8-44C8-8145-11340076B77C@umn.edu>

zpool scrub runs without any output and the dd command you gave does  
not produce any output in dmesg, however the checksum has definitely  
shot up for those two disks after running each of those:

bash-3.2$ zpool status
   pool: cpool
  state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
         attempt was made to correct the error.  Applications are  
unaffected.
action: Determine if the device needs to be replaced, and clear the  
errors
         using 'zpool clear' or replace the device with 'zpool replace'.
    see: http://www.sun.com/msg/ZFS-8000-9P
  scrub: scrub in progress, 0.57% done, 11h25m to go
config:

         NAME         STATE     READ WRITE CKSUM
         cpool        ONLINE       0     0     0
           raidz2     ONLINE       0     0     0
             disk2s2  ONLINE       0     0     0
             disk3s2  ONLINE       0     0     0
             disk4s2  ONLINE       0     0     0
             disk6s2  ONLINE       0     0     0
             disk5s2  ONLINE       0     0     0
             disk7s2  ONLINE       0     0  1007
             disk8s2  ONLINE       0     0 1.17K

errors: No known data errors

Is this a sign of failing disks? If so is there any simple way to  
identify which physical drives they are other then pulling drives  
randomly and rebooting?

Thanks,

Carl


On Jun 11, 2009, at 12:59 PM, Christian Kujau wrote:

> On Thu, 11 Jun 2009, Carl Magnuson wrote:
>> 	    disk7s2  ONLINE       0     0   102
>> 	    disk8s2  ONLINE       0     0   104
>
> I'm glad the disks are back in the pool, but these errors: are they
> increasing when you're writing to the pool? Did "zpool scrub" find
> something odd? I wouldn't trust those disks yet. Maybe you should  
> "dd if=/dev/disk7 of=/dev/null bs=1024k" and see if this runs  
> through without
> any errors (see dmesg). If so, you could go ahead and "zpool clear"  
> those
> errors and hope they won't come back.
>
> Christian.
> -- 
> BOFH excuse #343:
>
> The ATM board has run out of 10 pound notes.  We are having a whip  
> round to refill it, care to contribute ?


From lists at nerdbynature.de  Thu Jun 11 16:55:38 2009
From: lists at nerdbynature.de (Christian Kujau)
Date: Thu, 11 Jun 2009 16:55:38 -0700 (PDT)
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <59029BB5-5EB8-44C8-8145-11340076B77C@umn.edu>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>
	<1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>
	<C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM>
	<00B2033E-D08B-4BE3-BB09-4614F1F4A8B5@umn.edu>
	<80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM>
	<F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>
	<alpine.DEB.2.01.0906102034050.6439@bogon>
	<987DA95C-3D41-485D-8FDE-5412D1697518@umn.edu>
	<alpine.DEB.2.01.0906111053270.6439@bogon>
	<59029BB5-5EB8-44C8-8145-11340076B77C@umn.edu>
Message-ID: <alpine.DEB.2.01.0906111646450.6439@bogon>

On Thu, 11 Jun 2009, Carl Magnuson wrote:
> zpool scrub runs without any output

You can see its status with "zpool status" as yu did below.

> and the dd command you gave does not produce any output in dmesg,

Hm, that's strange, because:

> however the checksum has definitely shot up for
> those two disks after running each of those:

ZFS is correcting errors for you, yet the disks can be read from w/o 
errors. Maybe it's bad RAM: the disks are ok, but ZFS has to correct 
spurious bit flips, but it's strange that only disk7/8 would be affected 
by this. Do you have a replacement disk at hand so that you could *really* 
replace disk7/8 instead of using the old ones? (Sooner or later you'd need 
a replacement disk anyway).

> Is this a sign of failing disks? If so is there any simple way to identify
> which physical drives they are other then pulling drives randomly and
> rebooting?

We know already it's disk7 and disk8 making trouble:

>        NAME         STATE     READ WRITE CKSUM
>        cpool        ONLINE       0     0     0
>          raidz2     ONLINE       0     0     0
>            disk7s2  ONLINE       0     0  1007
>            disk8s2  ONLINE       0     0 1.17K

And system_profiler told you the model and the serial# of the disks, so 
you could look them up in the disk cage.

Christian.
-- 
BOFH excuse #446:

Mailer-daemon is busy burning your message in hell.

From richard.elling at gmail.com  Thu Jun 11 16:59:21 2009
From: richard.elling at gmail.com (Richard Elling)
Date: Thu, 11 Jun 2009 16:59:21 -0700
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <alpine.DEB.2.01.0906111646450.6439@bogon>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>	<49EE4052.8030105@jrv.org>	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>	<alpine.DEB.2.01.0906081835030.6439@bogon>	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>	<alpine.DEB.2.01.0906091242590.6439@bogon>	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>	<1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>	<C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM>	<00B2033E-D08B-4BE3-BB09-4614F1F4A8B5@umn.edu>	<80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM>	<F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>	<alpine.DEB.2.01.0906102034050.6439@bogon>	<987DA95C-3D41-485D-8FDE-5412D1697518@umn.edu>	<alpine.DEB.2.01.0906111053270.6439@bogon>	<59029BB5-5EB8-44C8-8145-11340076B77C@umn.edu>
	<alpine.DEB.2.01.0906111646450.6439@bogon>
Message-ID: <4A319A59.6040905@gmail.com>

A shot in the dark: we've also seen that sort of behaviour from systems
with flaky cables, electrically noisy environments & flaky cables, or 
marginal
power supplies.
 -- richard

Christian Kujau wrote:
> On Thu, 11 Jun 2009, Carl Magnuson wrote:
>   
>> zpool scrub runs without any output
>>     
>
> You can see its status with "zpool status" as yu did below.
>
>   
>> and the dd command you gave does not produce any output in dmesg,
>>     
>
> Hm, that's strange, because:
>
>   
>> however the checksum has definitely shot up for
>> those two disks after running each of those:
>>     
>
> ZFS is correcting errors for you, yet the disks can be read from w/o 
> errors. Maybe it's bad RAM: the disks are ok, but ZFS has to correct 
> spurious bit flips, but it's strange that only disk7/8 would be affected 
> by this. Do you have a replacement disk at hand so that you could *really* 
> replace disk7/8 instead of using the old ones? (Sooner or later you'd need 
> a replacement disk anyway).
>
>   
>> Is this a sign of failing disks? If so is there any simple way to identify
>> which physical drives they are other then pulling drives randomly and
>> rebooting?
>>     
>
> We know already it's disk7 and disk8 making trouble:
>
>   
>>        NAME         STATE     READ WRITE CKSUM
>>        cpool        ONLINE       0     0     0
>>          raidz2     ONLINE       0     0     0
>>            disk7s2  ONLINE       0     0  1007
>>            disk8s2  ONLINE       0     0 1.17K
>>     
>
> And system_profiler told you the model and the serial# of the disks, so 
> you could look them up in the disk cage.
>
> Christian.
>   

From Jonathan.Edwards at Sun.COM  Thu Jun 11 19:43:31 2009
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Thu, 11 Jun 2009 22:43:31 -0400
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <59029BB5-5EB8-44C8-8145-11340076B77C@umn.edu>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>
	<1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>
	<C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM>
	<00B2033E-D08B-4BE3-BB09-4614F1F4A8B5@umn.edu>
	<80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM>
	<F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>
	<alpine.DEB.2.01.0906102034050.6439@bogon>
	<987DA95C-3D41-485D-8FDE-5412D1697518@umn.edu>
Message-ID: <5169B9BD-75C4-401E-AC80-B293BA59FBA1@Sun.COM>


On Jun 11, 2009, at 7:00 PM, Carl Magnuson wrote:

> zpool scrub runs without any output and the dd command you gave does  
> not produce any output in dmesg, however the checksum has definitely  
> shot up for those two disks after running each of those:

sure .. since the dd command would be adding more data to the pool and  
zfs is already busy validating data that was written to the disks that  
were previously missing (disk8/disk9 .. now disk7/disk8) .. i'd wait  
for the scrub to complete (adding a spare would be non-intrusive  
here), and you should see the status message go away (you can also do  
a status -xv to look for changes or a zpool iostat -v cpool 5 to watch  
activity) .. looks like you've got about 6.5 hours to go now ..

> bash-3.2$ zpool status
>  pool: cpool
> state: ONLINE
> status: One or more devices has experienced an unrecoverable error.   
> An
>        attempt was made to correct the error.  Applications are  
> unaffected.
> action: Determine if the device needs to be replaced, and clear the  
> errors
>        using 'zpool clear' or replace the device with 'zpool replace'.
>   see: http://www.sun.com/msg/ZFS-8000-9P
> scrub: scrub in progress, 0.57% done, 11h25m to go
> config:
>
>        NAME         STATE     READ WRITE CKSUM
>        cpool        ONLINE       0     0     0
>          raidz2     ONLINE       0     0     0
>            disk2s2  ONLINE       0     0     0
>            disk3s2  ONLINE       0     0     0
>            disk4s2  ONLINE       0     0     0
>            disk6s2  ONLINE       0     0     0
>            disk5s2  ONLINE       0     0     0
>            disk7s2  ONLINE       0     0  1007
>            disk8s2  ONLINE       0     0 1.17K
>
> errors: No known data errors
>
> Is this a sign of failing disks? If so is there any simple way to  
> identify which physical drives they are other then pulling drives  
> randomly and rebooting?

can't tell at this point .. if you are seeing excessive cksums or  
frequent scrubs that take a long time to complete then you could have  
a problem .. but then from previous emails - it appears that you were  
having some issues with disks disappearing from the expander .. so  
that could have been some of it there

---
.je

From magnu213 at umn.edu  Thu Jun 11 20:23:19 2009
From: magnu213 at umn.edu (Carl Magnuson)
Date: Thu, 11 Jun 2009 22:23:19 -0500
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <5169B9BD-75C4-401E-AC80-B293BA59FBA1@Sun.COM>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>
	<1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>
	<C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM>
	<00B2033E-D08B-4BE3-BB09-4614F1F4A8B5@umn.edu>
	<80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM>
	<F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>
	<alpine.DEB.2.01.0906102034050.6439@bogon>
	<987DA95C-3D41-485D-8FDE-5412D1697518@umn.edu>
	<5169B9BD-75C4-401E-AC80-B293BA59FBA1@Sun.COM>
Message-ID: <B63F5CBD-FDF3-4C3E-A9BF-0112F5BC0BE3@umn.edu>


On Jun 11, 2009, at 9:43 PM, Jonathan Edwards wrote:

>
> On Jun 11, 2009, at 7:00 PM, Carl Magnuson wrote:
>
>> zpool scrub runs without any output and the dd command you gave  
>> does not produce any output in dmesg, however the checksum has  
>> definitely shot up for those two disks after running each of those:
>
> sure .. since the dd command would be adding more data to the pool  
> and zfs is already busy validating data that was written to the  
> disks that were previously missing (disk8/disk9 .. now disk7/ 
> disk8) .. i'd wait for the scrub to complete (adding a spare would  
> be non-intrusive here), and you should see the status message go  
> away (you can also do a status -xv to look for changes or a zpool  
> iostat -v cpool 5 to watch activity) .. looks like you've got about  
> 6.5 hours to go now ..

After the scrub completed I ran the given dd command on both disk7 and  
disk8, I do not see the checksum going up for either afterwords. Out  
of curiosity I tried running another scrub and I do see checksum going  
up for both disk7 and disk8 again but none of the other disks.  The  
last scrub left the checksums at 1.96K and 2.13K.  I'm not sure how to  
take this, but have two new drives on the way to be safe.

Thanks,

Carl

From alex.blewitt at gmail.com  Thu Jun 11 21:56:44 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Fri, 12 Jun 2009 05:56:44 +0100
Subject: [zfs-discuss] Kernel Panic and Missing Pool
In-Reply-To: <B63F5CBD-FDF3-4C3E-A9BF-0112F5BC0BE3@umn.edu>
References: <mailman.138.1239858279.597.zfs-discuss@lists.macosforge.org>
	<b7dd15cb0904211326n58acbf13p94e0ec1152b66634@mail.gmail.com>
	<2F64E272-B3A3-4691-BEE5-F850DF2ED656@umn.edu>
	<49EE4052.8030105@jrv.org>
	<3DB34638-44CA-49D9-A156-3FA016D6F6FD@umn.edu>
	<6FFF32EB-85FA-4E0F-94A6-DF91741C4872@umn.edu>
	<alpine.DEB.2.01.0906081835030.6439@bogon>
	<6132D768-35F5-4DCF-8487-F76C323756F6@umn.edu>
	<alpine.DEB.2.01.0906091242590.6439@bogon>
	<587CC22A-0E88-48BA-9144-C7FD97403F20@umn.edu>
	<327b821f0906091438p54cf6fb7g28b04f05f1ccb70e@mail.gmail.com>
	<7C9510D8-3656-4CD3-A0DC-DE33E012E05F@umn.edu>
	<D5167F9F-27E6-4C4B-A0B1-400E87706AC9@sun.com>
	<1C0EAA6E-107D-4FB8-B8F5-35F10EFA2028@umn.edu>
	<C80F63FA-010E-4D3D-AF09-9890A49C2D89@Sun.COM>
	<00B2033E-D08B-4BE3-BB09-4614F1F4A8B5@umn.edu>
	<80C98667-F34E-4C23-AF9A-3890FCCD519E@Sun.COM>
	<F03CAF93-B4D9-4607-BBD4-7AAC5D771CE1@umn.edu>
	<alpine.DEB.2.01.0906102034050.6439@bogon>
	<987DA95C-3D41-485D-8FDE-5412D1697518@umn.edu>
	<5169B9BD-75C4-401E-AC80-B293BA59FBA1@Sun.COM>
	<B63F5CBD-FDF3-4C3E-A9BF-0112F5BC0BE3@umn.edu>
Message-ID: <A34491A7-5299-47FF-806B-4AD5FD81868E@gmail.com>

Fwiw I found issues with an earlier build of ZFS and external USB  
enclosures. I think the fault was the enclosure but I saw similar  
symptoms. There were no read errors because I had used mirroring and  
data duplication.

Are disk7/8 using a different interface or enclosure than the others?

Alex

Sent from my (new) iPhone

On 12 Jun 2009, at 04:23, Carl Magnuson <magnu213 at umn.edu> wrote:

>
> On Jun 11, 2009, at 9:43 PM, Jonathan Edwards wrote:
>
>>
>> On Jun 11, 2009, at 7:00 PM, Carl Magnuson wrote:
>>
>>> zpool scrub runs without any output and the dd command you gave  
>>> does not produce any output in dmesg, however the checksum has  
>>> definitely shot up for those two disks after running each of those:
>>
>> sure .. since the dd command would be adding more data to the pool  
>> and zfs is already busy validating data that was written to the  
>> disks that were previously missing (disk8/disk9 .. now disk7/ 
>> disk8) .. i'd wait for the scrub to complete (adding a spare would  
>> be non-intrusive here), and you should see the status message go  
>> away (you can also do a status -xv to look for changes or a zpool  
>> iostat -v cpool 5 to watch activity) .. looks like you've got about  
>> 6.5 hours to go now ..
>
> After the scrub completed I ran the given dd command on both disk7  
> and disk8, I do not see the checksum going up for either afterwords.  
> Out of curiosity I tried running another scrub and I do see checksum  
> going up for both disk7 and disk8 again but none of the other  
> disks.  The last scrub left the checksums at 1.96K and 2.13K.  I'm  
> not sure how to take this, but have two new drives on the way to be  
> safe.
>
> Thanks,
>
> Carl
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From dirkschelfhout at mac.com  Fri Jun 12 10:17:42 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Fri, 12 Jun 2009 19:17:42 +0200
Subject: [zfs-discuss] zfs alternative
Message-ID: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>

Since probably no longer supported by apple, I would like to move my 4  
guid disks to a new machine running another os.

Any suggestions which os would be best ? Is there a *nix version that  
runs afp ? Will they recognize the disks ?

Also what kind off machine would I need, just to run as a zfs server  
for 2 to 3 clients. ( on a gigabit ethernet )
( as cheap as possible consuming as little power as possible. with the  
latter being more important to me ( and many disk bays ) )

Dirk 

From mcamou at tecnoguru.com  Fri Jun 12 10:20:52 2009
From: mcamou at tecnoguru.com (Mario Camou)
Date: Fri, 12 Jun 2009 19:20:52 +0200
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
Message-ID: <762437f0906121020t56074e11j19d256adb7df63d4@mail.gmail.com>

Linux supports AFP but the ZFS support is rather bad (via UserFS). I would
recommend OpenSolaris and installing the Netatalk package (I'm not sure if
it's available as a package or if you have to recompile it).
-Mario.

--
I want to change the world but they won't give me the source code.


On Fri, Jun 12, 2009 at 19:17, Dirk Schelfhout <dirkschelfhout at mac.com>wrote:

> Since probably no longer supported by apple, I would like to move my 4 guid
> disks to a new machine running another os.
>
> Any suggestions which os would be best ? Is there a *nix version that runs
> afp ? Will they recognize the disks ?
>
> Also what kind off machine would I need, just to run as a zfs server for 2
> to 3 clients. ( on a gigabit ethernet )
> ( as cheap as possible consuming as little power as possible. with the
> latter being more important to me ( and many disk bays ) )
>
> Dirk_______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090612/76a42152/attachment.html>

From aorchid at mac.com  Fri Jun 12 10:20:42 2009
From: aorchid at mac.com (Aric Gregson)
Date: Fri, 12 Jun 2009 10:20:42 -0700
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
Message-ID: <20090612102042.2625beb4.aorchid@mac.com>

On Fri, 12 Jun 2009 19:17:42 +0200
Dirk Schelfhout <dirkschelfhout at mac.com> wrote:

> Since probably no longer supported by apple, I would like to move my
> 4 guid disks to a new machine running another os.
> 
> Any suggestions which os would be best ?

OpenSolaris.

Maybe your decision is premature. Are we absolutely certain that the
new OS from Mac will no longer support ZFS? That seems unlikely. 

aric

From eableson at mac.com  Fri Jun 12 10:42:19 2009
From: eableson at mac.com (Erik Ableson)
Date: Fri, 12 Jun 2009 19:42:19 +0200
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
Message-ID: <F83052A9-6A2D-4FE7-B931-48E8124EEF77@mac.com>

Anorher vote for OpenSolaris. You can use the netatalk package if you  
really want to, but OS X is a first class NFS client and with the  
GlobalSAN initiator you can also use iSCSI.

I'm currently using NFS & iSCSI with great performance and reliability  
under OpenSolaris. Even when Apple does integrate ZFS, I'm probably  
going to keep the OS server for the form factor: 10 disks in a tower  
with one plug is a design I'm unlikely to see from Apple. Given that  
my OS X "server" is a Macbook pro, internal storage is not an option  
and daisy chaining drives and the attendant cable spaghetti drove me  
nuts.

I'm currently running 4 clients off a Core 2 Duo PC quite happily  
(details at www.infrageeks.com, search for "better NAS") with direct  
GbE for my OS X Server and one other client with the rest over 11n.  
The disks are more than capable of saturating the GbE connection and  
even with multiple Time Machine backups running concurrently the  
source disks are the bottleneck rather than the network or the server.

Assuming the disks are part of a zpool they should be readable by  
OpenSolaris, but the usual Apple ZFS configuration adrdesses a  
partition so you may have to tweak a little to get the zpool mounted.

Cordialement,

Erik Ableson

Envoy? depuis mon iPhone

On 12 juin 09, at 19:17, Dirk Schelfhout <dirkschelfhout at mac.com> wrote:

> Since probably no longer supported by apple, I would like to move my  
> 4 guid disks to a new machine running another os.
>
> Any suggestions which os would be best ? Is there a *nix version  
> that runs afp ? Will they recognize the disks ?
>
> Also what kind off machine would I need, just to run as a zfs server  
> for 2 to 3 clients. ( on a gigabit ethernet )
> ( as cheap as possible consuming as little power as possible. with  
> the latter being more important to me ( and many disk bays ) )
>
> Dirk_______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From Matthew.Ahrens at sun.com  Fri Jun 12 10:38:41 2009
From: Matthew.Ahrens at sun.com (Matthew Ahrens)
Date: Fri, 12 Jun 2009 10:38:41 -0700
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <762437f0906121020t56074e11j19d256adb7df63d4@mail.gmail.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<762437f0906121020t56074e11j19d256adb7df63d4@mail.gmail.com>
Message-ID: <4A3292A1.4060001@sun.com>

MacOS also supports SMB (CIFS) and NFS.  Using OpenSolaris with ZFS serving 
out SMB to MacOS is working well for me so far.  Case-insensitivity even works!

--matt

Mario Camou wrote:
> Linux supports AFP but the ZFS support is rather bad (via UserFS). I 
> would recommend OpenSolaris and installing the Netatalk package (I'm not 
> sure if it's available as a package or if you have to recompile it).
> 
> -Mario.
> 
> --
> I want to change the world but they won't give me the source code.
> 
> 
> On Fri, Jun 12, 2009 at 19:17, Dirk Schelfhout <dirkschelfhout at mac.com 
> <mailto:dirkschelfhout at mac.com>> wrote:
> 
>     Since probably no longer supported by apple, I would like to move my
>     4 guid disks to a new machine running another os.
> 
>     Any suggestions which os would be best ? Is there a *nix version
>     that runs afp ? Will they recognize the disks ?
> 
>     Also what kind off machine would I need, just to run as a zfs server
>     for 2 to 3 clients. ( on a gigabit ethernet )
>     ( as cheap as possible consuming as little power as possible. with
>     the latter being more important to me ( and many disk bays ) )
> 
>     Dirk_______________________________________________
>     zfs-discuss mailing list
>     zfs-discuss at lists.macosforge.org
>     <mailto:zfs-discuss at lists.macosforge.org>
>     http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> 
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From mcamou at tecnoguru.com  Fri Jun 12 10:46:34 2009
From: mcamou at tecnoguru.com (Mario Camou)
Date: Fri, 12 Jun 2009 19:46:34 +0200
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <F83052A9-6A2D-4FE7-B931-48E8124EEF77@mac.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com> 
	<F83052A9-6A2D-4FE7-B931-48E8124EEF77@mac.com>
Message-ID: <762437f0906121046r31d8242fle5baf55e2dfe8f8b@mail.gmail.com>

I actually had to do that (mount an OS X pool on a Solaris box) a while ago
to debug some problems. IIRC, just exporting the pool from OS X and
importing to Solaris did the job, no tweaking required.
-Mario.

--
I want to change the world but they won't give me the source code.


On Fri, Jun 12, 2009 at 19:42, Erik Ableson <eableson at mac.com> wrote:

> Anorher vote for OpenSolaris. You can use the netatalk package if you
> really want to, but OS X is a first class NFS client and with the GlobalSAN
> initiator you can also use iSCSI.
>
> I'm currently using NFS & iSCSI with great performance and reliability
> under OpenSolaris. Even when Apple does integrate ZFS, I'm probably going to
> keep the OS server for the form factor: 10 disks in a tower with one plug is
> a design I'm unlikely to see from Apple. Given that my OS X "server" is a
> Macbook pro, internal storage is not an option and daisy chaining drives and
> the attendant cable spaghetti drove me nuts.
>
> I'm currently running 4 clients off a Core 2 Duo PC quite happily (details
> at www.infrageeks.com, search for "better NAS") with direct GbE for my OS
> X Server and one other client with the rest over 11n. The disks are more
> than capable of saturating the GbE connection and even with multiple Time
> Machine backups running concurrently the source disks are the bottleneck
> rather than the network or the server.
>
> Assuming the disks are part of a zpool they should be readable by
> OpenSolaris, but the usual Apple ZFS configuration adrdesses a partition so
> you may have to tweak a little to get the zpool mounted.
>
> Cordialement,
>
> Erik Ableson
>
> Envoy? depuis mon iPhone
>
>
> On 12 juin 09, at 19:17, Dirk Schelfhout <dirkschelfhout at mac.com> wrote:
>
>  Since probably no longer supported by apple, I would like to move my 4
>> guid disks to a new machine running another os.
>>
>> Any suggestions which os would be best ? Is there a *nix version that runs
>> afp ? Will they recognize the disks ?
>>
>> Also what kind off machine would I need, just to run as a zfs server for 2
>> to 3 clients. ( on a gigabit ethernet )
>> ( as cheap as possible consuming as little power as possible. with the
>> latter being more important to me ( and many disk bays ) )
>>
>> Dirk_______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090612/739a5efa/attachment.html>

From lists at loveturtle.net  Fri Jun 12 10:50:31 2009
From: lists at loveturtle.net (Dillon Kass)
Date: Fri, 12 Jun 2009 13:50:31 -0400
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <762437f0906121020t56074e11j19d256adb7df63d4@mail.gmail.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<762437f0906121020t56074e11j19d256adb7df63d4@mail.gmail.com>
Message-ID: <4A329567.1010502@loveturtle.net>

You should wait until 10.6 is released. I find it very unlikely that ZFS 
is really gone. I find it more likely that it has been pulled from the 
official release because something wasn't ready and they don't want to 
officially support it. They will likely update the macosforge page upon 
release.

There were significant updates to ZFS in some of the 10.6 betas before 
it was pulled and I can't imagine throwing away all that work when it's 
already done. Not to mention a decent amount of folks at Apple are Unix 
geeks and are probably just as excited about ZFS as the rest of us are.

This isn't that dissimilar to 10.5, If anyone recalls some early 10.5 
betas had ZFS support and then it was pulled as we got close to the 
release just to show up in it's current unoffically supported form after 
release.

That said, I would go with FreeBSD. If you're familiar with it... 
7-STABLE (which will become 7.3) has a much improved ZFS implementation. 
ZFS v13, very fast, very stable, ZFS boot on GPT & MBR is supported (I 
have personally tested both, though the procedure is probably over the 
head of anyone who isn't familiar with the specifics of FreeBSD). You 
will be able to import your pools from OS-X.

NOTE: 7.2-RELEASE which is what normal users would use has ZFS v6 which 
will not work with your OS-X pool if you have upgraded it to v8. You 
must use 7-STABLE which is a development branch. This is probably over 
your head if you're not familiar with FreeBSD. Though if you have enough 
years of Linux experience you should be able to keep up just fine. If 
that turns you off OpenSolaris is a good alternative but the learning 
curve is steep and the clusterfuck of solaris & gnu tools needs another 
year or two of polishing if you ask me. I'd rather use SXCE, at least I 
don't have to worry about shitty gnu ls not understanding my ACL's.

Relax and wait until September.


On 6/12/09 1:20 PM, Mario Camou wrote:
> Linux supports AFP but the ZFS support is rather bad (via UserFS). I 
> would recommend OpenSolaris and installing the Netatalk package (I'm 
> not sure if it's available as a package or if you have to recompile it).
>
> -Mario.
>
> --
> I want to change the world but they won't give me the source code.
>
>
> On Fri, Jun 12, 2009 at 19:17, Dirk Schelfhout <dirkschelfhout at mac.com 
> <mailto:dirkschelfhout at mac.com>> wrote:
>
>     Since probably no longer supported by apple, I would like to move
>     my 4 guid disks to a new machine running another os.
>
>     Any suggestions which os would be best ? Is there a *nix version
>     that runs afp ? Will they recognize the disks ?
>
>     Also what kind off machine would I need, just to run as a zfs
>     server for 2 to 3 clients. ( on a gigabit ethernet )
>     ( as cheap as possible consuming as little power as possible. with
>     the latter being more important to me ( and many disk bays ) )
>
>     Dirk_______________________________________________
>     zfs-discuss mailing list
>     zfs-discuss at lists.macosforge.org
>     <mailto:zfs-discuss at lists.macosforge.org>
>     http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
> ------------------------------------------------------------------------
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>    

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090612/ef340796/attachment-0001.html>

From eableson at mac.com  Fri Jun 12 10:53:04 2009
From: eableson at mac.com (Erik Ableson)
Date: Fri, 12 Jun 2009 19:53:04 +0200
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <762437f0906121046r31d8242fle5baf55e2dfe8f8b@mail.gmail.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<F83052A9-6A2D-4FE7-B931-48E8124EEF77@mac.com>
	<762437f0906121046r31d8242fle5baf55e2dfe8f8b@mail.gmail.com>
Message-ID: <FCE29B4C-9EB8-4B05-8446-F61FD9E86D17@mac.com>

Cool. That's good to know.

Cordialement,

Erik Ableson

+33.6.80.83.58.28
Envoy? depuis mon iPhone

On 12 juin 09, at 19:46, Mario Camou <mcamou at tecnoguru.com> wrote:

> I actually had to do that (mount an OS X pool on a Solaris box) a  
> while ago to debug some problems. IIRC, just exporting the pool from  
> OS X and importing to Solaris did the job, no tweaking required.
> -Mario.
>
> --
> I want to change the world but they won't give me the source code.
>
>
> On Fri, Jun 12, 2009 at 19:42, Erik Ableson <eableson at mac.com> wrote:
> Anorher vote for OpenSolaris. You can use the netatalk package if  
> you really want to, but OS X is a first class NFS client and with  
> the GlobalSAN initiator you can also use iSCSI.
>
> I'm currently using NFS & iSCSI with great performance and  
> reliability under OpenSolaris. Even when Apple does integrate ZFS,  
> I'm probably going to keep the OS server for the form factor: 10  
> disks in a tower with one plug is a design I'm unlikely to see from  
> Apple. Given that my OS X "server" is a Macbook pro, internal  
> storage is not an option and daisy chaining drives and the attendant  
> cable spaghetti drove me nuts.
>
> I'm currently running 4 clients off a Core 2 Duo PC quite happily  
> (details at www.infrageeks.com, search for "better NAS") with direct  
> GbE for my OS X Server and one other client with the rest over 11n.  
> The disks are more than capable of saturating the GbE connection and  
> even with multiple Time Machine backups running concurrently the  
> source disks are the bottleneck rather than the network or the server.
>
> Assuming the disks are part of a zpool they should be readable by  
> OpenSolaris, but the usual Apple ZFS configuration adrdesses a  
> partition so you may have to tweak a little to get the zpool mounted.
>
> Cordialement,
>
> Erik Ableson
>
> Envoy? depuis mon iPhone
>
>
> On 12 juin 09, at 19:17, Dirk Schelfhout <dirkschelfhout at mac.com>  
> wrote:
>
> Since probably no longer supported by apple, I would like to move my  
> 4 guid disks to a new machine running another os.
>
> Any suggestions which os would be best ? Is there a *nix version  
> that runs afp ? Will they recognize the disks ?
>
> Also what kind off machine would I need, just to run as a zfs server  
> for 2 to 3 clients. ( on a gigabit ethernet )
> ( as cheap as possible consuming as little power as possible. with  
> the latter being more important to me ( and many disk bays ) )
>
> Dirk_______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090612/ca6a6889/attachment.html>

From Jonathan.Edwards at Sun.COM  Fri Jun 12 11:32:47 2009
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Fri, 12 Jun 2009 14:32:47 -0400
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <762437f0906121046r31d8242fle5baf55e2dfe8f8b@mail.gmail.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<F83052A9-6A2D-4FE7-B931-48E8124EEF77@mac.com>
	<762437f0906121046r31d8242fle5baf55e2dfe8f8b@mail.gmail.com>
Message-ID: <5170CC05-6DA5-4AFE-BF1A-74A934CF8319@sun.com>

i do this all the time to share data between my laptop and my  
opensolaris boxes, or solaris 10 servers i'm working on .. only thing  
to be careful is are the zfs and zpool versions you're working with -  
you'll want to be on a common denominator across your systems to make  
sure the pool is importable:
# zpool upgrade -v
# zfs upgrade -v

you can also test with the latest OpenSolaris LiveCD (i run this in a  
virtual machine [virtualbox is free and pretty nice now on the mac],  
and import my usb drive(s) there if i need to test or repair something)

---
.je

On Jun 12, 2009, at 1:46 PM, Mario Camou wrote:

> I actually had to do that (mount an OS X pool on a Solaris box) a  
> while ago to debug some problems. IIRC, just exporting the pool from  
> OS X and importing to Solaris did the job, no tweaking required.
> -Mario.
>
> --
> I want to change the world but they won't give me the source code.
>
>
> On Fri, Jun 12, 2009 at 19:42, Erik Ableson <eableson at mac.com> wrote:
> Anorher vote for OpenSolaris. You can use the netatalk package if  
> you really want to, but OS X is a first class NFS client and with  
> the GlobalSAN initiator you can also use iSCSI.
>
> I'm currently using NFS & iSCSI with great performance and  
> reliability under OpenSolaris. Even when Apple does integrate ZFS,  
> I'm probably going to keep the OS server for the form factor: 10  
> disks in a tower with one plug is a design I'm unlikely to see from  
> Apple. Given that my OS X "server" is a Macbook pro, internal  
> storage is not an option and daisy chaining drives and the attendant  
> cable spaghetti drove me nuts.
>
> I'm currently running 4 clients off a Core 2 Duo PC quite happily  
> (details at www.infrageeks.com, search for "better NAS") with direct  
> GbE for my OS X Server and one other client with the rest over 11n.  
> The disks are more than capable of saturating the GbE connection and  
> even with multiple Time Machine backups running concurrently the  
> source disks are the bottleneck rather than the network or the server.
>
> Assuming the disks are part of a zpool they should be readable by  
> OpenSolaris, but the usual Apple ZFS configuration adrdesses a  
> partition so you may have to tweak a little to get the zpool mounted.
>
> Cordialement,
>
> Erik Ableson
>
> Envoy? depuis mon iPhone
>
>
> On 12 juin 09, at 19:17, Dirk Schelfhout <dirkschelfhout at mac.com>  
> wrote:
>
> Since probably no longer supported by apple, I would like to move my  
> 4 guid disks to a new machine running another os.
>
> Any suggestions which os would be best ? Is there a *nix version  
> that runs afp ? Will they recognize the disks ?
>
> Also what kind off machine would I need, just to run as a zfs server  
> for 2 to 3 clients. ( on a gigabit ethernet )
> ( as cheap as possible consuming as little power as possible. with  
> the latter being more important to me ( and many disk bays ) )
>
> Dirk_______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From sebastian at pixelmilk.com  Fri Jun 12 12:01:43 2009
From: sebastian at pixelmilk.com (=?ISO-8859-1?Q?Sebastian_D=F6ll?=)
Date: Fri, 12 Jun 2009 21:01:43 +0200
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
Message-ID: <411220A1-11DE-4069-AACC-77AFD5437123@pixelmilk.com>

What I got curious about was what Bertrand Serlet said on the keynote.
He has spoken of filesystem compression, so maybe this will be  
available in HFS+
throughout the system, would be cool. I really believe in 10.6 server  
we'll see ZFS
and later maybe in Xsan.

Am 12.06.2009 um 19:17 schrieb Dirk Schelfhout:

> Since probably no longer supported by apple, I would like to move my  
> 4 guid disks to a new machine running another os.
>
> Any suggestions which os would be best ? Is there a *nix version  
> that runs afp ? Will they recognize the disks ?
>
> Also what kind off machine would I need, just to run as a zfs server  
> for 2 to 3 clients. ( on a gigabit ethernet )
> ( as cheap as possible consuming as little power as possible. with  
> the latter being more important to me ( and many disk bays ) )
>
> Dirk_______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4818 bytes
Desc: not available
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090612/d774861e/attachment.bin>

From alex.blewitt at gmail.com  Fri Jun 12 12:13:15 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Fri, 12 Jun 2009 20:13:15 +0100
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <411220A1-11DE-4069-AACC-77AFD5437123@pixelmilk.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<411220A1-11DE-4069-AACC-77AFD5437123@pixelmilk.com>
Message-ID: <636fd28e0906121213r6b505f74n212a2caa83886376@mail.gmail.com>

On Fri, Jun 12, 2009 at 8:01 PM, Sebastian D?ll<sebastian at pixelmilk.com> wrote:
> What I got curious about was what Bertrand Serlet said on the keynote.
> He has spoken of filesystem compression, so maybe this will be available in
> HFS+

I think it might be a euphemism for (a) we're only shipping Intel
binaries, not fat ones, and (b) we're not going to include any
developer rubbish that we ship with the apps (b) not shipping
designable.nibs (http://digg.com/apple/designable_nib). That's save
space and yet suddenly 'they've compressed it all'.

So take it with a pinch of NaCl - having said that, compression in the
FS is definitely a good thing to do and pretty cheap these days, esp.
with multi-core systems.

Alex

From sebastian at pixelmilk.com  Fri Jun 12 12:16:34 2009
From: sebastian at pixelmilk.com (=?ISO-8859-1?Q?Sebastian_D=F6ll?=)
Date: Fri, 12 Jun 2009 21:16:34 +0200
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <636fd28e0906121213r6b505f74n212a2caa83886376@mail.gmail.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<411220A1-11DE-4069-AACC-77AFD5437123@pixelmilk.com>
	<636fd28e0906121213r6b505f74n212a2caa83886376@mail.gmail.com>
Message-ID: <4C957F5C-8E65-49E9-BBBF-C7D820AAE99F@pixelmilk.com>

Yeah, I know what they've done to the system, but he really said,
"thx to filesystem compression", so i'm just guessing, but you got
such things in most modern filesystems. Wouldn't be such a suprise :).

Am 12.06.2009 um 21:13 schrieb Alex Blewitt:

> On Fri, Jun 12, 2009 at 8:01 PM, Sebastian D?ll<sebastian at pixelmilk.com 
> > wrote:
>> What I got curious about was what Bertrand Serlet said on the  
>> keynote.
>> He has spoken of filesystem compression, so maybe this will be  
>> available in
>> HFS+
>
> I think it might be a euphemism for (a) we're only shipping Intel
> binaries, not fat ones, and (b) we're not going to include any
> developer rubbish that we ship with the apps (b) not shipping
> designable.nibs (http://digg.com/apple/designable_nib). That's save
> space and yet suddenly 'they've compressed it all'.
>
> So take it with a pinch of NaCl - having said that, compression in the
> FS is definitely a good thing to do and pretty cheap these days, esp.
> with multi-core systems.
>
> Alex

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4818 bytes
Desc: not available
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090612/4b3e8128/attachment-0001.bin>

From dwebb at erbco.com  Fri Jun 12 13:21:58 2009
From: dwebb at erbco.com (Dustin Webb)
Date: Fri, 12 Jun 2009 16:21:58 -0400
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <4C957F5C-8E65-49E9-BBBF-C7D820AAE99F@pixelmilk.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<411220A1-11DE-4069-AACC-77AFD5437123@pixelmilk.com>
	<636fd28e0906121213r6b505f74n212a2caa83886376@mail.gmail.com>
	<4C957F5C-8E65-49E9-BBBF-C7D820AAE99F@pixelmilk.com>
Message-ID: <CDF880EB-1470-4C88-913F-36642FAD15D2@erbco.com>

I'm a big fan of these guys.  http://www.nexenta.org
They make a weird hybrid Opensolaris/Ubuntu OS called Nexenta.  They  
also have a for pay version called NexentaStor which strips it down to  
a headless file server.  Everything is easily managed through a web  
browser and if you are using less than 1 Terabyte of storage then it's  
free.  They have put together a very nice product, it supports iSCSI  
and anything else imaginable.

Dustin

From sebastian at pixelmilk.com  Fri Jun 12 13:49:37 2009
From: sebastian at pixelmilk.com (=?ISO-8859-1?Q?Sebastian_D=F6ll?=)
Date: Fri, 12 Jun 2009 22:49:37 +0200
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <CDF880EB-1470-4C88-913F-36642FAD15D2@erbco.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<411220A1-11DE-4069-AACC-77AFD5437123@pixelmilk.com>
	<636fd28e0906121213r6b505f74n212a2caa83886376@mail.gmail.com>
	<4C957F5C-8E65-49E9-BBBF-C7D820AAE99F@pixelmilk.com>
	<CDF880EB-1470-4C88-913F-36642FAD15D2@erbco.com>
Message-ID: <980A636A-1F0F-485A-A1A0-82D29948F047@pixelmilk.com>

The guys from FreeNAS are also working on ZFS support. I've a Jetway  
board (VIA),
with Gbit LAN and FreeNAS running from a CF Card. Very energy  
efficient and silent.


Am 12.06.2009 um 22:21 schrieb Dustin Webb:

> I'm a big fan of these guys.  http://www.nexenta.org
> They make a weird hybrid Opensolaris/Ubuntu OS called Nexenta.  They  
> also have a for pay version called NexentaStor which strips it down  
> to a headless file server.  Everything is easily managed through a  
> web browser and if you are using less than 1 Terabyte of storage  
> then it's free.  They have put together a very nice product, it  
> supports iSCSI and anything else imaginable.
>
> Dustin
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4818 bytes
Desc: not available
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090612/71747ac5/attachment.bin>

From eableson at mac.com  Sat Jun 13 04:00:53 2009
From: eableson at mac.com (Erik Ableson)
Date: Sat, 13 Jun 2009 13:00:53 +0200
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <4A329567.1010502@loveturtle.net>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<762437f0906121020t56074e11j19d256adb7df63d4@mail.gmail.com>
	<4A329567.1010502@loveturtle.net>
Message-ID: <B56419DE-AC4A-4604-9D30-0669F36ADFDB@mac.com>

e 12 juin 2009 ? 19:50, Dillon Kass a ?crit :

> You should wait until 10.6 is released. I find it very unlikely that  
> ZFS is really gone. I find it more likely that it has been pulled  
> from the official release because something wasn't ready and they  
> don't want to officially support it. They will likely update the  
> macosforge page upon release.

ZFS is not in the latest GM build from WWDC. I think that we can  
consider that it will not be a core OS component as part of the  
standard install. I don't have the Server version yet so I'll have to  
check that out.

> There were significant updates to ZFS in some of the 10.6 betas  
> before it was pulled and I can't imagine throwing away all that work  
> when it's already done. Not to mention a decent amount of folks at  
> Apple are Unix geeks and are probably just as excited about ZFS as  
> the rest of us are.

The work certainly won't be thrown away, but it won't be seen as an  
integrated component until 10.6.x or more likely 10.7.  It's possible  
that they will release it as a standalone package with a separate  
marketing message since generally point releases are not permitted to  
include important new functionality.

I expect that the unsupported beta releases on macosforge will reopen  
after 10.6 hits in September.

Cheers,

Erik
Note - I don't have any kind of inside information - just ideas and  
opinions based on dealing with Apple for a long time.

>
> This isn't that dissimilar to 10.5, If anyone recalls some early  
> 10.5 betas had ZFS support and then it was pulled as we got close to  
> the release just to show up in it's current unoffically supported  
> form after release.
>
>
> On 6/12/09 1:20 PM, Mario Camou wrote:
>>
>> Linux supports AFP but the ZFS support is rather bad (via UserFS).  
>> I would recommend OpenSolaris and installing the Netatalk package  
>> (I'm not sure if it's available as a package or if you have to  
>> recompile it).
>>
>> -Mario.
>>
>> --
>> I want to change the world but they won't give me the source code.
>>
>>
>> On Fri, Jun 12, 2009 at 19:17, Dirk Schelfhout <dirkschelfhout at mac.com 
>> > wrote:
>> Since probably no longer supported by apple, I would like to move  
>> my 4 guid disks to a new machine running another os.
>>
>> Any suggestions which os would be best ? Is there a *nix version  
>> that runs afp ? Will they recognize the disks ?
>>
>> Also what kind off machine would I need, just to run as a zfs  
>> server for 2 to 3 clients. ( on a gigabit ethernet )
>> ( as cheap as possible consuming as little power as possible. with  
>> the latter being more important to me ( and many disk bays ) )

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090613/367333c1/attachment.html>

From nancejk at phys.washington.edu  Sat Jun 13 09:42:02 2009
From: nancejk at phys.washington.edu (Jared Nance)
Date: Sat, 13 Jun 2009 09:42:02 -0700
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <B56419DE-AC4A-4604-9D30-0669F36ADFDB@mac.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>	<762437f0906121020t56074e11j19d256adb7df63d4@mail.gmail.com>	<4A329567.1010502@loveturtle.net>
	<B56419DE-AC4A-4604-9D30-0669F36ADFDB@mac.com>
Message-ID: <4A33D6DA.5030701@phys.washington.edu>

Is it fair to assume that work will continue with the version on
macosforge?  My understanding is that the version that will (hopefully)
eventually be in core mac OS will be derived from the work that is
currently represented there.  If so, I suppose I can wait.

Jared N

Erik Ableson wrote:
> e 12 juin 2009 ? 19:50, Dillon Kass a ?crit :
>
>> You should wait until 10.6 is released. I find it very unlikely that
>> ZFS is really gone. I find it more likely that it has been pulled
>> from the official release because something wasn't ready and they
>> don't want to officially support it. They will likely update the
>> macosforge page upon release.
>
> ZFS is not in the latest GM build from WWDC. I think that we can
> consider that it will not be a core OS component as part of the
> standard install. I don't have the Server version yet so I'll have to
> check that out.
>
>> There were significant updates to ZFS in some of the 10.6 betas
>> before it was pulled and I can't imagine throwing away all that work
>> when it's already done. Not to mention a decent amount of folks at
>> Apple are Unix geeks and are probably just as excited about ZFS as
>> the rest of us are.
>
> The work certainly won't be thrown away, but it won't be seen as an
> integrated component until 10.6.x or more likely 10.7.  It's possible
> that they will release it as a standalone package with a separate
> marketing message since generally point releases are not permitted to
> include important new functionality.
>
> I expect that the unsupported beta releases on macosforge will reopen
> after 10.6 hits in September.
>
> Cheers,
>
> Erik
> Note - I don't have any kind of inside information - just ideas and
> opinions based on dealing with Apple for a long time.
>
>>
>> This isn't that dissimilar to 10.5, If anyone recalls some early 10.5
>> betas had ZFS support and then it was pulled as we got close to the
>> release just to show up in it's current unoffically supported form
>> after release.
>>
>>  
>> On 6/12/09 1:20 PM, Mario Camou wrote:
>>> Linux supports AFP but the ZFS support is rather bad (via UserFS). I
>>> would recommend OpenSolaris and installing the Netatalk package (I'm
>>> not sure if it's available as a package or if you have to recompile
>>> it).
>>>
>>> -Mario.
>>>
>>> --
>>> I want to change the world but they won't give me the source code.
>>>
>>>
>>> On Fri, Jun 12, 2009 at 19:17, Dirk Schelfhout
>>> <dirkschelfhout at mac.com <mailto:dirkschelfhout at mac.com>> wrote:
>>>
>>>     Since probably no longer supported by apple, I would like to
>>>     move my 4 guid disks to a new machine running another os.
>>>
>>>     Any suggestions which os would be best ? Is there a *nix version
>>>     that runs afp ? Will they recognize the disks ?
>>>
>>>     Also what kind off machine would I need, just to run as a zfs
>>>     server for 2 to 3 clients. ( on a gigabit ethernet )
>>>     ( as cheap as possible consuming as little power as possible.
>>>     with the latter being more important to me ( and many disk bays ) )
>>>
>
> ------------------------------------------------------------------------
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>   


From s at avoidant.org  Sat Jun 13 11:39:56 2009
From: s at avoidant.org (sammy ominsky)
Date: Sat, 13 Jun 2009 21:39:56 +0300
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <F83052A9-6A2D-4FE7-B931-48E8124EEF77@mac.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<F83052A9-6A2D-4FE7-B931-48E8124EEF77@mac.com>
Message-ID: <DBEF4C60-418B-4416-A0A8-AA19E82BCA10@avoidant.org>

On 12/06/2009, at 20:42, Erik Ableson wrote:

> Assuming the disks are part of a zpool they should be readable by  
> OpenSolaris, but the usual Apple ZFS configuration adrdesses a  
> partition so you may have to tweak a little to get the zpool mounted.

Happily, no.  I have created ZFS pools on OS X and sent the disks to a  
friend running Solaris.  He had no problem whatsoever importing the  
pools.

One thing to be careful of, though, is to make sure the controller is  
supported in Solaris.  Te first attempt I made to send a disk to my  
Solaris friend failed because the USB chip in the external case wasn't  
recognized.

--sambo


From Jonathan.Edwards at Sun.COM  Sat Jun 13 15:29:24 2009
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Sat, 13 Jun 2009 18:29:24 -0400
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <DBEF4C60-418B-4416-A0A8-AA19E82BCA10@avoidant.org>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<F83052A9-6A2D-4FE7-B931-48E8124EEF77@mac.com>
	<DBEF4C60-418B-4416-A0A8-AA19E82BCA10@avoidant.org>
Message-ID: <81485A8B-140C-4DAC-BD13-DDCA53D7EC1D@sun.com>


On Jun 13, 2009, at 2:39 PM, sammy ominsky wrote:

> On 12/06/2009, at 20:42, Erik Ableson wrote:
>
>> Assuming the disks are part of a zpool they should be readable by  
>> OpenSolaris, but the usual Apple ZFS configuration adrdesses a  
>> partition so you may have to tweak a little to get the zpool mounted.
>
> Happily, no.  I have created ZFS pools on OS X and sent the disks to  
> a friend running Solaris.  He had no problem whatsoever importing  
> the pools.
>
> One thing to be careful of, though, is to make sure the controller  
> is supported in Solaris.  Te first attempt I made to send a disk to  
> my Solaris friend failed because the USB chip in the external case  
> wasn't recognized.

also make sure you followed the procedure on the macosforge page to  
format the disk as EFI/GPT .. they'll look a little funny to Solaris/ 
OpenSolaris (broken GPT implementation), but should read just fine ..  
if you somehow managed to build a zpool on the older pdisk layout -  
then you''ll potentially run into issues .. there is a work around,  
but it's a little messy

---
.je

From segfault_de at yahoo.de  Sun Jun 14 00:50:02 2009
From: segfault_de at yahoo.de (segfault_de at yahoo.de)
Date: Sun, 14 Jun 2009 07:50:02 +0000 (GMT)
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <4A33D6DA.5030701@phys.washington.edu>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<762437f0906121020t56074e11j19d256adb7df63d4@mail.gmail.com>
	<4A329567.1010502@loveturtle.net>
	<B56419DE-AC4A-4604-9D30-0669F36ADFDB@mac.com>
	<4A33D6DA.5030701@phys.washington.edu>
Message-ID: <806713.12326.qm@web24003.mail.ird.yahoo.com>



While I agree, that we should wait -- however, it's really bad for those of 
us who like to stay on top of things, so that we won't be left scrambling 
at the last minute. I believe Apple will continue to integrate ZFS into the 
future OS. The problem, however, is the immediate term at the release of 
10.6. Would it have read/write ZFS support? Would it still include readonly 
support for ZFS? Let's assume the worst -- 10.6 has *no* support for ZFS. 
With the amount of kernel changes, it would probably break macforge ZFS 
kernel extension -- so what are we left with? Either sticking with 10.5, or 
hope that someone will hack to make macforge ZFS work in 10.6, and hope that 
some of the Apple engineers return to continue contribute and release codes 
so those of us early adopters have something to work with.

Past that, we, as ZFS on Mac users have little recourse but to either 
consider the possibility of eventual abandonment of our beloved ZFS or 
abandon our beloved Mac OS X. Note to Apple: usually the FS wins.

L.

________________________________
Von: Jared Nance <nancejk at phys.washington.edu>
An: Erik Ableson <eableson at mac.com>
CC: ZFS on OSX mailing list mailing list <zfs-discuss at lists.macosforge.org>
Gesendet: Samstag, den 13. Juni 2009, 09:42:02 Uhr
Betreff: Re: [zfs-discuss] zfs alternative

Is it fair to assume that work will continue with the version on
macosforge?  My understanding is that the version that will (hopefully)
eventually be in core mac OS will be derived from the work that is
currently represented there.  If so, I suppose I can wait.

Jared N

Erik Ableson wrote:
> e 12 juin 2009 ? 19:50, Dillon Kass a ?crit :
>
>> You should wait until 10.6 is released. I find it very unlikely that
>> ZFS is really gone. I find it more likely that it has been pulled
>> from the official release because something wasn't ready and they
>> don't want to officially support it. They will likely update the
>> macosforge page upon release.
>
> ZFS is not in the latest GM build from WWDC. I think that we can
> consider that it will not be a core OS component as part of the
> standard install. I don't have the Server version yet so I'll have to
> check that out.
>
>> There were significant updates to ZFS in some of the 10.6 betas
>> before it was pulled and I can't imagine throwing away all that work
>> when it's already done. Not to mention a decent amount of folks at
>> Apple are Unix geeks and are probably just as excited about ZFS as
>> the rest of us are.
>
> The work certainly won't be thrown away, but it won't be seen as an
> integrated component until 10.6.x or more likely 10.7.  It's possible
> that they will release it as a standalone package with a separate
> marketing message since generally point releases are not permitted to
> include important new functionality.
>
> I expect that the unsupported beta releases on macosforge will reopen
> after 10.6 hits in September.
>
> Cheers,
>
> Erik
> Note - I don't have any kind of inside information - just ideas and
> opinions based on dealing with Apple for a long time.
>
>>
>> This isn't that dissimilar to 10.5, If anyone recalls some early 10.5
>> betas had ZFS support and then it was pulled as we got close to the
>> release just to show up in it's current unoffically supported form
>> after release.
>>
>>  
>> On 6/12/09 1:20 PM, Mario Camou wrote:
>>> Linux supports AFP but the ZFS support is rather bad (via UserFS). I
>>> would recommend OpenSolaris and installing the Netatalk package (I'm
>>> not sure if it's available as a package or if you have to recompile
>>> it).
>>>
>>> -Mario.
>>>
>>> --
>>> I want to change the world but they won't give me the source code.
>>>
>>>
>>> On Fri, Jun 12, 2009 at 19:17, Dirk Schelfhout
>>> <dirkschelfhout at mac.com <mailto:dirkschelfhout at mac.com>> wrote:
>>>
>>>     Since probably no longer supported by apple, I would like to
>>>     move my 4 guid disks to a new machine running another os.
>>>
>>>     Any suggestions which os would be best ? Is there a *nix version
>>>     that runs afp ? Will they recognize the disks ?
>>>
>>>     Also what kind off machine would I need, just to run as a zfs
>>>     server for 2 to 3 clients. ( on a gigabit ethernet )
>>>     ( as cheap as possible consuming as little power as possible.
>>>     with the latter being more important to me ( and many disk bays ) )
>>>
>
> ------------------------------------------------------------------------
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>  

_______________________________________________
zfs-discuss mailing list
zfs-discuss at lists.macosforge.org
http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss



      

From hendrik.beskow at googlemail.com  Sun Jun 14 01:55:18 2009
From: hendrik.beskow at googlemail.com (Hendrik Beskow)
Date: Sun, 14 Jun 2009 10:55:18 +0200
Subject: [zfs-discuss] zfs alternative
Message-ID: <AF45A9DB-D022-4A04-9989-FEC6637ED013@googlemail.com>

As far as i can tell, unless you have a fully supported machine (a via  
epia sn isn't supported it seams) the performance of OpenSolaris and/ 
or Nexenta is... worse than a usb2.0 external drive. The iSCSI  
implementation of OpenSolaris is also a mess, the latest Release has a  
bug that gives you around 10-15MB/s with either implementation they  
give you. via netatalk/nfs/samba i get Data by.. chunks it varies  
between 50KB/s and 30MB/s every Second. With Linux on that Box i get  
around 70MB/s, constantly.

From hendrik.beskow at googlemail.com  Sun Jun 14 01:57:18 2009
From: hendrik.beskow at googlemail.com (Hendrik Beskow)
Date: Sun, 14 Jun 2009 10:57:18 +0200
Subject: [zfs-discuss] zfs alternative
Message-ID: <E34D7C66-0C60-472F-9C09-1F9B5A0A5D60@googlemail.com>

I also wonder,
how do you reply to Threads with Apple Mail here? Or what do you use  
for Mailing lists?

From hendrik.beskow at googlemail.com  Sun Jun 14 02:19:23 2009
From: hendrik.beskow at googlemail.com (Hendrik Beskow)
Date: Sun, 14 Jun 2009 11:19:23 +0200
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <3185331E-3D5B-41CE-ABF8-96B81A6FD955@rehnmark.net>
References: <AF45A9DB-D022-4A04-9989-FEC6637ED013@googlemail.com>
	<3185331E-3D5B-41CE-ABF8-96B81A6FD955@rehnmark.net>
Message-ID: <E5E06885-FCB0-4ADE-8693-663662E8C1E4@googlemail.com>

I get that speed with this setup:

Raid-5 with 3x Seagate Baracuda ES.2 -> LVM -> Ext4

On 14.06.2009, at 11:16, Robert Rehnmark wrote:

>
> 14 jun 2009 kl. 10:55 skrev Hendrik Beskow:
>
>> As far as i can tell, unless you have a fully supported machine (a  
>> via epia sn isn't supported it seams) the performance of  
>> OpenSolaris and/or Nexenta is... worse than a usb2.0 external  
>> drive. The iSCSI implementation of OpenSolaris is also a mess, the  
>> latest Release has a bug that gives you around 10-15MB/s with  
>> either implementation they give you. via netatalk/nfs/samba i get  
>> Data by.. chunks it varies between 50KB/s and 30MB/s every Second.  
>> With Linux on that Box i get around 70MB/s, constantly.
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> When you say you get 70MB/s on Linux, are you talking about  
> ZFS_on_FUSE or any other filesytem?
> Getting data in chunks over the network seems to be a real problem  
> that nobody is talking much about, it was exactly the sam with ZFS  
> on OSX.
> I got 60-70MB/s with OSX and HFS+ but with ZFS it was a disaster.
> I'm running OpenSolaris 2009-06 on the server now and that is the  
> best solution I have tried so far.
> My Pool is version 14 since it's created on NexentaStor and I don't  
> know if it will be importable under any other OS except OpenSolaris  
> right now.
>
> /Robert

From Jonathan.Edwards at Sun.COM  Sun Jun 14 03:10:47 2009
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Sun, 14 Jun 2009 06:10:47 -0400
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <E5E06885-FCB0-4ADE-8693-663662E8C1E4@googlemail.com>
References: <AF45A9DB-D022-4A04-9989-FEC6637ED013@googlemail.com>
	<3185331E-3D5B-41CE-ABF8-96B81A6FD955@rehnmark.net>
	<E5E06885-FCB0-4ADE-8693-663662E8C1E4@googlemail.com>
Message-ID: <9E38E7CC-BBA5-4221-B8C2-F942BF4BCB34@sun.com>

the next question is obviously how are you testing? .. typically a  
test like:
time dd if=/dev/zero of=/mypool/testfile.out
may not go that well, or be that much of a real world example unless  
you typically write out 512B null blocks to a single file (at least  
set a higher blocksize, run more threads, and look at aggregate  
operations with zpool iostat 5 or zpool iostat -v 5) .. sun engineers  
also worked on a framework for testing, or constructing workloads and  
doing microbenchmarks called filebench .. there's still a fair amount  
of community work to do some cleanup on this, but for the most part  
you should be able to generate some decent workloads to simulate  
things like file serving, web requests, or some database loads -  
depending on what you'd like to do

what you might be seeing with the cyclical nature of the I/O  
throughput (if you're watching the disks) has to do with the txg  
pushes .. I/O aggregates in the ARC and attempts to push out more  
optimal full stripe writes at higher block sizes to take advantage of  
better backend utilization if you have it .. however, if you're  
finding better utilization on your backend storage with smaller  
blocksize writes - then you might want to tune the blocksize or  
volblocksize on the FS (or zvol)

also note that if you're doing a lot of sync() or dysnc() writes (eg:  
you do a lot of COMMIT operations on NFS) - you can run into  
situations where you might be doing incomplete txg pushes .. using a  
write optimized SSD for a slog device on later versions of zfs can  
help here.  similarly for read issues, read optimized SSDs used as  
L2ARC can also help tremendously .. you can see the zfs/zpool version  
descriptions here along with what build they were integrated in the  
sxce tree:
http://www.opensolaris.org/os/community/zfs/version/N/
http://www.opensolaris.org/os/community/zfs/version/zpl/N/
(macosforge build 119 is currently at zpool version 8, zfs version 2)


On Jun 14, 2009, at 5:19 AM, Hendrik Beskow wrote:

> I get that speed with this setup:
>
> Raid-5 with 3x Seagate Baracuda ES.2 -> LVM -> Ext4
>
> On 14.06.2009, at 11:16, Robert Rehnmark wrote:
>
>>
>> 14 jun 2009 kl. 10:55 skrev Hendrik Beskow:
>>
>>> As far as i can tell, unless you have a fully supported machine (a  
>>> via epia sn isn't supported it seams) the performance of  
>>> OpenSolaris and/or Nexenta is... worse than a usb2.0 external  
>>> drive. The iSCSI implementation of OpenSolaris is also a mess, the  
>>> latest Release has a bug that gives you around 10-15MB/s with  
>>> either implementation they give you. via netatalk/nfs/samba i get  
>>> Data by.. chunks it varies between 50KB/s and 30MB/s every Second.  
>>> With Linux on that Box i get around 70MB/s, constantly.
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> When you say you get 70MB/s on Linux, are you talking about  
>> ZFS_on_FUSE or any other filesytem?
>> Getting data in chunks over the network seems to be a real problem  
>> that nobody is talking much about, it was exactly the sam with ZFS  
>> on OSX.
>> I got 60-70MB/s with OSX and HFS+ but with ZFS it was a disaster.
>> I'm running OpenSolaris 2009-06 on the server now and that is the  
>> best solution I have tried so far.
>> My Pool is version 14 since it's created on NexentaStor and I don't  
>> know if it will be importable under any other OS except OpenSolaris  
>> right now.
>>
>> /Robert
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From nathan.stocks at gmail.com  Sun Jun 14 15:21:34 2009
From: nathan.stocks at gmail.com (Nathan)
Date: Sun, 14 Jun 2009 16:21:34 -0600
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <E34D7C66-0C60-472F-9C09-1F9B5A0A5D60@googlemail.com>
References: <E34D7C66-0C60-472F-9C09-1F9B5A0A5D60@googlemail.com>
Message-ID: <96c9d6a80906141521h27e84e1aibff432dbf50c66d9@mail.gmail.com>

On Sun, Jun 14, 2009 at 2:57 AM, Hendrik
Beskow<hendrik.beskow at googlemail.com> wrote:
> I also wonder,
> how do you reply to Threads with Apple Mail here? Or what do you use for
> Mailing lists?

In Mail.app, "View -> Organize by Thread"  -- though I personally use
a personal gmail account for all my mailing list correspondence,
specifically because of gmail's excellent handling of email threads.

~ Nathan

From dhoffmann at uwalumni.com  Sun Jun 14 22:07:41 2009
From: dhoffmann at uwalumni.com (Dominik Hoffmann)
Date: Mon, 15 Jun 2009 01:07:41 -0400
Subject: [zfs-discuss] How to recover from a FAULTED RAID-Z?
Message-ID: <D5223CF5-DBA2-4797-824C-2BBD2ACD47EB@uwalumni.com>

I built a three-drive RAID-Z, which was working beautifully until one  
of the drives died (WD Black Caviar, only two years old; had purchased  
a pair; the other one had died only about four months ago). Now when I  
issue the command

	zpool import -f

I get his output

	  pool: BackupRAID
	    id: 7691335677431272370
	 state: FAULTED
	status: One or more devices contains corrupted data.
	action: The pool cannot be imported due to damaged devices or data.
		The pool may be active on on another system, but can be imported using
		the '-f' flag.
	   see: http://www.sun.com/msg/ZFS-8000-5E
	config:

		BackupRAID   FAULTED  corrupted data
		  raidz1     DEGRADED
		    disk0s2  ONLINE
		    disk1s2  FAULTED  corrupted data
		    disk1s2  ONLINE

Sun's ZFS documentation leaves me little hope that I can restore the  
pool once I can replace the FAULTED drive with a new one. I'd love for  
anyone to show me that I'm wrong.

Dominik Hoffmann
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 2427 bytes
Desc: not available
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090615/edfb5d9a/attachment.bin>

From mattsnow at gmail.com  Sun Jun 14 23:02:24 2009
From: mattsnow at gmail.com (Matt Snow)
Date: Sun, 14 Jun 2009 23:02:24 -0700
Subject: [zfs-discuss] How to recover from a FAULTED RAID-Z?
In-Reply-To: <D5223CF5-DBA2-4797-824C-2BBD2ACD47EB@uwalumni.com>
References: <D5223CF5-DBA2-4797-824C-2BBD2ACD47EB@uwalumni.com>
Message-ID: <6879ebc80906142302m751e83b9mea3d375c14a27613@mail.gmail.com>

Try this.

zpool import -f BackupRAID

If you can get this to work, shut down, replace the disk(verify by serial
number in `system_profiler` output), boot the system up.

Then:

zpool replace BackupRAID disk1s2 disk1s2

I had a similar experience when 1 of 5 disks died in my RAIDZ.

..Matt


On Sun, Jun 14, 2009 at 10:07 PM, Dominik Hoffmann
<dhoffmann at uwalumni.com>wrote:

> I built a three-drive RAID-Z, which was working beautifully until one of
> the drives died (WD Black Caviar, only two years old; had purchased a pair;
> the other one had died only about four months ago). Now when I issue the
> command
>
>        zpool import -f
>
> I get his output
>
>          pool: BackupRAID
>            id: 7691335677431272370
>         state: FAULTED
>        status: One or more devices contains corrupted data.
>        action: The pool cannot be imported due to damaged devices or data.
>                The pool may be active on on another system, but can be
> imported using
>                the '-f' flag.
>           see: http://www.sun.com/msg/ZFS-8000-5E
>        config:
>
>                BackupRAID   FAULTED  corrupted data
>                  raidz1     DEGRADED
>                    disk0s2  ONLINE
>                    disk1s2  FAULTED  corrupted data
>                    disk1s2  ONLINE
>
> Sun's ZFS documentation leaves me little hope that I can restore the pool
> once I can replace the FAULTED drive with a new one. I'd love for anyone to
> show me that I'm wrong.
>
> Dominik Hoffmann
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090614/f4ce7d59/attachment.html>

From lee at ourstage.com  Tue Jun 16 08:29:03 2009
From: lee at ourstage.com (Lee Fyock)
Date: Tue, 16 Jun 2009 11:29:03 -0400
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <806713.12326.qm@web24003.mail.ird.yahoo.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<762437f0906121020t56074e11j19d256adb7df63d4@mail.gmail.com>
	<4A329567.1010502@loveturtle.net>
	<B56419DE-AC4A-4604-9D30-0669F36ADFDB@mac.com>
	<4A33D6DA.5030701@phys.washington.edu>
	<806713.12326.qm@web24003.mail.ird.yahoo.com>
Message-ID: <A6D459B5-68E6-4AFE-A7AC-66B34B1E6A01@ourstage.com>

"usually the FS wins"? Could you please provide examples?

Personally, I'm thinking of moving my 4-drive ZFS array over to a 4- 
drive Drobo. There, I've said it! :-)

I've been running the ZFS seeds since they first appeared in Leopard  
pre-releases, but if I'm going to be unable to upgrade to Snow Leopard  
because my movies and music are on an unsupported filesystem, the  
filesystem is what's going to get replaced, not my OS.

If anyone has any feedback, good or bad, on using a Drobo for non- 
critical mass storage, please email me (or post if you think it's  
appropriate). I've read good things and bad things, but having  
personally lost a terabyte of data to ZFS by not following the correct  
half of the two different instructions in the Mac ZFS readme... I have  
two coworkers who recently started using a 4-drive Drobo and an 8- 
drive Drobo, respectively, and they're quite happy so far.

Lee


On Jun 14, 2009, at 3:50 AM, segfault_de at yahoo.de wrote:

>
>
> While I agree, that we should wait -- however, it's really bad for  
> those of
> us who like to stay on top of things, so that we won't be left  
> scrambling
> at the last minute. I believe Apple will continue to integrate ZFS  
> into the
> future OS. The problem, however, is the immediate term at the  
> release of
> 10.6. Would it have read/write ZFS support? Would it still include  
> readonly
> support for ZFS? Let's assume the worst -- 10.6 has *no* support for  
> ZFS.
> With the amount of kernel changes, it would probably break macforge  
> ZFS
> kernel extension -- so what are we left with? Either sticking with  
> 10.5, or
> hope that someone will hack to make macforge ZFS work in 10.6, and  
> hope that
> some of the Apple engineers return to continue contribute and  
> release codes
> so those of us early adopters have something to work with.
>
> Past that, we, as ZFS on Mac users have little recourse but to either
> consider the possibility of eventual abandonment of our beloved ZFS or
> abandon our beloved Mac OS X. Note to Apple: usually the FS wins.
>
> L.


From james at themacplace.co.uk  Tue Jun 16 09:09:17 2009
From: james at themacplace.co.uk (James Relph)
Date: Tue, 16 Jun 2009 17:09:17 +0100
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <A6D459B5-68E6-4AFE-A7AC-66B34B1E6A01@ourstage.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<762437f0906121020t56074e11j19d256adb7df63d4@mail.gmail.com>
	<4A329567.1010502@loveturtle.net>
	<B56419DE-AC4A-4604-9D30-0669F36ADFDB@mac.com>
	<4A33D6DA.5030701@phys.washington.edu>
	<806713.12326.qm@web24003.mail.ird.yahoo.com>
	<A6D459B5-68E6-4AFE-A7AC-66B34B1E6A01@ourstage.com>
Message-ID: <DBAB739B-2519-43B1-A622-4A8DEB5B562A@themacplace.co.uk>

> If anyone has any feedback, good or bad, on using a Drobo for non- 
> critical mass storage, please email me (or post if you think it's  
> appropriate). I've read good things and bad things, but having  
> personally lost a terabyte of data to ZFS by not following the  
> correct half of the two different instructions in the Mac ZFS  
> readme... I have two coworkers who recently started using a 4-drive  
> Drobo and an 8-drive Drobo, respectively, and they're quite happy so  
> far.
>
> Lee

We've used them quite a lot for small-mid sized studios and they can  
occasionally go a bit nuts (we've had them repeatedly crash on large  
file transfers).  The 2nd gen ones (with firewire) seem a lot better  
and the big benefit is that they're very user-manageable in that you  
can give end-users some pretty straightforward instructions.  They're  
not particularly fast though.  FWIW we're actually looking at doing  
the opposite and moving to ZFS from a Drobo for some clients with  
10.6.  The current build from macosforge is pretty old, so even if  
Apple don't include ZFS officially with Snow Leopard Server I'm pretty  
positive that we'll get a new build at least which apart from anything  
else adds a year or more of development time onto the current  
publically available build.


James Relph
ACSA 10.5

www.themacplace.co.uk

From Jonathan.Edwards at Sun.COM  Tue Jun 16 09:20:35 2009
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Tue, 16 Jun 2009 12:20:35 -0400
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <A6D459B5-68E6-4AFE-A7AC-66B34B1E6A01@ourstage.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<762437f0906121020t56074e11j19d256adb7df63d4@mail.gmail.com>
	<4A329567.1010502@loveturtle.net>
	<B56419DE-AC4A-4604-9D30-0669F36ADFDB@mac.com>
	<4A33D6DA.5030701@phys.washington.edu>
	<806713.12326.qm@web24003.mail.ird.yahoo.com>
	<A6D459B5-68E6-4AFE-A7AC-66B34B1E6A01@ourstage.com>
Message-ID: <6DBA27AF-D203-4069-BE25-DD6604200556@sun.com>

actually - i'd say it differently .. it's the data that wins .. if it  
takes you too much time or pain to migrate data, or data services -  
then you're probably keeping your filesystem and storage in place and  
hence the FS wins

overall it's pretty trivial to setup an opensolaris box or appliance  
and import your disks/pools then share them back out over NFS or  
CIFS .. added advantage is the ability to run iSCSI targets and FC  
targets if you want off this new box to create more raw storage for  
whatever you'd like.

on a side note .. i've heard good things about Drobo .. but for my  
work - the main issues are:
1) only 4 or 8 drives
2) you're sinking a lot more money into the RAID controller
3) the RAID controller is also going to be of questionable integrity

the ZFS procedures and front-end need to be cleaned up a bit for the  
mac setup page, but the underlying code (now working on 4 years?) is  
much more stable .. particularly on Solaris/OpenSolaris

Jonathan

On Jun 16, 2009, at 11:29 AM, Lee Fyock wrote:

> "usually the FS wins"? Could you please provide examples?
>
> Personally, I'm thinking of moving my 4-drive ZFS array over to a 4- 
> drive Drobo. There, I've said it! :-)
>
> I've been running the ZFS seeds since they first appeared in Leopard  
> pre-releases, but if I'm going to be unable to upgrade to Snow  
> Leopard because my movies and music are on an unsupported  
> filesystem, the filesystem is what's going to get replaced, not my OS.
>
> If anyone has any feedback, good or bad, on using a Drobo for non- 
> critical mass storage, please email me (or post if you think it's  
> appropriate). I've read good things and bad things, but having  
> personally lost a terabyte of data to ZFS by not following the  
> correct half of the two different instructions in the Mac ZFS  
> readme... I have two coworkers who recently started using a 4-drive  
> Drobo and an 8-drive Drobo, respectively, and they're quite happy so  
> far.
>
> Lee
>
>
> On Jun 14, 2009, at 3:50 AM, segfault_de at yahoo.de wrote:
>
>>
>>
>> While I agree, that we should wait -- however, it's really bad for  
>> those of
>> us who like to stay on top of things, so that we won't be left  
>> scrambling
>> at the last minute. I believe Apple will continue to integrate ZFS  
>> into the
>> future OS. The problem, however, is the immediate term at the  
>> release of
>> 10.6. Would it have read/write ZFS support? Would it still include  
>> readonly
>> support for ZFS? Let's assume the worst -- 10.6 has *no* support  
>> for ZFS.
>> With the amount of kernel changes, it would probably break macforge  
>> ZFS
>> kernel extension -- so what are we left with? Either sticking with  
>> 10.5, or
>> hope that someone will hack to make macforge ZFS work in 10.6, and  
>> hope that
>> some of the Apple engineers return to continue contribute and  
>> release codes
>> so those of us early adopters have something to work with.
>>
>> Past that, we, as ZFS on Mac users have little recourse but to either
>> consider the possibility of eventual abandonment of our beloved ZFS  
>> or
>> abandon our beloved Mac OS X. Note to Apple: usually the FS wins.
>>
>> L.
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From toby at telegraphics.com.au  Tue Jun 16 13:45:17 2009
From: toby at telegraphics.com.au (Toby Thain)
Date: Tue, 16 Jun 2009 16:45:17 -0400
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <A6D459B5-68E6-4AFE-A7AC-66B34B1E6A01@ourstage.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<762437f0906121020t56074e11j19d256adb7df63d4@mail.gmail.com>
	<4A329567.1010502@loveturtle.net>
	<B56419DE-AC4A-4604-9D30-0669F36ADFDB@mac.com>
	<4A33D6DA.5030701@phys.washington.edu>
	<806713.12326.qm@web24003.mail.ird.yahoo.com>
	<A6D459B5-68E6-4AFE-A7AC-66B34B1E6A01@ourstage.com>
Message-ID: <9021E329-409A-4683-9409-6B63654DA5CA@telegraphics.com.au>


On 16-Jun-09, at 11:29 AM, Lee Fyock wrote:

> "usually the FS wins"? Could you please provide examples?

It would win if you need what ZFS offers - it's not exactly feature- 
interchangeable with any other filesystem. Personally I'd rather have  
checksumming and self-healing.

--Toby

>
> Personally, I'm thinking of moving my 4-drive ZFS array over to a 4- 
> drive Drobo. There, I've said it! :-)
>
> I've been running the ZFS seeds since they first appeared in  
> Leopard pre-releases, but if I'm going to be unable to upgrade to  
> Snow Leopard because my movies and music are on an unsupported  
> filesystem, the filesystem is what's going to get replaced, not my OS.
>
> If anyone has any feedback, good or bad, on using a Drobo for non- 
> critical mass storage, please email me (or post if you think it's  
> appropriate). I've read good things and bad things, but having  
> personally lost a terabyte of data to ZFS by not following the  
> correct half of the two different instructions in the Mac ZFS  
> readme... I have two coworkers who recently started using a 4-drive  
> Drobo and an 8-drive Drobo, respectively, and they're quite happy  
> so far.
>
> Lee
>
>
> On Jun 14, 2009, at 3:50 AM, segfault_de at yahoo.de wrote:
>
>>
>>
>> While I agree, that we should wait -- however, it's really bad for  
>> those of
>> us who like to stay on top of things, so that we won't be left  
>> scrambling
>> at the last minute. I believe Apple will continue to integrate ZFS  
>> into the
>> future OS. The problem, however, is the immediate term at the  
>> release of
>> 10.6. Would it have read/write ZFS support? Would it still include  
>> readonly
>> support for ZFS? Let's assume the worst -- 10.6 has *no* support  
>> for ZFS.
>> With the amount of kernel changes, it would probably break  
>> macforge ZFS
>> kernel extension -- so what are we left with? Either sticking with  
>> 10.5, or
>> hope that someone will hack to make macforge ZFS work in 10.6, and  
>> hope that
>> some of the Apple engineers return to continue contribute and  
>> release codes
>> so those of us early adopters have something to work with.
>>
>> Past that, we, as ZFS on Mac users have little recourse but to either
>> consider the possibility of eventual abandonment of our beloved  
>> ZFS or
>> abandon our beloved Mac OS X. Note to Apple: usually the FS wins.
>>
>> L.
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dirkschelfhout at mac.com  Thu Jun 18 14:20:56 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Thu, 18 Jun 2009 23:20:56 +0200
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <9021E329-409A-4683-9409-6B63654DA5CA@telegraphics.com.au>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<762437f0906121020t56074e11j19d256adb7df63d4@mail.gmail.com>
	<4A329567.1010502@loveturtle.net>
	<B56419DE-AC4A-4604-9D30-0669F36ADFDB@mac.com>
	<4A33D6DA.5030701@phys.washington.edu>
	<806713.12326.qm@web24003.mail.ird.yahoo.com>
	<A6D459B5-68E6-4AFE-A7AC-66B34B1E6A01@ourstage.com>
	<9021E329-409A-4683-9409-6B63654DA5CA@telegraphics.com.au>
Message-ID: <E660563A-7FE2-47F3-86BC-C447284B79A7@mac.com>

For now I have solaris running in virtualbox and spent half a day
getting 3 usb 1 Tb disks to show up in solaris. ( on a macpro )

For now I couldn't get netatalk compiled on solaris.
At the moment I have the zpool from solaris nfs mounted into osx,
and am running an rsync to copy the osx zpool to the solaris zpool.

too bad send recv doesn't work yet. but then would it work between  
different versions off zfs ?

the hardest part was finding this :
> VBoxManage internalcommands createrawvmdk -filename /Volumes/osx/ 
> Users/Shared/virtualbox/usbDisk1.vmdk -rawdisk /dev/disk6 -register

Not sure where I will be going with this from here, but at least I  
have a backup of my zpool now.

next test is to create a home directory on the nfs mounted zpool.

Dirk

From Jonathan.Edwards at Sun.COM  Thu Jun 18 20:22:35 2009
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Thu, 18 Jun 2009 23:22:35 -0400
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <E660563A-7FE2-47F3-86BC-C447284B79A7@mac.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<762437f0906121020t56074e11j19d256adb7df63d4@mail.gmail.com>
	<4A329567.1010502@loveturtle.net>
	<B56419DE-AC4A-4604-9D30-0669F36ADFDB@mac.com>
	<4A33D6DA.5030701@phys.washington.edu>
	<806713.12326.qm@web24003.mail.ird.yahoo.com>
	<A6D459B5-68E6-4AFE-A7AC-66B34B1E6A01@ourstage.com>
	<9021E329-409A-4683-9409-6B63654DA5CA@telegraphics.com.au>
	<E660563A-7FE2-47F3-86BC-C447284B79A7@mac.com>
Message-ID: <7D187FC5-3D12-48BE-AB85-B1544C7EBA35@sun.com>


On Jun 18, 2009, at 5:20 PM, Dirk Schelfhout wrote:

> For now I have solaris running in virtualbox and spent half a day
> getting 3 usb 1 Tb disks to show up in solaris. ( on a macpro )
>
> For now I couldn't get netatalk compiled on solaris.

take a look here for some hints on getting it to compile:
http://forums.sun.com/thread.jspa?threadID=5207965

(we really do need to take this on at as a project at some point)

> At the moment I have the zpool from solaris nfs mounted into osx,
> and am running an rsync to copy the osx zpool to the solaris zpool.
>
> too bad send recv doesn't work yet. but then would it work between  
> different versions off zfs ?

actually send/recv works to a file, so you can always set up a fifo as  
an intermediary .. (this was posted here a while back) - over the  
network it'd be something like:

# zfs snapshot mypool/fs1 at now
# mkfifo /var/tmp/zfs-fifo
# zfs send mypool/fs1 at now > /var/tmp/zfs-fifo &
# cat /var/tmp/zfs-fifo | ssh remote-host zfs recv mymac/newfs1

> the hardest part was finding this :
>> VBoxManage internalcommands createrawvmdk -filename /Volumes/osx/ 
>> Users/Shared/virtualbox/usbDisk1.vmdk -rawdisk /dev/disk6 -register

hehe .. actually the manual is pretty detailed if you read it, but yes  
it can be a pain to find the various command line components to do  
what you want ..

for USB on VBox, i typically setup a filter for the USB enclosure in  
the VM (when it's shut down) and then can typically see them in  
Solaris as root somewhere ..

> Not sure where I will be going with this from here, but at least I  
> have a backup of my zpool now.
>
> next test is to create a home directory on the nfs mounted zpool.

try using the automounter in leopard .. this works pretty well, you  
can typically navigate:
/net/<host>/<share>

or check the man page for auto_master for the syntax on modifying  
auto_home, or any other desired map files you might want to create


From alex.blewitt at gmail.com  Fri Jun 19 01:27:42 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Fri, 19 Jun 2009 09:27:42 +0100
Subject: [zfs-discuss] zfs alternative
In-Reply-To: <E660563A-7FE2-47F3-86BC-C447284B79A7@mac.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<762437f0906121020t56074e11j19d256adb7df63d4@mail.gmail.com>
	<4A329567.1010502@loveturtle.net>
	<B56419DE-AC4A-4604-9D30-0669F36ADFDB@mac.com>
	<4A33D6DA.5030701@phys.washington.edu>
	<806713.12326.qm@web24003.mail.ird.yahoo.com>
	<A6D459B5-68E6-4AFE-A7AC-66B34B1E6A01@ourstage.com>
	<9021E329-409A-4683-9409-6B63654DA5CA@telegraphics.com.au>
	<E660563A-7FE2-47F3-86BC-C447284B79A7@mac.com>
Message-ID: <57DED08B-9366-4800-8AB1-32250EB8558E@gmail.com>

Recv/send does work - you need to either dump to a file first or use  
mkfifo to get a virtual file. I've used this to do same system copies  
but it should be easy to do netcat or similar to punt it over a  
network connection too.

Why not, however, add the disk as a mirror and let zfs do the hard  
work? You should be able to export the solaris as a vdev I would have  
thought ...

Sent from my (new) iPhones

On 18 Jun 2009, at 22:20, Dirk Schelfhout <dirkschelfhout at mac.com>  
wrote:

> For now I have solaris running in virtualbox and spent half a day
> getting 3 usb 1 Tb disks to show up in solaris. ( on a macpro )
>
> For now I couldn't get netatalk compiled on solaris.
> At the moment I have the zpool from solaris nfs mounted into osx,
> and am running an rsync to copy the osx zpool to the solaris zpool.
>
> too bad send recv doesn't work yet. but then would it work between  
> different versions off zfs ?
>
> the hardest part was finding this :
>> VBoxManage internalcommands createrawvmdk -filename /Volumes/osx/ 
>> Users/Shared/virtualbox/usbDisk1.vmdk -rawdisk /dev/disk6 -register
>
> Not sure where I will be going with this from here, but at least I  
> have a backup of my zpool now.
>
> next test is to create a home directory on the nfs mounted zpool.
>
> Dirk
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From dirkschelfhout at mac.com  Fri Jun 19 06:49:38 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Fri, 19 Jun 2009 15:49:38 +0200
Subject: [zfs-discuss] Fwd:  zfs alternative
References: <3C6EA575-3EBF-4265-85DC-832A7F5C0409@mac.com>
Message-ID: <F34AC363-4CF4-486A-803C-09C869704F16@mac.com>

I forgot to copy the list for my problem, but fixed it.
might be useful for someone.

I had taken a snapshot yesterday after getting solaris up and running  
with my 3 raw usb disks attached to virtualbox.
After that It went wrong as you can see in the post below.
The fix is to create a new machine. ( or workaround ) As nothing else  
seemed to do the trick. ( like removing the snapshot , etc..... )
( I even started to edit xml files manually ( ouch ) )
And I won't be taking anymore snapshots :-)


Begin forwarded message:

> From: Dirk Schelfhout <dirkschelfhout at mac.com>
> Date: Fri 19 Jun 2009 13:41:13 GMT+02:00
> To: Dirk Schelfhout <dirkschelfhout at mac.com>
> Subject: Re: [zfs-discuss] zfs alternative
>
> It seems my setup isn't working.
> virtualbox does not seem to write to the disks but to files like  
> this : ( 3 on the top )
> ls -lt
> total 12277136
> -rw-------  1 schelfd  wheel   414646272 Jun 19 13:31  
> {5b88b289-88ef-4300-9d13-f0f3b5fd8e22}.vmdk
> -rw-------  1 schelfd  wheel   414515200 Jun 19 13:31  
> {bab7c742-542a-4195-8237-e328af9474e9}.vmdk
> -rw-------  1 schelfd  wheel   404422656 Jun 19 13:31  
> {1cc02d0c-6197-4e15-a830-209e9789b3af}.vmdk
> -rw-------+ 1 schelfd  wheel  5052285440 Jun 19 13:30  
> solarisEXpand.vdi
> -rw-r--r--@ 1 schelfd  wheel        6148 Jun 19 12:56 .DS_Store
> -rw-------  1 schelfd  wheel         631 Jun 18 18:15 usbDisk2.vmdk
> -rw-------  1 schelfd  wheel         631 Jun 18 18:15 usbDisk3.vmdk
> -rw-------  1 schelfd  wheel         631 Jun 18 18:15 usbDisk1.vmdk
>
> Whenever I create a file in the pool ( solaris in virtualbox ) the  
> usb disks spin up though.
> just tried an export to see if that flushed that data to the disks.  
> nope.
> The virtual media manager shows these 3 to be off type  
> Differencing(VMDK)
>
> Any suggestions ?
>
> Dirk
>
> On 18 Jun 2009, at 23:20, Dirk Schelfhout wrote:
>
>> For now I have solaris running in virtualbox and spent half a day
>> getting 3 usb 1 Tb disks to show up in solaris. ( on a macpro )
>>
>> For now I couldn't get netatalk compiled on solaris.
>> At the moment I have the zpool from solaris nfs mounted into osx,
>> and am running an rsync to copy the osx zpool to the solaris zpool.
>>
>> too bad send recv doesn't work yet. but then would it work between  
>> different versions off zfs ?
>>
>> the hardest part was finding this :
>>> VBoxManage internalcommands createrawvmdk -filename /Volumes/osx/ 
>>> Users/Shared/virtualbox/usbDisk1.vmdk -rawdisk /dev/disk6 -register
>>
>> Not sure where I will be going with this from here, but at least I  
>> have a backup of my zpool now.
>>
>> next test is to create a home directory on the nfs mounted zpool.
>>
>> Dirk
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090619/eed53568/attachment.html>

From nancejk at phys.washington.edu  Sun Jun 21 13:45:59 2009
From: nancejk at phys.washington.edu (Jared Nance)
Date: Sun, 21 Jun 2009 13:45:59 -0700
Subject: [zfs-discuss] strange behavior once pool is destroyed
Message-ID: <4A3E9C07.4040303@phys.washington.edu>

Hi All-
First of all, thanks in advance for any light you can shed on this
subject.  My ZFS installation on OSX is behaving very strangely after I
have destroyed a RAIDZ-1.  The setup is 3 external drives connected by
USB (used to be FW800, but a low quality cable was causing many issues),
RAIDZed.  I played around for a while, and eventually realized I could
use a clean sweep.  So, I destroyed the pool, formatted the drives, and
all of a sudden, ZFS refuses to re-create a pool of any kind of out
these drives.  Furthermore, a disk label that I applied as an HFS+
volume label seems to be quite persistent - no matter how many wipes I
do, I cannot change the name. 

Diskutil report:
/dev/disk0
   #:                       TYPE NAME                    SIZE      
IDENTIFIER
   0:      GUID_partition_scheme                        *111.8 Gi   disk0
   1:                        EFI                         200.0 Mi   disk0s1
   2:                  Apple_HFS Macintosh HD            111.5 Gi   disk0s2
/dev/disk1
   #:                       TYPE NAME                    SIZE      
IDENTIFIER
   0:      GUID_partition_scheme                        *596.2 Gi   disk1
   1:                        EFI                         200.0 Mi   disk1s1
   2:                 Apple_RAID                         595.9 Gi   disk1s2
   3:                 Apple_Boot Boot OSX                128.0 Mi   disk1s3
/dev/disk2
   #:                       TYPE NAME                    SIZE      
IDENTIFIER
   0:      GUID_partition_scheme                        *596.2 Gi   disk2
   1:                        EFI                         200.0 Mi   disk2s1
   2:                 Apple_RAID                         595.9 Gi   disk2s2
   3:                 Apple_Boot Boot OSX                128.0 Mi   disk2s3
/dev/disk3
   #:                       TYPE NAME                    SIZE      
IDENTIFIER
   0:                  Apple_HFS deepfreeze             *595.9 Gi   disk3
/dev/disk4
   #:                       TYPE NAME                    SIZE      
IDENTIFIER
   0:      GUID_partition_scheme                        *596.2 Gi   disk4
   1:                        EFI                         200.0 Mi   disk4s1
   2:                        ZFS d1                      595.9 Gi   disk4s2

If I try to create a simple pool of one drive:
heaviside-2% zpool create testpool /dev/disk4s2
cannot create 'testpool': one or more vdevs refer to the same device

Using any other drive results in the same issue.  Does anybody know
what's going on? 

Jared Nance

From hanche at math.ntnu.no  Sun Jun 21 14:28:29 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Sun, 21 Jun 2009 23:28:29 +0200 (CEST)
Subject: [zfs-discuss] strange behavior once pool is destroyed
In-Reply-To: <4A3E9C07.4040303@phys.washington.edu>
References: <4A3E9C07.4040303@phys.washington.edu>
Message-ID: <20090621.232829.20646236.hanche@math.ntnu.no>

+ Jared Nance <nancejk at phys.washington.edu>:

> heaviside-2% zpool create testpool /dev/disk4s2
> cannot create 'testpool': one or more vdevs refer to the same device
> 
> Using any other drive results in the same issue.  Does anybody know
> what's going on? 

I certainly don't, but I would try deleting /etc/zfs/zpool.cache to
see if that might help. Just make sure that any zpools are properly
exported first, and it might be a good idea to reboot afterwards.

And I haven't tried this on os x, but on freebsd a common trick used
to be to wipe the first few sectors of the disk before trying to set
up a new partitioning scheme:

  dd if=/dev/zero of=/dev/diskX bs=10240 count=16

which may be overdoing it, but anyway ... make sure you do it to the
right disk of course, and that it has no mounted filesystems on it.

- Harald

From nancejk at phys.washington.edu  Sun Jun 21 14:52:26 2009
From: nancejk at phys.washington.edu (Jared Nance)
Date: Sun, 21 Jun 2009 14:52:26 -0700
Subject: [zfs-discuss] strange behavior once pool is destroyed
In-Reply-To: <278BE663-A9B7-45A1-92D8-91902FF5E5B7@mac.com>
References: <4A3E9C07.4040303@phys.washington.edu>
	<278BE663-A9B7-45A1-92D8-91902FF5E5B7@mac.com>
Message-ID: <4A3EAB9A.8000809@phys.washington.edu>

Hi Dirk-
Yes, I think you are right - I just stumbled across a post some time ago
by one of the engineers who indicated that unlike Solaris, OSX ZFS does
-not- use zpool.cache to track label names... Hence my problem.  I am
currently zeroing the entire drive to avoid any issues.  I will let the
list know how this goes...

Jared N

Dirk Schelfhout wrote:
> I believe you have to use dd to wipe the drive at certain points.
> look for it in the archives of the list.
> Dirk
> On 21 Jun 2009, at 22:45, Jared Nance wrote:
>
>> Hi All-
>> First of all, thanks in advance for any light you can shed on this
>> subject.  My ZFS installation on OSX is behaving very strangely after I
>> have destroyed a RAIDZ-1.  The setup is 3 external drives connected by
>> USB (used to be FW800, but a low quality cable was causing many issues),
>> RAIDZed.  I played around for a while, and eventually realized I could
>> use a clean sweep.  So, I destroyed the pool, formatted the drives, and
>> all of a sudden, ZFS refuses to re-create a pool of any kind of out
>> these drives.  Furthermore, a disk label that I applied as an HFS+
>> volume label seems to be quite persistent - no matter how many wipes I
>> do, I cannot change the name.
>>
>> Diskutil report:
>> /dev/disk0
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:      GUID_partition_scheme                        *111.8 Gi   disk0
>>   1:                        EFI                         200.0 Mi  
>> disk0s1
>>   2:                  Apple_HFS Macintosh HD            111.5 Gi  
>> disk0s2
>> /dev/disk1
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:      GUID_partition_scheme                        *596.2 Gi   disk1
>>   1:                        EFI                         200.0 Mi  
>> disk1s1
>>   2:                 Apple_RAID                         595.9 Gi  
>> disk1s2
>>   3:                 Apple_Boot Boot OSX                128.0 Mi  
>> disk1s3
>> /dev/disk2
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:      GUID_partition_scheme                        *596.2 Gi   disk2
>>   1:                        EFI                         200.0 Mi  
>> disk2s1
>>   2:                 Apple_RAID                         595.9 Gi  
>> disk2s2
>>   3:                 Apple_Boot Boot OSX                128.0 Mi  
>> disk2s3
>> /dev/disk3
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:                  Apple_HFS deepfreeze             *595.9 Gi   disk3
>> /dev/disk4
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:      GUID_partition_scheme                        *596.2 Gi   disk4
>>   1:                        EFI                         200.0 Mi  
>> disk4s1
>>   2:                        ZFS d1                      595.9 Gi  
>> disk4s2
>>
>> If I try to create a simple pool of one drive:
>> heaviside-2% zpool create testpool /dev/disk4s2
>> cannot create 'testpool': one or more vdevs refer to the same device
>>
>> Using any other drive results in the same issue.  Does anybody know
>> what's going on?
>>
>> Jared Nance
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From Jonathan.Edwards at Sun.COM  Sun Jun 21 21:14:28 2009
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Mon, 22 Jun 2009 00:14:28 -0400
Subject: [zfs-discuss] strange behavior once pool is destroyed
In-Reply-To: <4A3EAB9A.8000809@phys.washington.edu>
References: <4A3E9C07.4040303@phys.washington.edu>
	<278BE663-A9B7-45A1-92D8-91902FF5E5B7@mac.com>
	<4A3EAB9A.8000809@phys.washington.edu>
Message-ID: <7469E858-51B1-4FB1-83CB-B56E88258D29@sun.com>

no need to zero the whole disk .. you can use gpt(8) to destroy the  
EFI label ..
then you probably want to relabel with the procedures on the  
macosforge page

it appears that on disk4 you had a pool named d1 still on it
(btw - you can still import destroyed pools with a zpool import -D  
<pool>)

now if you wanted to force creation - just use create -f <newpool>

On Jun 21, 2009, at 5:52 PM, Jared Nance wrote:

> Hi Dirk-
> Yes, I think you are right - I just stumbled across a post some time  
> ago
> by one of the engineers who indicated that unlike Solaris, OSX ZFS  
> does
> -not- use zpool.cache to track label names... Hence my problem.  I am
> currently zeroing the entire drive to avoid any issues.  I will let  
> the
> list know how this goes...
>
> Jared N
>
> Dirk Schelfhout wrote:
>> I believe you have to use dd to wipe the drive at certain points.
>> look for it in the archives of the list.
>> Dirk
>> On 21 Jun 2009, at 22:45, Jared Nance wrote:
>>
>>> Hi All-
>>> First of all, thanks in advance for any light you can shed on this
>>> subject.  My ZFS installation on OSX is behaving very strangely  
>>> after I
>>> have destroyed a RAIDZ-1.  The setup is 3 external drives  
>>> connected by
>>> USB (used to be FW800, but a low quality cable was causing many  
>>> issues),
>>> RAIDZed.  I played around for a while, and eventually realized I  
>>> could
>>> use a clean sweep.  So, I destroyed the pool, formatted the  
>>> drives, and
>>> all of a sudden, ZFS refuses to re-create a pool of any kind of out
>>> these drives.  Furthermore, a disk label that I applied as an HFS+
>>> volume label seems to be quite persistent - no matter how many  
>>> wipes I
>>> do, I cannot change the name.
>>>
>>> Diskutil report:
>>> /dev/disk0
>>>  #:                       TYPE NAME                    SIZE
>>> IDENTIFIER
>>>  0:      GUID_partition_scheme                        *111.8 Gi    
>>> disk0
>>>  1:                        EFI                         200.0 Mi
>>> disk0s1
>>>  2:                  Apple_HFS Macintosh HD            111.5 Gi
>>> disk0s2
>>> /dev/disk1
>>>  #:                       TYPE NAME                    SIZE
>>> IDENTIFIER
>>>  0:      GUID_partition_scheme                        *596.2 Gi    
>>> disk1
>>>  1:                        EFI                         200.0 Mi
>>> disk1s1
>>>  2:                 Apple_RAID                         595.9 Gi
>>> disk1s2
>>>  3:                 Apple_Boot Boot OSX                128.0 Mi
>>> disk1s3
>>> /dev/disk2
>>>  #:                       TYPE NAME                    SIZE
>>> IDENTIFIER
>>>  0:      GUID_partition_scheme                        *596.2 Gi    
>>> disk2
>>>  1:                        EFI                         200.0 Mi
>>> disk2s1
>>>  2:                 Apple_RAID                         595.9 Gi
>>> disk2s2
>>>  3:                 Apple_Boot Boot OSX                128.0 Mi
>>> disk2s3
>>> /dev/disk3
>>>  #:                       TYPE NAME                    SIZE
>>> IDENTIFIER
>>>  0:                  Apple_HFS deepfreeze             *595.9 Gi    
>>> disk3
>>> /dev/disk4
>>>  #:                       TYPE NAME                    SIZE
>>> IDENTIFIER
>>>  0:      GUID_partition_scheme                        *596.2 Gi    
>>> disk4
>>>  1:                        EFI                         200.0 Mi
>>> disk4s1
>>>  2:                        ZFS d1                      595.9 Gi
>>> disk4s2
>>>
>>> If I try to create a simple pool of one drive:
>>> heaviside-2% zpool create testpool /dev/disk4s2
>>> cannot create 'testpool': one or more vdevs refer to the same device
>>>
>>> Using any other drive results in the same issue.  Does anybody know
>>> what's going on?
>>>
>>> Jared Nance
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From nancejk at phys.washington.edu  Sun Jun 21 21:54:44 2009
From: nancejk at phys.washington.edu (Jared Nance)
Date: Sun, 21 Jun 2009 21:54:44 -0700
Subject: [zfs-discuss] strange behavior once pool is destroyed
In-Reply-To: <278BE663-A9B7-45A1-92D8-91902FF5E5B7@mac.com>
References: <4A3E9C07.4040303@phys.washington.edu>
	<278BE663-A9B7-45A1-92D8-91902FF5E5B7@mac.com>
Message-ID: <4A3F0E94.6000006@phys.washington.edu>

OK, I have fixed the issue.  For those unfamiliar with the GPT Format,
wikipedia has a nice article on it.  Basically there is a duplicate of
the GUID partition record at the end of the drive, which is used as a
backup.  Wiping the drive, particularly with %noformat% as an option,
will preserve this final record and ZFS will remember it.  The elegant
way to undo this is to hose the first and last 1MB or so of the drive
using dd.  The inelegant method (which I used) is to use diskutil to
format the drive in MBR format, and ask it to zero partition 1.  Since
the old GPT partition will sit on the new partition, it will get zeroed. 

Thanks for the help-
Jared N
> I believe you have to use dd to wipe the drive at certain points.
> look for it in the archives of the list.
> Dirk
> On 21 Jun 2009, at 22:45, Jared Nance wrote:
>
>> Hi All-
>> First of all, thanks in advance for any light you can shed on this
>> subject.  My ZFS installation on OSX is behaving very strangely after I
>> have destroyed a RAIDZ-1.  The setup is 3 external drives connected by
>> USB (used to be FW800, but a low quality cable was causing many issues),
>> RAIDZed.  I played around for a while, and eventually realized I could
>> use a clean sweep.  So, I destroyed the pool, formatted the drives, and
>> all of a sudden, ZFS refuses to re-create a pool of any kind of out
>> these drives.  Furthermore, a disk label that I applied as an HFS+
>> volume label seems to be quite persistent - no matter how many wipes I
>> do, I cannot change the name.
>>
>> Diskutil report:
>> /dev/disk0
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:      GUID_partition_scheme                        *111.8 Gi   disk0
>>   1:                        EFI                         200.0 Mi  
>> disk0s1
>>   2:                  Apple_HFS Macintosh HD            111.5 Gi  
>> disk0s2
>> /dev/disk1
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:      GUID_partition_scheme                        *596.2 Gi   disk1
>>   1:                        EFI                         200.0 Mi  
>> disk1s1
>>   2:                 Apple_RAID                         595.9 Gi  
>> disk1s2
>>   3:                 Apple_Boot Boot OSX                128.0 Mi  
>> disk1s3
>> /dev/disk2
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:      GUID_partition_scheme                        *596.2 Gi   disk2
>>   1:                        EFI                         200.0 Mi  
>> disk2s1
>>   2:                 Apple_RAID                         595.9 Gi  
>> disk2s2
>>   3:                 Apple_Boot Boot OSX                128.0 Mi  
>> disk2s3
>> /dev/disk3
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:                  Apple_HFS deepfreeze             *595.9 Gi   disk3
>> /dev/disk4
>>   #:                       TYPE NAME                    SIZE
>> IDENTIFIER
>>   0:      GUID_partition_scheme                        *596.2 Gi   disk4
>>   1:                        EFI                         200.0 Mi  
>> disk4s1
>>   2:                        ZFS d1                      595.9 Gi  
>> disk4s2
>>
>> If I try to create a simple pool of one drive:
>> heaviside-2% zpool create testpool /dev/disk4s2
>> cannot create 'testpool': one or more vdevs refer to the same device
>>
>> Using any other drive results in the same issue.  Does anybody know
>> what's going on?
>>
>> Jared Nance
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From nancejk at phys.washington.edu  Mon Jun 22 07:21:26 2009
From: nancejk at phys.washington.edu (Jared Nance)
Date: Mon, 22 Jun 2009 07:21:26 -0700
Subject: [zfs-discuss] strange behavior once pool is destroyed
In-Reply-To: <7469E858-51B1-4FB1-83CB-B56E88258D29@sun.com>
References: <4A3E9C07.4040303@phys.washington.edu>
	<278BE663-A9B7-45A1-92D8-91902FF5E5B7@mac.com>
	<4A3EAB9A.8000809@phys.washington.edu>
	<7469E858-51B1-4FB1-83CB-B56E88258D29@sun.com>
Message-ID: <4A3F9366.1050305@phys.washington.edu>

Hi Jonathan-
Thanks for the reply - very useful information for the future.  Of
course I usually learn the hard way! 

Yes, the pool d1 'exists', although what really happened was a format
with diskutil where I named the ZFS volume.  After doing that, I could
no longer create pools, but 'zpool status' would kernel panic my system. 

Thanks for all the help.
Jared N


Jonathan Edwards wrote:
> no need to zero the whole disk .. you can use gpt(8) to destroy the
> EFI label ..
> then you probably want to relabel with the procedures on the
> macosforge page
>
> it appears that on disk4 you had a pool named d1 still on it
> (btw - you can still import destroyed pools with a zpool import -D
> <pool>)
>
> now if you wanted to force creation - just use create -f <newpool>
>
> On Jun 21, 2009, at 5:52 PM, Jared Nance wrote:
>
>> Hi Dirk-
>> Yes, I think you are right - I just stumbled across a post some time ago
>> by one of the engineers who indicated that unlike Solaris, OSX ZFS does
>> -not- use zpool.cache to track label names... Hence my problem.  I am
>> currently zeroing the entire drive to avoid any issues.  I will let the
>> list know how this goes...
>>
>> Jared N
>>
>> Dirk Schelfhout wrote:
>>> I believe you have to use dd to wipe the drive at certain points.
>>> look for it in the archives of the list.
>>> Dirk
>>> On 21 Jun 2009, at 22:45, Jared Nance wrote:
>>>
>>>> Hi All-
>>>> First of all, thanks in advance for any light you can shed on this
>>>> subject.  My ZFS installation on OSX is behaving very strangely
>>>> after I
>>>> have destroyed a RAIDZ-1.  The setup is 3 external drives connected by
>>>> USB (used to be FW800, but a low quality cable was causing many
>>>> issues),
>>>> RAIDZed.  I played around for a while, and eventually realized I could
>>>> use a clean sweep.  So, I destroyed the pool, formatted the drives,
>>>> and
>>>> all of a sudden, ZFS refuses to re-create a pool of any kind of out
>>>> these drives.  Furthermore, a disk label that I applied as an HFS+
>>>> volume label seems to be quite persistent - no matter how many wipes I
>>>> do, I cannot change the name.
>>>>
>>>> Diskutil report:
>>>> /dev/disk0
>>>>  #:                       TYPE NAME                    SIZE
>>>> IDENTIFIER
>>>>  0:      GUID_partition_scheme                        *111.8 Gi  
>>>> disk0
>>>>  1:                        EFI                         200.0 Mi
>>>> disk0s1
>>>>  2:                  Apple_HFS Macintosh HD            111.5 Gi
>>>> disk0s2
>>>> /dev/disk1
>>>>  #:                       TYPE NAME                    SIZE
>>>> IDENTIFIER
>>>>  0:      GUID_partition_scheme                        *596.2 Gi  
>>>> disk1
>>>>  1:                        EFI                         200.0 Mi
>>>> disk1s1
>>>>  2:                 Apple_RAID                         595.9 Gi
>>>> disk1s2
>>>>  3:                 Apple_Boot Boot OSX                128.0 Mi
>>>> disk1s3
>>>> /dev/disk2
>>>>  #:                       TYPE NAME                    SIZE
>>>> IDENTIFIER
>>>>  0:      GUID_partition_scheme                        *596.2 Gi  
>>>> disk2
>>>>  1:                        EFI                         200.0 Mi
>>>> disk2s1
>>>>  2:                 Apple_RAID                         595.9 Gi
>>>> disk2s2
>>>>  3:                 Apple_Boot Boot OSX                128.0 Mi
>>>> disk2s3
>>>> /dev/disk3
>>>>  #:                       TYPE NAME                    SIZE
>>>> IDENTIFIER
>>>>  0:                  Apple_HFS deepfreeze             *595.9 Gi  
>>>> disk3
>>>> /dev/disk4
>>>>  #:                       TYPE NAME                    SIZE
>>>> IDENTIFIER
>>>>  0:      GUID_partition_scheme                        *596.2 Gi  
>>>> disk4
>>>>  1:                        EFI                         200.0 Mi
>>>> disk4s1
>>>>  2:                        ZFS d1                      595.9 Gi
>>>> disk4s2
>>>>
>>>> If I try to create a simple pool of one drive:
>>>> heaviside-2% zpool create testpool /dev/disk4s2
>>>> cannot create 'testpool': one or more vdevs refer to the same device
>>>>
>>>> Using any other drive results in the same issue.  Does anybody know
>>>> what's going on?
>>>>
>>>> Jared Nance
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From dirkschelfhout at mac.com  Mon Jun 22 08:26:14 2009
From: dirkschelfhout at mac.com (Dirk Schelfhout)
Date: Mon, 22 Jun 2009 17:26:14 +0200
Subject: [zfs-discuss] follow-up Re:  zfs alternative
In-Reply-To: <E660563A-7FE2-47F3-86BC-C447284B79A7@mac.com>
References: <CAF27478-7AE1-4489-83A4-A204E68A62E5@mac.com>
	<762437f0906121020t56074e11j19d256adb7df63d4@mail.gmail.com>
	<4A329567.1010502@loveturtle.net>
	<B56419DE-AC4A-4604-9D30-0669F36ADFDB@mac.com>
	<4A33D6DA.5030701@phys.washington.edu>
	<806713.12326.qm@web24003.mail.ird.yahoo.com>
	<A6D459B5-68E6-4AFE-A7AC-66B34B1E6A01@ourstage.com>
	<9021E329-409A-4683-9409-6B63654DA5CA@telegraphics.com.au>
	<E660563A-7FE2-47F3-86BC-C447284B79A7@mac.com>
Message-ID: <8EF296D0-951C-45EA-9733-37E48E404B79@mac.com>

This seems to be far from stable, so I am stopping this.
virtualbox has crashed my macpro the hard way. ( it sat overnight with  
no fan's running ) ( ouch )
nothing seems to be damaged on the macpro :-)
solaris crashes virtualbox now when I try to import the pool ( usb  
disks )
Had to reinstall solaris, as the crash somehow damaged the install.  
( usb driver ? )
I may try installing solaris on a mac mini later. ( anyone tried  
this ? )

Dirk
On 18 Jun 2009, at 23:20, Dirk Schelfhout wrote:

> For now I have solaris running in virtualbox and spent half a day
> getting 3 usb 1 Tb disks to show up in solaris. ( on a macpro )
>
> For now I couldn't get netatalk compiled on solaris.
> At the moment I have the zpool from solaris nfs mounted into osx,
> and am running an rsync to copy the osx zpool to the solaris zpool.
>
> too bad send recv doesn't work yet. but then would it work between  
> different versions off zfs ?
>
> the hardest part was finding this :
>> VBoxManage internalcommands createrawvmdk -filename /Volumes/osx/ 
>> Users/Shared/virtualbox/usbDisk1.vmdk -rawdisk /dev/disk6 -register
>
> Not sure where I will be going with this from here, but at least I  
> have a backup of my zpool now.
>
> next test is to create a home directory on the nfs mounted zpool.
>
> Dirk
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From the.atz at gmail.com  Fri Jun 26 04:48:52 2009
From: the.atz at gmail.com (Atze de Vries)
Date: Fri, 26 Jun 2009 13:48:52 +0200
Subject: [zfs-discuss] Removing ZFS USB device
In-Reply-To: <4A3F9366.1050305@phys.washington.edu>
References: <4A3E9C07.4040303@phys.washington.edu>
	<278BE663-A9B7-45A1-92D8-91902FF5E5B7@mac.com>
	<4A3EAB9A.8000809@phys.washington.edu>
	<7469E858-51B1-4FB1-83CB-B56E88258D29@sun.com>
	<4A3F9366.1050305@phys.washington.edu>
Message-ID: <55531A0A-BB0E-4FF4-B8E4-AB33341951D4@gmail.com>

Hello All,

I have a problem. My zfs volume ran out of diskspace. Removing files  
gives the (strange) error, 'No space left on device' . The only thing  
i could do is use my usbdisk to add some space to the zfs-pool. I've  
deleted some stuff and now have plenty of diskspace.
I want to remove my usbstick from the zpool.

Using:
zpool remove DataStore /dev/disk3 gives:
cannot remove /dev/disk3: only inactive hot spares can be removed

and using
zpool offline DataStore /dev/disk3 gives:
cannot offline /dev/disk3: no valid replicas


i also tried to export the pool, remove the usbstick from my Mac and  
then import the pool again, but this gives also an error.

Can anybody help?


Greetinx Atze

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090626/8fd4deeb/attachment.html>

From lists at loveturtle.net  Fri Jun 26 05:10:00 2009
From: lists at loveturtle.net (Dillon Kass)
Date: Fri, 26 Jun 2009 08:10:00 -0400
Subject: [zfs-discuss] Removing ZFS USB device
In-Reply-To: <55531A0A-BB0E-4FF4-B8E4-AB33341951D4@gmail.com>
References: <4A3E9C07.4040303@phys.washington.edu>	<278BE663-A9B7-45A1-92D8-91902FF5E5B7@mac.com>	<4A3EAB9A.8000809@phys.washington.edu>	<7469E858-51B1-4FB1-83CB-B56E88258D29@sun.com>	<4A3F9366.1050305@phys.washington.edu>
	<55531A0A-BB0E-4FF4-B8E4-AB33341951D4@gmail.com>
Message-ID: <4A44BA98.7090501@loveturtle.net>

There's nothing you can do except replace your usb disk a equal or 
greater sized partition or disk somewhere else. zpools can't be shrunk. 
you just learned the hard way not to do what you just did.

Atze de Vries wrote:
> Hello All,
>
> I have a problem. My zfs volume ran out of diskspace. Removing files 
> gives the (strange) error, 'No space left on device' . The only thing 
> i could do is use my usbdisk to add some space to the zfs-pool. I've 
> deleted some stuff and now have plenty of diskspace. 
> I want to remove my usbstick from the zpool.
>
> Using:
> zpool remove DataStore /dev/disk3 gives:
> cannot remove /dev/disk3: only inactive hot spares can be removed
>
> and using 
> zpool offline DataStore /dev/disk3 gives:
> cannot offline /dev/disk3: no valid replicas
>
>
> i also tried to export the pool, remove the usbstick from my Mac and 
> then import the pool again, but this gives also an error.
>
> Can anybody help?
>
>
> Greetinx Atze
>
> ------------------------------------------------------------------------
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>   


From hanche at math.ntnu.no  Fri Jun 26 05:12:18 2009
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Fri, 26 Jun 2009 14:12:18 +0200 (CEST)
Subject: [zfs-discuss] Removing ZFS USB device
In-Reply-To: <55531A0A-BB0E-4FF4-B8E4-AB33341951D4@gmail.com>
References: <7469E858-51B1-4FB1-83CB-B56E88258D29@sun.com>
	<4A3F9366.1050305@phys.washington.edu>
	<55531A0A-BB0E-4FF4-B8E4-AB33341951D4@gmail.com>
Message-ID: <20090626.141218.200880233.hanche@math.ntnu.no>

+ Atze de Vries <the.atz at gmail.com>:

> I have a problem. My zfs volume ran out of diskspace. Removing files
> gives the (strange) error, 'No space left on device' . The only
> thing i could do is use my usbdisk to add some space to the
> zfs-pool.

I am afraid I don't know how you can recover from this,
hopefully someone else can.

What I can do is explain what happened: Removing a file can cost extra
disk space because of snapshot magic: The disk block containing
directory information regarding the removed file will need to be
duplicated so you can keep the old copy in a snapshot, the new copy in
the live filesystem. Or something like that. Anyway, the common way to
regain control in a disk full situation is to destroy a snapshot.

- Harald

From the.atz at gmail.com  Fri Jun 26 05:15:52 2009
From: the.atz at gmail.com (Atze de Vries)
Date: Fri, 26 Jun 2009 14:15:52 +0200
Subject: [zfs-discuss] Removing ZFS USB device
In-Reply-To: <20090626.141218.200880233.hanche@math.ntnu.no>
References: <7469E858-51B1-4FB1-83CB-B56E88258D29@sun.com>
	<4A3F9366.1050305@phys.washington.edu>
	<55531A0A-BB0E-4FF4-B8E4-AB33341951D4@gmail.com>
	<20090626.141218.200880233.hanche@math.ntnu.no>
Message-ID: <60FEEB49-A565-48B4-AC65-3EB3F79E7EAE@gmail.com>


Op 26 jun 2009, om 14:12 heeft Harald Hanche-Olsen het volgende  
geschreven:

> + Atze de Vries <the.atz at gmail.com>:
>
>> I have a problem. My zfs volume ran out of diskspace. Removing files
>> gives the (strange) error, 'No space left on device' . The only
>> thing i could do is use my usbdisk to add some space to the
>> zfs-pool.
>
> I am afraid I don't know how you can recover from this,
> hopefully someone else can.
>
> What I can do is explain what happened: Removing a file can cost extra
> disk space because of snapshot magic: The disk block containing
> directory information regarding the removed file will need to be
> duplicated so you can keep the old copy in a snapshot, the new copy in
> the live filesystem. Or something like that. Anyway, the common way to
> regain control in a disk full situation is to destroy a snapshot.
>
> - Harald


mmm sound like a problem. I thought of somewhat a solution. On my OS  
drive, which still has many free space i can create a temponary drive  
using:
  mkfile 2048m /tmp/disk1
and add replace this with my usbdisk.
Is this going to work?


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090626/28939518/attachment.html>

From the.atz at gmail.com  Fri Jun 26 05:24:26 2009
From: the.atz at gmail.com (Atze de Vries)
Date: Fri, 26 Jun 2009 14:24:26 +0200
Subject: [zfs-discuss] Removing ZFS USB device
In-Reply-To: <20090626.141218.200880233.hanche@math.ntnu.no>
References: <7469E858-51B1-4FB1-83CB-B56E88258D29@sun.com>
	<4A3F9366.1050305@phys.washington.edu>
	<55531A0A-BB0E-4FF4-B8E4-AB33341951D4@gmail.com>
	<20090626.141218.200880233.hanche@math.ntnu.no>
Message-ID: <0B75AC5A-DE28-4EFB-B51D-1E9CDD424251@gmail.com>


Op 26 jun 2009, om 14:12 heeft Harald Hanche-Olsen het volgende  
geschreven:

> + Atze de Vries <the.atz at gmail.com>:
>
>> I have a problem. My zfs volume ran out of diskspace. Removing files
>> gives the (strange) error, 'No space left on device' . The only
>> thing i could do is use my usbdisk to add some space to the
>> zfs-pool.
>
> I am afraid I don't know how you can recover from this,
> hopefully someone else can.
>
> What I can do is explain what happened: Removing a file can cost extra
> disk space because of snapshot magic: The disk block containing
> directory information regarding the removed file will need to be
> duplicated so you can keep the old copy in a snapshot, the new copy in
> the live filesystem. Or something like that. Anyway, the common way to
> regain control in a disk full situation is to destroy a snapshot.
>
> - Harald

mmm sounds like a problem. I thought of somewhat a solution. On my OS  
drive, which still has many free space i can create a temponary drive  
using:
  mkfile 2048m /tmp/disk1
and add replace this with my usbdisk.
Is this going to work?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090626/9af8f222/attachment.html>

From lists at loveturtle.net  Fri Jun 26 05:52:13 2009
From: lists at loveturtle.net (Dillon Kass)
Date: Fri, 26 Jun 2009 08:52:13 -0400
Subject: [zfs-discuss] Removing ZFS USB device
In-Reply-To: <0B75AC5A-DE28-4EFB-B51D-1E9CDD424251@gmail.com>
References: <7469E858-51B1-4FB1-83CB-B56E88258D29@sun.com>	<4A3F9366.1050305@phys.washington.edu>	<55531A0A-BB0E-4FF4-B8E4-AB33341951D4@gmail.com>	<20090626.141218.200880233.hanche@math.ntnu.no>
	<0B75AC5A-DE28-4EFB-B51D-1E9CDD424251@gmail.com>
Message-ID: <4A44C47D.4030202@loveturtle.net>

Should work well enough to let you remove the usb disk.

Atze de Vries wrote:
>
> Op 26 jun 2009, om 14:12 heeft Harald Hanche-Olsen het volgende 
> geschreven:
>
>> + Atze de Vries <the.atz at gmail.com <mailto:the.atz at gmail.com>>:
>>
>>> I have a problem. My zfs volume ran out of diskspace. Removing files
>>> gives the (strange) error, 'No space left on device' . The only
>>> thing i could do is use my usbdisk to add some space to the
>>> zfs-pool.
>>
>> I am afraid I don't know how you can recover from this,
>> hopefully someone else can.
>>
>> What I can do is explain what happened: Removing a file can cost extra
>> disk space because of snapshot magic: The disk block containing
>> directory information regarding the removed file will need to be
>> duplicated so you can keep the old copy in a snapshot, the new copy in
>> the live filesystem. Or something like that. Anyway, the common way to
>> regain control in a disk full situation is to destroy a snapshot.
>>
>> - Harald
> mmm sounds like a problem. I thought of somewhat a solution. On my OS 
> drive, which still has many free space i can create a temponary drive 
> using:
> | mkfile 2048m /tmp/disk1 |
> and add replace this with my usbdisk.
> Is this going to work?
>
> ------------------------------------------------------------------------
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>   


From alex.blewitt at gmail.com  Fri Jun 26 06:00:12 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Fri, 26 Jun 2009 14:00:12 +0100
Subject: [zfs-discuss] Removing ZFS USB device
In-Reply-To: <55531A0A-BB0E-4FF4-B8E4-AB33341951D4@gmail.com>
References: <4A3E9C07.4040303@phys.washington.edu>
	<278BE663-A9B7-45A1-92D8-91902FF5E5B7@mac.com>
	<4A3EAB9A.8000809@phys.washington.edu>
	<7469E858-51B1-4FB1-83CB-B56E88258D29@sun.com>
	<4A3F9366.1050305@phys.washington.edu>
	<55531A0A-BB0E-4FF4-B8E4-AB33341951D4@gmail.com>
Message-ID: <19E1346C-F15A-4937-BFDB-373E07F3958C@gmail.com>

On 26 Jun 2009, at 12:48, Atze de Vries <the.atz at gmail.com> wrote:

> Hello All,
>
> I have a problem. My zfs volume ran out of diskspace. Removing files  
> gives the (strange) error, 'No space left on device' .

You probably had the space taken up with snapshots etc. Given that ZFS  
doesn't delete your file (but rather write a new entry saying it's not  
there any more) you can paradoxicly not have enough space to delete  
files.

Quotas can help and sometimes root can access areas normal users  
can't, but by then I suspect it wouldn't help with ZFS anyway.

> The only thing i could do is use my usbdisk to add some space to the  
> zfs-pool. I've deleted some stuff and now have plenty of diskspace.
> I want to remove my usbstick from the zpool.

Unfortunately what you have done is extended the size of the pool, and  
since it isn't mirrored (but rather striped) it's now a permanent  
part. Whilst it would be feasible for a hypothetical tool to exist to  
ensure that no snapshots are part of the pool on a device and thus be  
able to remove it, that tool doesn't exist.

> i also tried to export the pool, remove the usbstick from my Mac and  
> then import the pool again, but this gives also an error.

The reason you can't is because it is a necessary part of the pool.

> Can anybody help?

I doubt this has helped :-)

What you can do now is either

1) Use an external disk and zfs send/receive your snapshots to, re- 
create the DataStore pool and copy your data back with zfs send/receive
2) Add a mirror of the disk+usb on an external drive, then do a  
replace with that image
3) Live with the USB as part of your setup

Note that this isn't much different to if you had added another disk  
into a different RAID1 setup - adding is easy, removing is very  
difficult. ZFS might make it easy to add, but removing is tricky.  
However, zfs's send/receive make it much easier to rebuild an array  
when needed.

Alex

From alex.blewitt at gmail.com  Fri Jun 26 06:10:27 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Fri, 26 Jun 2009 14:10:27 +0100
Subject: [zfs-discuss] Removing ZFS USB device
In-Reply-To: <0B75AC5A-DE28-4EFB-B51D-1E9CDD424251@gmail.com>
References: <7469E858-51B1-4FB1-83CB-B56E88258D29@sun.com>
	<4A3F9366.1050305@phys.washington.edu>
	<55531A0A-BB0E-4FF4-B8E4-AB33341951D4@gmail.com>
	<20090626.141218.200880233.hanche@math.ntnu.no>
	<0B75AC5A-DE28-4EFB-B51D-1E9CDD424251@gmail.com>
Message-ID: <2A74123D-6593-4F7C-8D94-7C20D80B4729@gmail.com>



Sent from my (new) iPhone

On 26 Jun 2009, at 13:24, Atze de Vries <the.atz at gmail.com> wrote:

>
> Op 26 jun 2009, om 14:12 heeft Harald Hanche-Olsen het volgende  
> geschreven:
>
>> + Atze de Vries <the.atz at gmail.com>:
>>
>>> I have a problem. My zfs volume ran out of diskspace. Removing files
>>> gives the (strange) error, 'No space left on device' . The only
>>> thing i could do is use my usbdisk to add some space to the
>>> zfs-pool.
>>
>> I am afraid I don't know how you can recover from this,
>> hopefully someone else can.
>>
>> What I can do is explain what happened: Removing a file can cost  
>> extra
>> disk space because of snapshot magic: The disk block containing
>> directory information regarding the removed file will need to be
>> duplicated so you can keep the old copy in a snapshot, the new copy  
>> in
>> the live filesystem. Or something like that. Anyway, the common way  
>> to
>> regain control in a disk full situation is to destroy a snapshot.
>>
>> - Harald
>
> mmm sounds like a problem. I thought of somewhat a solution. On my  
> OS drive, which still has many free space i can create a temponary  
> drive using:
>  mkfile 2048m /tmp/disk1
> and add replace this with my usbdisk.
> Is this going to work?

If by "work" you mean "completely hose my system next reboot" then  
yes, it will work.

Mac OSX will clean /tmp on each reboot. So whilst you could replace  
the USB what would happen on reboot is that it would be deleted, and  
thus the pool would never mount again.

Alex
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090626/b8963483/attachment.html>

From richard.elling at gmail.com  Fri Jun 26 08:30:53 2009
From: richard.elling at gmail.com (Richard Elling)
Date: Fri, 26 Jun 2009 08:30:53 -0700
Subject: [zfs-discuss] Removing ZFS USB device
In-Reply-To: <19E1346C-F15A-4937-BFDB-373E07F3958C@gmail.com>
References: <4A3E9C07.4040303@phys.washington.edu>	<278BE663-A9B7-45A1-92D8-91902FF5E5B7@mac.com>	<4A3EAB9A.8000809@phys.washington.edu>	<7469E858-51B1-4FB1-83CB-B56E88258D29@sun.com>	<4A3F9366.1050305@phys.washington.edu>	<55531A0A-BB0E-4FF4-B8E4-AB33341951D4@gmail.com>
	<19E1346C-F15A-4937-BFDB-373E07F3958C@gmail.com>
Message-ID: <4A44E9AD.2030905@gmail.com>

Alex Blewitt wrote:
> On 26 Jun 2009, at 12:48, Atze de Vries <the.atz at gmail.com> wrote:
>
>> Hello All,
>>
>> I have a problem. My zfs volume ran out of diskspace. Removing files 
>> gives the (strange) error, 'No space left on device' .
>
> You probably had the space taken up with snapshots etc. Given that ZFS 
> doesn't delete your file (but rather write a new entry saying it's not 
> there any more) you can paradoxicly not have enough space to delete 
> files.
>

Modern ZFS reserves 32 MBytes or 1/64th of the pool size
for COW and ZIL, so I think this problem may be due to the
age of the available bits.
 -- richard


From lee at ourstage.com  Mon Jun 29 11:57:43 2009
From: lee at ourstage.com (Lee Fyock)
Date: Mon, 29 Jun 2009 14:57:43 -0400
Subject: [zfs-discuss] Dismantling an array?
Message-ID: <12E74928-D0F7-48A1-8323-78F42E9F3F59@ourstage.com>

Hi--

I plan on dismantling my zfs array and moving the data elsewhere,  
where "elsewhere" includes some of the same drives moved into a Drobo.

I have a pool containing two 1 TB drives mirrored, plus two 500 GB  
drives mirrored. Is it feasible to remove one of the drives from each  
mirror set and put those into the Drobo, then copy the data from the  
zfs array to the Drobo? (I have another 1 TB drive for the Drobo, so I  
should be OK space-wise.)

I'm guessing I'll want to wipe the drives I'm moving so that zfs won't  
just pick them up in their new location. After the move I'll want to  
destroy the pool, then initialize the remaining drives, correct?

I realize I'll be running in a dangerous non-redundant state during  
the move, but this data is replaceable.

Is there anything in particular to watch out for?

Thanks,
Lee


From alex.blewitt at gmail.com  Mon Jun 29 16:59:06 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Tue, 30 Jun 2009 00:59:06 +0100
Subject: [zfs-discuss] ZFS and kernel panics
Message-ID: <2A311451-8C6E-4D8D-898D-50145DB662AA@gmail.com>

I've noticed since 10.5.6 (or 10.5.7) I've been getting kernel panics  
more regularly than I used to before (i.e. not). The symptom is the  
same; a kernel panic which requires rebooting.

At midnight, I have a (potentially raceful) condition of a zpool scrub  
and a zfs snapshot.

@daily  /usr/sbin/zpool scrub Data
@daily  /usr/sbin/zfs snapshot -r Data at AutoD-`date +"\%F"`

Over the last week, my laptop  (when it's been on over midnight) has  
kernel paniced at exactly midnight. In addition, my Mac Mini (with its  
external disks making up the Data pool) has the same process. The Mac  
Mini tends to barf over midnight, though sometimes not until the early  
hours (presumably, when the ZFS scrub is finishing; it takes a lot  
longer on that box due to the size of data it has to process).

There are no problems with the pool itself; upon reboot, the disk is  
there and a subsequent scrub does not trigger the kernel panic. I can  
go days (on the Mini) without needing this, but the laptop seems to  
have triggered the kernel panic fairly regularly. Of course, crontab  
doesn't run when the machine is off, so I only see this over the  
midnight boundary (like tonight) when I happen to be up.

Has anyone else noticed a lack of stability since applying the 10.5.6  
or 10.5.7 patches? Before then, I never used to get a kernel panic and  
I've been using the same pool continuously since then.

Alex

From nancejk at phys.washington.edu  Mon Jun 29 18:01:17 2009
From: nancejk at phys.washington.edu (Jared Nance)
Date: Mon, 29 Jun 2009 18:01:17 -0700
Subject: [zfs-discuss] ZFS and kernel panics
In-Reply-To: <2A311451-8C6E-4D8D-898D-50145DB662AA@gmail.com>
References: <2A311451-8C6E-4D8D-898D-50145DB662AA@gmail.com>
Message-ID: <4A4963DD.2050404@phys.washington.edu>

Alex-
Indeed I have had almost exactly the same issue.  My setup is 3 external
hard drives in a ZRAID1 pool.  Everything is fine until late evening, at
which point I can no longer stat my zfs pool at all - zpool status
works, zfs list is fine, but when I try to run anything at FS level e.g.
ls, the system locks up completely.  Not a kernel panic, but becomes
totally unresponsive and requires a reboot.  I moved two of the drives
to a standard software RAID1, and kept one as a zpool, just for
experimental purposes - my thought was that perhaps it was some i/o
issue that was causing the problem.  However, even when the zpool
contains no data that the OS is accessing regularly, the lockup occurs. 
Very distressing. 

Jared N


Alex Blewitt wrote:
> I've noticed since 10.5.6 (or 10.5.7) I've been getting kernel panics
> more regularly than I used to before (i.e. not). The symptom is the
> same; a kernel panic which requires rebooting.
>
> At midnight, I have a (potentially raceful) condition of a zpool scrub
> and a zfs snapshot.
>
> @daily  /usr/sbin/zpool scrub Data
> @daily  /usr/sbin/zfs snapshot -r Data at AutoD-`date +"\%F"`
>
> Over the last week, my laptop  (when it's been on over midnight) has
> kernel paniced at exactly midnight. In addition, my Mac Mini (with its
> external disks making up the Data pool) has the same process. The Mac
> Mini tends to barf over midnight, though sometimes not until the early
> hours (presumably, when the ZFS scrub is finishing; it takes a lot
> longer on that box due to the size of data it has to process).
>
> There are no problems with the pool itself; upon reboot, the disk is
> there and a subsequent scrub does not trigger the kernel panic. I can
> go days (on the Mini) without needing this, but the laptop seems to
> have triggered the kernel panic fairly regularly. Of course, crontab
> doesn't run when the machine is off, so I only see this over the
> midnight boundary (like tonight) when I happen to be up.
>
> Has anyone else noticed a lack of stability since applying the 10.5.6
> or 10.5.7 patches? Before then, I never used to get a kernel panic and
> I've been using the same pool continuously since then.
>
> Alex
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From mike666 at mac.com  Tue Jun 30 14:02:20 2009
From: mike666 at mac.com (Mike Prather)
Date: Tue, 30 Jun 2009 14:02:20 -0700
Subject: [zfs-discuss] zfs-discuss Digest, Vol 18, Issue 22
In-Reply-To: <mailman.27.1246370403.89898.zfs-discuss@lists.macosforge.org>
References: <mailman.27.1246370403.89898.zfs-discuss@lists.macosforge.org>
Message-ID: <A9C90B7E-AE78-4490-BEE2-E5F0FC835B67@mac.com>

Do either of you use Apple Remote Desktop admin?  If so make sure any  
automatic report generation options for your zfs machines are turned  
off.  I was getting the same thing after my initial zfs setup and this  
was the culprit.

Mike

On Jun 30, 2009, at 7:00 AM, zfs-discuss-request at lists.macosforge.org  
wrote:

> Alex-
> Indeed I have had almost exactly the same issue.  My setup is 3  
> external
> hard drives in a ZRAID1 pool.  Everything is fine until late  
> evening, at
> which point I can no longer stat my zfs pool at all - zpool status
> works, zfs list is fine, but when I try to run anything at FS level  
> e.g.
> ls, the system locks up completely.  Not a kernel panic, but becomes
> totally unresponsive and requires a reboot.  I moved two of the drives
> to a standard software RAID1, and kept one as a zpool, just for
> experimental purposes - my thought was that perhaps it was some i/o
> issue that was causing the problem.  However, even when the zpool
> contains no data that the OS is accessing regularly, the lockup  
> occurs.
> Very distressing.
>
> Jared N
>
>
> Alex Blewitt wrote:
>> I've noticed since 10.5.6 (or 10.5.7) I've been getting kernel panics
>> more regularly than I used to before (i.e. not). The symptom is the
>> same; a kernel panic which requires rebooting.
>>
>> At midnight, I have a (potentially raceful) condition of a zpool  
>> scrub
>> and a zfs snapshot.
>>
>> @daily  /usr/sbin/zpool scrub Data
>> @daily  /usr/sbin/zfs snapshot -r Data at AutoD-`date +"\%F"`
>>
>> Over the last week, my laptop  (when it's been on over midnight) has
>> kernel paniced at exactly midnight. In addition, my Mac Mini (with  
>> its
>> external disks making up the Data pool) has the same process. The Mac
>> Mini tends to barf over midnight, though sometimes not until the  
>> early
>> hours (presumably, when the ZFS scrub is finishing; it takes a lot
>> longer on that box due to the size of data it has to process).
>>
>> There are no problems with the pool itself; upon reboot, the disk is
>> there and a subsequent scrub does not trigger the kernel panic. I can
>> go days (on the Mini) without needing this, but the laptop seems to
>> have triggered the kernel panic fairly regularly. Of course, crontab
>> doesn't run when the machine is off, so I only see this over the
>> midnight boundary (like tonight) when I happen to be up.
>>
>> Has anyone else noticed a lack of stability since applying the 10.5.6
>> or 10.5.7 patches? Before then, I never used to get a kernel panic  
>> and
>> I've been using the same pool continuously since then.
>>
>> Alex
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
>
> ------------------------------
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090630/56df8d91/attachment.html>

From alex.blewitt at gmail.com  Tue Jun 30 15:01:06 2009
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Tue, 30 Jun 2009 23:01:06 +0100
Subject: [zfs-discuss] zfs-discuss Digest, Vol 18, Issue 22
In-Reply-To: <A9C90B7E-AE78-4490-BEE2-E5F0FC835B67@mac.com>
References: <mailman.27.1246370403.89898.zfs-discuss@lists.macosforge.org>
	<A9C90B7E-AE78-4490-BEE2-E5F0FC835B67@mac.com>
Message-ID: <46B09628-4795-4BC5-AEBD-1565046371C1@gmail.com>

I don't, and I've been using ZFS for a long time. The issues have just  
started along the last couple of point releases of OSX.

Alex

Sent from my (new) iPhone

On 30 Jun 2009, at 22:02, Mike Prather <mike666 at mac.com> wrote:

> Do either of you use Apple Remote Desktop admin?  If so make sure  
> any automatic report generation options for your zfs machines are  
> turned off.  I was getting the same thing after my initial zfs setup  
> and this was the culprit.
>
> Mike
>
> On Jun 30, 2009, at 7:00 AM, zfs-discuss- 
> request at lists.macosforge.org wrote:
>
>> Alex-
>> Indeed I have had almost exactly the same issue.  My setup is 3  
>> external
>> hard drives in a ZRAID1 pool.  Everything is fine until late  
>> evening, at
>> which point I can no longer stat my zfs pool at all - zpool status
>> works, zfs list is fine, but when I try to run anything at FS level  
>> e.g.
>> ls, the system locks up completely.  Not a kernel panic, but becomes
>> totally unresponsive and requires a reboot.  I moved two of the  
>> drives
>> to a standard software RAID1, and kept one as a zpool, just for
>> experimental purposes - my thought was that perhaps it was some i/o
>> issue that was causing the problem.  However, even when the zpool
>> contains no data that the OS is accessing regularly, the lockup  
>> occurs.
>> Very distressing.
>>
>> Jared N
>>
>>
>> Alex Blewitt wrote:
>>> I've noticed since 10.5.6 (or 10.5.7) I've been getting kernel  
>>> panics
>>> more regularly than I used to before (i.e. not). The symptom is the
>>> same; a kernel panic which requires rebooting.
>>>
>>> At midnight, I have a (potentially raceful) condition of a zpool  
>>> scrub
>>> and a zfs snapshot.
>>>
>>> @daily  /usr/sbin/zpool scrub Data
>>> @daily  /usr/sbin/zfs snapshot -r Data at AutoD-`date +"\%F"`
>>>
>>> Over the last week, my laptop  (when it's been on over midnight) has
>>> kernel paniced at exactly midnight. In addition, my Mac Mini (with  
>>> its
>>> external disks making up the Data pool) has the same process. The  
>>> Mac
>>> Mini tends to barf over midnight, though sometimes not until the  
>>> early
>>> hours (presumably, when the ZFS scrub is finishing; it takes a lot
>>> longer on that box due to the size of data it has to process).
>>>
>>> There are no problems with the pool itself; upon reboot, the disk is
>>> there and a subsequent scrub does not trigger the kernel panic. I  
>>> can
>>> go days (on the Mini) without needing this, but the laptop seems to
>>> have triggered the kernel panic fairly regularly. Of course, crontab
>>> doesn't run when the machine is off, so I only see this over the
>>> midnight boundary (like tonight) when I happen to be up.
>>>
>>> Has anyone else noticed a lack of stability since applying the  
>>> 10.5.6
>>> or 10.5.7 patches? Before then, I never used to get a kernel panic  
>>> and
>>> I've been using the same pool continuously since then.
>>>
>>> Alex
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>>
>>
>> ------------------------------
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090630/bdfb2e09/attachment.html>

From nancejk at phys.washington.edu  Tue Jun 30 15:02:00 2009
From: nancejk at phys.washington.edu (Jared Nance)
Date: Tue, 30 Jun 2009 15:02:00 -0700
Subject: [zfs-discuss] zfs-discuss Digest, Vol 18, Issue 22
In-Reply-To: <46B09628-4795-4BC5-AEBD-1565046371C1@gmail.com>
References: <mailman.27.1246370403.89898.zfs-discuss@lists.macosforge.org>	<A9C90B7E-AE78-4490-BEE2-E5F0FC835B67@mac.com>
	<46B09628-4795-4BC5-AEBD-1565046371C1@gmail.com>
Message-ID: <4A4A8B58.3000706@phys.washington.edu>

Right, neither do I.  Things were stable until the last release for me.

JN

Alex Blewitt wrote:
> I don't, and I've been using ZFS for a long time. The issues have just
> started along the last couple of point releases of OSX. 
>
> Alex
>
> Sent from my (new) iPhone
>
> On 30 Jun 2009, at 22:02, Mike Prather <mike666 at mac.com
> <mailto:mike666 at mac.com>> wrote:
>
>> Do either of you use Apple Remote Desktop admin?  If so make sure any
>> automatic report generation options for your zfs machines are turned
>> off.  I was getting the same thing after my initial zfs setup and
>> this was the culprit.
>>
>> Mike
>>
>> On Jun 30, 2009, at 7:00 AM, zfs-discuss-request at lists.macosforge.org
>> <mailto:zfs-discuss-request at lists.macosforge.org> wrote:
>>
>>> Alex-
>>> Indeed I have had almost exactly the same issue.  My setup is 3 external
>>> hard drives in a ZRAID1 pool.  Everything is fine until late evening, at
>>> which point I can no longer stat my zfs pool at all - zpool status
>>> works, zfs list is fine, but when I try to run anything at FS level e.g.
>>> ls, the system locks up completely.  Not a kernel panic, but becomes
>>> totally unresponsive and requires a reboot.  I moved two of the drives
>>> to a standard software RAID1, and kept one as a zpool, just for
>>> experimental purposes - my thought was that perhaps it was some i/o
>>> issue that was causing the problem.  However, even when the zpool
>>> contains no data that the OS is accessing regularly, the lockup occurs. 
>>> Very distressing. 
>>>
>>> Jared N
>>>
>>>
>>> Alex Blewitt wrote:
>>>> I've noticed since 10.5.6 (or 10.5.7) I've been getting kernel panics
>>>> more regularly than I used to before (i.e. not). The symptom is the
>>>> same; a kernel panic which requires rebooting.
>>>>
>>>> At midnight, I have a (potentially raceful) condition of a zpool scrub
>>>> and a zfs snapshot.
>>>>
>>>> @daily  /usr/sbin/zpool scrub Data
>>>> @daily  /usr/sbin/zfs snapshot -r Data at AutoD-`date +"\%F"`
>>>>
>>>> Over the last week, my laptop  (when it's been on over midnight) has
>>>> kernel paniced at exactly midnight. In addition, my Mac Mini (with its
>>>> external disks making up the Data pool) has the same process. The Mac
>>>> Mini tends to barf over midnight, though sometimes not until the early
>>>> hours (presumably, when the ZFS scrub is finishing; it takes a lot
>>>> longer on that box due to the size of data it has to process).
>>>>
>>>> There are no problems with the pool itself; upon reboot, the disk is
>>>> there and a subsequent scrub does not trigger the kernel panic. I can
>>>> go days (on the Mini) without needing this, but the laptop seems to
>>>> have triggered the kernel panic fairly regularly. Of course, crontab
>>>> doesn't run when the machine is off, so I only see this over the
>>>> midnight boundary (like tonight) when I happen to be up.
>>>>
>>>> Has anyone else noticed a lack of stability since applying the 10.5.6
>>>> or 10.5.7 patches? Before then, I never used to get a kernel panic and
>>>> I've been using the same pool continuously since then.
>>>>
>>>> Alex
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> <mailto:zfs-discuss at lists.macosforge.org>
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>>
>>>
>>> ------------------------------
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> <mailto:zfs-discuss at lists.macosforge.org>
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> <mailto:zfs-discuss at lists.macosforge.org>
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> ------------------------------------------------------------------------
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>   


