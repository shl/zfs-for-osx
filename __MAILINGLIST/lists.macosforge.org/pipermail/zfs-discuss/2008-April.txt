From franzschmalzl at spamfreemail.de  Tue Apr  1 12:21:31 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Tue Apr  1 12:20:40 2008
Subject: [zfs-discuss] backup
Message-ID: <54E5DB6F-78C7-470F-93DC-8BB1BD2B18D9@spamfreemail.de>

hi!

i wast just wondering if zfs on mac osx supports resource forks and  
acl's ?

i wanted to use my external raidz as backup volume, and make sure  
*everything* is backed up when running my backup script.

regards

franz 
From alex.blewitt at gmail.com  Tue Apr  1 13:22:57 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Tue Apr  1 13:21:52 2008
Subject: [zfs-discuss] backup
In-Reply-To: <54E5DB6F-78C7-470F-93DC-8BB1BD2B18D9@spamfreemail.de>
References: <54E5DB6F-78C7-470F-93DC-8BB1BD2B18D9@spamfreemail.de>
Message-ID: <636fd28e0804011322t64fc5fe8x59646283ec9644e1@mail.gmail.com>

On Tue, Apr 1, 2008 at 8:21 PM, Franz Schmalzl
<franzschmalzl@spamfreemail.de> wrote:
> hi!
>
>  i wast just wondering if zfs on mac osx supports resource forks and
>  acl's ?
>
>  i wanted to use my external raidz as backup volume, and make sure
>  *everything* is backed up when running my backup script.

I'd do some testing to verify it does what you expect. I've played
around with ZFS using 'xattr' to set/get attributes on the filing
system, and they seem to do the right thing. Not used ACLs though.

If you export to a different machine via AFS, I'd imagine that the
client would also be able to use them. It doesn't work over NFS
though.

Alex
From franzschmalzl at spamfreemail.de  Tue Apr  1 13:36:20 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Tue Apr  1 13:35:28 2008
Subject: [zfs-discuss] backup
In-Reply-To: <636fd28e0804011322t64fc5fe8x59646283ec9644e1@mail.gmail.com>
References: <54E5DB6F-78C7-470F-93DC-8BB1BD2B18D9@spamfreemail.de>
	<636fd28e0804011322t64fc5fe8x59646283ec9644e1@mail.gmail.com>
Message-ID: <49CCE637-346D-4C7F-A646-3407E55D655F@spamfreemail.de>

very well

thank you...

i'm going to run a backup now and see if everything works...

i'll keep you posted


regards

franz




On 01.04.2008, at 22:22, Alex Blewitt wrote:

> On Tue, Apr 1, 2008 at 8:21 PM, Franz Schmalzl
> <franzschmalzl@spamfreemail.de> wrote:
>> hi!
>>
>> i wast just wondering if zfs on mac osx supports resource forks and
>> acl's ?
>>
>> i wanted to use my external raidz as backup volume, and make sure
>> *everything* is backed up when running my backup script.
>
> I'd do some testing to verify it does what you expect. I've played
> around with ZFS using 'xattr' to set/get attributes on the filing
> system, and they seem to do the right thing. Not used ACLs though.
>
> If you export to a different machine via AFS, I'd imagine that the
> client would also be able to use them. It doesn't work over NFS
> though.
>
> Alex

From james-zfsosx at jrv.org  Tue Apr  1 18:55:19 2008
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Tue Apr  1 18:54:53 2008
Subject: [zfs-discuss] RAIDZ component UNAVAIL, but actually ok?
Message-ID: <47F2E787.3010502@jrv.org>

I have seen a number of cases with RAIDZs in pools on removable devices 
where the RAIDZ comes up degraded due to drive UNAVAIL, yet there 
appears to be nothing wrong with the drive and no difficulty accessing 
it with "dd if=/dev/disk12s2 of=/dev/null size=65536".

The test case is a 15" Intel MacBook Pro with a Sans Digital TR4U 
4-drive external USB enclosure with 4 Seagate SATA 750GB drives.  The 
pool "d01" was created on this Mac on disk{2,3,4,5}s2 IIRC.  I created a 
VMware virtual machine with FreeBSD 7 to test and have moved the 
enclosure back and forth between the VM and OSX to test.  It seems to 
work fine under Freebsd but comes up DEGRADED under OSX.  I can't detect 
any problem with the UNAVAIL disk below.  Note that name "da4p2" is the 
name the device had under FreeBSD and diskutil shows it as disk12s2 
under OSX.

Note that another pool "v01" was created after "d01".  I don't know if 
its existence plays a role.

Is it possible that when ZFS is loading the RAIDZ it first tries to 
access the components by the name in the on-disk pool metadata, and when 
that fails ZFS brings up the RAIDZ anyway since it is possible to do so, 
albeit in a degraded state, on the last RAIDZ component?

bash-3.2# zpool status
  pool: d01
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas 
exist for
    the pool to continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://www.sun.com/msg/ZFS-8000-2Q
 scrub: resilver completed with 0 errors on Tue Apr  1 20:02:31 2008
config:

    NAME          STATE     READ WRITE CKSUM
    d01           DEGRADED     0     0     0
      raidz1      DEGRADED     0     0     0
        disk8s2   ONLINE       0     0     0
        disk10s2  ONLINE       0     0     0
        disk11s2  ONLINE       0     0     0
        da4p2     UNAVAIL      0     0     0  cannot open

errors: No known data errors

  pool: v01
 state: ONLINE
status: The pool is formatted using an older on-disk format.  The pool can
    still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
    pool will no longer be accessible on older software versions.
 scrub: none requested
config:

    NAME         STATE     READ WRITE CKSUM
    v01          ONLINE       0     0     0
      raidz1     ONLINE       0     0     0
        disk2s2  ONLINE       0     0     0
        disk4s2  ONLINE       0     0     0
        disk5s2  ONLINE       0     0     0
        disk6s2  ONLINE       0     0     0

errors: No known data errors
bash-3.2#


/dev/disk8
   #:                       TYPE NAME                    SIZE       
IDENTIFIER
   0:      GUID_partition_scheme                        *698.6 Gi   disk8
   1:                        EFI                         200.0 Mi   disk8s1
   2:                        ZFS d01                     698.3 Gi   disk8s2
/dev/disk9
...
/dev/disk10
   #:                       TYPE NAME                    SIZE       
IDENTIFIER
   0:      GUID_partition_scheme                        *698.6 Gi   disk10
   1:                        EFI                         200.0 Mi   disk10s1
   2:                        ZFS d01                     698.3 Gi   disk10s2
/dev/disk11
   #:                       TYPE NAME                    SIZE       
IDENTIFIER
   0:      GUID_partition_scheme                        *698.6 Gi   disk11
   1:                        EFI                         200.0 Mi   disk11s1
   2:                        ZFS d01                     698.3 Gi   disk11s2
/dev/disk12
   #:                       TYPE NAME                    SIZE       
IDENTIFIER
   0:      GUID_partition_scheme                        *698.6 Gi   disk12
   1:                        EFI                         200.0 Mi   disk12s1
   2:                        ZFS d01                     698.3 Gi   disk12s2

From i_see at macnews.de  Wed Apr  2 13:28:17 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Wed Apr  2 13:28:37 2008
Subject: [zfs-discuss] Re: RAIDZ component UNAVAIL, but actually ok?
In-Reply-To: <20080402015455.3BBCF23C761@lists.macosforge.org>
References: <20080402015455.3BBCF23C761@lists.macosforge.org>
Message-ID: <A8214DC4-8513-41E3-95EF-5541D5A1FC4D@macnews.de>

Hi,
I also occasionally have this problem and reported it to No?l.
This problem will occur if
The physical device name changes (i.e. d24p2 -> disk12s2)
AND
the disk is slow to become available, so it i not auto-discovered when  
Mac OS X's zfs implementation searches for available volumes.
For me exporting the pool and re-importing is a viable workaround.
In future versions you should be able to issue zpool online <pool>  
<device> and it should check the label and realize da4p2 is now indeed  
disk12s2.
Hope this helps,
	ralf
Am 02.04.2008 um 03:54 schrieb zfs-discuss-request@lists.macosforge.org:
> [zfs-discuss] RAIDZ component UNAVAIL, but actually ok?
>
>
> I have seen a number of cases with RAIDZs in pools on removable  
> devices where the RAIDZ comes up degraded due to drive UNAVAIL, yet  
> there appears to be nothing wrong with the drive and no difficulty  
> accessing it with "dd if=/dev/disk12s2 of=/dev/null size=65536".
>
> The test case is a 15" Intel MacBook Pro with a Sans Digital TR4U 4- 
> drive external USB enclosure with 4 Seagate SATA 750GB drives.  The  
> pool "d01" was created on this Mac on disk{2,3,4,5}s2 IIRC.  I  
> created a VMware virtual machine with FreeBSD 7 to test and have  
> moved the enclosure back and forth between the VM and OSX to test.   
> It seems to work fine under Freebsd but comes up DEGRADED under  
> OSX.  I can't detect any problem with the UNAVAIL disk below.  Note  
> that name "da4p2" is the name the device had under FreeBSD and  
> diskutil shows it as disk12s2 under OSX.
>
> Note that another pool "v01" was created after "d01".  I don't know  
> if its existence plays a role.
>
> Is it possible that when ZFS is loading the RAIDZ it first tries to  
> access the components by the name in the on-disk pool metadata, and  
> when that fails ZFS brings up the RAIDZ anyway since it is possible  
> to do so, albeit in a degraded state, on the last RAIDZ component?
>
> bash-3.2# zpool status
> pool: d01
> state: DEGRADED
> status: One or more devices could not be opened.  Sufficient  
> replicas exist for
>   the pool to continue functioning in a degraded state.
> action: Attach the missing device and online it using 'zpool online'.
>  see: http://www.sun.com/msg/ZFS-8000-2Q
> scrub: resilver completed with 0 errors on Tue Apr  1 20:02:31 2008
> config:
>
>   NAME          STATE     READ WRITE CKSUM
>   d01           DEGRADED     0     0     0
>     raidz1      DEGRADED     0     0     0
>       disk8s2   ONLINE       0     0     0
>       disk10s2  ONLINE       0     0     0
>       disk11s2  ONLINE       0     0     0
>       da4p2     UNAVAIL      0     0     0  cannot open
>
> errors: No known data errors

From franzschmalzl at spamfreemail.de  Wed Apr  2 14:18:24 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Wed Apr  2 14:17:30 2008
Subject: [zfs-discuss] Re: RAIDZ component UNAVAIL, but actually ok?
In-Reply-To: <A8214DC4-8513-41E3-95EF-5541D5A1FC4D@macnews.de>
References: <20080402015455.3BBCF23C761@lists.macosforge.org>
	<A8214DC4-8513-41E3-95EF-5541D5A1FC4D@macnews.de>
Message-ID: <3BC8DF76-A98C-4EF9-9195-E76CA18F9EF5@spamfreemail.de>

Same here, happens to me when i attach my pool when logged in and reboot
(since i use filevault, my device names change for my disks because my  
home user sparse bundle is was mounted before )

exporting and importing again helps..



On Apr 2, 2008, at 10:28 PM, Ralf Bertling wrote:

> Hi,
> I also occasionally have this problem and reported it to No?l.
> This problem will occur if
> The physical device name changes (i.e. d24p2 -> disk12s2)
> AND
> the disk is slow to become available, so it i not auto-discovered  
> when Mac OS X's zfs implementation searches for available volumes.
> For me exporting the pool and re-importing is a viable workaround.
> In future versions you should be able to issue zpool online <pool>  
> <device> and it should check the label and realize da4p2 is now  
> indeed disk12s2.
> Hope this helps,
> 	ralf
> Am 02.04.2008 um 03:54 schrieb zfs-discuss-request@lists.macosforge.org 
> :
>> [zfs-discuss] RAIDZ component UNAVAIL, but actually ok?
>>
>>
>> I have seen a number of cases with RAIDZs in pools on removable  
>> devices where the RAIDZ comes up degraded due to drive UNAVAIL, yet  
>> there appears to be nothing wrong with the drive and no difficulty  
>> accessing it with "dd if=/dev/disk12s2 of=/dev/null size=65536".
>>
>> The test case is a 15" Intel MacBook Pro with a Sans Digital TR4U 4- 
>> drive external USB enclosure with 4 Seagate SATA 750GB drives.  The  
>> pool "d01" was created on this Mac on disk{2,3,4,5}s2 IIRC.  I  
>> created a VMware virtual machine with FreeBSD 7 to test and have  
>> moved the enclosure back and forth between the VM and OSX to test.   
>> It seems to work fine under Freebsd but comes up DEGRADED under  
>> OSX.  I can't detect any problem with the UNAVAIL disk below.  Note  
>> that name "da4p2" is the name the device had under FreeBSD and  
>> diskutil shows it as disk12s2 under OSX.
>>
>> Note that another pool "v01" was created after "d01".  I don't know  
>> if its existence plays a role.
>>
>> Is it possible that when ZFS is loading the RAIDZ it first tries to  
>> access the components by the name in the on-disk pool metadata, and  
>> when that fails ZFS brings up the RAIDZ anyway since it is possible  
>> to do so, albeit in a degraded state, on the last RAIDZ component?
>>
>> bash-3.2# zpool status
>> pool: d01
>> state: DEGRADED
>> status: One or more devices could not be opened.  Sufficient  
>> replicas exist for
>>  the pool to continue functioning in a degraded state.
>> action: Attach the missing device and online it using 'zpool online'.
>> see: http://www.sun.com/msg/ZFS-8000-2Q
>> scrub: resilver completed with 0 errors on Tue Apr  1 20:02:31 2008
>> config:
>>
>>  NAME          STATE     READ WRITE CKSUM
>>  d01           DEGRADED     0     0     0
>>    raidz1      DEGRADED     0     0     0
>>      disk8s2   ONLINE       0     0     0
>>      disk10s2  ONLINE       0     0     0
>>      disk11s2  ONLINE       0     0     0
>>      da4p2     UNAVAIL      0     0     0  cannot open
>>
>> errors: No known data errors
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From William.Winnett at Sun.COM  Thu Apr  3 06:10:22 2008
From: William.Winnett at Sun.COM (Bill Winnett)
Date: Thu Apr  3 06:09:12 2008
Subject: [zfs-discuss] zfs assertion/kernel panic
Message-ID: <4311F237-7743-4AC5-B960-E02DDA1AD2D0@sun.com>

Hi,


I have a zfs set that consistently will crash the kernel when a scrub  
operation is performed.  This is not a serious problem for me, as the  
data is unimportant.
However, what is important is that the zfs volume is composed of 4  
disks that includes a known disk with faulty sectors.  Is this panic a  
result of the disk
with faulty sectors, or does it point to some other issue?


=====
Thu Apr  3 09:03:42 2008
panic(cpu 1 caller 0x00CF31EC): "[ZFS]: assertion failed in /Volumes/ 
pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/zfs_kext/zfs/spa.c  
line 3128: vdev_config_sync(rvd, txg) == 0"@/Volumes/pixie_dust/home/ 
ndellofano/zfs-work/wiki/zfs-102A/zfs_kext/zfs/spa.c:3128
Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x5518fdb8 : 0x12b0e1 (0x457024 0x5518fdec 0x13321a 0x0)
0x5518fe08 : 0xcf31ec (0xd5644c 0xd547f8 0xc38 0xd5642c)
0x5518ff58 : 0xcdc22a (0x7a53000 0xc8d51 0x0 0x129218)
0x5518ffc8 : 0x19e93c (0x7b30200 0x0 0x1a20b5 0x7c5be40)
Backtrace terminated-invalid frame pointer 0
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0xcab000->0xd76fff

BSD process name corresponding to current thread: kernel_task

Mac OS version:
9C31

Kernel version:
Darwin Kernel Version 9.2.0: Tue Feb  5 16:13:22 PST 2008;  
root:xnu-1228.3.13~1/RELEASE_I386
System model name: iMac7,1 (Mac-F4238CC8)

From me at joedunn.com  Thu Apr  3 18:58:36 2008
From: me at joedunn.com (Joe Dunn)
Date: Thu Apr  3 18:57:23 2008
Subject: [zfs-discuss] Kernel Panics
Message-ID: <c61bd13c0804031858x362fb3cbmabd3bba9fe5004fd@mail.gmail.com>

Hey Guys

Just wondering how often you experience kernel panics from the zfs-101a
install? I just got my second of the night, here it is below

Thu Apr  3 21:49:15 2008
panic(cpu 0 caller 0x345AFDEC): "ZFS: I/O failure (write on <unknown> off 0:
zio 0x5448ee0 [L0 ZFS plain file] 20000L/20000P DVA[0]=<0:8b10c80000:20000>
fletcher2 uncompressed LE contiguous birth=18742 fill=1
cksum=3d6e6c81d83ac4fc:a6bdb8401f61f4b8:a44578ca38daf3f:cb762ce2540a4cf):
error "
"6"@/Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/zfs_kext/zfs/zio.c:918
Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x3453be48 : 0x12b0f7 (0x4581f4 0x3453be7c 0x133230 0x0)
0x3453be98 : 0x345afdec (0x3461db90 0x3461db84 0x34619690 0x3464c6ec)
0x3453bf18 : 0x345ac4c0 (0x5448ee0 0x176 0x3453bf38 0x345f3267)
0x3453bf58 : 0x3460b0b0 (0x5448ee0 0x1 0x34656110 0x90f5b30)
0x3453bfc8 : 0x19eadc (0x90f5b30 0x0 0x1a20b5 0xafa1128)
Backtrace terminated-invalid frame pointer 0
      Kernel loadable modules in backtrace (with dependencies):
         com.apple.filesystems.zfs(8.0)@0x34598000->0x34663fff

BSD process name corresponding to current thread: kernel_task

Mac OS version:
9C7010
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080403/7ad4b3c1/attachment.html
From btm at pobox.com  Fri Apr  4 05:32:29 2008
From: btm at pobox.com (Brett Ault-McCoy)
Date: Fri Apr  4 05:31:17 2008
Subject: [zfs-discuss] missing space found by du
Message-ID: <7bccd8dc0804040532g762929dmfb458db364c5e3e1@mail.gmail.com>

I had a problem with free space disappearing on my zfs pool today.  I had
11G free showing and I deleted about 12G from a filesystem.  There isn't a
snapshot of this filesystem so the space should have been freed up, but
after the delete (using 'rm' from the command line) it still showed 11G
free.  I thought it must have been due to some process having those files
open, but I couldn't find anything that looked like a likely culprit.  So, I
found another large file to delete, ran lsof on it to make sure nothing had
it open, and deleted it.  It was >2G, but I still showed 11G free.  I
decided to try unmounting the filesystem to make sure nothing was holding
the files open.  However, a 'zfs umount' hung.  I tried doing a 'zpool
export' of the pool and that wound up hanging as well.  I finally had to
reboot my machine to get things working again.  When it came back up the
pool still showed 9G free (I'd written some more to it between when I'd
first started messing with it and finally rebooted).  I ran 'du' on the
filesystem that I'd been messing with and came up with 87G, but the 'USED'
and 'REFER' columns under 'zfs list' showed 112G.  The USED for the pool was
showing 219G, so I ran a du across the entire pool.  When it finished it
said there was 194G in the pool.  I ran 'zfs list' to check and it now shows
194G used for the pool and only 87G for the filesystem in question and now
shows 34G AVAIL.

So, it would seem that some sort of asynchronous garbage collection is
happening that just took a while to notice the freed space, or the du caused
it to notice the free space or something.  Does anyone have any ideas what
would have caused this?

++Brett;


zfs list output before running a du across the entire pool:

NAME                            USED  AVAIL  REFER  MOUNTPOINT
A                               219G  8.98G   310K  /Volumes/A
A/Applications                 1.34G  8.98G  1.34G  /Volumes/A/Applications
A/Downloads                     155M  8.98G   155M  /Volumes/A/Downloads
A/Movies                        112G  8.98G   112G  /Volumes/A/Movies
A/Pictures                     36.9G  8.98G  36.9G  /Volumes/A/Pictures
A/Pictures@20080307211826        18K      -  36.8G  -
A/Pictures@20080314134126        19K      -  36.8G  -
A/PicturesOld                  2.49G  8.98G  2.49G  /Volumes/A/PicturesOld
A/PicturesOld@20080304101257     16K      -  2.49G  -
A/PicturesOld@20080311144506     16K      -  2.49G  -
A/Stuff                         193K  8.98G   193K  /Volumes/A/Stuff
A/iTunes                       59.7G  8.98G  59.7G  /Volumes/A/iTunes
A/iTunes@20080326102020        93.5K      -  59.7G  -
A/iTunes@20080326144342          88K      -  59.7G  -
A/iTunes@20080331133543         148K      -  59.7G  -
A/iTunes@20080403130441          93K      -  59.7G  -
A/opt                          6.78G  8.98G  6.78G  /Volumes/A/opt

End result from du:

194G    .


zfs list output after running a du across the entire pool:

NAME                            USED  AVAIL  REFER  MOUNTPOINT
A                               194G  34.3G   308K  /Volumes/A
A/Applications                 1.34G  34.3G  1.34G  /Volumes/A/Applications
A/Downloads                     155M  34.3G   155M  /Volumes/A/Downloads
A/Movies                       86.7G  34.3G  86.7G  /Volumes/A/Movies
A/Pictures                     36.9G  34.3G  36.9G  /Volumes/A/Pictures
A/Pictures@20080307211826        18K      -  36.8G  -
A/Pictures@20080314134126        19K      -  36.8G  -
A/PicturesOld                  2.49G  34.3G  2.49G  /Volumes/A/PicturesOld
A/PicturesOld@20080304101257     16K      -  2.49G  -
A/PicturesOld@20080311144506     16K      -  2.49G  -
A/Stuff                         190K  34.3G   190K  /Volumes/A/Stuff
A/iTunes                       59.7G  34.3G  59.7G  /Volumes/A/iTunes
A/iTunes@20080326102020        93.5K      -  59.7G  -
A/iTunes@20080326144342          88K      -  59.7G  -
A/iTunes@20080331133543         148K      -  59.7G  -
A/iTunes@20080403130441          93K      -  59.7G  -
A/opt                          6.78G  34.3G  6.78G  /Volumes/A/opt
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080404/a7ecd6c2/attachment-0001.html
From justin at justincooley.com  Fri Apr  4 07:20:56 2008
From: justin at justincooley.com (Justin Cooley)
Date: Fri Apr  4 07:19:57 2008
Subject: [zfs-discuss] Panic on scrub
Message-ID: <7AE74633-1EFE-45C7-972F-5F1BBFCC5538@justincooley.com>

Will,

I was just about to write to the list because I have also been getting  
panics on scrubs.  My panics just started recently; previously I was  
able to scrub the same pool.  My panic log is pasted after Will's  
message below.

Any ideas?

Thanks,

Justin


> Message: 3
> Date: Thu, 03 Apr 2008 09:10:22 -0400
> From: Bill Winnett <William.Winnett@Sun.COM>
> Subject: [zfs-discuss] zfs assertion/kernel panic
> To: zfs-discuss@lists.macosforge.org
> Message-ID: <4311F237-7743-4AC5-B960-E02DDA1AD2D0@sun.com>
>
> Hi,
>
> I have a zfs set that consistently will crash the kernel when a scrub
> operation is performed.  This is not a serious problem for me, as the
> data is unimportant.
> However, what is important is that the zfs volume is composed of 4
> disks that includes a known disk with faulty sectors.  Is this panic a
> result of the disk
> with faulty sectors, or does it point to some other issue?




Thu Apr  3 00:04:26 2008
panic(cpu 0 caller 0x001A8C8A): Kernel trap at 0x003618c1, type  
14=page fault, registers:
CR0: 0x80010033, CR2: 0x0000000c, CR3: 0x00f8b000, CR4: 0x00000660
EAX: 0x00000000, EBX: 0x345dfde8, ECX: 0x1f000000, EDX: 0x00000000
CR2: 0x0000000c, EBP: 0x345dfd18, ESI: 0x00000200, EDI: 0x052ee000
EFL: 0x00010202, EIP: 0x003618c1, CS:  0x00000008, DS:  0x03c00010
Error code: 0x00000000

Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x345dfb38 : 0x12b0f7 (0x4581f4 0x345dfb6c 0x133230 0x0)
0x345dfb88 : 0x1a8c8a (0x461720 0x3618c1 0xe 0x460ed0)
0x345dfc68 : 0x19ece5 (0x345dfc80 0x4c5e2e8 0x345dfd18 0x3618c1)
0x345dfc78 : 0x3618c1 (0xe 0x48 0x4b30010 0x190010)
0x345dfd18 : 0x54a20ff8 (0x0 0x1bf2a 0x3000 0x0)
0x345dfd38 : 0x54a289dd (0x0 0x0 0x345dfd58 0x1a236f)
0x345dfd78 : 0x202bea (0x1f000000 0xcf1c5a20 0x52ee000 0x3)
0x345dfdb8 : 0x1f6039 (0x345dfde8 0x246 0x345dfe18 0x1da207)
0x345dfe18 : 0x1ec2f6 (0x4ad7910 0xcf1c5a20 0x52ee000 0x3)
0x345dfe78 : 0x3653a7 (0x433f500 0xcf1c5a20 0x52ee000 0x345dff50)
0x345dfe98 : 0x38bd28 (0x433f500 0xcf1c5a20 0x52ee000 0x345dff50)
0x345dff78 : 0x3dcf13 (0x44b57b0 0x4c57680 0x4c576c4 0x3e5a3c0)
0x345dffc8 : 0x19f1c3 (0x4c4c120 0x0 0x1a20b5 0x4c4c120)
No mapping exists for frame pointer
Backtrace terminated-invalid frame pointer 0xbfffab38
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0x549c5000->0x54a90fff

BSD process name corresponding to current thread: zpool

Mac OS version:
9C7010

Kernel version:
Darwin Kernel Version 9.2.2: Tue Mar  4 21:17:34 PST 2008;  
root:xnu-1228.4.31~1/RELEASE_I386
System model name: MacBookAir1,1 (Mac-F42C8CC8)

>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080404/9dde9260/attachment.html
From eberlein at att.net  Fri Apr  4 08:30:40 2008
From: eberlein at att.net (Michael Eberlein)
Date: Fri Apr  4 08:29:34 2008
Subject: [zfs-discuss] Panic on scrub
In-Reply-To: <7AE74633-1EFE-45C7-972F-5F1BBFCC5538@justincooley.com>
References: <7AE74633-1EFE-45C7-972F-5F1BBFCC5538@justincooley.com>
Message-ID: <F1DC1A57-FFC2-4C89-8A7C-0223E38624BA@att.net>

I guess it's time for me to delurk.  I also experienced consistent and  
repeated kernel panics - not on scrubs, but on the command 'zpool  
status'.  Here is the scenario where I ran into the problem:

I setup a pool with raidz using 4 500GB external drives.  One of the  
drive enclosures (with 2 drives in it) turned out to be bad, but I  
initially misdiagnosed the problem, so I declared one of the drives  
faulted and continued operation with the degraded raidz.

Several days later, with continued errors, I identified the bad  
enclosure and replaced it - everything was good - so I thought...   
Unfortunately, some errors were introduced as the next zpool scrub  
showed several dozen errors (on the pool, not any drive), and the  
wonderful message:

status: One or more devices has experienced an error resulting in data
corruption. Applications may be affected.

An attempt to use "zpool status -v" resulted in an immediate kernel  
panic.  Any followon attempts to do a basic zpool status then also  
resulted in a panic.

I'm recovering from a backup now and plan on destroying the pool and  
recreating, but my observation is that the existence of errors  
(particularly data errors in my case) seems to cause zpool status to  
cause a kernel panic.  In my case, though, zpool scrub worked (I just  
couldn't check the status) and resilvering took place correctly when I  
finally replaced the 4th drive.

Unfortunately, I don't have a copy of the panic log handy and I am not  
in a position where I can intentionally cause one..

Mike

On Apr 4, 2008, at 4:20 PM, Justin Cooley wrote:

> Will,
>
> I was just about to write to the list because I have also been  
> getting panics on scrubs.  My panics just started recently;  
> previously I was able to scrub the same pool.  My panic log is  
> pasted after Will's message below.
>
> Any ideas?
>
> Thanks,
>
> Justin
>
>
>> Message: 3
>> Date: Thu, 03 Apr 2008 09:10:22 -0400
>> From: Bill Winnett <William.Winnett@Sun.COM>
>> Subject: [zfs-discuss] zfs assertion/kernel panic
>> To: zfs-discuss@lists.macosforge.org
>> Message-ID: <4311F237-7743-4AC5-B960-E02DDA1AD2D0@sun.com>
>>
>> Hi,
>>
>> I have a zfs set that consistently will crash the kernel when a scrub
>> operation is performed.  This is not a serious problem for me, as the
>> data is unimportant.
>> However, what is important is that the zfs volume is composed of 4
>> disks that includes a known disk with faulty sectors.  Is this  
>> panic a
>> result of the disk
>> with faulty sectors, or does it point to some other issue?
>
>
>
>
> Thu Apr  3 00:04:26 2008
> panic(cpu 0 caller 0x001A8C8A): Kernel trap at 0x003618c1, type  
> 14=page fault, registers:
> CR0: 0x80010033, CR2: 0x0000000c, CR3: 0x00f8b000, CR4: 0x00000660
> EAX: 0x00000000, EBX: 0x345dfde8, ECX: 0x1f000000, EDX: 0x00000000
> CR2: 0x0000000c, EBP: 0x345dfd18, ESI: 0x00000200, EDI: 0x052ee000
> EFL: 0x00010202, EIP: 0x003618c1, CS:  0x00000008, DS:  0x03c00010
> Error code: 0x00000000
>
> Backtrace, Format - Frame : Return Address (4 potential args on stack)
> 0x345dfb38 : 0x12b0f7 (0x4581f4 0x345dfb6c 0x133230 0x0)
> 0x345dfb88 : 0x1a8c8a (0x461720 0x3618c1 0xe 0x460ed0)
> 0x345dfc68 : 0x19ece5 (0x345dfc80 0x4c5e2e8 0x345dfd18 0x3618c1)
> 0x345dfc78 : 0x3618c1 (0xe 0x48 0x4b30010 0x190010)
> 0x345dfd18 : 0x54a20ff8 (0x0 0x1bf2a 0x3000 0x0)
> 0x345dfd38 : 0x54a289dd (0x0 0x0 0x345dfd58 0x1a236f)
> 0x345dfd78 : 0x202bea (0x1f000000 0xcf1c5a20 0x52ee000 0x3)
> 0x345dfdb8 : 0x1f6039 (0x345dfde8 0x246 0x345dfe18 0x1da207)
> 0x345dfe18 : 0x1ec2f6 (0x4ad7910 0xcf1c5a20 0x52ee000 0x3)
> 0x345dfe78 : 0x3653a7 (0x433f500 0xcf1c5a20 0x52ee000 0x345dff50)
> 0x345dfe98 : 0x38bd28 (0x433f500 0xcf1c5a20 0x52ee000 0x345dff50)
> 0x345dff78 : 0x3dcf13 (0x44b57b0 0x4c57680 0x4c576c4 0x3e5a3c0)
> 0x345dffc8 : 0x19f1c3 (0x4c4c120 0x0 0x1a20b5 0x4c4c120)
> No mapping exists for frame pointer
> Backtrace terminated-invalid frame pointer 0xbfffab38
>       Kernel loadable modules in backtrace (with dependencies):
>          com.apple.filesystems.zfs(8.0)@0x549c5000->0x54a90fff
>
> BSD process name corresponding to current thread: zpool
>
> Mac OS version:
> 9C7010
>
> Kernel version:
> Darwin Kernel Version 9.2.2: Tue Mar  4 21:17:34 PST 2008;  
> root:xnu-1228.4.31~1/RELEASE_I386
> System model name: MacBookAir1,1 (Mac-F42C8CC8)
>
>>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080404/ea6547af/attachment.html
From justin at justincooley.com  Fri Apr  4 08:38:41 2008
From: justin at justincooley.com (Justin Cooley)
Date: Fri Apr  4 08:37:25 2008
Subject: [zfs-discuss] Panic on scrub
In-Reply-To: <F1DC1A57-FFC2-4C89-8A7C-0223E38624BA@att.net>
References: <7AE74633-1EFE-45C7-972F-5F1BBFCC5538@justincooley.com>
	<F1DC1A57-FFC2-4C89-8A7C-0223E38624BA@att.net>
Message-ID: <A27B93CA-F52E-48B4-8459-3BCCD237A848@justincooley.com>

On Apr 4, 2008, at 8:30 AM, Michael Eberlein wrote:

> In my case, though, zpool scrub worked (I just couldn't check the  
> status) and resilvering took place correctly when I finally replaced  
> the 4th drive.


That's what I get for writing too early in the morning.  I should add  
to my earlier post that the KP happens when I 'zpool status' AFTER  
starting a scrub, not on the scrub itself.  Sorry for the  
miscommunication.

Thanks Michael,

Justin

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080404/fcfb6092/attachment.html
From ndellofano at apple.com  Fri Apr  4 13:56:39 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri Apr  4 13:56:50 2008
Subject: [zfs-discuss] Kernel Panics
In-Reply-To: <c61bd13c0804031858x362fb3cbmabd3bba9fe5004fd@mail.gmail.com>
References: <c61bd13c0804031858x362fb3cbmabd3bba9fe5004fd@mail.gmail.com>
Message-ID: <0A113794-353C-45B6-AA9E-C994C3CAC03C@apple.com>

Hey Joe,

This panic is from a device either being removed or failing to  
respond.  Since ZFS is trying to write to it and it's a non replicated  
pool, then the write *not* flagged as ZIO_FLAG_CANFAIL.  Since the  
write must succeed and the drive appears to either be missing or  
malfunctioning, then a panic ensues.  We are working on changing this  
behaviour such that instead of panicking we will handle this error  
more gracefully while still trying to maintain on disk consistency.

Noel

On Apr 3, 2008, at 6:58 PM, Joe Dunn wrote:

> Hey Guys
>
> Just wondering how often you experience kernel panics from the  
> zfs-101a install? I just got my second of the night, here it is below
>
> Thu Apr  3 21:49:15 2008
> panic(cpu 0 caller 0x345AFDEC): "ZFS: I/O failure (write on  
> <unknown> off 0: zio 0x5448ee0 [L0 ZFS plain file] 20000L/20000P  
> DVA[0]=<0:8b10c80000:20000> fletcher2 uncompressed LE contiguous  
> birth=18742 fill=1  
> cksum 
> =3d6e6c81d83ac4fc:a6bdb8401f61f4b8:a44578ca38daf3f:cb762ce2540a4cf):  
> error " "6"@/Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/ 
> zfs-102A/zfs_kext/zfs/zio.c:918
> Backtrace, Format - Frame : Return Address (4 potential args on stack)
> 0x3453be48 : 0x12b0f7 (0x4581f4 0x3453be7c 0x133230 0x0)
> 0x3453be98 : 0x345afdec (0x3461db90 0x3461db84 0x34619690 0x3464c6ec)
> 0x3453bf18 : 0x345ac4c0 (0x5448ee0 0x176 0x3453bf38 0x345f3267)
> 0x3453bf58 : 0x3460b0b0 (0x5448ee0 0x1 0x34656110 0x90f5b30)
> 0x3453bfc8 : 0x19eadc (0x90f5b30 0x0 0x1a20b5 0xafa1128)
> Backtrace terminated-invalid frame pointer 0
>       Kernel loadable modules in backtrace (with dependencies):
>          com.apple.filesystems.zfs(8.0)@0x34598000->0x34663fff
>
> BSD process name corresponding to current thread: kernel_task
>
> Mac OS version:
> 9C7010
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From eberlein at att.net  Fri Apr  4 14:10:46 2008
From: eberlein at att.net (Michael Eberlein)
Date: Fri Apr  4 14:09:45 2008
Subject: [zfs-discuss] Panic on scrub
In-Reply-To: <7AE74633-1EFE-45C7-972F-5F1BBFCC5538@justincooley.com>
References: <7AE74633-1EFE-45C7-972F-5F1BBFCC5538@justincooley.com>
Message-ID: <FC2D3EC8-68AD-44ED-A8B5-C6C43E1629D1@att.net>

Justin,

   Here is the panic log I've been getting on my problems as listed  
below - same problem.  I am using an Imac 24" with 4 drives connected  
via USB and firewire, though the problem persists with any manner of  
swapping of the interfaces.

I *KNOW* I have data corruption issues here, so I think that's an  
underlying cause here.

Any help appreciated.

Mike

Fri Apr  4 23:05:40 2008
panic(cpu 1 caller 0x001A8C8A): Kernel trap at 0x003618c1, type  
14=page fault, registers:
CR0: 0x80010033, CR2: 0x0000000c, CR3: 0x011e1000, CR4: 0x00000660
EAX: 0x00000000, EBX: 0x35f13de8, ECX: 0x1f000000, EDX: 0x00000000
CR2: 0x0000000c, EBP: 0x35f13d18, ESI: 0x00000200, EDI: 0x09f14000
EFL: 0x00010202, EIP: 0x003618c1, CS:  0x00000008, DS:  0x05840010
Error code: 0x00000000

Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x35f13b38 : 0x12b0f7 (0x4581f4 0x35f13b6c 0x133230 0x0)
0x35f13b88 : 0x1a8c8a (0x461720 0x3618c1 0xe 0x460ed0)
0x35f13c68 : 0x19ece5 (0x35f13c80 0x5693000 0x35f13d18 0x3618c1)
0x35f13c78 : 0x3618c1 (0xe 0x48 0x4a8a0010 0x10)
0x35f13d18 : 0xc71ff8 (0x0 0x6a11f 0x804000 0x0)
0x35f13d38 : 0xc799dd (0x0 0x0 0x35f13d88 0x6a11f000)
0x35f13d78 : 0x202bea (0x1f000000 0xcf1c5a20 0x9f14000 0x3)
0x35f13db8 : 0x1f6039 (0x35f13de8 0x246 0x35f13e18 0x1da207)
0x35f13e18 : 0x1ec2f6 (0x4941050 0xcf1c5a20 0x9f14000 0x3)
0x35f13e78 : 0x3653a7 (0x4ccb970 0xcf1c5a20 0x9f14000 0x35f13f50)
0x35f13e98 : 0x38bd28 (0x4ccb970 0xcf1c5a20 0x9f14000 0x35f13f50)
0x35f13f78 : 0x3dcf13 (0x59cde70 0x5458340 0x5458384 0x4303)
0x35f13fc8 : 0x19f1c3 (0x569bac0 0x0 0x1a20b5 0x569bac0)
No mapping exists for frame pointer
Backtrace terminated-invalid frame pointer 0xbfffab18
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0xc16000->0xce1fff

BSD process name corresponding to current thread: zpool

Mac OS version:
9C7010

Kernel version:
Darwin Kernel Version 9.2.2: Tue Mar  4 21:17:34 PST 2008;  
root:xnu-1228.4.31~1/RELEASE_I386
System model name: iMac7,1 (Mac-F42386C8)

On Apr 4, 2008, at 4:20 PM, Justin Cooley wrote:

> Will,
>
> I was just about to write to the list because I have also been  
> getting panics on scrubs.  My panics just started recently;  
> previously I was able to scrub the same pool.  My panic log is  
> pasted after Will's message below.
>
> Any ideas?
>
> Thanks,
>
> Justin
>
>
>> Message: 3
>> Date: Thu, 03 Apr 2008 09:10:22 -0400
>> From: Bill Winnett <William.Winnett@Sun.COM>
>> Subject: [zfs-discuss] zfs assertion/kernel panic
>> To: zfs-discuss@lists.macosforge.org
>> Message-ID: <4311F237-7743-4AC5-B960-E02DDA1AD2D0@sun.com>
>>
>> Hi,
>>
>> I have a zfs set that consistently will crash the kernel when a scrub
>> operation is performed.  This is not a serious problem for me, as the
>> data is unimportant.
>> However, what is important is that the zfs volume is composed of 4
>> disks that includes a known disk with faulty sectors.  Is this  
>> panic a
>> result of the disk
>> with faulty sectors, or does it point to some other issue?
>
>
>
>
> Thu Apr  3 00:04:26 2008
> panic(cpu 0 caller 0x001A8C8A): Kernel trap at 0x003618c1, type  
> 14=page fault, registers:
> CR0: 0x80010033, CR2: 0x0000000c, CR3: 0x00f8b000, CR4: 0x00000660
> EAX: 0x00000000, EBX: 0x345dfde8, ECX: 0x1f000000, EDX: 0x00000000
> CR2: 0x0000000c, EBP: 0x345dfd18, ESI: 0x00000200, EDI: 0x052ee000
> EFL: 0x00010202, EIP: 0x003618c1, CS:  0x00000008, DS:  0x03c00010
> Error code: 0x00000000
>
> Backtrace, Format - Frame : Return Address (4 potential args on stack)
> 0x345dfb38 : 0x12b0f7 (0x4581f4 0x345dfb6c 0x133230 0x0)
> 0x345dfb88 : 0x1a8c8a (0x461720 0x3618c1 0xe 0x460ed0)
> 0x345dfc68 : 0x19ece5 (0x345dfc80 0x4c5e2e8 0x345dfd18 0x3618c1)
> 0x345dfc78 : 0x3618c1 (0xe 0x48 0x4b30010 0x190010)
> 0x345dfd18 : 0x54a20ff8 (0x0 0x1bf2a 0x3000 0x0)
> 0x345dfd38 : 0x54a289dd (0x0 0x0 0x345dfd58 0x1a236f)
> 0x345dfd78 : 0x202bea (0x1f000000 0xcf1c5a20 0x52ee000 0x3)
> 0x345dfdb8 : 0x1f6039 (0x345dfde8 0x246 0x345dfe18 0x1da207)
> 0x345dfe18 : 0x1ec2f6 (0x4ad7910 0xcf1c5a20 0x52ee000 0x3)
> 0x345dfe78 : 0x3653a7 (0x433f500 0xcf1c5a20 0x52ee000 0x345dff50)
> 0x345dfe98 : 0x38bd28 (0x433f500 0xcf1c5a20 0x52ee000 0x345dff50)
> 0x345dff78 : 0x3dcf13 (0x44b57b0 0x4c57680 0x4c576c4 0x3e5a3c0)
> 0x345dffc8 : 0x19f1c3 (0x4c4c120 0x0 0x1a20b5 0x4c4c120)
> No mapping exists for frame pointer
> Backtrace terminated-invalid frame pointer 0xbfffab38
>       Kernel loadable modules in backtrace (with dependencies):
>          com.apple.filesystems.zfs(8.0)@0x549c5000->0x54a90fff
>
> BSD process name corresponding to current thread: zpool
>
> Mac OS version:
> 9C7010
>
> Kernel version:
> Darwin Kernel Version 9.2.2: Tue Mar  4 21:17:34 PST 2008;  
> root:xnu-1228.4.31~1/RELEASE_I386
> System model name: MacBookAir1,1 (Mac-F42C8CC8)
>
>>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080404/16501d7c/attachment.html
From i_see at macnews.de  Fri Apr  4 14:41:24 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Fri Apr  4 14:40:07 2008
Subject: [zfs-discuss] On panics scrubbing and zpool status
In-Reply-To: <20080404205651.1DC9323EB4A@lists.macosforge.org>
References: <20080404205651.1DC9323EB4A@lists.macosforge.org>
Message-ID: <5F42638F-C89A-465E-9EEA-94A35CEEADF5@macnews.de>

hi list,
as mentioned earlier, I had similar problems with my initial pool,  
(probably caused by bad labels that in turn came from drives without a  
GUID disk label).

ZFS on Mac OS X can not handle a couple of things gracefully:
1. The pool becoming unavailable (hot unplugging of too many drives or  
errors on non-replicated pools)
2. Corrupt disk labels (can lead to consistant kernel panics on zpool  
status
3. Disks taking a long time to answer requests (this tends to result  
in deadlocks instead of a traditional kernel panic. the problem is,  
you may be unable to export the pool.

For 2. You can try to work with a degraded pool to get your data out  
of the pool and start over.
For 3. if you know the drive causing the problem (I do have one  
FireWire enclosure that occasionally locks up), you may try to export  
the pool (do not unmount anything prior to that not even disk images  
mounted from your ZFS Volumes).
If that does not work and you have a replicated pool, turning off the  
locked device helps but may not be recognized by zfs quickly. You may  
then have to export/reimport your pool or export and reboot to get  
everything back to normal.
Hope this helps,
	ralf
From james-zfsosx at jrv.org  Fri Apr  4 16:08:17 2008
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Fri Apr  4 16:07:04 2008
Subject: [zfs-discuss] missing space found by du
In-Reply-To: <7bccd8dc0804040532g762929dmfb458db364c5e3e1@mail.gmail.com>
References: <7bccd8dc0804040532g762929dmfb458db364c5e3e1@mail.gmail.com>
Message-ID: <47F6B4E1.2000202@jrv.org>

Brett Ault-McCoy wrote:
> So, it would seem that some sort of asynchronous garbage collection is 
> happening that just took a while to notice the freed space, or the du 
> caused it to notice the free space or something.  Does anyone have any 
> ideas what would have caused this?
>

The ZFS on-disk specification PDF has this text that is likely related:

Section 6.1: ZPL Filesystem Layout
...
Name: DELETE_QUEUE
Value: 64 bit object number for the delete queue object
Description: The delete queue provides a list of deletes that were 
in-progress when
the filesystem was force unmounted or as a result of a system failure 
such as a
power outage.   Upon the next mount of the filesystem, the delete queue is
processed to remove the files/dirs that are in the delete queue.  This 
mechanism is
used to avoid leaking files and directories in the filesystem.

From james-zfsosx at jrv.org  Fri Apr  4 16:29:56 2008
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Fri Apr  4 16:28:42 2008
Subject: [zfs-discuss] Kernel Panics
In-Reply-To: <0A113794-353C-45B6-AA9E-C994C3CAC03C@apple.com>
References: <c61bd13c0804031858x362fb3cbmabd3bba9fe5004fd@mail.gmail.com>
	<0A113794-353C-45B6-AA9E-C994C3CAC03C@apple.com>
Message-ID: <47F6B9F4.4040500@jrv.org>

No?l Dellofano wrote:
> This panic is from a device either being removed or failing to 
> respond.  Since ZFS is trying to write to it and it's a non replicated 
> pool, then the write *not* flagged as ZIO_FLAG_CANFAIL.  Since the 
> write must succeed and the drive appears to either be missing or 
> malfunctioning, then a panic ensues.  We are working on changing this 
> behaviour such that instead of panicking we will handle this error 
> more gracefully while still trying to maintain on disk consistency.
>
I don't understand this.  The on-disk format is always consistent, 
right?  As I understand it that failed write can be discarded (so long 
as subsequent I/O is not written!) and the on-disk image remains 
consistent?  I'm having trouble seeing how this "write failure" event 
differs from a "loss of power" event as far as on-disk consistency goes.


As an aside, I assume ZFS requires the underlying devices honor either 
write fencing or ordering of some kind in order to keep the on-disk 
structures consistent.  Is it known that the Firewire and USB drivers in 
OSX honor this requirement?

I have had a couple of data loss / pool corruption events experimenting 
with ZFS on FreeBSD in a VMware machine on OSX.  My assumption is that 
VMware is not passing through some write-fencing from ZFS/FreeBSD to OSX 
correctly.  But I'm wondering if the OSX drivers have been looked at to 
make sure whatever it is ZFS needs is met by current OSX drivers.  What 
the disk drive itself does to fenced/ordered writes is another problem 
entirely...

I haven't had any data loss or data corruption under OSX yet, nor any 
panics (but then again I have 4GB of RAM) with correctly labeled disks.

From robert at rehnmark.net  Sat Apr  5 08:08:59 2008
From: robert at rehnmark.net (Robert Rehnmark)
Date: Sat Apr  5 08:07:45 2008
Subject: [zfs-discuss] Known problems in day-to-day usage?
Message-ID: <3F5538AB-4565-4D91-A7E8-D4712CBFE7BD@rehnmark.net>

I'm setting up my home server for backups, media storage etc ...

I think this question has been posted here before but I can't find any  
good answers.
As ZFS is now on OSX, can it be used without risking dataloss under  
normal use?
Will it preserve all data in all files?
... like data forks, different kind of files, icons, etc, etc ...
Can I hope for it to work fine?

I also tested ACL's on a simple 1-disk pool and it seems to work just  
fine.
It's easy in the terminal but even esier and faster with TinkerTool  
System, I can really recommend that app/tool. (10 $ I think)

Thanks.

/Robert
From lopez.on.the.lists at yellowspace.net  Sat Apr  5 14:24:50 2008
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Sat Apr  5 14:23:28 2008
Subject: [zfs-discuss] Status of ZFS on Mac OS X?
Message-ID: <fb9be13f644474a1cb6845fa4b9a4c3a@yellowspace.net>

Hello,

I'm looking forward on deploying ZFS fileystems on Mac OS X 10.5
environments (both client and server), as I've already done with Solaris
and FreeBSD (the latter on development systems). 

There's no doubt to me that ZFS is a great idea and a huge leap forward in
Mac OS X filesystems, and I'm willing to start using zfs on osx (and share
my experience here) as soon as it gets stable enough for daily usage (ok.
maybe a tech-aware usage).

Since there's nothing on the website about a timeline or a roadmap, and
since the current open bugs on trac are anything but encouraging, I'd like
to dare making one question here: is there anybody on this list from the
"mothership" allowed to make statements on the progress/current status on
those bugs and on zfs on Mac OS X in general?


Regards,

Lorenzo



From lists at loveturtle.net  Sat Apr  5 16:47:44 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Sat Apr  5 16:46:30 2008
Subject: [zfs-discuss] Status of ZFS on Mac OS X?
In-Reply-To: <fb9be13f644474a1cb6845fa4b9a4c3a@yellowspace.net>
References: <fb9be13f644474a1cb6845fa4b9a4c3a@yellowspace.net>
Message-ID: <47F80FA0.2000604@loveturtle.net>

Hi, I'm not from the mothership and this isn't an answer but I just 
wanted to comment.

"I'm willing to start using zfs on osx (and sharemy experience here) as 
soon as it gets stable enough for daily usage (ok. maybe a tech-aware 
usage)."

It's already there. Your own mileage may vary, I have a two disk zpool 
in my mac pro for my home dir, /opt, and storage.
Other than the finder weirdness I have no serious problems. No panics 
triggered by anything I do on a daily basis. I haven't had a single 
panic on the mac pro since I started using the current build. That's not 
to say that it's perfect, just that nothing I do triggers it.  Depending 
on your needs and configuration it may be just fine for what you do, or 
it may not.

So I would tell you what I'd tell anyone. Give it a shot.


Lorenzo Perone wrote:
> Hello,
>
> I'm looking forward on deploying ZFS fileystems on Mac OS X 10.5
> environments (both client and server), as I've already done with Solaris
> and FreeBSD (the latter on development systems). 
>
> There's no doubt to me that ZFS is a great idea and a huge leap forward in
> Mac OS X filesystems, and I'm willing to start using zfs on osx (and share
> my experience here) as soon as it gets stable enough for daily usage (ok.
> maybe a tech-aware usage).
>
> Since there's nothing on the website about a timeline or a roadmap, and
> since the current open bugs on trac are anything but encouraging, I'd like
> to dare making one question here: is there anybody on this list from the
> "mothership" allowed to make statements on the progress/current status on
> those bugs and on zfs on Mac OS X in general?
>
>
> Regards,
>
> Lorenzo
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>   

From jbsnyder at gmail.com  Sat Apr  5 17:03:08 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Sat Apr  5 17:01:47 2008
Subject: [zfs-discuss] Status of ZFS on Mac OS X?
In-Reply-To: <47F80FA0.2000604@loveturtle.net>
References: <fb9be13f644474a1cb6845fa4b9a4c3a@yellowspace.net>
	<47F80FA0.2000604@loveturtle.net>
Message-ID: <33644d3c0804051703j79c39b6fr2567c049aa18ffa6@mail.gmail.com>

I'll chime in on a similar note.  Aside from the known issues with
having to empty trash by rm'ing and not being able to use spotlight,
it has been good to me on my single-disk MacBook.  I have 60 GB for
applications and boot on HFS+ and 100GB home dir on ZFS, and it has
not given me any panics except for just after getting started with it.
 After some initial stability issues everything seems pretty happy.
I'm even running a bunch of applications out of ~/Applications and
most of those seem fine as well, with the exception that installing
firefox add-ons doesn't seem to work correctly for some reason.

I've been using zetaback (adjusted slightly for my own usage) for
snapshots to one machine, and rsync 3 to another machine for
additional backups, with no real trouble.

Keep in mind that it is case sensitive, as well, and you should be
mostly fine, I think.  I've not lost any data on it aside from things
that haven't been written to disk at the time of panics.  I've not had
any corruption that I know of.

One thing I might stay away from, at the moment, is attaching and
detaching disks a lot from mirrors (like if you want to do something
like this with a memory stick or external drive:
http://solarisdesktop.blogspot.com/2007/02/stick-to-zfs-or-laptop-with-mirrored.html).
  From my recollection it can be possible to trigger some panics
there.

I'm not sure how I would rate it in comparison to the FreeBSD port.
That one is relatively stable as well, though there are the known
memory issues and some deadlocks there that I don't think I've seen on
OS X.

On Sat, Apr 5, 2008 at 6:47 PM, Dillon Kass <lists@loveturtle.net> wrote:
> Hi, I'm not from the mothership and this isn't an answer but I just wanted
> to comment.
>
>  "I'm willing to start using zfs on osx (and sharemy experience here) as
> soon as it gets stable enough for daily usage (ok. maybe a tech-aware
> usage)."
>
>  It's already there. Your own mileage may vary, I have a two disk zpool in
> my mac pro for my home dir, /opt, and storage.
>  Other than the finder weirdness I have no serious problems. No panics
> triggered by anything I do on a daily basis. I haven't had a single panic on
> the mac pro since I started using the current build. That's not to say that
> it's perfect, just that nothing I do triggers it.  Depending on your needs
> and configuration it may be just fine for what you do, or it may not.
>
>  So I would tell you what I'd tell anyone. Give it a shot.
>
>
>
>
>  Lorenzo Perone wrote:
>
> > Hello,
> >
> > I'm looking forward on deploying ZFS fileystems on Mac OS X 10.5
> > environments (both client and server), as I've already done with Solaris
> > and FreeBSD (the latter on development systems).
> > There's no doubt to me that ZFS is a great idea and a huge leap forward in
> > Mac OS X filesystems, and I'm willing to start using zfs on osx (and share
> > my experience here) as soon as it gets stable enough for daily usage (ok.
> > maybe a tech-aware usage).
> >
> > Since there's nothing on the website about a timeline or a roadmap, and
> > since the current open bugs on trac are anything but encouraging, I'd like
> > to dare making one question here: is there anybody on this list from the
> > "mothership" allowed to make statements on the progress/current status on
> > those bugs and on zfs on Mac OS X in general?
> >
> >
> > Regards,
> >
> > Lorenzo
> >
> >
> >
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss@lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> >
> >
>
>  _______________________________________________
>  zfs-discuss mailing list
>  zfs-discuss@lists.macosforge.org
>  http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
From mail at jameslegg.co.uk  Sat Apr  5 17:28:11 2008
From: mail at jameslegg.co.uk (James Legg)
Date: Sat Apr  5 17:26:51 2008
Subject: [zfs-discuss] Kernel Panics
In-Reply-To: <47F6B9F4.4040500@jrv.org>
References: <c61bd13c0804031858x362fb3cbmabd3bba9fe5004fd@mail.gmail.com>
	<0A113794-353C-45B6-AA9E-C994C3CAC03C@apple.com>
	<47F6B9F4.4040500@jrv.org>
Message-ID: <026CF653-3A84-43EF-90A3-EDD98AE25682@jameslegg.co.uk>

James whats your failmode property set to on your pool?

root@frank / $ zpool get failmode tank
NAME  PROPERTY  VALUE     SOURCE
tank  failmode  wait      default

Cheers
James

On 5 Apr 2008, at 00:29, James R. Van Artsdalen wrote:

> No?l Dellofano wrote:
>> This panic is from a device either being removed or failing to  
>> respond.  Since ZFS is trying to write to it and it's a non  
>> replicated pool, then the write *not* flagged as ZIO_FLAG_CANFAIL.   
>> Since the write must succeed and the drive appears to either be  
>> missing or malfunctioning, then a panic ensues.  We are working on  
>> changing this behaviour such that instead of panicking we will  
>> handle this error more gracefully while still trying to maintain on  
>> disk consistency.
>>
> I don't understand this.  The on-disk format is always consistent,  
> right?  As I understand it that failed write can be discarded (so  
> long as subsequent I/O is not written!) and the on-disk image  
> remains consistent?  I'm having trouble seeing how this "write  
> failure" event differs from a "loss of power" event as far as on- 
> disk consistency goes.
>
>
> As an aside, I assume ZFS requires the underlying devices honor  
> either write fencing or ordering of some kind in order to keep the  
> on-disk structures consistent.  Is it known that the Firewire and  
> USB drivers in OSX honor this requirement?
>
> I have had a couple of data loss / pool corruption events  
> experimenting with ZFS on FreeBSD in a VMware machine on OSX.  My  
> assumption is that VMware is not passing through some write-fencing  
> from ZFS/FreeBSD to OSX correctly.  But I'm wondering if the OSX  
> drivers have been looked at to make sure whatever it is ZFS needs is  
> met by current OSX drivers.  What the disk drive itself does to  
> fenced/ordered writes is another problem entirely...
>
> I haven't had any data loss or data corruption under OSX yet, nor  
> any panics (but then again I have 4GB of RAM) with correctly labeled  
> disks.
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From mail at jameslegg.co.uk  Sat Apr  5 17:41:31 2008
From: mail at jameslegg.co.uk (James Legg)
Date: Sat Apr  5 17:40:17 2008
Subject: [zfs-discuss] Kernel Panics
In-Reply-To: <026CF653-3A84-43EF-90A3-EDD98AE25682@jameslegg.co.uk>
References: <c61bd13c0804031858x362fb3cbmabd3bba9fe5004fd@mail.gmail.com>
	<0A113794-353C-45B6-AA9E-C994C3CAC03C@apple.com>
	<47F6B9F4.4040500@jrv.org>
	<026CF653-3A84-43EF-90A3-EDD98AE25682@jameslegg.co.uk>
Message-ID: <16008F0A-18FB-43F0-9879-D93F2D2B8DDC@jameslegg.co.uk>

Sorry James,

I just read back up the thread again and it wasn't you that had the  
panic originally (I should maybe go and get some sleep).

Maybe Jo can take a look at his failmode property.

I'm guessing it is set to panic.

Cheers
James

On 6 Apr 2008, at 01:28, James Legg wrote:

> James whats your failmode property set to on your pool?
>
> root@frank / $ zpool get failmode tank
> NAME  PROPERTY  VALUE     SOURCE
> tank  failmode  wait      default
>
> Cheers
> James
>
> On 5 Apr 2008, at 00:29, James R. Van Artsdalen wrote:
>
>> No?l Dellofano wrote:
>>> This panic is from a device either being removed or failing to  
>>> respond.  Since ZFS is trying to write to it and it's a non  
>>> replicated pool, then the write *not* flagged as  
>>> ZIO_FLAG_CANFAIL.  Since the write must succeed and the drive  
>>> appears to either be missing or malfunctioning, then a panic  
>>> ensues.  We are working on changing this behaviour such that  
>>> instead of panicking we will handle this error more gracefully  
>>> while still trying to maintain on disk consistency.
>>>
>> I don't understand this.  The on-disk format is always consistent,  
>> right?  As I understand it that failed write can be discarded (so  
>> long as subsequent I/O is not written!) and the on-disk image  
>> remains consistent?  I'm having trouble seeing how this "write  
>> failure" event differs from a "loss of power" event as far as on- 
>> disk consistency goes.
>>
>>
>> As an aside, I assume ZFS requires the underlying devices honor  
>> either write fencing or ordering of some kind in order to keep the  
>> on-disk structures consistent.  Is it known that the Firewire and  
>> USB drivers in OSX honor this requirement?
>>
>> I have had a couple of data loss / pool corruption events  
>> experimenting with ZFS on FreeBSD in a VMware machine on OSX.  My  
>> assumption is that VMware is not passing through some write-fencing  
>> from ZFS/FreeBSD to OSX correctly.  But I'm wondering if the OSX  
>> drivers have been looked at to make sure whatever it is ZFS needs  
>> is met by current OSX drivers.  What the disk drive itself does to  
>> fenced/ordered writes is another problem entirely...
>>
>> I haven't had any data loss or data corruption under OSX yet, nor  
>> any panics (but then again I have 4GB of RAM) with correctly  
>> labeled disks.
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From mail at jameslegg.co.uk  Sat Apr  5 18:59:16 2008
From: mail at jameslegg.co.uk (James Legg)
Date: Sat Apr  5 18:57:55 2008
Subject: [zfs-discuss] Kernel Panics
In-Reply-To: <56CE525C-E53A-4173-A624-993EC9BA0117@bandlem.com>
References: <c61bd13c0804031858x362fb3cbmabd3bba9fe5004fd@mail.gmail.com>
	<0A113794-353C-45B6-AA9E-C994C3CAC03C@apple.com>
	<47F6B9F4.4040500@jrv.org>
	<026CF653-3A84-43EF-90A3-EDD98AE25682@jameslegg.co.uk>
	<56CE525C-E53A-4173-A624-993EC9BA0117@bandlem.com>
Message-ID: <0C9DD2F3-62EF-45F0-A265-6CAE13DDF6BC@jameslegg.co.uk>

failmode is set on my solaris zfs box (I don't have a mac with read  
write installed handy)

Looking a bit harder the property was implemented in build 77 of  
Nevada (Solaris 11)
http://opensolaris.org/os/community/arc/caselog/2007/567/

the sync doc in the zfs code browser states that the code in osx at  
the moment is in build 72 of Nevada.
http://trac.macosforge.org/projects/zfs/browser/Open_Solaris_Synchronization

Up until that point zfs on solaris exhibited the same sort of  
behaviour (panics) when failing to write. Someone from Sun tried to  
explain the justification to me at one point - I sort of understand  
why a read failing might be cause for a panic I never really  
understood why a write failing was.

The gist of it would seem to be failures to read/write on an un- 
mirrored pool (it has no other disks to try) will panic at the moment,  
but I am *guessing* that is by design not by an error in the mac port  
of zfs. Thats my take on things - I could be wrong!

Cheers
James


On 6 Apr 2008, at 02:36, Alex Blewitt wrote:

> When I do that (on the pool), I get an error saying that there's no  
> such attribute 'failmode'. Where does that come from?
>
> apple[~] zpool get all dyskWorld
> NAME  PROPERTY     VALUE       SOURCE
> dyskWorld  bootfs       -           default
> dyskWorld  autoreplace  off         default
> dyskWorld  delegation   off         default
>
> Alex
>
> On Apr 6, 2008, at 01:28, James Legg wrote:
>
>> James whats your failmode property set to on your pool?
>>
>> root@frank / $ zpool get failmode tank
>> NAME  PROPERTY  VALUE     SOURCE
>> tank  failmode  wait      default
>>
>> Cheers
>> James
>>
>> On 5 Apr 2008, at 00:29, James R. Van Artsdalen wrote:
>>
>>> No?l Dellofano wrote:
>>>> This panic is from a device either being removed or failing to  
>>>> respond.  Since ZFS is trying to write to it and it's a non  
>>>> replicated pool, then the write *not* flagged as  
>>>> ZIO_FLAG_CANFAIL.  Since the write must succeed and the drive  
>>>> appears to either be missing or malfunctioning, then a panic  
>>>> ensues.  We are working on changing this behaviour such that  
>>>> instead of panicking we will handle this error more gracefully  
>>>> while still trying to maintain on disk consistency.
>>>>
>>> I don't understand this.  The on-disk format is always consistent,  
>>> right?  As I understand it that failed write can be discarded (so  
>>> long as subsequent I/O is not written!) and the on-disk image  
>>> remains consistent?  I'm having trouble seeing how this "write  
>>> failure" event differs from a "loss of power" event as far as on- 
>>> disk consistency goes.
>>>
>>>
>>> As an aside, I assume ZFS requires the underlying devices honor  
>>> either write fencing or ordering of some kind in order to keep the  
>>> on-disk structures consistent.  Is it known that the Firewire and  
>>> USB drivers in OSX honor this requirement?
>>>
>>> I have had a couple of data loss / pool corruption events  
>>> experimenting with ZFS on FreeBSD in a VMware machine on OSX.  My  
>>> assumption is that VMware is not passing through some write- 
>>> fencing from ZFS/FreeBSD to OSX correctly.  But I'm wondering if  
>>> the OSX drivers have been looked at to make sure whatever it is  
>>> ZFS needs is met by current OSX drivers.  What the disk drive  
>>> itself does to fenced/ordered writes is another problem entirely...
>>>
>>> I haven't had any data loss or data corruption under OSX yet, nor  
>>> any panics (but then again I have 4GB of RAM) with correctly  
>>> labeled disks.
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

From sah.list at gmail.com  Sat Apr  5 23:20:59 2008
From: sah.list at gmail.com (Sean Hafeez)
Date: Sat Apr  5 23:19:44 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>
References: <47E33231.40107@jrv.org>
	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>
	<47E42E60.6040506@jrv.org>
	<6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>
Message-ID: <4AADF1E6-FC85-4829-9D20-9E2CAF2B16C9@gmail.com>

So I have been ripping my DVDs to watch on my AppleTV. In the past I  
have stored the files to be converted with Handbrake on my RAID box. I  
switched that box over to ZFS a few weeks ago. Handbrake seems to  
choke on the files when trying to open them now. If I copy them over  
to an HFS+ volume it works just fine.

Thanks!

From lu at cachescrubber.org  Sun Apr  6 03:44:43 2008
From: lu at cachescrubber.org (Lars Uffmann)
Date: Sun Apr  6 03:43:25 2008
Subject: [zfs-discuss] /dev/zfs permissions
Message-ID: <683B94E7-2925-419E-A013-DA2C00BCDEF9@cachescrubber.org>


while playing with zfs on OS X I found that  /dev/zfs is world writable:

monstermac:~ lu$ ls -ld /dev/zfs
crw-rw-rw-  1 root  wheel   31,   0  6 Apr 12:11 /dev/zfs

This means any unprivileged user on the system can list, create and  
drop filesystems on existing zpools.

monstermac:etc lu$ id
uid=503(lu) gid=20(staff) groups=20(staff),98(_lpadmin), 
101(com.apple.sharepoint.group.1),102(com.apple.sharepoint.group.2)

monstermac:etc lu$ zfs list
NAME           USED  AVAIL  REFER  MOUNTPOINT
Data          10,0G   281G  9,57G  /Volumes/Data
Data/opt       465M  14,5G   465M  /opt
Data/package    19K  15,0G    19K  /package

monstermac:etc lu$ sudo chmod 660 /dev/zfs

monstermac:etc lu$ zfs list
internal error: failed to initialize ZFS library

Since Mac OS X uses devfs, changes to the device permissions won't   
persist after a reboot.

I have no clue whether the most OS X like solution to permanently set  
the permissions for /dev/zfs would be to

- somehow configure devfs (how ?)

- somehow configure the zfs driver (how?, I see /etc/zfs/ exists)

- simply add a lauchd item / startup script

Bye, Lars
From robert at rehnmark.net  Sun Apr  6 07:35:11 2008
From: robert at rehnmark.net (Robert Rehnmark)
Date: Sun Apr  6 07:33:54 2008
Subject: [zfs-discuss] Performance problems - Tips?
Message-ID: <A9AF4F48-446D-4B10-B0E1-69B4866153EC@rehnmark.net>

When I copy files over the network to/from my server (10.5.2 client  
single-disk volume) I get 66-70 MB/sec.
If I do the same thing to the same machine with a single-disk zpool I  
get 0-30/40 MB/sec.
To a zpool mirror I get 0-55 MB/sec.
The performance/operation is very "janky" and when the speed is up the  
CPU use allmost  tops out too.
Does this have anything to do with caching etc.?
The pools are version 8, allmost empty and made from the same disks.

I want to be able to use ZFS but if I am not able to fix the  
performance problems HFS+ with daily backups will be preferred.

I want to hear about ather peoples experiences and tips, please.

/Robert
From me at joedunn.com  Sun Apr  6 08:04:32 2008
From: me at joedunn.com (Joe Dunn)
Date: Sun Apr  6 08:03:07 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <4AADF1E6-FC85-4829-9D20-9E2CAF2B16C9@gmail.com>
References: <47E33231.40107@jrv.org>
	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>
	<47E42E60.6040506@jrv.org>
	<6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>
	<4AADF1E6-FC85-4829-9D20-9E2CAF2B16C9@gmail.com>
Message-ID: <c61bd13c0804060804h22a131f1oa5924caeca084aa7@mail.gmail.com>

Hi Sean

Is the volume mirrored or in raidz configuration? Anything in your syslog
about what is happening?

Cheers

Joe

On Sun, Apr 6, 2008 at 2:20 AM, Sean Hafeez <sah.list@gmail.com> wrote:

> So I have been ripping my DVDs to watch on my AppleTV. In the past I have
> stored the files to be converted with Handbrake on my RAID box. I switched
> that box over to ZFS a few weeks ago. Handbrake seems to choke on the files
> when trying to open them now. If I copy them over to an HFS+ volume it works
> just fine.
>
> Thanks!
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080406/c7ef64b5/attachment.html
From dloose at gmail.com  Sun Apr  6 10:50:55 2008
From: dloose at gmail.com (Dave Loose)
Date: Sun Apr  6 10:49:29 2008
Subject: [zfs-discuss] Version incompatibility when importing zpool from
	Solaris
Message-ID: <4bc43a2b0804061050w292384bfn229eb15023363003@mail.gmail.com>

About a year ago, I put a Solaris box together to play with ZFS. I
mainly use it to serve my iTunes library and DVD rips. I used samba
for this, and I was continually frustrated by its flakiness. Both
samba and Solaris's built-in smb server don't seem to play well with
the other computers in my house (all of which are Macs).

I finally bit the bullet and bought a Mac Pro (I'd been needing a new
desktop for a while) and I decided I'd try to migrate the zpool to my
new machine. The problem is that I upgraded my Solaris machine rather
frequently and now when I try to 'zpool import storage' I get this
message: cannot import 'storage': pool is formatted using a newer ZFS
version

I kept my Solaris machine pretty current. It has Nevada release 84 and
the zpool format is whatever was most current for that release. I
don't remember which version it is off the top of my head.

Is there any way for me to get my zfs volume running under OS X? I
read the FAQ and it seems to suggest that OS X creates new zpools
using an older version, but is there any way to import a newer
version? Is there any way to "downgrade" the version?

Thanks for the help,

Dave
From alex.blewitt at gmail.com  Sun Apr  6 10:55:57 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Sun Apr  6 10:54:38 2008
Subject: [zfs-discuss] Version incompatibility when importing zpool from
	Solaris
In-Reply-To: <4bc43a2b0804061050w292384bfn229eb15023363003@mail.gmail.com>
References: <4bc43a2b0804061050w292384bfn229eb15023363003@mail.gmail.com>
Message-ID: <8002085F-7B16-41D8-BFB2-CD7FEA1C8B5E@gmail.com>

By default, the Mac OS X read-only drivers create filesystem pools  
with version 6 on disk format. The read-only version that ships with  
Mac OS X is a little old; I don't know what version it supports up to.

You can download a newer build from the macosforge site; I've packaged  
it up as a binary installer (from http://alblue.blogspot.com/2008/03/zfs-on-mac.html) 
  which can import version 8  on disk format. I don't know what  
Solaris had a year ago, but I'd be surprised if it was newer than V8.

I don't believe there's a way of downgrading short of recreating and  
importing, but if you can't do the import in the first place that's  
not likely to help.

Alex

On Apr 6, 2008, at 18:50, Dave Loose wrote:

> About a year ago, I put a Solaris box together to play with ZFS. I
> mainly use it to serve my iTunes library and DVD rips. I used samba
> for this, and I was continually frustrated by its flakiness. Both
> samba and Solaris's built-in smb server don't seem to play well with
> the other computers in my house (all of which are Macs).
>
> I finally bit the bullet and bought a Mac Pro (I'd been needing a new
> desktop for a while) and I decided I'd try to migrate the zpool to my
> new machine. The problem is that I upgraded my Solaris machine rather
> frequently and now when I try to 'zpool import storage' I get this
> message: cannot import 'storage': pool is formatted using a newer ZFS
> version
>
> I kept my Solaris machine pretty current. It has Nevada release 84 and
> the zpool format is whatever was most current for that release. I
> don't remember which version it is off the top of my head.
>
> Is there any way for me to get my zfs volume running under OS X? I
> read the FAQ and it seems to suggest that OS X creates new zpools
> using an older version, but is there any way to import a newer
> version? Is there any way to "downgrade" the version?
>
> Thanks for the help,
>
> Dave
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From mattsnow at gmail.com  Sun Apr  6 16:14:08 2008
From: mattsnow at gmail.com (Matt Snow)
Date: Sun Apr  6 16:12:41 2008
Subject: [zfs-discuss] Version incompatibility when importing zpool from
	Solaris
In-Reply-To: <4bc43a2b0804061050w292384bfn229eb15023363003@mail.gmail.com>
References: <4bc43a2b0804061050w292384bfn229eb15023363003@mail.gmail.com>
Message-ID: <6879ebc80804061614q308a2a31wcf862551dab9dc1f@mail.gmail.com>

Hi Dave,
I too was in this same situation and i've not been able to find a way to
'downgrade' ZFS versions on an existing pool. the darwin ZFS port would need
to merge version 12 changes (or whatever the bleeding edge is in nevada) and
start from there.

I'll probably end up migrating my fileserver back to Solaris for more stable
and better performing ZFS since things on my setup are not as stable as I
would like it to be.

..Matt

On Sun, Apr 6, 2008 at 10:50 AM, Dave Loose <dloose@gmail.com> wrote:

> About a year ago, I put a Solaris box together to play with ZFS. I
> mainly use it to serve my iTunes library and DVD rips. I used samba
> for this, and I was continually frustrated by its flakiness. Both
> samba and Solaris's built-in smb server don't seem to play well with
> the other computers in my house (all of which are Macs).
>
> I finally bit the bullet and bought a Mac Pro (I'd been needing a new
> desktop for a while) and I decided I'd try to migrate the zpool to my
> new machine. The problem is that I upgraded my Solaris machine rather
> frequently and now when I try to 'zpool import storage' I get this
> message: cannot import 'storage': pool is formatted using a newer ZFS
> version
>
> I kept my Solaris machine pretty current. It has Nevada release 84 and
> the zpool format is whatever was most current for that release. I
> don't remember which version it is off the top of my head.
>
> Is there any way for me to get my zfs volume running under OS X? I
> read the FAQ and it seems to suggest that OS X creates new zpools
> using an older version, but is there any way to import a newer
> version? Is there any way to "downgrade" the version?
>
> Thanks for the help,
>
> Dave
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080406/9e2209ef/attachment-0001.html
From sah.list at gmail.com  Sun Apr  6 20:08:57 2008
From: sah.list at gmail.com (Sean Hafeez)
Date: Sun Apr  6 20:07:31 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <c61bd13c0804060804h22a131f1oa5924caeca084aa7@mail.gmail.com>
References: <47E33231.40107@jrv.org>
	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>
	<47E42E60.6040506@jrv.org>
	<6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>
	<4AADF1E6-FC85-4829-9D20-9E2CAF2B16C9@gmail.com>
	<c61bd13c0804060804h22a131f1oa5924caeca084aa7@mail.gmail.com>
Message-ID: <8C0F3494-FB86-431C-9872-3B57B5C2BB2B@gmail.com>

RAIDZ
Here is the output from the Handbrake GUI

[20:03:36] hb_init: checking cpu count
[20:03:36] hb_init: starting libhb thread
[20:03:36] thread b0147000 started ("libhb")
[20:04:05] macgui: trying to open video_ts folder (parent directory  
chosen)

Then it just stops like I never pointed it to the DVD image.

Here is the system.log:

Apr  6 20:00:03 rioja MacTheRipper[661]: MTR: DVDCopyTileVobX (ts)1 (v)4
Apr  6 20:03:09 rioja MacTheRipper[661]: MTR: copyContent
Apr  6 20:03:11 rioja MacTheRipper[661]: we got here
Apr  6 20:03:11 rioja MacTheRipper[661]: MTR: resetUI_state
Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: extractMethod
Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: getDiskSize
Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: DVDFreeTitlesInfo
Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: DVDFreeTitleSetInfo
Apr  6 20:03:22 rioja MacTheRipper[661]: MTR: popupControl
Apr  6 20:03:31 rioja MacTheRipper[661]: MTR: savePrefs
Apr  6 20:04:05 rioja HandBrake[734]: *** +[NSString  
stringWithCString:]: NULL cString


zpool list:

NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
data2                  1.16T    672G    518G    56%  ONLINE     -



-Sean


On Apr 6, 2008, at 8:04 AM, Joe Dunn wrote:

> Hi Sean
>
> Is the volume mirrored or in raidz configuration? Anything in your  
> syslog about what is happening?
>
> Cheers
>
> Joe
>
> On Sun, Apr 6, 2008 at 2:20 AM, Sean Hafeez <sah.list@gmail.com>  
> wrote:
> So I have been ripping my DVDs to watch on my AppleTV. In the past I  
> have stored the files to be converted with Handbrake on my RAID box.  
> I switched that box over to ZFS a few weeks ago. Handbrake seems to  
> choke on the files when trying to open them now. If I copy them over  
> to an HFS+ volume it works just fine.
>
> Thanks!
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080406/8309854b/attachment.html
From me at joedunn.com  Sun Apr  6 22:49:36 2008
From: me at joedunn.com (Joe Dunn)
Date: Sun Apr  6 22:48:08 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <8C0F3494-FB86-431C-9872-3B57B5C2BB2B@gmail.com>
References: <47E33231.40107@jrv.org>
	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>
	<47E42E60.6040506@jrv.org>
	<6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>
	<4AADF1E6-FC85-4829-9D20-9E2CAF2B16C9@gmail.com>
	<c61bd13c0804060804h22a131f1oa5924caeca084aa7@mail.gmail.com>
	<8C0F3494-FB86-431C-9872-3B57B5C2BB2B@gmail.com>
Message-ID: <c61bd13c0804062249r8e2b1a8iec50b66665cc710b@mail.gmail.com>

What version of handbrake? Might be something to tell the handbrake
developer about. I see this message listed on apple's website

http://developer.apple.com/documentation/Cocoa/Reference/Foundation/Classes/NSString_Class/DeprecationAppendix/AppendixADeprecatedAPI.html

Seems like that got rid of that in 10.4.

I could be totally wrong, and if so hopefully someone smarter then me points
it out :)

Joe

On Sun, Apr 6, 2008 at 11:08 PM, Sean Hafeez <sah.list@gmail.com> wrote:

> RAIDZ
> Here is the output from the Handbrake GUI
>
> [20:03:36] hb_init: checking cpu count
> [20:03:36] hb_init: starting libhb thread
> [20:03:36] thread b0147000 started ("libhb")
> [20:04:05] macgui: trying to open video_ts folder (parent directory
> chosen)
>
> Then it just stops like I never pointed it to the DVD image.
>
> Here is the system.log:
>
> Apr  6 20:00:03 rioja MacTheRipper[661]: MTR: DVDCopyTileVobX (ts)1 (v)4
> Apr  6 20:03:09 rioja MacTheRipper[661]: MTR: copyContent
> Apr  6 20:03:11 rioja MacTheRipper[661]: we got here
> Apr  6 20:03:11 rioja MacTheRipper[661]: MTR: resetUI_state
> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: extractMethod
> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: getDiskSize
> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: DVDFreeTitlesInfo
> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: DVDFreeTitleSetInfo
> Apr  6 20:03:22 rioja MacTheRipper[661]: MTR: popupControl
> Apr  6 20:03:31 rioja MacTheRipper[661]: MTR: savePrefs
> Apr  6 20:04:05 rioja HandBrake[734]: *** +[NSString stringWithCString:]:
> NULL cString
>
>
> zpool list:
>
> NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
> data2                  1.16T    672G    518G    56%  ONLINE     -
>
>
>
> -Sean
>
>
> On Apr 6, 2008, at 8:04 AM, Joe Dunn wrote:
>
> Hi Sean
>
> Is the volume mirrored or in raidz configuration? Anything in your syslog
> about what is happening?
>
> Cheers
>
> Joe
>
> On Sun, Apr 6, 2008 at 2:20 AM, Sean Hafeez <sah.list@gmail.com> wrote:
>
> > So I have been ripping my DVDs to watch on my AppleTV. In the past I
> > have stored the files to be converted with Handbrake on my RAID box. I
> > switched that box over to ZFS a few weeks ago. Handbrake seems to choke on
> > the files when trying to open them now. If I copy them over to an HFS+
> > volume it works just fine.
> >
> > Thanks!
> >
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss@lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> >
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080407/e322a0d8/attachment.html
From sah.list at gmail.com  Sun Apr  6 23:47:06 2008
From: sah.list at gmail.com (Sean Hafeez)
Date: Sun Apr  6 23:45:40 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <c61bd13c0804062249r8e2b1a8iec50b66665cc710b@mail.gmail.com>
References: <47E33231.40107@jrv.org>
	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>
	<47E42E60.6040506@jrv.org>
	<6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>
	<4AADF1E6-FC85-4829-9D20-9E2CAF2B16C9@gmail.com>
	<c61bd13c0804060804h22a131f1oa5924caeca084aa7@mail.gmail.com>
	<8C0F3494-FB86-431C-9872-3B57B5C2BB2B@gmail.com>
	<c61bd13c0804062249r8e2b1a8iec50b66665cc710b@mail.gmail.com>
Message-ID: <6DFAF372-EFF8-4E58-8B23-69F1161DD807@gmail.com>

I goofed - you can ignore the system log. That is from the DVD ripper  
prg, not the Handbrake encoder. So no errors there. It just does not  
work. Does not seem to like something on ZFS.

-Sean

On Apr 6, 2008, at 10:49 PM, Joe Dunn wrote:

> What version of handbrake? Might be something to tell the handbrake  
> developer about. I see this message listed on apple's website
>
> http://developer.apple.com/documentation/Cocoa/Reference/Foundation/Classes/NSString_Class/DeprecationAppendix/AppendixADeprecatedAPI.html
>
> Seems like that got rid of that in 10.4.
>
> I could be totally wrong, and if so hopefully someone smarter then  
> me points it out :)
>
> Joe
>
> On Sun, Apr 6, 2008 at 11:08 PM, Sean Hafeez <sah.list@gmail.com>  
> wrote:
> RAIDZ
> Here is the output from the Handbrake GUI
>
> [20:03:36] hb_init: checking cpu count
> [20:03:36] hb_init: starting libhb thread
> [20:03:36] thread b0147000 started ("libhb")
> [20:04:05] macgui: trying to open video_ts folder (parent directory  
> chosen)
>
> Then it just stops like I never pointed it to the DVD image.
>
> Here is the system.log:
>
> Apr  6 20:00:03 rioja MacTheRipper[661]: MTR: DVDCopyTileVobX (ts)1  
> (v)4
> Apr  6 20:03:09 rioja MacTheRipper[661]: MTR: copyContent
> Apr  6 20:03:11 rioja MacTheRipper[661]: we got here
> Apr  6 20:03:11 rioja MacTheRipper[661]: MTR: resetUI_state
> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: extractMethod
> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: getDiskSize
> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: DVDFreeTitlesInfo
> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: DVDFreeTitleSetInfo
> Apr  6 20:03:22 rioja MacTheRipper[661]: MTR: popupControl
> Apr  6 20:03:31 rioja MacTheRipper[661]: MTR: savePrefs
> Apr  6 20:04:05 rioja HandBrake[734]: *** +[NSString  
> stringWithCString:]: NULL cString
>
>
> zpool list:
>
> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
> ALTROOT
> data2                  1.16T    672G    518G    56%  ONLINE     -
>
>
>
> -Sean
>
>
> On Apr 6, 2008, at 8:04 AM, Joe Dunn wrote:
>> Hi Sean
>>
>> Is the volume mirrored or in raidz configuration? Anything in your  
>> syslog about what is happening?
>>
>> Cheers
>>
>> Joe
>>
>> On Sun, Apr 6, 2008 at 2:20 AM, Sean Hafeez <sah.list@gmail.com>  
>> wrote:
>> So I have been ripping my DVDs to watch on my AppleTV. In the past  
>> I have stored the files to be converted with Handbrake on my RAID  
>> box. I switched that box over to ZFS a few weeks ago. Handbrake  
>> seems to choke on the files when trying to open them now. If I copy  
>> them over to an HFS+ volume it works just fine.
>>
>> Thanks!
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080406/290170f9/attachment-0001.html
From alex.blewitt at gmail.com  Mon Apr  7 00:57:52 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Mon Apr  7 00:56:34 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <6DFAF372-EFF8-4E58-8B23-69F1161DD807@gmail.com>
References: <47E33231.40107@jrv.org>
	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>
	<47E42E60.6040506@jrv.org>
	<6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>
	<4AADF1E6-FC85-4829-9D20-9E2CAF2B16C9@gmail.com>
	<c61bd13c0804060804h22a131f1oa5924caeca084aa7@mail.gmail.com>
	<8C0F3494-FB86-431C-9872-3B57B5C2BB2B@gmail.com>
	<c61bd13c0804062249r8e2b1a8iec50b66665cc710b@mail.gmail.com>
	<6DFAF372-EFF8-4E58-8B23-69F1161DD807@gmail.com>
Message-ID: <CFA57BDA-2C10-4CD5-B899-D425AD960427@gmail.com>

I presume it's a local, rather than networked share? I've found that  
ZFS over NFS doesn't like locking on my setup. That fails when  
extended attributes are set (e.g. via xattr). It might be doing the  
same thing with Handbrake.

Alex.

On Apr 7, 2008, at 07:47, Sean Hafeez wrote:

> I goofed - you can ignore the system log. That is from the DVD  
> ripper prg, not the Handbrake encoder. So no errors there. It just  
> does not work. Does not seem to like something on ZFS.
>
> -Sean
>
> On Apr 6, 2008, at 10:49 PM, Joe Dunn wrote:
>> What version of handbrake? Might be something to tell the handbrake  
>> developer about. I see this message listed on apple's website
>>
>> http://developer.apple.com/documentation/Cocoa/Reference/Foundation/Classes/NSString_Class/DeprecationAppendix/AppendixADeprecatedAPI.html
>>
>> Seems like that got rid of that in 10.4.
>>
>> I could be totally wrong, and if so hopefully someone smarter then  
>> me points it out :)
>>
>> Joe
>>
>> On Sun, Apr 6, 2008 at 11:08 PM, Sean Hafeez <sah.list@gmail.com>  
>> wrote:
>> RAIDZ
>> Here is the output from the Handbrake GUI
>>
>> [20:03:36] hb_init: checking cpu count
>> [20:03:36] hb_init: starting libhb thread
>> [20:03:36] thread b0147000 started ("libhb")
>> [20:04:05] macgui: trying to open video_ts folder (parent directory  
>> chosen)
>>
>> Then it just stops like I never pointed it to the DVD image.
>>
>> Here is the system.log:
>>
>> Apr  6 20:00:03 rioja MacTheRipper[661]: MTR: DVDCopyTileVobX (ts)1  
>> (v)4
>> Apr  6 20:03:09 rioja MacTheRipper[661]: MTR: copyContent
>> Apr  6 20:03:11 rioja MacTheRipper[661]: we got here
>> Apr  6 20:03:11 rioja MacTheRipper[661]: MTR: resetUI_state
>> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: extractMethod
>> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: getDiskSize
>> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: DVDFreeTitlesInfo
>> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: DVDFreeTitleSetInfo
>> Apr  6 20:03:22 rioja MacTheRipper[661]: MTR: popupControl
>> Apr  6 20:03:31 rioja MacTheRipper[661]: MTR: savePrefs
>> Apr  6 20:04:05 rioja HandBrake[734]: *** +[NSString  
>> stringWithCString:]: NULL cString
>>
>>
>> zpool list:
>>
>> NAME                    SIZE    USED   AVAIL    CAP  HEALTH      
>> ALTROOT
>> data2                  1.16T    672G    518G    56%  ONLINE     -
>>
>>
>>
>> -Sean
>>
>>
>> On Apr 6, 2008, at 8:04 AM, Joe Dunn wrote:
>>> Hi Sean
>>>
>>> Is the volume mirrored or in raidz configuration? Anything in your  
>>> syslog about what is happening?
>>>
>>> Cheers
>>>
>>> Joe
>>>
>>> On Sun, Apr 6, 2008 at 2:20 AM, Sean Hafeez <sah.list@gmail.com>  
>>> wrote:
>>> So I have been ripping my DVDs to watch on my AppleTV. In the past  
>>> I have stored the files to be converted with Handbrake on my RAID  
>>> box. I switched that box over to ZFS a few weeks ago. Handbrake  
>>> seems to choke on the files when trying to open them now. If I  
>>> copy them over to an HFS+ volume it works just fine.
>>>
>>> Thanks!
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>>
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080407/86905a80/attachment.html
From robert at rehnmark.net  Mon Apr  7 02:38:06 2008
From: robert at rehnmark.net (Robert Rehnmark)
Date: Mon Apr  7 02:36:57 2008
Subject: [zfs-discuss] Performance problems - AFP!
In-Reply-To: <A9AF4F48-446D-4B10-B0E1-69B4866153EC@rehnmark.net>
References: <A9AF4F48-446D-4B10-B0E1-69B4866153EC@rehnmark.net>
Message-ID: <F1CE4772-22AB-46EC-A755-39CEE0B588DA@rehnmark.net>

Ugh!
The pools work fine locally but when used over Apple File Sharing  
Protocol it tends to read/write in short bursts.
The effect is speeds cut in half or worse.

Can anybody verify this or maybe even share a solution?

Please.

/Robert



6 apr 2008 kl. 16:35 skrev Robert Rehnmark:
> When I copy files over the network to/from my server (10.5.2 client  
> single-disk volume) I get 66-70 MB/sec.
> If I do the same thing to the same machine with a single-disk zpool  
> I get 0-30/40 MB/sec.
> To a zpool mirror I get 0-55 MB/sec.
> The performance/operation is very "janky" and when the speed is up  
> the CPU use allmost  tops out too.
> Does this have anything to do with caching etc.?
> The pools are version 8, allmost empty and made from the same disks.
>
> I want to be able to use ZFS but if I am not able to fix the  
> performance problems HFS+ with daily backups will be preferred.
>
> I want to hear about ather peoples experiences and tips, please.
>
> /Robert
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From info at martin-hauser.net  Mon Apr  7 03:03:13 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Mon Apr  7 03:02:15 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <CFA57BDA-2C10-4CD5-B899-D425AD960427@gmail.com>
References: <47E33231.40107@jrv.org>
	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>
	<47E42E60.6040506@jrv.org>
	<6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>
	<4AADF1E6-FC85-4829-9D20-9E2CAF2B16C9@gmail.com>
	<c61bd13c0804060804h22a131f1oa5924caeca084aa7@mail.gmail.com>
	<8C0F3494-FB86-431C-9872-3B57B5C2BB2B@gmail.com>
	<c61bd13c0804062249r8e2b1a8iec50b66665cc710b@mail.gmail.com>
	<6DFAF372-EFF8-4E58-8B23-69F1161DD807@gmail.com>
	<CFA57BDA-2C10-4CD5-B899-D425AD960427@gmail.com>
Message-ID: <11DDD05B-E100-4115-A5A7-07224EECE11A@martin-hauser.net>

I do not have any problems with handbrake running on a local ZFS. I  
encode from and to ZFS volumes, always using a custom mp4 setup. I  
assume it's a problem with ZFS + networking ?

kind regards

Martin
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080407/c3e2bb80/PGP.bin
From james-zfsosx at jrv.org  Mon Apr  7 04:43:19 2008
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Mon Apr  7 04:41:54 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <11DDD05B-E100-4115-A5A7-07224EECE11A@martin-hauser.net>
References: <47E33231.40107@jrv.org>	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>	<47E42E60.6040506@jrv.org>	<6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>	<4AADF1E6-FC85-4829-9D20-9E2CAF2B16C9@gmail.com>	<c61bd13c0804060804h22a131f1oa5924caeca084aa7@mail.gmail.com>	<8C0F3494-FB86-431C-9872-3B57B5C2BB2B@gmail.com>	<c61bd13c0804062249r8e2b1a8iec50b66665cc710b@mail.gmail.com>	<6DFAF372-EFF8-4E58-8B23-69F1161DD807@gmail.com>	<CFA57BDA-2C10-4CD5-B899-D425AD960427@gmail.com>
	<11DDD05B-E100-4115-A5A7-07224EECE11A@martin-hauser.net>
Message-ID: <47FA08D7.2010901@jrv.org>

I can reproduce his problem here using the Handbrake 0.9.2 GUI with no 
network involved.  It hangs after clicking OK to select the source 
material - it doesn't get to the point of encoding anything.  Handbrake 
does respond to a cmd-Q and is using about 5% CPU at that point.  The 
source material is a DVD copied to the hard disk.  It selects source 
material OK from a "Mac OS Extended" disk.

Martin Hauser wrote:
> I do not have any problems with handbrake running on a local ZFS. I 
> encode from and to ZFS volumes, always using a custom mp4 setup. I 
> assume it's a problem with ZFS + networking ?
>
> kind regards
>
> Martin
> ------------------------------------------------------------------------
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>   
From info at martin-hauser.net  Mon Apr  7 05:16:27 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Mon Apr  7 05:15:16 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <47FA08D7.2010901@jrv.org>
References: <47E33231.40107@jrv.org>	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>	<47E42E60.6040506@jrv.org>	<6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>	<4AADF1E6-FC85-4829-9D20-9E2CAF2B16C9@gmail.com>	<c61bd13c0804060804h22a131f1oa5924caeca084aa7@mail.gmail.com>	<8C0F3494-FB86-431C-9872-3B57B5C2BB2B@gmail.com>	<c61bd13c0804062249r8e2b1a8iec50b66665cc710b@mail.gmail.com>	<6DFAF372-EFF8-4E58-8B23-69F1161DD807@gmail.com>	<CFA57BDA-2C10-4CD5-B899-D425AD960427@gmail.com>
	<11DDD05B-E100-4115-A5A7-07224EECE11A@martin-hauser.net>
	<47FA08D7.2010901@jrv.org>
Message-ID: <C433CDE9-FEAF-4D7D-B8A3-7D5136FF96F1@martin-hauser.net>

Hmm,

I am encoding from .mpg files, maybe the difference is in directory  
handling, as DVD's are oriented that way?


On Apr 7, 2008, at 13:43 PM, James R. Van Artsdalen wrote:
> I can reproduce his problem here using the Handbrake 0.9.2 GUI with  
> no network involved.  It hangs after clicking OK to select the  
> source material - it doesn't get to the point of encoding anything.   
> Handbrake does respond to a cmd-Q and is using about 5% CPU at that  
> point.  The source material is a DVD copied to the hard disk.  It  
> selects source material OK from a "Mac OS Extended" disk.
>
> Martin Hauser wrote:
>> I do not have any problems with handbrake running on a local ZFS. I  
>> encode from and to ZFS volumes, always using a custom mp4 setup. I  
>> assume it's a problem with ZFS + networking ?
>>
>> kind regards
>>
>> Martin
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080407/0f2ebade/PGP-0001.bin
From schalk at unf.edu  Mon Apr  7 09:00:20 2008
From: schalk at unf.edu (Stuart Chalk)
Date: Mon Apr  7 08:58:51 2008
Subject: [zfs-discuss] Re: zfs-discuss Digest, Vol 4, Issue 8
In-Reply-To: <20080407121516.58FF7240AB6@lists.macosforge.org>
References: <20080407121516.58FF7240AB6@lists.macosforge.org>
Message-ID: <E19FD1F2-9D21-4B0B-B841-968615FB9587@unf.edu>

You have till tonight at 11pm.

On Apr 7, 2008, at 8:15 AM, zfs-discuss-request@lists.macosforge.org  
wrote:

> Send zfs-discuss mailing list submissions to
> 	zfs-discuss@lists.macosforge.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
> 	http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> or, via email, send a message with subject or body 'help' to
> 	zfs-discuss-request@lists.macosforge.org
>
> You can reach the person managing the list at
> 	zfs-discuss-owner@lists.macosforge.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of zfs-discuss digest..."
>
>
> Today's Topics:
>
>   1. Re: Handbrake on ZFS (Alex Blewitt)
>   2. Re: Performance problems - AFP! (Robert Rehnmark)
>   3. Re: Handbrake on ZFS (Martin Hauser)
>   4. Re: Handbrake on ZFS (James R. Van Artsdalen)
>   5. Re: Handbrake on ZFS (Martin Hauser)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Mon, 7 Apr 2008 08:57:52 +0100
> From: Alex Blewitt <alex.blewitt@gmail.com>
> Subject: Re: [zfs-discuss] Handbrake on ZFS
> To: Sean Hafeez <sah.list@gmail.com>
> Cc: zfs-discuss@lists.macosforge.org
> Message-ID: <CFA57BDA-2C10-4CD5-B899-D425AD960427@gmail.com>
> Content-Type: text/plain; charset="us-ascii"
>
> I presume it's a local, rather than networked share? I've found that
> ZFS over NFS doesn't like locking on my setup. That fails when
> extended attributes are set (e.g. via xattr). It might be doing the
> same thing with Handbrake.
>
> Alex.
>
> On Apr 7, 2008, at 07:47, Sean Hafeez wrote:
>
>> I goofed - you can ignore the system log. That is from the DVD
>> ripper prg, not the Handbrake encoder. So no errors there. It just
>> does not work. Does not seem to like something on ZFS.
>>
>> -Sean
>>
>> On Apr 6, 2008, at 10:49 PM, Joe Dunn wrote:
>>> What version of handbrake? Might be something to tell the handbrake
>>> developer about. I see this message listed on apple's website
>>>
>>> http://developer.apple.com/documentation/Cocoa/Reference/Foundation/Classes/NSString_Class/DeprecationAppendix/AppendixADeprecatedAPI.html
>>>
>>> Seems like that got rid of that in 10.4.
>>>
>>> I could be totally wrong, and if so hopefully someone smarter then
>>> me points it out :)
>>>
>>> Joe
>>>
>>> On Sun, Apr 6, 2008 at 11:08 PM, Sean Hafeez <sah.list@gmail.com>
>>> wrote:
>>> RAIDZ
>>> Here is the output from the Handbrake GUI
>>>
>>> [20:03:36] hb_init: checking cpu count
>>> [20:03:36] hb_init: starting libhb thread
>>> [20:03:36] thread b0147000 started ("libhb")
>>> [20:04:05] macgui: trying to open video_ts folder (parent directory
>>> chosen)
>>>
>>> Then it just stops like I never pointed it to the DVD image.
>>>
>>> Here is the system.log:
>>>
>>> Apr  6 20:00:03 rioja MacTheRipper[661]: MTR: DVDCopyTileVobX (ts)1
>>> (v)4
>>> Apr  6 20:03:09 rioja MacTheRipper[661]: MTR: copyContent
>>> Apr  6 20:03:11 rioja MacTheRipper[661]: we got here
>>> Apr  6 20:03:11 rioja MacTheRipper[661]: MTR: resetUI_state
>>> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: extractMethod
>>> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: getDiskSize
>>> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: DVDFreeTitlesInfo
>>> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: DVDFreeTitleSetInfo
>>> Apr  6 20:03:22 rioja MacTheRipper[661]: MTR: popupControl
>>> Apr  6 20:03:31 rioja MacTheRipper[661]: MTR: savePrefs
>>> Apr  6 20:04:05 rioja HandBrake[734]: *** +[NSString
>>> stringWithCString:]: NULL cString
>>>
>>>
>>> zpool list:
>>>
>>> NAME                    SIZE    USED   AVAIL    CAP  HEALTH
>>> ALTROOT
>>> data2                  1.16T    672G    518G    56%  ONLINE     -
>>>
>>>
>>>
>>> -Sean
>>>
>>>
>>> On Apr 6, 2008, at 8:04 AM, Joe Dunn wrote:
>>>> Hi Sean
>>>>
>>>> Is the volume mirrored or in raidz configuration? Anything in your
>>>> syslog about what is happening?
>>>>
>>>> Cheers
>>>>
>>>> Joe
>>>>
>>>> On Sun, Apr 6, 2008 at 2:20 AM, Sean Hafeez <sah.list@gmail.com>
>>>> wrote:
>>>> So I have been ripping my DVDs to watch on my AppleTV. In the past
>>>> I have stored the files to be converted with Handbrake on my RAID
>>>> box. I switched that box over to ZFS a few weeks ago. Handbrake
>>>> seems to choke on the files when trying to open them now. If I
>>>> copy them over to an HFS+ volume it works just fine.
>>>>
>>>> Thanks!
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>>
>>>
>>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080407/86905a80/attachment-0001.html
>
> ------------------------------
>
> Message: 2
> Date: Mon, 7 Apr 2008 11:38:06 +0200
> From: Robert Rehnmark <robert@rehnmark.net>
> Subject: Re: [zfs-discuss] Performance problems - AFP!
> To: ZFS on OSX mailing list mailing list
> 	<zfs-discuss@lists.macosforge.org>
> Message-ID: <F1CE4772-22AB-46EC-A755-39CEE0B588DA@rehnmark.net>
> Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes
>
> Ugh!
> The pools work fine locally but when used over Apple File Sharing
> Protocol it tends to read/write in short bursts.
> The effect is speeds cut in half or worse.
>
> Can anybody verify this or maybe even share a solution?
>
> Please.
>
> /Robert
>
>
>
> 6 apr 2008 kl. 16:35 skrev Robert Rehnmark:
>> When I copy files over the network to/from my server (10.5.2 client
>> single-disk volume) I get 66-70 MB/sec.
>> If I do the same thing to the same machine with a single-disk zpool
>> I get 0-30/40 MB/sec.
>> To a zpool mirror I get 0-55 MB/sec.
>> The performance/operation is very "janky" and when the speed is up
>> the CPU use allmost  tops out too.
>> Does this have anything to do with caching etc.?
>> The pools are version 8, allmost empty and made from the same disks.
>>
>> I want to be able to use ZFS but if I am not able to fix the
>> performance problems HFS+ with daily backups will be preferred.
>>
>> I want to hear about ather peoples experiences and tips, please.
>>
>> /Robert
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
>
>
> ------------------------------
>
> Message: 3
> Date: Mon, 7 Apr 2008 12:03:13 +0200
> From: Martin Hauser <info@martin-hauser.net>
> Subject: Re: [zfs-discuss] Handbrake on ZFS
> To: Sean Hafeez <sah.list@gmail.com>
> Cc: ZFS on OSX mailing list mailing list
> 	<zfs-discuss@lists.macosforge.org>
> Message-ID: <11DDD05B-E100-4115-A5A7-07224EECE11A@martin-hauser.net>
> Content-Type: text/plain; charset="us-ascii"
>
> I do not have any problems with handbrake running on a local ZFS. I
> encode from and to ZFS volumes, always using a custom mp4 setup. I
> assume it's a problem with ZFS + networking ?
>
> kind regards
>
> Martin
> -------------- next part --------------
> A non-text attachment was scrubbed...
> Name: PGP.sig
> Type: application/pgp-signature
> Size: 186 bytes
> Desc: This is a digitally signed message part
> Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080407/c3e2bb80/PGP-0001.bin
>
> ------------------------------
>
> Message: 4
> Date: Mon, 07 Apr 2008 06:43:19 -0500
> From: "James R. Van Artsdalen" <james-zfsosx@jrv.org>
> Subject: Re: [zfs-discuss] Handbrake on ZFS
> To: Martin Hauser <info@martin-hauser.net>
> Cc: ZFS on OSX mailing list mailing list
> 	<zfs-discuss@lists.macosforge.org>
> Message-ID: <47FA08D7.2010901@jrv.org>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
> I can reproduce his problem here using the Handbrake 0.9.2 GUI with no
> network involved.  It hangs after clicking OK to select the source
> material - it doesn't get to the point of encoding anything.   
> Handbrake
> does respond to a cmd-Q and is using about 5% CPU at that point.  The
> source material is a DVD copied to the hard disk.  It selects source
> material OK from a "Mac OS Extended" disk.
>
> Martin Hauser wrote:
>> I do not have any problems with handbrake running on a local ZFS. I
>> encode from and to ZFS volumes, always using a custom mp4 setup. I
>> assume it's a problem with ZFS + networking ?
>>
>> kind regards
>>
>> Martin
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>
>
> ------------------------------
>
> Message: 5
> Date: Mon, 7 Apr 2008 14:16:27 +0200
> From: Martin Hauser <info@martin-hauser.net>
> Subject: Re: [zfs-discuss] Handbrake on ZFS
> To: James R.Van Artsdalen <james-zfsosx@jrv.org>
> Cc: ZFS on OSX mailing list mailing list
> 	<zfs-discuss@lists.macosforge.org>
> Message-ID: <C433CDE9-FEAF-4D7D-B8A3-7D5136FF96F1@martin-hauser.net>
> Content-Type: text/plain; charset="us-ascii"
>
> Hmm,
>
> I am encoding from .mpg files, maybe the difference is in directory
> handling, as DVD's are oriented that way?
>
>
> On Apr 7, 2008, at 13:43 PM, James R. Van Artsdalen wrote:
>> I can reproduce his problem here using the Handbrake 0.9.2 GUI with
>> no network involved.  It hangs after clicking OK to select the
>> source material - it doesn't get to the point of encoding anything.
>> Handbrake does respond to a cmd-Q and is using about 5% CPU at that
>> point.  The source material is a DVD copied to the hard disk.  It
>> selects source material OK from a "Mac OS Extended" disk.
>>
>> Martin Hauser wrote:
>>> I do not have any problems with handbrake running on a local ZFS. I
>>> encode from and to ZFS volumes, always using a custom mp4 setup. I
>>> assume it's a problem with ZFS + networking ?
>>>
>>> kind regards
>>>
>>> Martin
>>> ------------------------------------------------------------------------
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>
> -------------- next part --------------
> A non-text attachment was scrubbed...
> Name: PGP.sig
> Type: application/pgp-signature
> Size: 186 bytes
> Desc: This is a digitally signed message part
> Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080407/0f2ebade/PGP.bin
>
> ------------------------------
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
>
> End of zfs-discuss Digest, Vol 4, Issue 8
> *****************************************

From zorg at sogeeky.net  Mon Apr  7 09:56:38 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Mon Apr  7 09:55:54 2008
Subject: [zfs-discuss] Re: zfs-discuss Digest, Vol 4, Issue 8
In-Reply-To: <E19FD1F2-9D21-4B0B-B841-968615FB9587@unf.edu>
References: <20080407121516.58FF7240AB6@lists.macosforge.org>
	<E19FD1F2-9D21-4B0B-B841-968615FB9587@unf.edu>
Message-ID: <E0315DDB-8EA0-47CF-810A-448BF1397304@sogeeky.net>

Then what?!?!?

On Apr 7, 2008, at 9:00 AM, Stuart Chalk <schalk@unf.edu> wrote:

> You have till tonight at 11pm.
>
> On Apr 7, 2008, at 8:15 AM, zfs-discuss-request@lists.macosforge.org  
> wrote:
>
>> Send zfs-discuss mailing list submissions to
>>    zfs-discuss@lists.macosforge.org
>>
>> To subscribe or unsubscribe via the World Wide Web, visit
>>    http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>> or, via email, send a message with subject or body 'help' to
>>    zfs-discuss-request@lists.macosforge.org
>>
>> You can reach the person managing the list at
>>    zfs-discuss-owner@lists.macosforge.org
>>
>> When replying, please edit your Subject line so it is more specific
>> than "Re: Contents of zfs-discuss digest..."
>>
>>
>> Today's Topics:
>>
>>  1. Re: Handbrake on ZFS (Alex Blewitt)
>>  2. Re: Performance problems - AFP! (Robert Rehnmark)
>>  3. Re: Handbrake on ZFS (Martin Hauser)
>>  4. Re: Handbrake on ZFS (James R. Van Artsdalen)
>>  5. Re: Handbrake on ZFS (Martin Hauser)
>>
>>
>> --- 
>> -------------------------------------------------------------------
>>
>> Message: 1
>> Date: Mon, 7 Apr 2008 08:57:52 +0100
>> From: Alex Blewitt <alex.blewitt@gmail.com>
>> Subject: Re: [zfs-discuss] Handbrake on ZFS
>> To: Sean Hafeez <sah.list@gmail.com>
>> Cc: zfs-discuss@lists.macosforge.org
>> Message-ID: <CFA57BDA-2C10-4CD5-B899-D425AD960427@gmail.com>
>> Content-Type: text/plain; charset="us-ascii"
>>
>> I presume it's a local, rather than networked share? I've found that
>> ZFS over NFS doesn't like locking on my setup. That fails when
>> extended attributes are set (e.g. via xattr). It might be doing the
>> same thing with Handbrake.
>>
>> Alex.
>>
>> On Apr 7, 2008, at 07:47, Sean Hafeez wrote:
>>
>>> I goofed - you can ignore the system log. That is from the DVD
>>> ripper prg, not the Handbrake encoder. So no errors there. It just
>>> does not work. Does not seem to like something on ZFS.
>>>
>>> -Sean
>>>
>>> On Apr 6, 2008, at 10:49 PM, Joe Dunn wrote:
>>>> What version of handbrake? Might be something to tell the handbrake
>>>> developer about. I see this message listed on apple's website
>>>>
>>>> http://developer.apple.com/documentation/Cocoa/Reference/Foundation/Classes/NSString_Class/DeprecationAppendix/AppendixADeprecatedAPI.html
>>>>
>>>> Seems like that got rid of that in 10.4.
>>>>
>>>> I could be totally wrong, and if so hopefully someone smarter then
>>>> me points it out :)
>>>>
>>>> Joe
>>>>
>>>> On Sun, Apr 6, 2008 at 11:08 PM, Sean Hafeez <sah.list@gmail.com>
>>>> wrote:
>>>> RAIDZ
>>>> Here is the output from the Handbrake GUI
>>>>
>>>> [20:03:36] hb_init: checking cpu count
>>>> [20:03:36] hb_init: starting libhb thread
>>>> [20:03:36] thread b0147000 started ("libhb")
>>>> [20:04:05] macgui: trying to open video_ts folder (parent directory
>>>> chosen)
>>>>
>>>> Then it just stops like I never pointed it to the DVD image.
>>>>
>>>> Here is the system.log:
>>>>
>>>> Apr  6 20:00:03 rioja MacTheRipper[661]: MTR: DVDCopyTileVobX (ts)1
>>>> (v)4
>>>> Apr  6 20:03:09 rioja MacTheRipper[661]: MTR: copyContent
>>>> Apr  6 20:03:11 rioja MacTheRipper[661]: we got here
>>>> Apr  6 20:03:11 rioja MacTheRipper[661]: MTR: resetUI_state
>>>> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: extractMethod
>>>> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: getDiskSize
>>>> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: DVDFreeTitlesInfo
>>>> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: DVDFreeTitleSetInfo
>>>> Apr  6 20:03:22 rioja MacTheRipper[661]: MTR: popupControl
>>>> Apr  6 20:03:31 rioja MacTheRipper[661]: MTR: savePrefs
>>>> Apr  6 20:04:05 rioja HandBrake[734]: *** +[NSString
>>>> stringWithCString:]: NULL cString
>>>>
>>>>
>>>> zpool list:
>>>>
>>>> NAME                    SIZE    USED   AVAIL    CAP  HEALTH
>>>> ALTROOT
>>>> data2                  1.16T    672G    518G    56%  ONLINE     -
>>>>
>>>>
>>>>
>>>> -Sean
>>>>
>>>>
>>>> On Apr 6, 2008, at 8:04 AM, Joe Dunn wrote:
>>>>> Hi Sean
>>>>>
>>>>> Is the volume mirrored or in raidz configuration? Anything in your
>>>>> syslog about what is happening?
>>>>>
>>>>> Cheers
>>>>>
>>>>> Joe
>>>>>
>>>>> On Sun, Apr 6, 2008 at 2:20 AM, Sean Hafeez <sah.list@gmail.com>
>>>>> wrote:
>>>>> So I have been ripping my DVDs to watch on my AppleTV. In the past
>>>>> I have stored the files to be converted with Handbrake on my RAID
>>>>> box. I switched that box over to ZFS a few weeks ago. Handbrake
>>>>> seems to choke on the files when trying to open them now. If I
>>>>> copy them over to an HFS+ volume it works just fine.
>>>>>
>>>>> Thanks!
>>>>>
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss@lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>>>
>>>>
>>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>> -------------- next part --------------
>> An HTML attachment was scrubbed...
>> URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080407/86905a80/attachment-0001.html
>>
>> ------------------------------
>>
>> Message: 2
>> Date: Mon, 7 Apr 2008 11:38:06 +0200
>> From: Robert Rehnmark <robert@rehnmark.net>
>> Subject: Re: [zfs-discuss] Performance problems - AFP!
>> To: ZFS on OSX mailing list mailing list
>>    <zfs-discuss@lists.macosforge.org>
>> Message-ID: <F1CE4772-22AB-46EC-A755-39CEE0B588DA@rehnmark.net>
>> Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes
>>
>> Ugh!
>> The pools work fine locally but when used over Apple File Sharing
>> Protocol it tends to read/write in short bursts.
>> The effect is speeds cut in half or worse.
>>
>> Can anybody verify this or maybe even share a solution?
>>
>> Please.
>>
>> /Robert
>>
>>
>>
>> 6 apr 2008 kl. 16:35 skrev Robert Rehnmark:
>>> When I copy files over the network to/from my server (10.5.2 client
>>> single-disk volume) I get 66-70 MB/sec.
>>> If I do the same thing to the same machine with a single-disk zpool
>>> I get 0-30/40 MB/sec.
>>> To a zpool mirror I get 0-55 MB/sec.
>>> The performance/operation is very "janky" and when the speed is up
>>> the CPU use allmost  tops out too.
>>> Does this have anything to do with caching etc.?
>>> The pools are version 8, allmost empty and made from the same disks.
>>>
>>> I want to be able to use ZFS but if I am not able to fix the
>>> performance problems HFS+ with daily backups will be preferred.
>>>
>>> I want to hear about ather peoples experiences and tips, please.
>>>
>>> /Robert
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>>
>>
>> ------------------------------
>>
>> Message: 3
>> Date: Mon, 7 Apr 2008 12:03:13 +0200
>> From: Martin Hauser <info@martin-hauser.net>
>> Subject: Re: [zfs-discuss] Handbrake on ZFS
>> To: Sean Hafeez <sah.list@gmail.com>
>> Cc: ZFS on OSX mailing list mailing list
>>    <zfs-discuss@lists.macosforge.org>
>> Message-ID: <11DDD05B-E100-4115-A5A7-07224EECE11A@martin-hauser.net>
>> Content-Type: text/plain; charset="us-ascii"
>>
>> I do not have any problems with handbrake running on a local ZFS. I
>> encode from and to ZFS volumes, always using a custom mp4 setup. I
>> assume it's a problem with ZFS + networking ?
>>
>> kind regards
>>
>> Martin
>> -------------- next part --------------
>> A non-text attachment was scrubbed...
>> Name: PGP.sig
>> Type: application/pgp-signature
>> Size: 186 bytes
>> Desc: This is a digitally signed message part
>> Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080407/c3e2bb80/PGP-0001.bin
>>
>> ------------------------------
>>
>> Message: 4
>> Date: Mon, 07 Apr 2008 06:43:19 -0500
>> From: "James R. Van Artsdalen" <james-zfsosx@jrv.org>
>> Subject: Re: [zfs-discuss] Handbrake on ZFS
>> To: Martin Hauser <info@martin-hauser.net>
>> Cc: ZFS on OSX mailing list mailing list
>>    <zfs-discuss@lists.macosforge.org>
>> Message-ID: <47FA08D7.2010901@jrv.org>
>> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>>
>> I can reproduce his problem here using the Handbrake 0.9.2 GUI with  
>> no
>> network involved.  It hangs after clicking OK to select the source
>> material - it doesn't get to the point of encoding anything.   
>> Handbrake
>> does respond to a cmd-Q and is using about 5% CPU at that point.  The
>> source material is a DVD copied to the hard disk.  It selects source
>> material OK from a "Mac OS Extended" disk.
>>
>> Martin Hauser wrote:
>>> I do not have any problems with handbrake running on a local ZFS. I
>>> encode from and to ZFS volumes, always using a custom mp4 setup. I
>>> assume it's a problem with ZFS + networking ?
>>>
>>> kind regards
>>>
>>> Martin
>>> --- 
>>> --- 
>>> ------------------------------------------------------------------
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>>
>>
>> ------------------------------
>>
>> Message: 5
>> Date: Mon, 7 Apr 2008 14:16:27 +0200
>> From: Martin Hauser <info@martin-hauser.net>
>> Subject: Re: [zfs-discuss] Handbrake on ZFS
>> To: James R.Van Artsdalen <james-zfsosx@jrv.org>
>> Cc: ZFS on OSX mailing list mailing list
>>    <zfs-discuss@lists.macosforge.org>
>> Message-ID: <C433CDE9-FEAF-4D7D-B8A3-7D5136FF96F1@martin-hauser.net>
>> Content-Type: text/plain; charset="us-ascii"
>>
>> Hmm,
>>
>> I am encoding from .mpg files, maybe the difference is in directory
>> handling, as DVD's are oriented that way?
>>
>>
>> On Apr 7, 2008, at 13:43 PM, James R. Van Artsdalen wrote:
>>> I can reproduce his problem here using the Handbrake 0.9.2 GUI with
>>> no network involved.  It hangs after clicking OK to select the
>>> source material - it doesn't get to the point of encoding anything.
>>> Handbrake does respond to a cmd-Q and is using about 5% CPU at that
>>> point.  The source material is a DVD copied to the hard disk.  It
>>> selects source material OK from a "Mac OS Extended" disk.
>>>
>>> Martin Hauser wrote:
>>>> I do not have any problems with handbrake running on a local ZFS. I
>>>> encode from and to ZFS volumes, always using a custom mp4 setup. I
>>>> assume it's a problem with ZFS + networking ?
>>>>
>>>> kind regards
>>>>
>>>> Martin
>>>> --- 
>>>> --- 
>>>> ------------------------------------------------------------------
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>>
>>
>> -------------- next part --------------
>> A non-text attachment was scrubbed...
>> Name: PGP.sig
>> Type: application/pgp-signature
>> Size: 186 bytes
>> Desc: This is a digitally signed message part
>> Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080407/0f2ebade/PGP.bin
>>
>> ------------------------------
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>>
>> End of zfs-discuss Digest, Vol 4, Issue 8
>> *****************************************
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
From schalk at unf.edu  Mon Apr  7 10:32:23 2008
From: schalk at unf.edu (Stuart Chalk)
Date: Mon Apr  7 10:32:13 2008
Subject: [zfs-discuss] Re: zfs-discuss Digest, Vol 4, Issue 8
In-Reply-To: <20080407121516.58FF7240AB6@lists.macosforge.org>
References: <20080407121516.58FF7240AB6@lists.macosforge.org>
Message-ID: <2368B0E3-BF26-4C8F-91D9-D8282C9A5801@unf.edu>

Sorry, I sent the last email to this list rather than its intended  
recipient.  Please ignore.

On Apr 7, 2008, at 8:15 AM, zfs-discuss-request@lists.macosforge.org  
wrote:

> Send zfs-discuss mailing list submissions to
> 	zfs-discuss@lists.macosforge.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
> 	http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> or, via email, send a message with subject or body 'help' to
> 	zfs-discuss-request@lists.macosforge.org
>
> You can reach the person managing the list at
> 	zfs-discuss-owner@lists.macosforge.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of zfs-discuss digest..."
>
>
> Today's Topics:
>
>   1. Re: Handbrake on ZFS (Alex Blewitt)
>   2. Re: Performance problems - AFP! (Robert Rehnmark)
>   3. Re: Handbrake on ZFS (Martin Hauser)
>   4. Re: Handbrake on ZFS (James R. Van Artsdalen)
>   5. Re: Handbrake on ZFS (Martin Hauser)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Mon, 7 Apr 2008 08:57:52 +0100
> From: Alex Blewitt <alex.blewitt@gmail.com>
> Subject: Re: [zfs-discuss] Handbrake on ZFS
> To: Sean Hafeez <sah.list@gmail.com>
> Cc: zfs-discuss@lists.macosforge.org
> Message-ID: <CFA57BDA-2C10-4CD5-B899-D425AD960427@gmail.com>
> Content-Type: text/plain; charset="us-ascii"
>
> I presume it's a local, rather than networked share? I've found that
> ZFS over NFS doesn't like locking on my setup. That fails when
> extended attributes are set (e.g. via xattr). It might be doing the
> same thing with Handbrake.
>
> Alex.
>
> On Apr 7, 2008, at 07:47, Sean Hafeez wrote:
>
>> I goofed - you can ignore the system log. That is from the DVD
>> ripper prg, not the Handbrake encoder. So no errors there. It just
>> does not work. Does not seem to like something on ZFS.
>>
>> -Sean
>>
>> On Apr 6, 2008, at 10:49 PM, Joe Dunn wrote:
>>> What version of handbrake? Might be something to tell the handbrake
>>> developer about. I see this message listed on apple's website
>>>
>>> http://developer.apple.com/documentation/Cocoa/Reference/Foundation/Classes/NSString_Class/DeprecationAppendix/AppendixADeprecatedAPI.html
>>>
>>> Seems like that got rid of that in 10.4.
>>>
>>> I could be totally wrong, and if so hopefully someone smarter then
>>> me points it out :)
>>>
>>> Joe
>>>
>>> On Sun, Apr 6, 2008 at 11:08 PM, Sean Hafeez <sah.list@gmail.com>
>>> wrote:
>>> RAIDZ
>>> Here is the output from the Handbrake GUI
>>>
>>> [20:03:36] hb_init: checking cpu count
>>> [20:03:36] hb_init: starting libhb thread
>>> [20:03:36] thread b0147000 started ("libhb")
>>> [20:04:05] macgui: trying to open video_ts folder (parent directory
>>> chosen)
>>>
>>> Then it just stops like I never pointed it to the DVD image.
>>>
>>> Here is the system.log:
>>>
>>> Apr  6 20:00:03 rioja MacTheRipper[661]: MTR: DVDCopyTileVobX (ts)1
>>> (v)4
>>> Apr  6 20:03:09 rioja MacTheRipper[661]: MTR: copyContent
>>> Apr  6 20:03:11 rioja MacTheRipper[661]: we got here
>>> Apr  6 20:03:11 rioja MacTheRipper[661]: MTR: resetUI_state
>>> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: extractMethod
>>> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: getDiskSize
>>> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: DVDFreeTitlesInfo
>>> Apr  6 20:03:16 rioja MacTheRipper[661]: MTR: DVDFreeTitleSetInfo
>>> Apr  6 20:03:22 rioja MacTheRipper[661]: MTR: popupControl
>>> Apr  6 20:03:31 rioja MacTheRipper[661]: MTR: savePrefs
>>> Apr  6 20:04:05 rioja HandBrake[734]: *** +[NSString
>>> stringWithCString:]: NULL cString
>>>
>>>
>>> zpool list:
>>>
>>> NAME                    SIZE    USED   AVAIL    CAP  HEALTH
>>> ALTROOT
>>> data2                  1.16T    672G    518G    56%  ONLINE     -
>>>
>>>
>>>
>>> -Sean
>>>
>>>
>>> On Apr 6, 2008, at 8:04 AM, Joe Dunn wrote:
>>>> Hi Sean
>>>>
>>>> Is the volume mirrored or in raidz configuration? Anything in your
>>>> syslog about what is happening?
>>>>
>>>> Cheers
>>>>
>>>> Joe
>>>>
>>>> On Sun, Apr 6, 2008 at 2:20 AM, Sean Hafeez <sah.list@gmail.com>
>>>> wrote:
>>>> So I have been ripping my DVDs to watch on my AppleTV. In the past
>>>> I have stored the files to be converted with Handbrake on my RAID
>>>> box. I switched that box over to ZFS a few weeks ago. Handbrake
>>>> seems to choke on the files when trying to open them now. If I
>>>> copy them over to an HFS+ volume it works just fine.
>>>>
>>>> Thanks!
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>>
>>>
>>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080407/86905a80/attachment-0001.html
>
> ------------------------------
>
> Message: 2
> Date: Mon, 7 Apr 2008 11:38:06 +0200
> From: Robert Rehnmark <robert@rehnmark.net>
> Subject: Re: [zfs-discuss] Performance problems - AFP!
> To: ZFS on OSX mailing list mailing list
> 	<zfs-discuss@lists.macosforge.org>
> Message-ID: <F1CE4772-22AB-46EC-A755-39CEE0B588DA@rehnmark.net>
> Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes
>
> Ugh!
> The pools work fine locally but when used over Apple File Sharing
> Protocol it tends to read/write in short bursts.
> The effect is speeds cut in half or worse.
>
> Can anybody verify this or maybe even share a solution?
>
> Please.
>
> /Robert
>
>
>
> 6 apr 2008 kl. 16:35 skrev Robert Rehnmark:
>> When I copy files over the network to/from my server (10.5.2 client
>> single-disk volume) I get 66-70 MB/sec.
>> If I do the same thing to the same machine with a single-disk zpool
>> I get 0-30/40 MB/sec.
>> To a zpool mirror I get 0-55 MB/sec.
>> The performance/operation is very "janky" and when the speed is up
>> the CPU use allmost  tops out too.
>> Does this have anything to do with caching etc.?
>> The pools are version 8, allmost empty and made from the same disks.
>>
>> I want to be able to use ZFS but if I am not able to fix the
>> performance problems HFS+ with daily backups will be preferred.
>>
>> I want to hear about ather peoples experiences and tips, please.
>>
>> /Robert
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
>
>
> ------------------------------
>
> Message: 3
> Date: Mon, 7 Apr 2008 12:03:13 +0200
> From: Martin Hauser <info@martin-hauser.net>
> Subject: Re: [zfs-discuss] Handbrake on ZFS
> To: Sean Hafeez <sah.list@gmail.com>
> Cc: ZFS on OSX mailing list mailing list
> 	<zfs-discuss@lists.macosforge.org>
> Message-ID: <11DDD05B-E100-4115-A5A7-07224EECE11A@martin-hauser.net>
> Content-Type: text/plain; charset="us-ascii"
>
> I do not have any problems with handbrake running on a local ZFS. I
> encode from and to ZFS volumes, always using a custom mp4 setup. I
> assume it's a problem with ZFS + networking ?
>
> kind regards
>
> Martin
> -------------- next part --------------
> A non-text attachment was scrubbed...
> Name: PGP.sig
> Type: application/pgp-signature
> Size: 186 bytes
> Desc: This is a digitally signed message part
> Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080407/c3e2bb80/PGP-0001.bin
>
> ------------------------------
>
> Message: 4
> Date: Mon, 07 Apr 2008 06:43:19 -0500
> From: "James R. Van Artsdalen" <james-zfsosx@jrv.org>
> Subject: Re: [zfs-discuss] Handbrake on ZFS
> To: Martin Hauser <info@martin-hauser.net>
> Cc: ZFS on OSX mailing list mailing list
> 	<zfs-discuss@lists.macosforge.org>
> Message-ID: <47FA08D7.2010901@jrv.org>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
> I can reproduce his problem here using the Handbrake 0.9.2 GUI with no
> network involved.  It hangs after clicking OK to select the source
> material - it doesn't get to the point of encoding anything.   
> Handbrake
> does respond to a cmd-Q and is using about 5% CPU at that point.  The
> source material is a DVD copied to the hard disk.  It selects source
> material OK from a "Mac OS Extended" disk.
>
> Martin Hauser wrote:
>> I do not have any problems with handbrake running on a local ZFS. I
>> encode from and to ZFS volumes, always using a custom mp4 setup. I
>> assume it's a problem with ZFS + networking ?
>>
>> kind regards
>>
>> Martin
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>
>
> ------------------------------
>
> Message: 5
> Date: Mon, 7 Apr 2008 14:16:27 +0200
> From: Martin Hauser <info@martin-hauser.net>
> Subject: Re: [zfs-discuss] Handbrake on ZFS
> To: James R.Van Artsdalen <james-zfsosx@jrv.org>
> Cc: ZFS on OSX mailing list mailing list
> 	<zfs-discuss@lists.macosforge.org>
> Message-ID: <C433CDE9-FEAF-4D7D-B8A3-7D5136FF96F1@martin-hauser.net>
> Content-Type: text/plain; charset="us-ascii"
>
> Hmm,
>
> I am encoding from .mpg files, maybe the difference is in directory
> handling, as DVD's are oriented that way?
>
>
> On Apr 7, 2008, at 13:43 PM, James R. Van Artsdalen wrote:
>> I can reproduce his problem here using the Handbrake 0.9.2 GUI with
>> no network involved.  It hangs after clicking OK to select the
>> source material - it doesn't get to the point of encoding anything.
>> Handbrake does respond to a cmd-Q and is using about 5% CPU at that
>> point.  The source material is a DVD copied to the hard disk.  It
>> selects source material OK from a "Mac OS Extended" disk.
>>
>> Martin Hauser wrote:
>>> I do not have any problems with handbrake running on a local ZFS. I
>>> encode from and to ZFS volumes, always using a custom mp4 setup. I
>>> assume it's a problem with ZFS + networking ?
>>>
>>> kind regards
>>>
>>> Martin
>>> ------------------------------------------------------------------------
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>
> -------------- next part --------------
> A non-text attachment was scrubbed...
> Name: PGP.sig
> Type: application/pgp-signature
> Size: 186 bytes
> Desc: This is a digitally signed message part
> Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080407/0f2ebade/PGP.bin
>
> ------------------------------
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
>
> End of zfs-discuss Digest, Vol 4, Issue 8
> *****************************************

From sah.list at gmail.com  Mon Apr  7 19:23:21 2008
From: sah.list at gmail.com (Sean Hafeez)
Date: Mon Apr  7 19:21:54 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <47FA08D7.2010901@jrv.org>
References: <47E33231.40107@jrv.org>	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>	<47E42E60.6040506@jrv.org>	<6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>	<4AADF1E6-FC85-4829-9D20-9E2CAF2B16C9@gmail.com>	<c61bd13c0804060804h22a131f1oa5924caeca084aa7@mail.gmail.com>	<8C0F3494-FB86-431C-9872-3B57B5C2BB2B@gmail.com>	<c61bd13c0804062249r8e2b1a8iec50b66665cc710b@mail.gmail.com>	<6DFAF372-EFF8-4E58-8B23-69F1161DD807@gmail.com>	<CFA57BDA-2C10-4CD5-B899-D425AD960427@gmail.com>
	<11DDD05B-E100-4115-A5A7-07224EECE11A@martin-hauser.net>
	<47FA08D7.2010901@jrv.org>
Message-ID: <042384E3-BBF9-415A-AEC9-DB4C20114FBE@gmail.com>

Bingo.

Not network, local. Does not work on ZFS. Works on HFS+. Same files,  
same disk.

I will try the list of cmd that were posted and see if I can figure  
out what it is doing.

Thanks!

On Apr 7, 2008, at 4:43 AM, James R. Van Artsdalen wrote:
> I can reproduce his problem here using the Handbrake 0.9.2 GUI with  
> no network involved.  It hangs after clicking OK to select the  
> source material - it doesn't get to the point of encoding anything.   
> Handbrake does respond to a cmd-Q and is using about 5% CPU at that  
> point.  The source material is a DVD copied to the hard disk.  It  
> selects source material OK from a "Mac OS Extended" disk.
>
> Martin Hauser wrote:
>> I do not have any problems with handbrake running on a local ZFS. I  
>> encode from and to ZFS volumes, always using a custom mp4 setup. I  
>> assume it's a problem with ZFS + networking ?
>>
>> kind regards
>>
>> Martin
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>

From canadrian at electricteaparty.net  Mon Apr  7 19:51:29 2008
From: canadrian at electricteaparty.net (Adrian Thornton)
Date: Mon Apr  7 19:49:56 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
In-Reply-To: <20080407173214.CFF94240E15@lists.macosforge.org>
References: <20080407173214.CFF94240E15@lists.macosforge.org>
Message-ID: <8F73D3A4-4B9D-4259-938B-87662ED78F41@electricteaparty.net>

For some reason, my RAID-Z is not automatically re-mounting when I  
reboot. I wouldn't mind too much, but as it is I get a million "disk  
not recognized" prompts every time I restart. Is there any reason you  
can think of why this may be happening? And is there any way for me to  
automate the re-mount without creating an Automator login item with  
"sudo zpool import -f Archives" in it? I'd rather have it re-mount  
before anyone logs in to ensure it's available already to the first  
logged in user.

Thanks,
Adrian
From mattsnow at gmail.com  Mon Apr  7 21:15:30 2008
From: mattsnow at gmail.com (Matt Snow)
Date: Mon Apr  7 21:13:59 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
In-Reply-To: <8F73D3A4-4B9D-4259-938B-87662ED78F41@electricteaparty.net>
References: <20080407173214.CFF94240E15@lists.macosforge.org>
	<8F73D3A4-4B9D-4259-938B-87662ED78F41@electricteaparty.net>
Message-ID: <6879ebc80804072115y4c3d3025m2460ea02cb585e91@mail.gmail.com>

Adrian,
I had the exact same problem. you have to use the command line diskutil to
create the EFI label and ZFS slice using this command:

diskutil partitiondisk /dev/disk2 GPTFormat ZFS %noformat% 100%


This is vaguely covered in the getting started page here:
http://trac.macosforge.org/projects/zfs/wiki/get_the_party_started

It would be nice if this was not required like in solaris. A simple "zpool
create tank disk1 disk2 disk3 disk4" did the trick when using entire disks.

..Matt

On Mon, Apr 7, 2008 at 7:51 PM, Adrian Thornton <
canadrian@electricteaparty.net> wrote:

> For some reason, my RAID-Z is not automatically re-mounting when I reboot.
> I wouldn't mind too much, but as it is I get a million "disk not recognized"
> prompts every time I restart. Is there any reason you can think of why this
> may be happening? And is there any way for me to automate the re-mount
> without creating an Automator login item with "sudo zpool import -f
> Archives" in it? I'd rather have it re-mount before anyone logs in to ensure
> it's available already to the first logged in user.
>
> Thanks,
> Adrian
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080407/10d2ce3a/attachment.html
From mattsnow at gmail.com  Tue Apr  8 07:53:51 2008
From: mattsnow at gmail.com (Matt Snow)
Date: Tue Apr  8 07:52:19 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
In-Reply-To: <4B0CBC09-2A7F-40AC-80DF-7A0A72E4D32C@electricteaparty.net>
References: <20080407173214.CFF94240E15@lists.macosforge.org>
	<8F73D3A4-4B9D-4259-938B-87662ED78F41@electricteaparty.net>
	<6879ebc80804072115y4c3d3025m2460ea02cb585e91@mail.gmail.com>
	<4B0CBC09-2A7F-40AC-80DF-7A0A72E4D32C@electricteaparty.net>
Message-ID: <6879ebc80804080753r571500bfg443be5b0a4e0cfa4@mail.gmail.com>

Using the entire disk works in Solaris perfectly. I would guess that it has
something to do with the OSX automounter.

I picked up a 750GB disk and backed up all of my data to it then recreated
the ZFS pool. Resilvering is an option if you want to test the feature out.
:)

..Matt

On Mon, Apr 7, 2008 at 9:27 PM, Adrian Thornton <
canadrian@electricteaparty.net> wrote:

> Matt,
> I suppose that's the issue then - I used the simple whole-disk command to
> create my pool, thinking ZFS would take care of itself. So... what do you
> figure - remove one disk at a time, re-partition as stated, re-add to the
> pool, resilver, repeat?
>
> - Adrian
>
> On Apr 7, 2008, at 22:15 , Matt Snow wrote:
>
> Adrian,
> I had the exact same problem. you have to use the command line diskutil to
> create the EFI label and ZFS slice using this command:
>
> diskutil partitiondisk /dev/disk2 GPTFormat ZFS %noformat% 100%
>
>
> This is vaguely covered in the getting started page here:
> http://trac.macosforge.org/projects/zfs/wiki/get_the_party_started
>
> It would be nice if this was not required like in solaris. A simple "zpool
> create tank disk1 disk2 disk3 disk4" did the trick when using entire disks.
>
> ..Matt
>
> On Mon, Apr 7, 2008 at 7:51 PM, Adrian Thornton <
> canadrian@electricteaparty.net> wrote:
>
> > For some reason, my RAID-Z is not automatically re-mounting when I
> > reboot. I wouldn't mind too much, but as it is I get a million "disk not
> > recognized" prompts every time I restart. Is there any reason you can think
> > of why this may be happening? And is there any way for me to automate the
> > re-mount without creating an Automator login item with "sudo zpool import -f
> > Archives" in it? I'd rather have it re-mount before anyone logs in to ensure
> > it's available already to the first logged in user.
> >
> > Thanks,
> > Adrian
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss@lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> >
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080408/46bbf851/attachment.html
From mattsnow at gmail.com  Tue Apr  8 07:57:16 2008
From: mattsnow at gmail.com (Matt Snow)
Date: Tue Apr  8 07:55:44 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
In-Reply-To: <CC6752AC-38AD-45A2-921E-09B4BB188A59@gmail.com>
References: <20080407173214.CFF94240E15@lists.macosforge.org>
	<8F73D3A4-4B9D-4259-938B-87662ED78F41@electricteaparty.net>
	<6879ebc80804072115y4c3d3025m2460ea02cb585e91@mail.gmail.com>
	<CC6752AC-38AD-45A2-921E-09B4BB188A59@gmail.com>
Message-ID: <6879ebc80804080757lbf08fc8s59255ad863cd5c62@mail.gmail.com>

Hi Maarten,
i'm not sure. That was one of the reasons I joined the list. :)

On Mon, Apr 7, 2008 at 11:36 PM, Maarten Billemont <lhunath@gmail.com>
wrote:

> Can this be done after creating a ZFS pool on the disk too without loosing
> the pool?
>
>
> On 08 Apr 2008, at 06:15, Matt Snow wrote:
>
> > Adrian,
> > I had the exact same problem. you have to use the command line diskutil
> > to create the EFI label and ZFS slice using this command:
> >
> > diskutil partitiondisk /dev/disk2 GPTFormat ZFS %noformat% 100%
> >
> >
> > This is vaguely covered in the getting started page here:
> > http://trac.macosforge.org/projects/zfs/wiki/get_the_party_started
> >
> > It would be nice if this was not required like in solaris. A simple
> > "zpool create tank disk1 disk2 disk3 disk4" did the trick when using entire
> > disks.
> >
> > ..Matt
> >
> > On Mon, Apr 7, 2008 at 7:51 PM, Adrian Thornton <
> > canadrian@electricteaparty.net> wrote:
> > For some reason, my RAID-Z is not automatically re-mounting when I
> > reboot. I wouldn't mind too much, but as it is I get a million "disk not
> > recognized" prompts every time I restart. Is there any reason you can think
> > of why this may be happening? And is there any way for me to automate the
> > re-mount without creating an Automator login item with "sudo zpool import -f
> > Archives" in it? I'd rather have it re-mount before anyone logs in to ensure
> > it's available already to the first logged in user.
> >
> > Thanks,
> > Adrian
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss@lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> >
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss@lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080408/1ae65662/attachment-0001.html
From alex.blewitt at gmail.com  Tue Apr  8 08:20:34 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Tue Apr  8 08:19:00 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
In-Reply-To: <6879ebc80804072115y4c3d3025m2460ea02cb585e91@mail.gmail.com>
References: <20080407173214.CFF94240E15@lists.macosforge.org>
	<8F73D3A4-4B9D-4259-938B-87662ED78F41@electricteaparty.net>
	<6879ebc80804072115y4c3d3025m2460ea02cb585e91@mail.gmail.com>
Message-ID: <636fd28e0804080820q310e5c1dk82d292d4caee3949@mail.gmail.com>

On Tue, Apr 8, 2008 at 5:15 AM, Matt Snow <mattsnow@gmail.com> wrote:
> Adrian,
> I had the exact same problem. you have to use the command line diskutil to
> create the EFI label and ZFS slice using this command:
>
> diskutil partitiondisk /dev/disk2 GPTFormat ZFS %noformat% 100%

I thought this was bad because (a) you end up with a disk2s2
partition, and (b) the on-disk format of ZFS leaves the first X bytes
free so that an EFI label can be put on there anyway? In any case, (c)
ZFS works better if it's given complete control of the disk since it
can use the on-disk cache properly, whereas if it's given a partition
(even if that partition is most of the system and the other partitions
are never written) then it doesn't know that it has the whole disk and
thus doesn't use the disk cache.

Alex.
From lists at loveturtle.net  Tue Apr  8 08:35:23 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Tue Apr  8 08:33:51 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
In-Reply-To: <636fd28e0804080820q310e5c1dk82d292d4caee3949@mail.gmail.com>
References: <20080407173214.CFF94240E15@lists.macosforge.org>	<8F73D3A4-4B9D-4259-938B-87662ED78F41@electricteaparty.net>	<6879ebc80804072115y4c3d3025m2460ea02cb585e91@mail.gmail.com>
	<636fd28e0804080820q310e5c1dk82d292d4caee3949@mail.gmail.com>
Message-ID: <47FB90BB.6060804@loveturtle.net>

With the whole disk vs partition cache issue from what I understand this 
is specific to the implementation.

It's true on Solaris, not true on FreeBSD, and I have no idea on OSX 
except to say that the method below is how Apple is telling us to do it. 
So it's safe to assume that it is the correct method.

Alex Blewitt wrote:
> On Tue, Apr 8, 2008 at 5:15 AM, Matt Snow <mattsnow@gmail.com> wrote:
>   
>> Adrian,
>> I had the exact same problem. you have to use the command line diskutil to
>> create the EFI label and ZFS slice using this command:
>>
>> diskutil partitiondisk /dev/disk2 GPTFormat ZFS %noformat% 100%
>>     
>
> I thought this was bad because (a) you end up with a disk2s2
> partition, and (b) the on-disk format of ZFS leaves the first X bytes
> free so that an EFI label can be put on there anyway? In any case, (c)
> ZFS works better if it's given complete control of the disk since it
> can use the on-disk cache properly, whereas if it's given a partition
> (even if that partition is most of the system and the other partitions
> are never written) then it doesn't know that it has the whole disk and
> thus doesn't use the disk cache.
>
> Alex.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>   

From alex.blewitt at gmail.com  Tue Apr  8 08:57:39 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Tue Apr  8 08:56:06 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
In-Reply-To: <47FB90BB.6060804@loveturtle.net>
References: <20080407173214.CFF94240E15@lists.macosforge.org>
	<8F73D3A4-4B9D-4259-938B-87662ED78F41@electricteaparty.net>
	<6879ebc80804072115y4c3d3025m2460ea02cb585e91@mail.gmail.com>
	<636fd28e0804080820q310e5c1dk82d292d4caee3949@mail.gmail.com>
	<47FB90BB.6060804@loveturtle.net>
Message-ID: <636fd28e0804080857r30fb79d0gecd929f5f46e3432@mail.gmail.com>

On Tue, Apr 8, 2008 at 4:35 PM, Dillon Kass <lists@loveturtle.net> wrote:
> With the whole disk vs partition cache issue from what I understand this is
> specific to the implementation.
>
> It's true on Solaris, not true on FreeBSD, and I have no idea on OSX except
> to say that the method below is how Apple is telling us to do it. So it's
> safe to assume that it is the correct method.

The man pages for zpool specifically suggest using a whole-disk
approach, and that the disk will be labeled accordingly:

       disk      A  block  device, typically located under "/dev". ZFS can use
                 individual slices or partitions, though the recommended  mode
                 of  operation  is to use whole disks. A disk can be specified
                 by a full path, or it can be a shorthand name  (the  relative
                 portion of the path under "/dev"). A whole disk can be speci-
                 fied by omitting the  slice  or  partition  designation.  For
                 example,  "disk1s2"  is  equivalent  to  "/dev/disk1s2". When
                 given a whole disk, ZFS automatically  labels  the  disk,  if
                 necessary.

It seems that the labelling (in particular, the EFI signature) isn't
being written during the creation on a whole disk. Perhaps it gets
created only when you do an 'export' (man zpool - export)

           For pools to be portable, you must give  the  zpool  command  whole
           disks, not just slices, so that ZFS can label the disks with porta-
           ble EFI labels. Otherwise, disk drivers on platforms  of  different
           endianness will not recognize the disks.

As far as the current implementation goes, I can imagine that it's
better to follow the advice on the mac page. But I suspect there may
be developments in the future which will allow whole disks to be used
for ZFS instead of a partition on a disk.

Alex
From james-zfsosx at jrv.org  Tue Apr  8 10:07:11 2008
From: james-zfsosx at jrv.org (James R. Van Artsdalen)
Date: Tue Apr  8 10:05:54 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
In-Reply-To: <636fd28e0804080857r30fb79d0gecd929f5f46e3432@mail.gmail.com>
References: <20080407173214.CFF94240E15@lists.macosforge.org>	<8F73D3A4-4B9D-4259-938B-87662ED78F41@electricteaparty.net>	<6879ebc80804072115y4c3d3025m2460ea02cb585e91@mail.gmail.com>	<636fd28e0804080820q310e5c1dk82d292d4caee3949@mail.gmail.com>	<47FB90BB.6060804@loveturtle.net>
	<636fd28e0804080857r30fb79d0gecd929f5f46e3432@mail.gmail.com>
Message-ID: <47FBA63F.3000503@jrv.org>


Alex Blewitt wrote:
> As far as the current implementation goes, I can imagine that it's
> better to follow the advice on the mac page.
Indeed.  This is a work in progress and needs updates to base OSX to 
smooth over the pointy parts.

Disks used whole for ZFS need to be repartitioned and reformed to make 
life simpler.  But before this you *must* do a "zpool destroy" or ZFS 
will detect droppings on the drive even after a repartition and reformat 
and become confused.

If you can't use "zpool destroy" for whatever reason then there is a 
post in the archive from No?l Dellofano to me on 3/21 titled "Re: How to 
erase cached zpool member info?" showing how to use dd(1) to erase the 
remnant ZFS whole-disk markings from a drive so it can be used again.

From franzschmalzl at spamfreemail.de  Tue Apr  8 12:35:45 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Tue Apr  8 12:46:19 2008
Subject: [zfs-discuss] mount/umount as user 
Message-ID: <077A7548-3A28-4A05-B897-CA28F838158D@spamfreemail.de>

is there a way to set permission to make zfs pools/filesystems  
umountable as normal (l)user ?

i can neither mount nor umount my zpool and filesystems as normal  
user, i always have to use sudo

any ideas ?

thanks

franz 
From i_see at macnews.de  Tue Apr  8 12:45:01 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Tue Apr  8 12:46:29 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
In-Reply-To: <20080408145543.2CBD4241AB3@lists.macosforge.org>
References: <20080408145543.2CBD4241AB3@lists.macosforge.org>
Message-ID: <77DCF6BD-D149-485B-B647-4BA90F9FAAF1@macnews.de>

Hi,
not currently, unless you have a set of slightly larger disks available.
If you use the whole disks the data partitions are slightly larger and  
right now ZFS is unable to replace devices by smaller ones, so you  
have to back up your data, destroy the pool and start ove with a new  
one..
The sad part is that for Solaris, it is actually recommended to use  
whole disks. It should be relatively simple to prevent this from  
happening in the mac os x port.
Tha ability to shrink pools will hopefully come some day (I could  
not,however, find a ZFS roadmap anywhere on the net so far.

Hope this helps,	ralf
Am 08.04.2008 um 16:55 schrieb zfs-discuss-request@lists.macosforge.org:
> Hi Maarten,
> i'm not sure. That was one of the reasons I joined the list. :)
>
> On Mon, Apr 7, 2008 at 11:36 PM, Maarten Billemont  
> <lhunath@gmail.com> wrote:
> Can this be done after creating a ZFS pool on the disk too without  
> loosing the pool?
>

From zorg at sogeeky.net  Tue Apr  8 15:27:10 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Tue Apr  8 15:25:49 2008
Subject: [zfs-discuss] mount/umount as user
In-Reply-To: <077A7548-3A28-4A05-B897-CA28F838158D@spamfreemail.de>
References: <077A7548-3A28-4A05-B897-CA28F838158D@spamfreemail.de>
Message-ID: <BF37964A-D0F1-4FD1-BB62-C6C03B2E025D@sogeeky.net>

Yes. Using sudo you cab chown the /Volumes/whatever directory.

On Apr 8, 2008, at 12:35 PM, Franz Schmalzl <franzschmalzl@spamfreemail.de 
 > wrote:

> is there a way to set permission to make zfs pools/filesystems  
> umountable as normal (l)user ?
>
> i can neither mount nor umount my zpool and filesystems as normal  
> user, i always have to use sudo
>
> any ideas ?
>
> thanks
>
> franz_______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
From lopez.on.the.lists at yellowspace.net  Tue Apr  8 17:24:53 2008
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Tue Apr  8 17:24:33 2008
Subject: [zfs-discuss] Status of ZFS on Mac OS X?
In-Reply-To: <33644d3c0804051703j79c39b6fr2567c049aa18ffa6@mail.gmail.com>
References: <fb9be13f644474a1cb6845fa4b9a4c3a@yellowspace.net>
	<47F80FA0.2000604@loveturtle.net>
	<33644d3c0804051703j79c39b6fr2567c049aa18ffa6@mail.gmail.com>
Message-ID: <A384DE4E-6348-4728-BE8B-B112376FD2D2@yellowspace.net>

Thanx for Your contributions, first of all.

On 06.04.2008, at 02:03, James Snyder wrote:

> Aside from the known issues with
> having to empty trash by rm'ing and not being able to use spotlight,

These are two nasty limitations for daily usage (which would be
one important use to test it, would it?), if you ask me ;/
I use spotlight a lot, so I couldn't put my home on it.

On the other hand, it would be interesting to see what happens when
using File Vault (provided that it is possible to symlink it to
an encrypted sparse image within a zfs volume). Since zfs snapshots
are block-wise, at least that would be a way to backup the sparse-
monster incrementally - and to have a "portable" time machine.

Of course it would be great for things like the iTunes library,
as well as volumes containing big files such as Paralels VM
images, etc.

> One thing I might stay away from, at the moment, is attaching and
> detaching disks a lot from mirrors

Personally, I don't think that this example is one of the most notable
features of zfs (besides you can do this with diskutil as well)...

> I'm not sure how I would rate it in comparison to the FreeBSD port.
> That one is relatively stable as well, though there are the known
> memory issues and some deadlocks there that I don't think I've seen on
> OS X.

I've been using zfs on freebsd/amd64 since FreeBSD 7.0-BETA4
and it runs smoothly with appropriate kernel memory settings.
One of my systems receives rsync as well as psyncX backups from
other Unixes and from Macs, and has yet to show any problem.

As for a feature comparison, it looks to me like the Mac OS X
port has implemented more features (ACLs and things like
labels look like they're being kept, at least from a
Finder perspective). I've seen it has problems with resource
forks however (at least when copying recursively with cp).

Finally, I took some more time to read the past posts
on this lists, and have found that No?l Dellofano from inside the
"mothership" is keeping us up to date at a fair pace, and even
posted about a new release shortly.
I look forward to it and will start using zfs on a macbook pro
for the beginning...

Thanx a lot and Regards,


Lorenzo



From franzschmalzl at spamfreemail.de  Wed Apr  9 06:57:03 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Wed Apr  9 06:55:39 2008
Subject: [zfs-discuss] mount/umount as user
In-Reply-To: <BF37964A-D0F1-4FD1-BB62-C6C03B2E025D@sogeeky.net>
References: <077A7548-3A28-4A05-B897-CA28F838158D@spamfreemail.de>
	<BF37964A-D0F1-4FD1-BB62-C6C03B2E025D@sogeeky.net>
Message-ID: <85AC84DC-CD99-4DAF-90A1-D45589E541F0@spamfreemail.de>

thanks for your reply

yap, did that
did not work

i own the directories
also tried setting rwxrwxrxw...

did not work either


On Apr 9, 2008, at 12:27 AM, Mr. Zorg wrote:

> Yes. Using sudo you cab chown the /Volumes/whatever directory.
>
> On Apr 8, 2008, at 12:35 PM, Franz Schmalzl <franzschmalzl@spamfreemail.de 
> > wrote:
>
>> is there a way to set permission to make zfs pools/filesystems  
>> umountable as normal (l)user ?
>>
>> i can neither mount nor umount my zpool and filesystems as normal  
>> user, i always have to use sudo
>>
>> any ideas ?
>>
>> thanks
>>
>> franz_______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From karl.gyllstrom at gmail.com  Wed Apr  9 08:40:36 2008
From: karl.gyllstrom at gmail.com (Karl Gyllstrom)
Date: Wed Apr  9 08:38:59 2008
Subject: [zfs-discuss] Version incompatibility when importing zpool from
	Solaris
In-Reply-To: <4bc43a2b0804061050w292384bfn229eb15023363003@mail.gmail.com>
References: <4bc43a2b0804061050w292384bfn229eb15023363003@mail.gmail.com>
Message-ID: <C7481F86-29E3-4AF3-86AC-2B23A13F984F@gmail.com>

If you have the space, maybe you can just make a new pool on the new  
machine and send the snapshot of the old one.

On Apr 6, 2008, at 1:50 PM, Dave Loose wrote:

> About a year ago, I put a Solaris box together to play with ZFS. I
> mainly use it to serve my iTunes library and DVD rips. I used samba
> for this, and I was continually frustrated by its flakiness. Both
> samba and Solaris's built-in smb server don't seem to play well with
> the other computers in my house (all of which are Macs).
>
> I finally bit the bullet and bought a Mac Pro (I'd been needing a new
> desktop for a while) and I decided I'd try to migrate the zpool to my
> new machine. The problem is that I upgraded my Solaris machine rather
> frequently and now when I try to 'zpool import storage' I get this
> message: cannot import 'storage': pool is formatted using a newer ZFS
> version
>
> I kept my Solaris machine pretty current. It has Nevada release 84 and
> the zpool format is whatever was most current for that release. I
> don't remember which version it is off the top of my head.
>
> Is there any way for me to get my zfs volume running under OS X? I
> read the FAQ and it seems to suggest that OS X creates new zpools
> using an older version, but is there any way to import a newer
> version? Is there any way to "downgrade" the version?
>
> Thanks for the help,
>
> Dave
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From zorg at sogeeky.net  Wed Apr  9 08:54:08 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Wed Apr  9 08:52:53 2008
Subject: [zfs-discuss] mount/umount as user
In-Reply-To: <85AC84DC-CD99-4DAF-90A1-D45589E541F0@spamfreemail.de>
References: <077A7548-3A28-4A05-B897-CA28F838158D@spamfreemail.de>
	<BF37964A-D0F1-4FD1-BB62-C6C03B2E025D@sogeeky.net>
	<85AC84DC-CD99-4DAF-90A1-D45589E541F0@spamfreemail.de>
Message-ID: <487CC5F9-E05B-4C70-966C-8A3D3C2E4C24@sogeeky.net>

Huh. I could have sworn that worked for me. Maybe I haven't actually  
tried unmounting it. I guess I'll have to try again. :)

On Apr 9, 2008, at 6:57 AM, Franz Schmalzl <franzschmalzl@spamfreemail.de 
 > wrote:

> thanks for your reply
>
> yap, did that
> did not work
>
> i own the directories
> also tried setting rwxrwxrxw...
>
> did not work either
>
>
> On Apr 9, 2008, at 12:27 AM, Mr. Zorg wrote:
>
>> Yes. Using sudo you cab chown the /Volumes/whatever directory.
>>
>> On Apr 8, 2008, at 12:35 PM, Franz Schmalzl <franzschmalzl@spamfreemail.de 
>> > wrote:
>>
>>> is there a way to set permission to make zfs pools/filesystems  
>>> umountable as normal (l)user ?
>>>
>>> i can neither mount nor umount my zpool and filesystems as normal  
>>> user, i always have to use sudo
>>>
>>> any ideas ?
>>>
>>> thanks
>>>
>>> franz_______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
From franzschmalzl at spamfreemail.de  Wed Apr  9 09:39:54 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Wed Apr  9 09:38:27 2008
Subject: [zfs-discuss] zpool status -v
Message-ID: <17348603-B0C9-4B6E-AF13-3E2B16325BBC@spamfreemail.de>

zpool status  -v gives me a complete system freeze....

any ideas ?


regards

franz schmalzl 
From i_see at macnews.de  Wed Apr  9 10:20:23 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Wed Apr  9 10:18:43 2008
Subject: [zfs-discuss] Status of ZFS on Mac OS X?
In-Reply-To: <20080409002448.318F2242096@lists.macosforge.org>
References: <20080409002448.318F2242096@lists.macosforge.org>
Message-ID: <25A4DF94-3D2A-498B-92DF-484FA781D276@macnews.de>

Hi James,
I don't use FileVault, but unencrypted images to store most of my data.
Except for the fact that deleted space is not as easily reclaimed as  
with the normal filesystem, emptying the trash and spotlight work great.
I am using ZFS since december and if you can take the trouble of  
occasionally going to the command line to issue zpool commands (like  
scrubing, exporting before shutdown etc.) it works very reliable.
ralf
Am 09.04.2008 um 02:24 schrieb zfs-discuss-request@lists.macosforge.org:
> Thanx for Your contributions, first of all.
>
> On 06.04.2008, at 02:03, James Snyder wrote:
>
>> Aside from the known issues with
>> having to empty trash by rm'ing and not being able to use spotlight,
>
> These are two nasty limitations for daily usage (which would be
> one important use to test it, would it?), if you ask me ;/
> I use spotlight a lot, so I couldn't put my home on it.
>
> On the other hand, it would be interesting to see what happens when
> using File Vault (provided that it is possible to symlink it to
> an encrypted sparse image within a zfs volume). Since zfs snapshots
> are block-wise, at least that would be a way to backup the sparse-
> monster incrementally - and to have a "portable" time machine.

From franzschmalzl at spamfreemail.de  Wed Apr  9 10:49:47 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Wed Apr  9 10:48:22 2008
Subject: [zfs-discuss] data corruption
Message-ID: <2E6ED8D5-7CC0-4CF5-8F64-0324165DBC3E@spamfreemail.de>

hi folks!


One or more devices has experienced an error resulting in data
	corruption.  Applications may be affected.
action: Restore the file in question if possible.  Otherwise restore the
	entire pool from backup.
    see: http://www.sun.com/msg/ZFS-8000-8A
  scrub: scrub in progress, 4.99% done, 3h48m to go
config:

	NAME         STATE     READ WRITE CKSUM
	raidtank     ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk2s2  ONLINE       0     0     0
	    disk1s2  ONLINE       0     0     0
	    disk3s2  ONLINE       0     0     0

errors: 1399 data errors, use '-v' for a list
jrg:zfs ruebezahl$


i got this recently when two of my stupid WD drives went to sleep  
during a resilver *argh*

now the data it was resilvering is completely unimportant ( it was a  
faulted test backup )

i deleted the files....

now i only hope the files wich were already present before are not  
affected...

first one went to sleep during the backup, i woke it up again and zfs  
started resilvering... during that process the other one went to sleep  
( for odd reasons )

it suddenly began to give me read write errors....

so no important data should be affected, the actual problem is that i  
can't verify becaus zpool status -v freezes my system comlpetely...


any ideas ?

kind regards


franz 
From eberlein at att.net  Wed Apr  9 11:06:27 2008
From: eberlein at att.net (Michael Eberlein)
Date: Wed Apr  9 11:05:18 2008
Subject: [zfs-discuss] data corruption
In-Reply-To: <2E6ED8D5-7CC0-4CF5-8F64-0324165DBC3E@spamfreemail.de>
References: <2E6ED8D5-7CC0-4CF5-8F64-0324165DBC3E@spamfreemail.de>
Message-ID: <0DC34084-F82E-4AFB-8469-CFBEA4C59DB7@att.net>

Franz,

   I had the same problem, which I mentioned a little earlier in the  
kernel panic discussion (zpool status -v caused an immediate kernel  
panic).  Additionally, attempting certain disk operations (such as  
moving the files via iTunes -> I was using my pool as an AV drive)  
would cause the computer to die with no way to figure out the problem.

In the process of copying the salvageable data from the pool, I found  
a workaround.  The damaged files will fail when you copy them over to  
a known good drive (I used cp -R -v and just located the files that  
generated error messages).  All other files were good.  (The bad files  
were partially copied over, but easily identified and deleted.   
Fortunately it was a video library so I could just rerip..)

Of course, this required copying the ENTIRE POOL.  Takes awhile for a  
400GB library over USB :)

After doing this, I destroyed the pool and started over.   Now I can  
finally do zpool status again...

Mike

On Apr 9, 2008, at 7:49 PM, Franz Schmalzl wrote:

> hi folks!
>
>
> One or more devices has experienced an error resulting in data
> 	corruption.  Applications may be affected.
> action: Restore the file in question if possible.  Otherwise restore  
> the
> 	entire pool from backup.
>   see: http://www.sun.com/msg/ZFS-8000-8A
> scrub: scrub in progress, 4.99% done, 3h48m to go
> config:
>
> 	NAME         STATE     READ WRITE CKSUM
> 	raidtank     ONLINE       0     0     0
> 	  raidz1     ONLINE       0     0     0
> 	    disk2s2  ONLINE       0     0     0
> 	    disk1s2  ONLINE       0     0     0
> 	    disk3s2  ONLINE       0     0     0
>
> errors: 1399 data errors, use '-v' for a list
> jrg:zfs ruebezahl$
>
>
> i got this recently when two of my stupid WD drives went to sleep  
> during a resilver *argh*
>
> now the data it was resilvering is completely unimportant ( it was a  
> faulted test backup )
>
> i deleted the files....
>
> now i only hope the files wich were already present before are not  
> affected...
>
> first one went to sleep during the backup, i woke it up again and  
> zfs started resilvering... during that process the other one went to  
> sleep ( for odd reasons )
>
> it suddenly began to give me read write errors....
>
> so no important data should be affected, the actual problem is that  
> i can't verify becaus zpool status -v freezes my system comlpetely...
>
>
> any ideas ?
>
> kind regards
>
>
> franz_______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From franzschmalzl at spamfreemail.de  Wed Apr  9 11:14:24 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Wed Apr  9 11:12:57 2008
Subject: [zfs-discuss] data corruption
In-Reply-To: <0DC34084-F82E-4AFB-8469-CFBEA4C59DB7@att.net>
References: <2E6ED8D5-7CC0-4CF5-8F64-0324165DBC3E@spamfreemail.de>
	<0DC34084-F82E-4AFB-8469-CFBEA4C59DB7@att.net>
Message-ID: <39462AF1-1B9C-4AC7-A3BC-04B23220DCD7@spamfreemail.de>

good workoround, i just don't happen to have 2 tb of space to copy the  
data off : )

but thanks for your reply tough....

the filesystems vere obvioulsy not corrupted because the ones who i  
care of are just sparsebundles, and they mount fine....



On Apr 9, 2008, at 8:06 PM, Michael Eberlein wrote:

> Franz,
>
>  I had the same problem, which I mentioned a little earlier in the  
> kernel panic discussion (zpool status -v caused an immediate kernel  
> panic).  Additionally, attempting certain disk operations (such as  
> moving the files via iTunes -> I was using my pool as an AV drive)  
> would cause the computer to die with no way to figure out the problem.
>
> In the process of copying the salvageable data from the pool, I  
> found a workaround.  The damaged files will fail when you copy them  
> over to a known good drive (I used cp -R -v and just located the  
> files that generated error messages).  All other files were good.   
> (The bad files were partially copied over, but easily identified and  
> deleted.  Fortunately it was a video library so I could just rerip..)
>
> Of course, this required copying the ENTIRE POOL.  Takes awhile for  
> a 400GB library over USB :)
>
> After doing this, I destroyed the pool and started over.   Now I can  
> finally do zpool status again...
>
> Mike
>
> On Apr 9, 2008, at 7:49 PM, Franz Schmalzl wrote:
>
>> hi folks!
>>
>>
>> One or more devices has experienced an error resulting in data
>> 	corruption.  Applications may be affected.
>> action: Restore the file in question if possible.  Otherwise  
>> restore the
>> 	entire pool from backup.
>>  see: http://www.sun.com/msg/ZFS-8000-8A
>> scrub: scrub in progress, 4.99% done, 3h48m to go
>> config:
>>
>> 	NAME         STATE     READ WRITE CKSUM
>> 	raidtank     ONLINE       0     0     0
>> 	  raidz1     ONLINE       0     0     0
>> 	    disk2s2  ONLINE       0     0     0
>> 	    disk1s2  ONLINE       0     0     0
>> 	    disk3s2  ONLINE       0     0     0
>>
>> errors: 1399 data errors, use '-v' for a list
>> jrg:zfs ruebezahl$
>>
>>
>> i got this recently when two of my stupid WD drives went to sleep  
>> during a resilver *argh*
>>
>> now the data it was resilvering is completely unimportant ( it was  
>> a faulted test backup )
>>
>> i deleted the files....
>>
>> now i only hope the files wich were already present before are not  
>> affected...
>>
>> first one went to sleep during the backup, i woke it up again and  
>> zfs started resilvering... during that process the other one went  
>> to sleep ( for odd reasons )
>>
>> it suddenly began to give me read write errors....
>>
>> so no important data should be affected, the actual problem is that  
>> i can't verify becaus zpool status -v freezes my system comlpetely...
>>
>>
>> any ideas ?
>>
>> kind regards
>>
>>
>> franz_______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

From bwaters at nrao.edu  Wed Apr  9 11:39:28 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Wed Apr  9 11:39:00 2008
Subject: [zfs-discuss] data corruption
In-Reply-To: <39462AF1-1B9C-4AC7-A3BC-04B23220DCD7@spamfreemail.de>
References: <2E6ED8D5-7CC0-4CF5-8F64-0324165DBC3E@spamfreemail.de>
	<0DC34084-F82E-4AFB-8469-CFBEA4C59DB7@att.net>
	<39462AF1-1B9C-4AC7-A3BC-04B23220DCD7@spamfreemail.de>
Message-ID: <7BA689E7-D084-4202-8C73-78606C75CBBB@nrao.edu>


On Apr 9, 2008, at 12:14 PM, Franz Schmalzl wrote:
> the filesystems vere obvioulsy not corrupted because the ones who i  
> care of are just sparsebundles, and they mount fine....

I'm not sure I trust that completely.

I recall that when I used FileVault last year (quite a few OS  
revisions ago, 10.4.7 or thereabouts), a crash could corrupt files -  
usually graphic image files or sounds. As if some multi-media metadata  
trolling had those files open when it crashed, and those files were  
*scragged*.

The encrypted filesystem passed an fsck_hfs, but had corrupted file  
data. Generally I'd recover from a backup in that case. It's also  
possible to run DiskWarrior on sparsebundles, you might want to do  
something like that.


My data-corruption problem had *nothing* to do with ZFS, of course,  
and I haven't seen it in a while. But I have seen disk images that  
passed fsck and yet had data corruption at the file level.


From ndellofano at apple.com  Wed Apr  9 13:18:59 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed Apr  9 13:18:52 2008
Subject: [zfs-discuss] zpool status -v
In-Reply-To: <17348603-B0C9-4B6E-AF13-3E2B16325BBC@spamfreemail.de>
References: <17348603-B0C9-4B6E-AF13-3E2B16325BBC@spamfreemail.de>
Message-ID: <AB3828E3-B960-4E58-8B64-979317C3BD53@apple.com>

yikes :(
What is your config?  Something huge?  what kind of system are you  
running on?  and is it only zpool status? or any zpool command freezes  
the system?

Noel

On Apr 9, 2008, at 9:39 AM, Franz Schmalzl wrote:

> zpool status  -v gives me a complete system freeze....
>
> any ideas ?
>
>
> regards
>
> franz schmalzl _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From jbsnyder at gmail.com  Thu Apr  3 11:32:52 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Wed Apr  9 13:21:20 2008
Subject: [zfs-discuss] Some More on Firefox
Message-ID: <33644d3c0804031132n278db33ey8da4b869971ef989@mail.gmail.com>

I had played with DTrace a little before with firefox and the plugin
installation bug, and I decided to pull it out again today when b5 of
Firefox 3 became available today.

Related to the previous "fcntl" troubles I noticed before, I've now
got a trace from Instruments, which may or may not be useful.

I've attached the trace file, and below is one of the stack traces
(attached has 3 since I let it try and update 3 extensions).

   0 libSystem.B.dylib fcntl$UNIX2003
   1 CarbonCore PBAllocateForkSync
   2 CarbonCore FSAllocateFork
   3 XUL XPT_Do64
   4 XUL XPT_Do64
   5 XUL XPT_Do64
   6 XUL XPT_Do64
   7 XUL NS_NewLocalFile_P
   8 XUL NS_NewLocalFile_P
   9 XUL NS_InvokeByIndex_P
  10 XUL XRE_GetFileFromPath
  11 XUL XRE_GetFileFromPath
  12 libmozjs.dylib js_Invoke
  13 libmozjs.dylib js_FreeStack
  14 libmozjs.dylib js_Invoke
  15 XUL XRE_GetFileFromPath
  16 XUL XRE_GetFileFromPath
  17 XUL NS_InvokeByIndex_P
  18 XUL NS_InvokeByIndex_P
  19 XUL XRE_GetFileFromPath
  20 XUL XRE_GetFileFromPath
  21 XUL XRE_GetFileFromPath
  22 XUL XRE_GetFileFromPath
  23 XUL XRE_GetFileFromPath
  24 XUL XRE_GetFileFromPath
  25 XUL JNIEnv_::CallStaticObjectMethod(_jclass*, _jmethodID*, ...)
  26 XUL NS_GetComponentRegistrar_P
  27 XUL GetSecurityContext(JNIEnv_*, nsISecurityContext**)
  28 XUL JSD_GetValueForObject
  29 XUL JSD_GetValueForObject
  30 CoreFoundation CFRunLoopRunSpecific
  31 CoreFoundation CFRunLoopRunInMode
  32 HIToolbox RunCurrentEventLoopInMode
  33 HIToolbox ReceiveNextEventCommon
  34 HIToolbox BlockUntilNextEventMatchingListInMode
  35 AppKit _DPSNextEvent
  36 AppKit -[NSApplication nextEventMatchingMask:untilDate:inMode:dequeue:]
  37 AppKit -[NSApplication run]
  38 XUL JSD_GetValueForObject
  39 XUL XRE_GetFileFromPath
  40 XUL XRE_main
  41 firefox-bin start
  42 firefox-bin start

The other thing I noticed in dtruss is this sequence of syscalls
leading up to the error:
89550/0x54a93c8:  getattrlist("/.vol/754974724/71903/tmp.xpi\0",
0xBFFFAEB0, 0xBFFFAD00)         = 0 0
89550/0x54a93c8:  setattrlist(0xBFFFA454, 0xBFFFABF8, 0xBFFFA3B0)        = 0 0
89550/0x54a93c8:  getattrlist("/.vol/754974724/867350\0", 0xBFFFA8B0,
0xBFFFA6E0)        = 0 0
89550/0x54a93c8:  getattrlist("/.vol/754974724/867350\0", 0xBFFFA8B0,
0xBFFFA6E0)        = 0 0
89550/0x54a93c8:  getattrlist("/.vol/754974724/867350\0", 0xBFFFA8B0,
0xBFFFA6E0)        = 0 0
89550/0x54a93c8:  getattrlist("/.vol/754974724/867362\0", 0xBFFFA510,
0xBFFFA340)        = 0 0
89550/0x54a93c8:  getattrlist("/.vol/754974724/867362\0", 0xBFFFA1C0,
0xBFFF9CE0)        = 0 0
89550/0x54a93c8:  open("/.vol/754974724/71903/tmp.xpi\0", 0x26, 0x1B6)
      = 65 0
89550/0x54a93c8:  getattrlist("/.vol/754974724/867350\0", 0xBFFFA1C0,
0xBFFF9CE0)        = 0 0
89550/0x54a93c8:  open("/.vol/754974724/581663/tmp.xpi\0", 0x0, 0x1B6)
      = 66 0
89550/0x54a93c8:  getattrlist("/.vol/754974724/71903/tmp.xpi\0",
0xBFFFA820, 0xBFFFA660)         = 0 0
89550/0x54a93c8:  fcntl(0x41, 0x2A, 0xFFFFFFFFBFFFABB0)      = -1 Err#45

What is .vol?  I'm not sure if these syscalls are connected with the
same file, but /.vol seems to be owned by root:wheel and have nothing
in it on my HFS+ volume.


-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Firefox_fcntl_error.trace.zip
Type: application/zip
Size: 665353 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080403/99818dd8/Firefox_fcntl_error.trace-0001.zip
From me at joedunn.com  Wed Apr  2 16:41:43 2008
From: me at joedunn.com (Joe Dunn)
Date: Wed Apr  9 13:21:26 2008
Subject: [zfs-discuss] ZFS Share Question
Message-ID: <c61bd13c0804021641k7f251003t75e02136a6cecd5e@mail.gmail.com>

Hi,

Long time lurker, first time poster ;)

Wondering if the sharenfs works in the zfs-102A binary? I am getting the
following

mbp:~ user$ sudo zfs set sharenfs=on zfstest
cannot set property for 'zfstest': the sharenfs property is not yet
supported

Please feel free to ask any questions of me if I missed something. I've been
playing with zfs in freebsd in vmware, this is the first time i've used
hardware.

Cheers

Joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080402/d555137d/attachment.html
From cdl at asgaard.org  Mon Apr  7 11:44:12 2008
From: cdl at asgaard.org (Christopher LILJENSTOLPE)
Date: Wed Apr  9 13:21:30 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <C433CDE9-FEAF-4D7D-B8A3-7D5136FF96F1@martin-hauser.net>
References: <47E33231.40107@jrv.org>	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>	<47E42E60.6040506@jrv.org>	<6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>	<4AADF1E6-FC85-4829-9D20-9E2CAF2B16C9@gmail.com>	<c61bd13c0804060804h22a131f1oa5924caeca084aa7@mail.gmail.com>	<8C0F3494-FB86-431C-9872-3B57B5C2BB2B@gmail.com>	<c61bd13c0804062249r8e2b1a8iec50b66665cc710b@mail.gmail.com>	<6DFAF372-EFF8-4E58-8B23-69F1161DD807@gmail.com>	<CFA57BDA-2C10-4CD5-B899-D425AD960427@gmail.com>
	<11DDD05B-E100-4115-A5A7-07224EECE11A@martin-hauser.net>
	<47FA08D7.2010901@jrv.org>
	<C433CDE9-FEAF-4D7D-B8A3-7D5136FF96F1@martin-hauser.net>
Message-ID: <3EF46CF4-B83D-4864-A572-EF86E5AF1539@asgaard.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Greetings,

	Can it be case insensitivity in Handbrake?

	Chris

On 07 Apr 2008, at 05.16, Martin Hauser wrote:
> Hmm,
>
> I am encoding from .mpg files, maybe the difference is in directory  
> handling, as DVD's are oriented that way?
>
>
> On Apr 7, 2008, at 13:43 PM, James R. Van Artsdalen wrote:
>> I can reproduce his problem here using the Handbrake 0.9.2 GUI with  
>> no network involved.  It hangs after clicking OK to select the  
>> source material - it doesn't get to the point of encoding  
>> anything.  Handbrake does respond to a cmd-Q and is using about 5%  
>> CPU at that point.  The source material is a DVD copied to the hard  
>> disk.  It selects source material OK from a "Mac OS Extended" disk.
>>
>> Martin Hauser wrote:
>>> I do not have any problems with handbrake running on a local ZFS.  
>>> I encode from and to ZFS volumes, always using a custom mp4 setup.  
>>> I assume it's a problem with ZFS + networking ?
>>>
>>> kind regards
>>>
>>> Martin
>>> ------------------------------------------------------------------------
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

- ---
???
Check my PGP key here:
http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B




-----BEGIN PGP SIGNATURE-----

iQEcBAEBAgAGBQJH+mt8AAoJEGmx2Mt/+Iw/YHcIAJKNkqo5bdUKe5I1IV9cmxrA
17hQ3SQ58n1QlIWQROPQXZuf/uGvYlp38LjQQF5Cs9CY0RrwbeORdGGcJL7ceo91
gvSstxaYi5Dx/EvNUEpurejIWo4SsPG53Iyv1s1gQneOmMX2ft9u3Y9tx0O9G3z/
Z2It1REpmfywGROx25M6bKhnfL7V/ZoCRB1JdEmbtoP9DtZxUJSJ1Kge4P4cgw64
pg5UJvnRz5cZ1mxLSB+uSOwxQqyZn9Zt4Sb28ReEwuCcsRGRPHWU0YT1A8OTwnKf
sURvxOfqPlixiSPHdxkFjPQq5JToiBbx9hbLWxQxhW4k7TPvNCZZgO5wJtOyu/k=
=wzgk
-----END PGP SIGNATURE-----
From ndellofano at apple.com  Wed Apr  9 13:34:51 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed Apr  9 13:34:42 2008
Subject: [zfs-discuss] ZFS Share Question
In-Reply-To: <c61bd13c0804021641k7f251003t75e02136a6cecd5e@mail.gmail.com>
References: <c61bd13c0804021641k7f251003t75e02136a6cecd5e@mail.gmail.com>
Message-ID: <AD0D1E27-1191-473E-A282-E56D9D00F787@apple.com>

Welcome :)

the sharenfs property isn't supported in Mac OSX ZFS because OSX  
doesn't have a share command (as in Solaris http://docs.sun.com/app/docs/doc/816-1055/6m7gh31j0?a=view) 
  or any notion of being able to dynamically share and unshare  
resources.

Hence you can use NFS and AFP and friends with ZFS but you need to  
share it the old fashioned way.

Noel
p.s. this is also the case with iscsi, zones and a few other zfs  
properties that just don't make sense on an OSX system

On Apr 2, 2008, at 4:41 PM, Joe Dunn wrote:

> Hi,
>
> Long time lurker, first time poster ;)
>
> Wondering if the sharenfs works in the zfs-102A binary? I am getting  
> the following
>
> mbp:~ user$ sudo zfs set sharenfs=on zfstest
> cannot set property for 'zfstest': the sharenfs property is not yet  
> supported
>
> Please feel free to ask any questions of me if I missed something.  
> I've been playing with zfs in freebsd in vmware, this is the first  
> time i've used hardware.
>
> Cheers
>
> Joe
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From me at joedunn.com  Wed Apr  9 13:38:27 2008
From: me at joedunn.com (Joe Dunn)
Date: Wed Apr  9 13:36:55 2008
Subject: [zfs-discuss] ZFS Share Question
In-Reply-To: <AD0D1E27-1191-473E-A282-E56D9D00F787@apple.com>
References: <c61bd13c0804021641k7f251003t75e02136a6cecd5e@mail.gmail.com>
	<AD0D1E27-1191-473E-A282-E56D9D00F787@apple.com>
Message-ID: <c61bd13c0804091338p2c4148a8wa7e5ad9b88c1585c@mail.gmail.com>

Noel

Yeah this was my bad and not reading the zfs-macforge FAQ before posting.
Thank you for the follow-up though.

Cheers

Joe

On Wed, Apr 9, 2008 at 4:34 PM, No?l Dellofano <ndellofano@apple.com> wrote:

> Welcome :)
>
> the sharenfs property isn't supported in Mac OSX ZFS because OSX doesn't
> have a share command (as in Solaris
> http://docs.sun.com/app/docs/doc/816-1055/6m7gh31j0?a=view) or any notion
> of being able to dynamically share and unshare resources.
>
> Hence you can use NFS and AFP and friends with ZFS but you need to share
> it the old fashioned way.
>
> Noel
> p.s. this is also the case with iscsi, zones and a few other zfs
> properties that just don't make sense on an OSX system
>
>
> On Apr 2, 2008, at 4:41 PM, Joe Dunn wrote:
>
>  Hi,
> >
> > Long time lurker, first time poster ;)
> >
> > Wondering if the sharenfs works in the zfs-102A binary? I am getting the
> > following
> >
> > mbp:~ user$ sudo zfs set sharenfs=on zfstest
> > cannot set property for 'zfstest': the sharenfs property is not yet
> > supported
> >
> > Please feel free to ask any questions of me if I missed something. I've
> > been playing with zfs in freebsd in vmware, this is the first time i've used
> > hardware.
> >
> > Cheers
> >
> > Joe
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss@lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080409/2379ae40/attachment.html
From bwaters at nrao.edu  Wed Apr  9 13:42:51 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Wed Apr  9 13:41:40 2008
Subject: [zfs-discuss] Status of ZFS on Mac OS X?
In-Reply-To: <A384DE4E-6348-4728-BE8B-B112376FD2D2@yellowspace.net>
References: <fb9be13f644474a1cb6845fa4b9a4c3a@yellowspace.net>
	<47F80FA0.2000604@loveturtle.net>
	<33644d3c0804051703j79c39b6fr2567c049aa18ffa6@mail.gmail.com>
	<A384DE4E-6348-4728-BE8B-B112376FD2D2@yellowspace.net>
Message-ID: <4FDC27DF-7B55-4223-A705-1EE54D189DE7@nrao.edu>


On Apr 8, 2008, at 6:24 PM, Lorenzo Perone wrote:
> On the other hand, it would be interesting to see what happens when
> using File Vault (provided that it is possible to symlink it to
> an encrypted sparse image within a zfs volume). Since zfs snapshots
> are block-wise, at least that would be a way to backup the sparse-
> monster incrementally - and to have a "portable" time machine.


I use FileVault. A lot. For years and years.

Every hour I copy my FileVault sparsebundle to the ZFS pool via rsync,  
then I create a snapshot.

You can mount the sparsebundle by double-clicking on it.

This actually seems to work just fine -- while the FileVault is  
mounted. I think the whole motivation behind implementing  
sparsebundles was to carve up a disk image into small chunks so that  
things like this would work. From hour to hour, only a handful of the  
8-megabyte "bands" change, so the rsync is fast, and of course the ZFS  
snapshots don't take up much room.

To be clear:  an internal 500 GB disk, boots the Mac, has my FileVault  
image on it, no change from a standard Mac setup.  An external disk  
formatted ZFS, every hour we rsync from the internal to the external,  
and make a zfs snapshot.

Here's the script:

==========


#! /bin/bash
#
# file name; /usr/local/bin/backup

# TODO: exclude some things from backup like vm swapfiles

h=$(/bin/hostname -s)

/usr/local/bin/rsync -aEHx  /  /Volumes/pool/backup/${h}/

/usr/sbin/zfs snapshot pool/backup/${h}@$(date +%FT%T)


# delete all but the last four snapshots

snaps=($(zfs list| grep ${h}@20 | cut -f 1 -d ' '))

fourth_from_last=$(( ${#snaps[@]} - 4 ))

i=0
while [ $i -lt $fourth_from_last ]
do
	zfs destroy ${snaps[$i]}
	let "i = $i + 1"
done


==========


and here's the LaunchDaemon that runs it every hour (or so)


====

<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd 
">
<plist version="1.0">
	<dict>
		<key>Disabled</key>
		<false/>
		<key>Label</key>
		<string>edu.nrao.bwaters.backup</string>
		<key>ProgramArguments</key>
		<array>
			<string>/usr/local/bin/backup</string>
		</array>
		<key>RunAtLoad</key>
		<true/>
		<key>StandardErrorPath</key>
		<string>/var/log/rsync-backup.log</string>
		<key>StandardOutPath</key>
		<string>/var/log/rsync-backup.log</string>
		<key>StartInterval</key>
		<integer>4380</integer>
	</dict>
</plist>


====



From ndellofano at apple.com  Wed Apr  9 13:45:03 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed Apr  9 13:45:03 2008
Subject: [zfs-discuss] Version incompatibility when importing zpool from
	Solaris
In-Reply-To: <C7481F86-29E3-4AF3-86AC-2B23A13F984F@gmail.com>
References: <4bc43a2b0804061050w292384bfn229eb15023363003@mail.gmail.com>
	<C7481F86-29E3-4AF3-86AC-2B23A13F984F@gmail.com>
Message-ID: <DDB4C0B1-955A-4C53-BFD5-4B130287F07E@apple.com>

Hey Dave,

We are currently synced with Solaris snv_72, which is ZFS version 8,  
however to maintain backward compatibility with the readonly version  
of ZFS (released on Leopard) we kept the writable version of ZFS  
version 6 by default, but it has the ability to understand version 8  
pools.  Since the highest version in the code is version 8, you should  
be allowed to import your new pool.  Can you double check your current  
pool is version 8?  (zpool upgrade will let you know what version your  
pool is).

  If so let me know and I'll gen you some custom bits with the default  
at 8 as it should be to try and import your pool.

Noel



On Apr 9, 2008, at 8:40 AM, Karl Gyllstrom wrote:

> If you have the space, maybe you can just make a new pool on the new  
> machine and send the snapshot of the old one.
>
> On Apr 6, 2008, at 1:50 PM, Dave Loose wrote:
>
>> About a year ago, I put a Solaris box together to play with ZFS. I
>> mainly use it to serve my iTunes library and DVD rips. I used samba
>> for this, and I was continually frustrated by its flakiness. Both
>> samba and Solaris's built-in smb server don't seem to play well with
>> the other computers in my house (all of which are Macs).
>>
>> I finally bit the bullet and bought a Mac Pro (I'd been needing a new
>> desktop for a while) and I decided I'd try to migrate the zpool to my
>> new machine. The problem is that I upgraded my Solaris machine rather
>> frequently and now when I try to 'zpool import storage' I get this
>> message: cannot import 'storage': pool is formatted using a newer ZFS
>> version
>>
>> I kept my Solaris machine pretty current. It has Nevada release 84  
>> and
>> the zpool format is whatever was most current for that release. I
>> don't remember which version it is off the top of my head.
>>
>> Is there any way for me to get my zfs volume running under OS X? I
>> read the FAQ and it seems to suggest that OS X creates new zpools
>> using an older version, but is there any way to import a newer
>> version? Is there any way to "downgrade" the version?
>>
>> Thanks for the help,
>>
>> Dave
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From jbsnyder at gmail.com  Wed Apr  9 15:27:32 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Wed Apr  9 15:25:53 2008
Subject: [zfs-discuss] Status of ZFS on Mac OS X?
In-Reply-To: <4FDC27DF-7B55-4223-A705-1EE54D189DE7@nrao.edu>
References: <fb9be13f644474a1cb6845fa4b9a4c3a@yellowspace.net>
	<47F80FA0.2000604@loveturtle.net>
	<33644d3c0804051703j79c39b6fr2567c049aa18ffa6@mail.gmail.com>
	<A384DE4E-6348-4728-BE8B-B112376FD2D2@yellowspace.net>
	<4FDC27DF-7B55-4223-A705-1EE54D189DE7@nrao.edu>
Message-ID: <33644d3c0804091527v50a33182tf4a6ee04a9b8de89@mail.gmail.com>

There's also EncFS and EncFSVault.  I've not used encfsvault with ZFS,
but I've used encfs quite a bit, seems to work fine.  It is just an
overlay filesystem that encrypts per-file, and is built on FUSE.  The
files and filenames are encrypted but still live as files on the
filesystem.

encfs homepage:
http://www.arg0.net/encfs
Intro to encfs:
http://www.arg0.net/encfsintro

MacOS Binaries & EncFSVault
http://code.google.com/p/encfsvault/

Nice and fairly elegant. You can then just skip the mounts and rsync
the encrypted files.  It works on OS X, Linux, and probably FreeBSD.
I think there was a windows client at one point, but it may not be
maintained.

I have not used it for a home, but it works great for individual
"private" directories that you want encrypted.

-jsnyder

On Wed, Apr 9, 2008 at 3:42 PM, Boyd Waters <bwaters@nrao.edu> wrote:
>
>  On Apr 8, 2008, at 6:24 PM, Lorenzo Perone wrote:
>
> > On the other hand, it would be interesting to see what happens when
> > using File Vault (provided that it is possible to symlink it to
> > an encrypted sparse image within a zfs volume). Since zfs snapshots
> > are block-wise, at least that would be a way to backup the sparse-
> > monster incrementally - and to have a "portable" time machine.
> >
>
>
>  I use FileVault. A lot. For years and years.
>
>  Every hour I copy my FileVault sparsebundle to the ZFS pool via rsync, then
> I create a snapshot.
>
>  You can mount the sparsebundle by double-clicking on it.
>
>  This actually seems to work just fine -- while the FileVault is mounted. I
> think the whole motivation behind implementing sparsebundles was to carve up
> a disk image into small chunks so that things like this would work. From
> hour to hour, only a handful of the 8-megabyte "bands" change, so the rsync
> is fast, and of course the ZFS snapshots don't take up much room.
>
>  To be clear:  an internal 500 GB disk, boots the Mac, has my FileVault
> image on it, no change from a standard Mac setup.  An external disk
> formatted ZFS, every hour we rsync from the internal to the external, and
> make a zfs snapshot.
>
>  Here's the script:
>
>  ==========
>
>
>  #! /bin/bash
>  #
>  # file name; /usr/local/bin/backup
>
>  # TODO: exclude some things from backup like vm swapfiles
>
>  h=$(/bin/hostname -s)
>
>  /usr/local/bin/rsync -aEHx  /  /Volumes/pool/backup/${h}/
>
>  /usr/sbin/zfs snapshot pool/backup/${h}@$(date +%FT%T)
>
>
>  # delete all but the last four snapshots
>
>  snaps=($(zfs list| grep ${h}@20 | cut -f 1 -d ' '))
>
>  fourth_from_last=$(( ${#snaps[@]} - 4 ))
>
>  i=0
>  while [ $i -lt $fourth_from_last ]
>  do
>         zfs destroy ${snaps[$i]}
>         let "i = $i + 1"
>  done
>
>
>  ==========
>
>
>  and here's the LaunchDaemon that runs it every hour (or so)
>
>
>  ====
>
>  <?xml version="1.0" encoding="UTF-8"?>
>  <!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN"
> "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
>  <plist version="1.0">
>         <dict>
>                 <key>Disabled</key>
>                 <false/>
>                 <key>Label</key>
>                 <string>edu.nrao.bwaters.backup</string>
>                 <key>ProgramArguments</key>
>                 <array>
>                         <string>/usr/local/bin/backup</string>
>                 </array>
>                 <key>RunAtLoad</key>
>                 <true/>
>                 <key>StandardErrorPath</key>
>                 <string>/var/log/rsync-backup.log</string>
>                 <key>StandardOutPath</key>
>                 <string>/var/log/rsync-backup.log</string>
>                 <key>StartInterval</key>
>                 <integer>4380</integer>
>         </dict>
>  </plist>
>
>
>  ====
>
>
>
>
>
>  _______________________________________________
>  zfs-discuss mailing list
>  zfs-discuss@lists.macosforge.org
>  http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
From ndellofano at apple.com  Wed Apr  9 15:31:05 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed Apr  9 15:30:54 2008
Subject: [zfs-discuss] Some More on Firefox
In-Reply-To: <33644d3c0804031132n278db33ey8da4b869971ef989@mail.gmail.com>
References: <33644d3c0804031132n278db33ey8da4b869971ef989@mail.gmail.com>
Message-ID: <EA776520-82BF-4CA5-9C45-CC8EC62D6E53@apple.com>

The '.vol' party you see is basically a mechanism for looking up an  
object by id.

The first number after the "/.vol/" is the fsid which specifies which  
file system and the second number is the object id.  ZFS supports this  
just fine (the calls cited below are all passing).

The fcntl 65 (0x41) seems suspicious as there is no fnctl selector  
defined for this value.

Noel

On Apr 3, 2008, at 11:32 AM, James Snyder wrote:

> I had played with DTrace a little before with firefox and the plugin
> installation bug, and I decided to pull it out again today when b5 of
> Firefox 3 became available today.
>
> Related to the previous "fcntl" troubles I noticed before, I've now
> got a trace from Instruments, which may or may not be useful.
>
> I've attached the trace file, and below is one of the stack traces
> (attached has 3 since I let it try and update 3 extensions).
>
>   0 libSystem.B.dylib fcntl$UNIX2003
>   1 CarbonCore PBAllocateForkSync
>   2 CarbonCore FSAllocateFork
>   3 XUL XPT_Do64
>   4 XUL XPT_Do64
>   5 XUL XPT_Do64
>   6 XUL XPT_Do64
>   7 XUL NS_NewLocalFile_P
>   8 XUL NS_NewLocalFile_P
>   9 XUL NS_InvokeByIndex_P
>  10 XUL XRE_GetFileFromPath
>  11 XUL XRE_GetFileFromPath
>  12 libmozjs.dylib js_Invoke
>  13 libmozjs.dylib js_FreeStack
>  14 libmozjs.dylib js_Invoke
>  15 XUL XRE_GetFileFromPath
>  16 XUL XRE_GetFileFromPath
>  17 XUL NS_InvokeByIndex_P
>  18 XUL NS_InvokeByIndex_P
>  19 XUL XRE_GetFileFromPath
>  20 XUL XRE_GetFileFromPath
>  21 XUL XRE_GetFileFromPath
>  22 XUL XRE_GetFileFromPath
>  23 XUL XRE_GetFileFromPath
>  24 XUL XRE_GetFileFromPath
>  25 XUL JNIEnv_::CallStaticObjectMethod(_jclass*, _jmethodID*, ...)
>  26 XUL NS_GetComponentRegistrar_P
>  27 XUL GetSecurityContext(JNIEnv_*, nsISecurityContext**)
>  28 XUL JSD_GetValueForObject
>  29 XUL JSD_GetValueForObject
>  30 CoreFoundation CFRunLoopRunSpecific
>  31 CoreFoundation CFRunLoopRunInMode
>  32 HIToolbox RunCurrentEventLoopInMode
>  33 HIToolbox ReceiveNextEventCommon
>  34 HIToolbox BlockUntilNextEventMatchingListInMode
>  35 AppKit _DPSNextEvent
>  36 AppKit -[NSApplication  
> nextEventMatchingMask:untilDate:inMode:dequeue:]
>  37 AppKit -[NSApplication run]
>  38 XUL JSD_GetValueForObject
>  39 XUL XRE_GetFileFromPath
>  40 XUL XRE_main
>  41 firefox-bin start
>  42 firefox-bin start
>
> The other thing I noticed in dtruss is this sequence of syscalls
> leading up to the error:
> 89550/0x54a93c8:  getattrlist("/.vol/754974724/71903/tmp.xpi\0",
> 0xBFFFAEB0, 0xBFFFAD00)         = 0 0
> 89550/0x54a93c8:  setattrlist(0xBFFFA454, 0xBFFFABF8,  
> 0xBFFFA3B0)        = 0 0
> 89550/0x54a93c8:  getattrlist("/.vol/754974724/867350\0", 0xBFFFA8B0,
> 0xBFFFA6E0)        = 0 0
> 89550/0x54a93c8:  getattrlist("/.vol/754974724/867350\0", 0xBFFFA8B0,
> 0xBFFFA6E0)        = 0 0
> 89550/0x54a93c8:  getattrlist("/.vol/754974724/867350\0", 0xBFFFA8B0,
> 0xBFFFA6E0)        = 0 0
> 89550/0x54a93c8:  getattrlist("/.vol/754974724/867362\0", 0xBFFFA510,
> 0xBFFFA340)        = 0 0
> 89550/0x54a93c8:  getattrlist("/.vol/754974724/867362\0", 0xBFFFA1C0,
> 0xBFFF9CE0)        = 0 0
> 89550/0x54a93c8:  open("/.vol/754974724/71903/tmp.xpi\0", 0x26, 0x1B6)
>      = 65 0
> 89550/0x54a93c8:  getattrlist("/.vol/754974724/867350\0", 0xBFFFA1C0,
> 0xBFFF9CE0)        = 0 0
> 89550/0x54a93c8:  open("/.vol/754974724/581663/tmp.xpi\0", 0x0, 0x1B6)
>      = 66 0
> 89550/0x54a93c8:  getattrlist("/.vol/754974724/71903/tmp.xpi\0",
> 0xBFFFA820, 0xBFFFA660)         = 0 0
> 89550/0x54a93c8:  fcntl(0x41, 0x2A, 0xFFFFFFFFBFFFABB0)      = -1  
> Err#45
>
> What is .vol?  I'm not sure if these syscalls are connected with the
> same file, but /.vol seems to be owned by root:wheel and have nothing
> in it on my HFS+ volume.
>
>
> -- 
> James Snyder
> Biomedical Engineering
> Northwestern University
> jbsnyder@gmail.com
> < 
> Firefox_fcntl_error 
> .trace.zip>_______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From info at martin-hauser.net  Thu Apr 10 00:27:55 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Thu Apr 10 01:25:10 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <3EF46CF4-B83D-4864-A572-EF86E5AF1539@asgaard.org>
References: <47E33231.40107@jrv.org>	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>	<47E42E60.6040506@jrv.org>	<6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>	<4AADF1E6-FC85-4829-9D20-9E2CAF2B16C9@gmail.com>	<c61bd13c0804060804h22a131f1oa5924caeca084aa7@mail.gmail.com>	<8C0F3494-FB86-431C-9872-3B57B5C2BB2B@gmail.com>	<c61bd13c0804062249r8e2b1a8iec50b66665cc710b@mail.gmail.com>	<6DFAF372-EFF8-4E58-8B23-69F1161DD807@gmail.com>	<CFA57BDA-2C10-4CD5-B899-D425AD960427@gmail.com>
	<11DDD05B-E100-4115-A5A7-07224EECE11A@martin-hauser.net>
	<47FA08D7.2010901@jrv.org>
	<C433CDE9-FEAF-4D7D-B8A3-7D5136FF96F1@martin-hauser.net>
	<3EF46CF4-B83D-4864-A572-EF86E5AF1539@asgaard.org>
Message-ID: <E34E4AD4-A863-4F52-BEA6-27FC4CE2EE2B@martin-hauser.net>

Very interesting, now that you mention it, my handbrake is on a case  
sensitive HFS+ and encodes from and to a ZFS (which is also case  
sensitive as we all know).

Maybe Sean's copy of Handbrake is stored on case insensetive HFS+ and  
that makes the difference ??

Martin

On Apr 7, 2008, at 20:44 PM, Christopher LILJENSTOLPE wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Greetings,
>
> 	Can it be case insensitivity in Handbrake?
>
> 	Chris
>
> On 07 Apr 2008, at 05.16, Martin Hauser wrote:
>> Hmm,
>>
>> I am encoding from .mpg files, maybe the difference is in directory  
>> handling, as DVD's are oriented that way?
>>
>>
>> On Apr 7, 2008, at 13:43 PM, James R. Van Artsdalen wrote:
>>> I can reproduce his problem here using the Handbrake 0.9.2 GUI  
>>> with no network involved.  It hangs after clicking OK to select  
>>> the source material - it doesn't get to the point of encoding  
>>> anything.  Handbrake does respond to a cmd-Q and is using about 5%  
>>> CPU at that point.  The source material is a DVD copied to the  
>>> hard disk.  It selects source material OK from a "Mac OS Extended"  
>>> disk.
>>>
>>> Martin Hauser wrote:
>>>> I do not have any problems with handbrake running on a local ZFS.  
>>>> I encode from and to ZFS volumes, always using a custom mp4  
>>>> setup. I assume it's a problem with ZFS + networking ?
>>>>
>>>> kind regards
>>>>
>>>> Martin
>>>> ------------------------------------------------------------------------
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> - ---
> ???
> Check my PGP key here:
> http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B
>
>
>
>
> -----BEGIN PGP SIGNATURE-----
>
> iQEcBAEBAgAGBQJH+mt8AAoJEGmx2Mt/+Iw/YHcIAJKNkqo5bdUKe5I1IV9cmxrA
> 17hQ3SQ58n1QlIWQROPQXZuf/uGvYlp38LjQQF5Cs9CY0RrwbeORdGGcJL7ceo91
> gvSstxaYi5Dx/EvNUEpurejIWo4SsPG53Iyv1s1gQneOmMX2ft9u3Y9tx0O9G3z/
> Z2It1REpmfywGROx25M6bKhnfL7V/ZoCRB1JdEmbtoP9DtZxUJSJ1Kge4P4cgw64
> pg5UJvnRz5cZ1mxLSB+uSOwxQqyZn9Zt4Sb28ReEwuCcsRGRPHWU0YT1A8OTwnKf
> sURvxOfqPlixiSPHdxkFjPQq5JToiBbx9hbLWxQxhW4k7TPvNCZZgO5wJtOyu/k=
> =wzgk
> -----END PGP SIGNATURE-----
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080410/d712a722/PGP.bin
From cdl at asgaard.org  Thu Apr 10 03:22:35 2008
From: cdl at asgaard.org (Christopher LILJENSTOLPE)
Date: Thu Apr 10 03:20:56 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <E34E4AD4-A863-4F52-BEA6-27FC4CE2EE2B@martin-hauser.net>
References: <47E33231.40107@jrv.org>	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>	<47E42E60.6040506@jrv.org>	<6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>	<4AADF1E6-FC85-4829-9D20-9E2CAF2B16C9@gmail.com>	<c61bd13c0804060804h22a131f1oa5924caeca084aa7@mail.gmail.com>	<8C0F3494-FB86-431C-9872-3B57B5C2BB2B@gmail.com>	<c61bd13c0804062249r8e2b1a8iec50b66665cc710b@mail.gmail.com>	<6DFAF372-EFF8-4E58-8B23-69F1161DD807@gmail.com>	<CFA57BDA-2C10-4CD5-B899-D425AD960427@gmail.com>
	<11DDD05B-E100-4115-A5A7-07224EECE11A@martin-hauser.net>
	<47FA08D7.2010901@jrv.org>
	<C433CDE9-FEAF-4D7D-B8A3-7D5136FF96F1@martin-hauser.net>
	<3EF46CF4-B83D-4864-A572-EF86E5AF1539@asgaard.org>
	<E34E4AD4-A863-4F52-BEA6-27FC4CE2EE2B@martin-hauser.net>
Message-ID: <C9D41D73-DEC3-4288-A905-4D5689481C8A@asgaard.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

It would make sense, if Sean's copy of Handbrake is not case  
sensitive, I think...  Too late, need sleep....

	Chris

On 10 Apr 2008, at 00.27, Martin Hauser wrote:
> Very interesting, now that you mention it, my handbrake is on a case  
> sensitive HFS+ and encodes from and to a ZFS (which is also case  
> sensitive as we all know).
>
> Maybe Sean's copy of Handbrake is stored on case insensetive HFS+  
> and that makes the difference ??
>
> Martin
>
> On Apr 7, 2008, at 20:44 PM, Christopher LILJENSTOLPE wrote:
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> Greetings,
>>
>> 	Can it be case insensitivity in Handbrake?
>>
>> 	Chris
>>
>> On 07 Apr 2008, at 05.16, Martin Hauser wrote:
>>> Hmm,
>>>
>>> I am encoding from .mpg files, maybe the difference is in  
>>> directory handling, as DVD's are oriented that way?
>>>
>>>
>>> On Apr 7, 2008, at 13:43 PM, James R. Van Artsdalen wrote:
>>>> I can reproduce his problem here using the Handbrake 0.9.2 GUI  
>>>> with no network involved.  It hangs after clicking OK to select  
>>>> the source material - it doesn't get to the point of encoding  
>>>> anything.  Handbrake does respond to a cmd-Q and is using about  
>>>> 5% CPU at that point.  The source material is a DVD copied to the  
>>>> hard disk.  It selects source material OK from a "Mac OS  
>>>> Extended" disk.
>>>>
>>>> Martin Hauser wrote:
>>>>> I do not have any problems with handbrake running on a local  
>>>>> ZFS. I encode from and to ZFS volumes, always using a custom mp4  
>>>>> setup. I assume it's a problem with ZFS + networking ?
>>>>>
>>>>> kind regards
>>>>>
>>>>> Martin
>>>>> ------------------------------------------------------------------------
>>>>>
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss@lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>> - ---
>> ???
>> Check my PGP key here:
>> http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B
>>
>>
>>
>>
>> -----BEGIN PGP SIGNATURE-----
>>
>> iQEcBAEBAgAGBQJH+mt8AAoJEGmx2Mt/+Iw/YHcIAJKNkqo5bdUKe5I1IV9cmxrA
>> 17hQ3SQ58n1QlIWQROPQXZuf/uGvYlp38LjQQF5Cs9CY0RrwbeORdGGcJL7ceo91
>> gvSstxaYi5Dx/EvNUEpurejIWo4SsPG53Iyv1s1gQneOmMX2ft9u3Y9tx0O9G3z/
>> Z2It1REpmfywGROx25M6bKhnfL7V/ZoCRB1JdEmbtoP9DtZxUJSJ1Kge4P4cgw64
>> pg5UJvnRz5cZ1mxLSB+uSOwxQqyZn9Zt4Sb28ReEwuCcsRGRPHWU0YT1A8OTwnKf
>> sURvxOfqPlixiSPHdxkFjPQq5JToiBbx9hbLWxQxhW4k7TPvNCZZgO5wJtOyu/k=
>> =wzgk
>> -----END PGP SIGNATURE-----
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

- ---
???
Check my PGP key here:
http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B




-----BEGIN PGP SIGNATURE-----

iQEcBAEBAgAGBQJH/eprAAoJEGmx2Mt/+Iw/s1EH+wWVlx/Gnd6PlvY8jEuyy3J4
V4YEwE+BZDdgFvVClxhWgq6eyl8XsHzt2qvsmb1BGOOH0kt+r7ixINQo1B7urhhI
vm2/TL8C2oGUwN5O4tunWUcZw/iG2IXPwKvXmtscAFXXgx3qX0jO2V4jnSAqo1s/
O0rbyvvXoprT/C2zChGVC/p4HmnOdUVkBcQXgQpZmmd3z+0sTzn+x6BslrpX13t0
b8dKA3tAhzZgbHMbpyqgqNPBz26E2FHV2vpOtSB+45DbpXx3N5aK8k2+Ynf8EQyK
7VesVR3VZ82ze7ut0ZLn6IzxQKLSKAsvJ7g/ey092/joapJ3gzWjkMPf0VjS6qE=
=sG5x
-----END PGP SIGNATURE-----
From david299792 at googlemail.com  Thu Apr 10 05:52:03 2008
From: david299792 at googlemail.com (David Ritchie)
Date: Thu Apr 10 05:50:28 2008
Subject: [zfs-discuss] VMWare Fusion and mmap on ZFS
Message-ID: <AA6A14FF-5DC1-4212-9290-FA578A5B991F@googlemail.com>

Hi,

I've been storing my VMWare fusion machines on my ZFS volume for some  
time, so that I can cheaply snapshot and clone them at will for one  
thing. The one problem I had with this was that they wouldn't suspend  
and resume properly. (not when I did any snapshots or clones anyway).  
I presume this is because zfs-102A isn't mmap coherent as Noel says.

In the course of trying to keep down the size of some snapshots I've  
found a workaround though. If you add in the following line to a VMX  
config file:-
	mainMem.useNamedFile = "false"

It stops VMWare from using a named mem file in the VM directory (on  
ZFS). It puts it somewhere else instead I think. This seems to keep  
VMWare happy as far as suspend/resume is concerned on ZFS. I even seem  
to be able, now to:-
	1. start up a VM from ZFS
	2. suspend it
	3. do a zfs snapshot
	4. clone it
	5. start up the cloned VM (VMware asks whether it was copied or moved  
and I click copied).
	6. start up the original VM

This is really incredibly useful. It uses much less disk space than  
just copying the whole VM directory.

-- David
From jeroenvandeven at gmail.com  Thu Apr 10 07:32:39 2008
From: jeroenvandeven at gmail.com (Jeroen van de Ven)
Date: Thu Apr 10 07:30:57 2008
Subject: [zfs-discuss] ZFS Share Question
In-Reply-To: <c61bd13c0804091338p2c4148a8wa7e5ad9b88c1585c@mail.gmail.com>
References: <c61bd13c0804021641k7f251003t75e02136a6cecd5e@mail.gmail.com>
	<AD0D1E27-1191-473E-A282-E56D9D00F787@apple.com>
	<c61bd13c0804091338p2c4148a8wa7e5ad9b88c1585c@mail.gmail.com>
Message-ID: <b99b378d0804100732l3da2cd2al3a76dce7e4a539b5@mail.gmail.com>

Hi all,

I just got a kernel panic shortly after deleting the Trash with
Terminal: sudo rm -rf /Volumes/puddle/.Trashes
I usually do this (or the same but with /* added to the end) to empty
the trash because I know it doesn't work the normal way, and this is
the first time it crashed. Here's the log:

Thu Apr 10 22:28:10 2008
panic(cpu 0 caller 0x001A83A6): Double fault at 0x00ad211c,
thread:0x6d484f0, trapno:0x8, err:0x0),registers:
CR0: 0x8001003b, CR2: 0x39a7bfc8, CR3: 0x0145b000, CR4: 0x00000660
EAX: 0x39a7c0ac, EBX: 0x565441e4, ECX: 0x052b26bc, EDX: 0x00004000
ESP: 0x39a7bfd0, EBP: 0x39a7c118, ESI: 0x00000000, EDI: 0x00000000
EFL: 0x00010246, EIP: 0x00ad211c

Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x4eae28 : 0x12b0f7 (0x4581f4 0x4eae5c 0x133230 0x0)
0x4eae78 : 0x1a83a6 (0x4611a4 0xad211c 0x6d484f0 0x8)
0x4eaf58 : 0x19fc73 (0x4eaf70 0x0 0x0 0x0)
0x39a7c118 : 0xac9f26 (0x52b26bc 0x424000 0x0 0x4000)
0x39a7c1a8 : 0xad76be (0x56544190 0x0 0x2 0xb42d98)
0x39a7c1f8 : 0xad794e (0x52c1f00 0x212e 0x0 0x1)
0x39a7c228 : 0xad2fde (0x52c1f00 0x212e 0x0 0xb3f9a4)
0x39a7c288 : 0xb08d92 (0x515e220 0x212e 0x0 0x0)
0x39a7c338 : 0xb11598 (0x39a7c4ec 0x0 0x0 0x39a7c4ec)
0x39a7c508 : 0xb07fa1 (0x566bd928 0x212 0x39a7c538 0x52d5230)
0x39a7c548 : 0xab1e5b (0x566bd928 0x1 0x39a7c598 0x52d5230)
0x39a7c588 : 0x1f3eb1 (0x39a7c5a4 0x212 0x39a7c5ac 0x0)
0x39a7c5c8 : 0x1db45f (0x51d7990 0x56efc64 0x0 0x2)
0x39a7c618 : 0x1db67b (0x51d7990 0x1 0x39a7c648 0x19d4b1)
0x39a7c668 : 0x1dd2e6 (0x0 0x4f43c0 0x39a7c708 0xaca404)
0x39a7c6e8 : 0xb08ca5 (0x0 0x34 0x39a7c71c 0x566baaf4)
	Backtrace continues...
      Kernel loadable modules in backtrace (with dependencies):
         com.apple.filesystems.zfs(8.0)@0xaaa000->0xb75fff

BSD process name corresponding to current thread: du

Mac OS version:
9C7010

Kernel version:
Darwin Kernel Version 9.2.2: Tue Mar  4 21:17:34 PST 2008;
root:xnu-1228.4.31~1/RELEASE_I386
System model name: MacBookPro3,1 (Mac-F4238BC8)

Is it of any help?

Sincerely,
Jeroen
From lists at loveturtle.net  Thu Apr 10 13:48:15 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Thu Apr 10 13:46:33 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <C9D41D73-DEC3-4288-A905-4D5689481C8A@asgaard.org>
References: <47E33231.40107@jrv.org>	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>	<47E42E60.6040506@jrv.org>	<6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>	<4AADF1E6-FC85-4829-9D20-9E2CAF2B16C9@gmail.com>	<c61bd13c0804060804h22a131f1oa5924caeca084aa7@mail.gmail.com>	<8C0F3494-FB86-431C-9872-3B57B5C2BB2B@gmail.com>	<c61bd13c0804062249r8e2b1a8iec50b66665cc710b@mail.gmail.com>	<6DFAF372-EFF8-4E58-8B23-69F1161DD807@gmail.com>	<CFA57BDA-2C10-4CD5-B899-D425AD960427@gmail.com>	<11DDD05B-E100-4115-A5A7-07224EECE11A@martin-hauser.net>	<47FA08D7.2010901@jrv.org>	<C433CDE9-FEAF-4D7D-B8A3-7D5136FF96F1@martin-hauser.net>	<3EF46CF4-B83D-4864-A572-EF86E5AF1539@asgaard.org>	<E34E4AD4-A863-4F52-BEA6-27FC4CE2EE2B@martin-hauser.net>
	<C9D41D73-DEC3-4288-A905-4D5689481C8A@asgaard.org>
Message-ID: <47FE7D0F.4050503@loveturtle.net>

I use handbrake and I'm unable to read from dvd directories stored on 
ZFS but if i copy them to my case sensitive HFS+ partition it works just 
fine. case sensitivity isn't the issue.

Christopher LILJENSTOLPE wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> It would make sense, if Sean's copy of Handbrake is not case 
> sensitive, I think...  Too late, need sleep....
>
>     Chris
>
> On 10 Apr 2008, at 00.27, Martin Hauser wrote:
>> Very interesting, now that you mention it, my handbrake is on a case 
>> sensitive HFS+ and encodes from and to a ZFS (which is also case 
>> sensitive as we all know).
>>
>> Maybe Sean's copy of Handbrake is stored on case insensetive HFS+ and 
>> that makes the difference ??
>>
>> Martin
>>
>> On Apr 7, 2008, at 20:44 PM, Christopher LILJENSTOLPE wrote:
>>> -----BEGIN PGP SIGNED MESSAGE-----
>>> Hash: SHA1
>>>
>>> Greetings,
>>>
>>>     Can it be case insensitivity in Handbrake?
>>>
>>>     Chris
>>>
>>> On 07 Apr 2008, at 05.16, Martin Hauser wrote:
>>>> Hmm,
>>>>
>>>> I am encoding from .mpg files, maybe the difference is in directory 
>>>> handling, as DVD's are oriented that way?
>>>>
>>>>
>>>> On Apr 7, 2008, at 13:43 PM, James R. Van Artsdalen wrote:
>>>>> I can reproduce his problem here using the Handbrake 0.9.2 GUI 
>>>>> with no network involved.  It hangs after clicking OK to select 
>>>>> the source material - it doesn't get to the point of encoding 
>>>>> anything.  Handbrake does respond to a cmd-Q and is using about 5% 
>>>>> CPU at that point.  The source material is a DVD copied to the 
>>>>> hard disk.  It selects source material OK from a "Mac OS Extended" 
>>>>> disk.
>>>>>
>>>>> Martin Hauser wrote:
>>>>>> I do not have any problems with handbrake running on a local ZFS. 
>>>>>> I encode from and to ZFS volumes, always using a custom mp4 
>>>>>> setup. I assume it's a problem with ZFS + networking ?
>>>>>>
>>>>>> kind regards
>>>>>>
>>>>>> Martin
>>>>>> ------------------------------------------------------------------------ 
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> zfs-discuss mailing list
>>>>>> zfs-discuss@lists.macosforge.org
>>>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>>>>
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>>> - ---
>>> ???
>>> Check my PGP key here:
>>> http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B
>>>
>>>
>>>
>>>
>>> -----BEGIN PGP SIGNATURE-----
>>>
>>> iQEcBAEBAgAGBQJH+mt8AAoJEGmx2Mt/+Iw/YHcIAJKNkqo5bdUKe5I1IV9cmxrA
>>> 17hQ3SQ58n1QlIWQROPQXZuf/uGvYlp38LjQQF5Cs9CY0RrwbeORdGGcJL7ceo91
>>> gvSstxaYi5Dx/EvNUEpurejIWo4SsPG53Iyv1s1gQneOmMX2ft9u3Y9tx0O9G3z/
>>> Z2It1REpmfywGROx25M6bKhnfL7V/ZoCRB1JdEmbtoP9DtZxUJSJ1Kge4P4cgw64
>>> pg5UJvnRz5cZ1mxLSB+uSOwxQqyZn9Zt4Sb28ReEwuCcsRGRPHWU0YT1A8OTwnKf
>>> sURvxOfqPlixiSPHdxkFjPQq5JToiBbx9hbLWxQxhW4k7TPvNCZZgO5wJtOyu/k=
>>> =wzgk
>>> -----END PGP SIGNATURE-----
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>
> - ---
> ???
> Check my PGP key here:
> http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B
>
>
>
>
> -----BEGIN PGP SIGNATURE-----
>
> iQEcBAEBAgAGBQJH/eprAAoJEGmx2Mt/+Iw/s1EH+wWVlx/Gnd6PlvY8jEuyy3J4
> V4YEwE+BZDdgFvVClxhWgq6eyl8XsHzt2qvsmb1BGOOH0kt+r7ixINQo1B7urhhI
> vm2/TL8C2oGUwN5O4tunWUcZw/iG2IXPwKvXmtscAFXXgx3qX0jO2V4jnSAqo1s/
> O0rbyvvXoprT/C2zChGVC/p4HmnOdUVkBcQXgQpZmmd3z+0sTzn+x6BslrpX13t0
> b8dKA3tAhzZgbHMbpyqgqNPBz26E2FHV2vpOtSB+45DbpXx3N5aK8k2+Ynf8EQyK
> 7VesVR3VZ82ze7ut0ZLn6IzxQKLSKAsvJ7g/ey092/joapJ3gzWjkMPf0VjS6qE=
> =sG5x
> -----END PGP SIGNATURE-----
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From ndellofano at apple.com  Thu Apr 10 13:53:34 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu Apr 10 13:53:18 2008
Subject: [zfs-discuss] Performance problems - AFP!
In-Reply-To: <F1CE4772-22AB-46EC-A755-39CEE0B588DA@rehnmark.net>
References: <A9AF4F48-446D-4B10-B0E1-69B4866153EC@rehnmark.net>
	<F1CE4772-22AB-46EC-A755-39CEE0B588DA@rehnmark.net>
Message-ID: <EB18039D-EA3C-49FC-A837-AECAB5F5B2F2@apple.com>

Sorry to be the bearer of bad news but the truth is I've done zero,  
and i do mean zero, work on performance for ZFS, hence the issues  
you're having are present.  Apologies, we're still trying to get  
features up and going and performance is second on the list to getting  
us stable and feature complete.

That being said, any feedback you have, like the below is super  
helpful as I can open a bug now to track this problem so then i can go  
back reproduce it and fix it.

thanks!
Noel

On Apr 7, 2008, at 2:38 AM, Robert Rehnmark wrote:

> Ugh!
> The pools work fine locally but when used over Apple File Sharing  
> Protocol it tends to read/write in short bursts.
> The effect is speeds cut in half or worse.
>
> Can anybody verify this or maybe even share a solution?
>
> Please.
>
> /Robert
>
>
>
> 6 apr 2008 kl. 16:35 skrev Robert Rehnmark:
>> When I copy files over the network to/from my server (10.5.2 client  
>> single-disk volume) I get 66-70 MB/sec.
>> If I do the same thing to the same machine with a single-disk zpool  
>> I get 0-30/40 MB/sec.
>> To a zpool mirror I get 0-55 MB/sec.
>> The performance/operation is very "janky" and when the speed is up  
>> the CPU use allmost  tops out too.
>> Does this have anything to do with caching etc.?
>> The pools are version 8, allmost empty and made from the same disks.
>>
>> I want to be able to use ZFS but if I am not able to fix the  
>> performance problems HFS+ with daily backups will be preferred.
>>
>> I want to hear about ather peoples experiences and tips, please.
>>
>> /Robert
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From ndellofano at apple.com  Thu Apr 10 13:53:44 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu Apr 10 13:53:27 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
In-Reply-To: <77DCF6BD-D149-485B-B647-4BA90F9FAAF1@macnews.de>
References: <20080408145543.2CBD4241AB3@lists.macosforge.org>
	<77DCF6BD-D149-485B-B647-4BA90F9FAAF1@macnews.de>
Message-ID: <F9AD4709-A1BF-47B7-8AE4-D61DC743026C@apple.com>

Ok, so I'll add this to the wiki too since it seems there is a lot of  
conflicting information out there :)

So, the deal is you MUST use the diskutil command to partition your  
drive first as a ZFS disk BEFORE you create the pool.  I can't stress  
this enough.  Otherwise with the way the system works, all kinds of  
weird things can occur. Namely, if you fail to label the drive before  
creating the pool, then you're pool won't import on boot.  Also, your  
drives will and can be misidentified by diskutil, and therefore your  
pool or pieces of it may just not come up on boot either.

If you want more detail read on:
So in Solaris, these two steps the labeling of the drive and the pool  
creation are done all at once, which I agree is just rad.  However,  
because of the way Mac OSX works, diskutil needs to do the labeling of  
the drive.  As Linus I'm sure would agree, it is a "rampant layerying  
violation" for ZFS to call the diskutil code, cause they are above us  
in the stack (let alone just not a good idea anyway and would be  
disaster).  So, that leaves us with a two step process for now.
1) use diskutil to label your drive for ZFS
2) create your pool

ZFS is still aware that if given the disk0s2 slice of a drive that it  
has the whole drive and thus all the goodness that comes with giving  
ZFS the whole drive will still occur and you'll be set.  It's just two  
steps instead of one.
For more info on how the lables are layed out on the drive you can see  
this thread:

http://lists.macosforge.org/pipermail/zfs-discuss/2008-March/000363.html

also, there is no way to fix this once you've failed to do it.  You'll  
have to zpool destroy the pool, wipe the drive, and then use diskutil  
to create the ZFS partition.  Sucks I know, but is the only way it  
works.  In the future hopefully we'll have this all in one fancy step,  
however for now, you need both :)

let me know if you have any questions on this,
Noel


On Apr 8, 2008, at 12:45 PM, Ralf Bertling wrote:

> Hi,
> not currently, unless you have a set of slightly larger disks  
> available.
> If you use the whole disks the data partitions are slightly larger  
> and right now ZFS is unable to replace devices by smaller ones, so  
> you have to back up your data, destroy the pool and start ove with a  
> new one..
> The sad part is that for Solaris, it is actually recommended to use  
> whole disks. It should be relatively simple to prevent this from  
> happening in the mac os x port.
> Tha ability to shrink pools will hopefully come some day (I could  
> not,however, find a ZFS roadmap anywhere on the net so far.
>
> Hope this helps,	ralf
> Am 08.04.2008 um 16:55 schrieb zfs-discuss-request@lists.macosforge.org 
> :
>> Hi Maarten,
>> i'm not sure. That was one of the reasons I joined the list. :)
>>
>> On Mon, Apr 7, 2008 at 11:36 PM, Maarten Billemont  
>> <lhunath@gmail.com> wrote:
>> Can this be done after creating a ZFS pool on the disk too without  
>> loosing the pool?
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From ndellofano at apple.com  Thu Apr 10 13:55:46 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu Apr 10 13:55:31 2008
Subject: [zfs-discuss] Status of ZFS on Mac OS X?
In-Reply-To: <fb9be13f644474a1cb6845fa4b9a4c3a@yellowspace.net>
References: <fb9be13f644474a1cb6845fa4b9a4c3a@yellowspace.net>
Message-ID: <380A13C9-ECB6-4FC0-9E49-C3D21FB9FB81@apple.com>

Hehe, I'm owner of the list and am from the "mothership" here at  
Apple :)

So I can't make any statements about timelines, roadmaps and things of  
that nature, but if you have specific questions on stuff or bugs I can  
hopefully help you out.  As of right now, I'm concentrating on getting  
snapshot browsing (known as .zfs) up and running, which it is up and  
running, but, um, well less then stable :)  My partner in crime, Don  
Brady, is working on ZFS booting among other things.  We're also  
trying to mow through panics as they come up (we don't have many other  
than ones that all seem to be related to traversing large data  
heirarchies) which we are still trying to nail down.  Things like the  
Finder trash problem are relatively low in the stack, since while  
annoying, is easy to workaround, since "rm -rf .Trash/*"  works just  
fine :)

I use ZFS as my home directory on my laptop that I use everyday  
(ironically to write more ZFS code, how fun!).  I've had no problems  
with it other then like some have mentioned, some apps for Mac expect  
case insensitivity so that breaks some things.  On the whole though,  
I'd give it a go.  If you've got something specific let me know.   
Feedback is always awesome.

apologies as well to the list in general, I try to answer emails as  
much as i can but i'm falling behind :)


Noel

On Apr 5, 2008, at 2:24 PM, Lorenzo Perone wrote:

> Hello,
>
> I'm looking forward on deploying ZFS fileystems on Mac OS X 10.5
> environments (both client and server), as I've already done with  
> Solaris
> and FreeBSD (the latter on development systems).
>
> There's no doubt to me that ZFS is a great idea and a huge leap  
> forward in
> Mac OS X filesystems, and I'm willing to start using zfs on osx (and  
> share
> my experience here) as soon as it gets stable enough for daily usage  
> (ok.
> maybe a tech-aware usage).
>
> Since there's nothing on the website about a timeline or a roadmap,  
> and
> since the current open bugs on trac are anything but encouraging,  
> I'd like
> to dare making one question here: is there anybody on this list from  
> the
> "mothership" allowed to make statements on the progress/current  
> status on
> those bugs and on zfs on Mac OS X in general?
>
>
> Regards,
>
> Lorenzo
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From cdl at asgaard.org  Thu Apr 10 14:49:07 2008
From: cdl at asgaard.org (Christopher LILJENSTOLPE)
Date: Thu Apr 10 14:47:25 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <47FE7D0F.4050503@loveturtle.net>
References: <47E33231.40107@jrv.org>	<605C34EB-44F8-45EE-9705-79B9123E471C@apple.com>	<47E42E60.6040506@jrv.org>	<6E4419B2-E4E5-4622-BD3C-BE636AA0EC5A@apple.com>	<4AADF1E6-FC85-4829-9D20-9E2CAF2B16C9@gmail.com>	<c61bd13c0804060804h22a131f1oa5924caeca084aa7@mail.gmail.com>	<8C0F3494-FB86-431C-9872-3B57B5C2BB2B@gmail.com>	<c61bd13c0804062249r8e2b1a8iec50b66665cc710b@mail.gmail.com>	<6DFAF372-EFF8-4E58-8B23-69F1161DD807@gmail.com>	<CFA57BDA-2C10-4CD5-B899-D425AD960427@gmail.com>	<11DDD05B-E100-4115-A5A7-07224EECE11A@martin-hauser.net>	<47FA08D7.2010901@jrv.org>	<C433CDE9-FEAF-4D7D-B8A3-7D5136FF96F1@martin-hauser.net>	<3EF46CF4-B83D-4864-A572-EF86E5AF1539@asgaard.org>	<E34E4AD4-A863-4F52-BEA6-27FC4CE2EE2B@martin-hauser.net>
	<C9D41D73-DEC3-4288-A905-4D5689481C8A@asgaard.org>
	<47FE7D0F.4050503@loveturtle.net>
Message-ID: <46949EA3-6ACA-4813-A410-65727242C689@asgaard.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Ok

	Chris

On 10 Apr 2008, at 13.48, Dillon Kass wrote:
> I use handbrake and I'm unable to read from dvd directories stored  
> on ZFS but if i copy them to my case sensitive HFS+ partition it  
> works just fine. case sensitivity isn't the issue.
>
> Christopher LILJENSTOLPE wrote:
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> It would make sense, if Sean's copy of Handbrake is not case  
>> sensitive, I think...  Too late, need sleep....
>>
>>    Chris
>>
>> On 10 Apr 2008, at 00.27, Martin Hauser wrote:
>>> Very interesting, now that you mention it, my handbrake is on a  
>>> case sensitive HFS+ and encodes from and to a ZFS (which is also  
>>> case sensitive as we all know).
>>>
>>> Maybe Sean's copy of Handbrake is stored on case insensetive HFS+  
>>> and that makes the difference ??
>>>
>>> Martin
>>>
>>> On Apr 7, 2008, at 20:44 PM, Christopher LILJENSTOLPE wrote:
>>>> -----BEGIN PGP SIGNED MESSAGE-----
>>>> Hash: SHA1
>>>>
>>>> Greetings,
>>>>
>>>>    Can it be case insensitivity in Handbrake?
>>>>
>>>>    Chris
>>>>
>>>> On 07 Apr 2008, at 05.16, Martin Hauser wrote:
>>>>> Hmm,
>>>>>
>>>>> I am encoding from .mpg files, maybe the difference is in  
>>>>> directory handling, as DVD's are oriented that way?
>>>>>
>>>>>
>>>>> On Apr 7, 2008, at 13:43 PM, James R. Van Artsdalen wrote:
>>>>>> I can reproduce his problem here using the Handbrake 0.9.2 GUI  
>>>>>> with no network involved.  It hangs after clicking OK to select  
>>>>>> the source material - it doesn't get to the point of encoding  
>>>>>> anything.  Handbrake does respond to a cmd-Q and is using about  
>>>>>> 5% CPU at that point.  The source material is a DVD copied to  
>>>>>> the hard disk.  It selects source material OK from a "Mac OS  
>>>>>> Extended" disk.
>>>>>>
>>>>>> Martin Hauser wrote:
>>>>>>> I do not have any problems with handbrake running on a local  
>>>>>>> ZFS. I encode from and to ZFS volumes, always using a custom  
>>>>>>> mp4 setup. I assume it's a problem with ZFS + networking ?
>>>>>>>
>>>>>>> kind regards
>>>>>>>
>>>>>>> Martin
>>>>>>> ------------------------------------------------------------------------
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> zfs-discuss mailing list
>>>>>>> zfs-discuss@lists.macosforge.org
>>>>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss@lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>>
>>>> - ---
>>>> ???
>>>> Check my PGP key here:
>>>> http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B
>>>>
>>>>
>>>>
>>>>
>>>> -----BEGIN PGP SIGNATURE-----
>>>>
>>>> iQEcBAEBAgAGBQJH+mt8AAoJEGmx2Mt/+Iw/YHcIAJKNkqo5bdUKe5I1IV9cmxrA
>>>> 17hQ3SQ58n1QlIWQROPQXZuf/uGvYlp38LjQQF5Cs9CY0RrwbeORdGGcJL7ceo91
>>>> gvSstxaYi5Dx/EvNUEpurejIWo4SsPG53Iyv1s1gQneOmMX2ft9u3Y9tx0O9G3z/
>>>> Z2It1REpmfywGROx25M6bKhnfL7V/ZoCRB1JdEmbtoP9DtZxUJSJ1Kge4P4cgw64
>>>> pg5UJvnRz5cZ1mxLSB+uSOwxQqyZn9Zt4Sb28ReEwuCcsRGRPHWU0YT1A8OTwnKf
>>>> sURvxOfqPlixiSPHdxkFjPQq5JToiBbx9hbLWxQxhW4k7TPvNCZZgO5wJtOyu/k=
>>>> =wzgk
>>>> -----END PGP SIGNATURE-----
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>>
>> - ---
>> ???
>> Check my PGP key here:
>> http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B
>>
>>
>>
>>
>> -----BEGIN PGP SIGNATURE-----
>>
>> iQEcBAEBAgAGBQJH/eprAAoJEGmx2Mt/+Iw/s1EH+wWVlx/Gnd6PlvY8jEuyy3J4
>> V4YEwE+BZDdgFvVClxhWgq6eyl8XsHzt2qvsmb1BGOOH0kt+r7ixINQo1B7urhhI
>> vm2/TL8C2oGUwN5O4tunWUcZw/iG2IXPwKvXmtscAFXXgx3qX0jO2V4jnSAqo1s/
>> O0rbyvvXoprT/C2zChGVC/p4HmnOdUVkBcQXgQpZmmd3z+0sTzn+x6BslrpX13t0
>> b8dKA3tAhzZgbHMbpyqgqNPBz26E2FHV2vpOtSB+45DbpXx3N5aK8k2+Ynf8EQyK
>> 7VesVR3VZ82ze7ut0ZLn6IzxQKLSKAsvJ7g/ey092/joapJ3gzWjkMPf0VjS6qE=
>> =sG5x
>> -----END PGP SIGNATURE-----
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

- ---
???
Check my PGP key here:
http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B




-----BEGIN PGP SIGNATURE-----

iQEcBAEBAgAGBQJH/otTAAoJEGmx2Mt/+Iw//WQH/08mI8oQOfT2xxVOXqsfLZdr
7OjQXsp81ZxDMVz+T9U9Y2ZjeT4q+yDPHPqKrPA5lHPDZiWwgfDoZSFQU94uf1cI
+s/ivBj6BxAdgxu9dyhARXEuGE5v6aHVM8dhv8MsnZZlr63yoB4TklL1BaC8l2d8
ggsOgImG1q4xgllusFUzZOfI3iJFcGhqBVfIppic0U31Cq5tbf7nTEv5ZpRlMsBF
X3fug5yYaXb84ZQrX9Of9q1Uuyzv56b2cwr8EGGIr188duK2Yxgdxy7YlwGa15XL
xfm2BYKpjIsnV7h8DkyxS8Fhw3947QzKUzVLXGpL7TuFbrdsVHxMHzJORdk12AQ=
=6s2o
-----END PGP SIGNATURE-----
From alex.blewitt at gmail.com  Thu Apr 10 14:52:06 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Thu Apr 10 14:50:21 2008
Subject: [zfs-discuss] Status of ZFS on Mac OS X?
In-Reply-To: <380A13C9-ECB6-4FC0-9E49-C3D21FB9FB81@apple.com>
References: <fb9be13f644474a1cb6845fa4b9a4c3a@yellowspace.net>
	<380A13C9-ECB6-4FC0-9E49-C3D21FB9FB81@apple.com>
Message-ID: <636fd28e0804101452h52d12d4alf559eed1afc635b9@mail.gmail.com>

On Thu, Apr 10, 2008 at 9:55 PM, No?l Dellofano <ndellofano@apple.com> wrote:
> Hehe, I'm owner of the list and am from the "mothership" here at Apple :)
>
> So I can't make any statements about timelines, roadmaps and things of that
> nature, but if you have specific questions on stuff or bugs I can hopefully
> help you out.  As of right now, I'm concentrating on getting snapshot
> browsing (known as .zfs) up and running, which it is up and running, but,
> um, well less then stable :)  My partner in crime, Don Brady, is working on
> ZFS booting among other things.

This is great news! The lack of a .zfs is a bit odd, and difficult to
demonstrate to naysayers how useful the idea of snapshots can be until
you've actually seen the .zfs in action.

Quick bit of feedback about .zfs - the 'rsync' program has an -C or
--cvs-exclude, which sets an exclude list to ignore certain
directories:

              The exclude list is initialized to:

              RCS SCCS CVS  CVS.adm  RCSLOG  cvslog.*  tags  TAGS  .make.state
              .nse_depinfo  *~ #* .#* ,* _$* *$ *.old *.bak *.BAK *.orig *.rej
              .del-* *.a *.olb *.o *.obj *.so *.exe *.Z *.elc *.ln core .svn/

I see that it's got .svn/ in there; I think a great addition would be
.zfs/ so that when you're rsync'ing across multiple directories you
don't get accidental snapshots carried with you.

And as for ZFS booting ... well, when that happens I'm not going back!

> We're also trying to mow through panics as
> they come up (we don't have many other than ones that all seem to be related
> to traversing large data heirarchies) which we are still trying to nail
> down.

I've seen more than a few panics but mostly these are related to
taking out the underling disks. It's a bit annoying to have a panic -
it will be great when we have the 'failmode' property that means we
can fail a little more elegantly than a system shutdown.

> On the whole though, I'd give it a go.  If you've  got something specific let me know.  Feedback is always awesome.

I speak for myself here, but I'm sure that the others on the list will
agree that you guys are doing a great job in bringing a great file
system to the Mac. Here's looking forward to His Jobness announcing
that everyone should test on ZFS in WWDC because it's going to be the
new default file system for the future. (Oh wait, Jonathan Schwartz
already said that.)

Seriously, you're doing a great service to the Mac community, and I
applaud you :-)

Alex
From canadrian at electricteaparty.net  Thu Apr 10 15:22:28 2008
From: canadrian at electricteaparty.net (Thornton Adrian)
Date: Thu Apr 10 15:21:07 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
In-Reply-To: <20080410215022.48B62243FCC@lists.macosforge.org>
References: <20080410215022.48B62243FCC@lists.macosforge.org>
Message-ID: <42E77539-C221-4486-B3FC-B40B2FB5DC34@electricteaparty.net>

Noel,

All I can say is... OOPS!

So is there any way I can fix my RAID-Z one disk at a time, removing  
and fixing one disk at a time, resilvering as I go, or am I completely  
SOL and need to find some way to get 2TB of data off the pool and back  
on again? If the answer is "tough luck," would you be able to explain  
why I would not be able to perform this slow and laborious process  
rather than the expensive and unwieldy option of buying expensive  
external drives for a complete re-build?

Many thanks,
Adrian

> ------------------------------
>
> Message: 2
> Date: Thu, 10 Apr 2008 13:53:44 -0700
> From: No?l Dellofano <ndellofano@apple.com>
> Subject: Re: [zfs-discuss] RAID-Z not re-mounting on reboot
> To: Ralf Bertling <i_see@macnews.de>
> Cc: zfs-discuss@lists.macosforge.org
> Message-ID: <F9AD4709-A1BF-47B7-8AE4-D61DC743026C@apple.com>
> Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes
>
> Ok, so I'll add this to the wiki too since it seems there is a lot of
> conflicting information out there :)
>
> So, the deal is you MUST use the diskutil command to partition your
> drive first as a ZFS disk BEFORE you create the pool.  I can't stress
> this enough.  Otherwise with the way the system works, all kinds of
> weird things can occur. Namely, if you fail to label the drive before
> creating the pool, then you're pool won't import on boot.  Also, your
> drives will and can be misidentified by diskutil, and therefore your
> pool or pieces of it may just not come up on boot either.
>
> If you want more detail read on:
> So in Solaris, these two steps the labeling of the drive and the pool
> creation are done all at once, which I agree is just rad.  However,
> because of the way Mac OSX works, diskutil needs to do the labeling of
> the drive.  As Linus I'm sure would agree, it is a "rampant layerying
> violation" for ZFS to call the diskutil code, cause they are above us
> in the stack (let alone just not a good idea anyway and would be
> disaster).  So, that leaves us with a two step process for now.
> 1) use diskutil to label your drive for ZFS
> 2) create your pool
>
> ZFS is still aware that if given the disk0s2 slice of a drive that it
> has the whole drive and thus all the goodness that comes with giving
> ZFS the whole drive will still occur and you'll be set.  It's just two
> steps instead of one.
> For more info on how the lables are layed out on the drive you can see
> this thread:
>
> http://lists.macosforge.org/pipermail/zfs-discuss/2008-March/000363.html
>
> also, there is no way to fix this once you've failed to do it.  You'll
> have to zpool destroy the pool, wipe the drive, and then use diskutil
> to create the ZFS partition.  Sucks I know, but is the only way it
> works.  In the future hopefully we'll have this all in one fancy step,
> however for now, you need both :)
>
> let me know if you have any questions on this,
> Noel
From i_see at macnews.de  Thu Apr 10 22:16:49 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Thu Apr 10 22:15:07 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
In-Reply-To: <F9AD4709-A1BF-47B7-8AE4-D61DC743026C@apple.com>
References: <20080408145543.2CBD4241AB3@lists.macosforge.org>
	<77DCF6BD-D149-485B-B647-4BA90F9FAAF1@macnews.de>
	<F9AD4709-A1BF-47B7-8AE4-D61DC743026C@apple.com>
Message-ID: <E66332DA-D965-4FCA-A66C-723296C4648B@macnews.de>

Hi,
maybe step 2 should read
2) create your pool, but be careful not to state full disks but only  
partitions.
i.e. disk27s2 instead of disk27 because otherwise zpool will overwrite  
the GUID-label and the disk will revert to a MBR/fdisk-style partition  
map.
ralf
Am 10.04.2008 um 22:53 schrieb No?l Dellofano:
> Ok, so I'll add this to the wiki too since it seems there is a lot  
> of conflicting information out there :)
>
> So, the deal is you MUST use the diskutil command to partition your  
> drive first as a ZFS disk BEFORE you create the pool.  I can't  
> stress this enough.  Otherwise with the way the system works, all  
> kinds of weird things can occur. Namely, if you fail to label the  
> drive before creating the pool, then you're pool won't import on  
> boot.  Also, your drives will and can be misidentified by diskutil,  
> and therefore your pool or pieces of it may just not come up on boot  
> either.
>
> If you want more detail read on:
> So in Solaris, these two steps the labeling of the drive and the  
> pool creation are done all at once, which I agree is just rad.   
> However, because of the way Mac OSX works, diskutil needs to do the  
> labeling of the drive.  As Linus I'm sure would agree, it is a  
> "rampant layerying violation" for ZFS to call the diskutil code,  
> cause they are above us in the stack (let alone just not a good idea  
> anyway and would be disaster).  So, that leaves us with a two step  
> process for now.
> 1) use diskutil to label your drive for ZFS
> 2) create your pool
>
> ZFS is still aware that if given the disk0s2 slice of a drive that  
> it has the whole drive and thus all the goodness that comes with  
> giving ZFS the whole drive will still occur and you'll be set.  It's  
> just two steps instead of one.
> For more info on how the lables are layed out on the drive you can  
> see this thread:
>
> http://lists.macosforge.org/pipermail/zfs-discuss/2008-March/000363.html
>
> also, there is no way to fix this once you've failed to do it.   
> You'll have to zpool destroy the pool, wipe the drive, and then use  
> diskutil to create the ZFS partition.  Sucks I know, but is the only  
> way it works.  In the future hopefully we'll have this all in one  
> fancy step, however for now, you need both :)
>
> let me know if you have any questions on this,
> Noel
>
>
> On Apr 8, 2008, at 12:45 PM, Ralf Bertling wrote:
>
>> Hi,
>> not currently, unless you have a set of slightly larger disks  
>> available.
>> If you use the whole disks the data partitions are slightly larger  
>> and right now ZFS is unable to replace devices by smaller ones, so  
>> you have to back up your data, destroy the pool and start ove with  
>> a new one..
>> The sad part is that for Solaris, it is actually recommended to use  
>> whole disks. It should be relatively simple to prevent this from  
>> happening in the mac os x port.
>> Tha ability to shrink pools will hopefully come some day (I could  
>> not,however, find a ZFS roadmap anywhere on the net so far.
>>
>> Hope this helps,	ralf
>> Am 08.04.2008 um 16:55 schrieb zfs-discuss-request@lists.macosforge.org 
>> :
>>> Hi Maarten,
>>> i'm not sure. That was one of the reasons I joined the list. :)
>>>
>>> On Mon, Apr 7, 2008 at 11:36 PM, Maarten Billemont <lhunath@gmail.com 
>>> > wrote:
>>> Can this be done after creating a ZFS pool on the disk too without  
>>> loosing the pool?
>>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

From Ben.Vaughn at united.com  Fri Apr 11 14:23:31 2008
From: Ben.Vaughn at united.com (Vaughn, Benjamin [WHQSE])
Date: Fri Apr 11 14:21:49 2008
Subject: [zfs-discuss] ZFS on an XServe RAID
Message-ID: <A30DC84845CE2241A2900B68D6111099F09CA6@D1VS1.D1.USA.NET>

Hello,
 
  I currently have an XServe RAID box with all 14 drive slots filled
(left side is 250GB, right side is 700GB disks.)  In the future when ZFS
is more stable and enterprise friendly, how would be the best way to
deploy ZFS?  Convert all the disks to JBOD and have RAIDZ do the
raiding, or let the XServe RAID do RAID5 and put ZFS on that?
 
Thanks for the info,
Ben
 

Benjamin Vaughn

Project Manager - Computer Investigations and Systems Integrity

United Airlines Corporate Security - OPCSE

+1 847 700 9505 Office

+1 630 329 7629 Mobile

Ben.Vaughn@united.com <mailto:Ben.Vaughn@united.com> 

P Please consider the environment before printing this e-mail.

 

 

 

 

 

 

 

 

 

 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080411/fbd8684e/attachment.html
From ndellofano at apple.com  Fri Apr 11 14:31:10 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri Apr 11 14:30:52 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
In-Reply-To: <E66332DA-D965-4FCA-A66C-723296C4648B@macnews.de>
References: <20080408145543.2CBD4241AB3@lists.macosforge.org>
	<77DCF6BD-D149-485B-B647-4BA90F9FAAF1@macnews.de>
	<F9AD4709-A1BF-47B7-8AE4-D61DC743026C@apple.com>
	<E66332DA-D965-4FCA-A66C-723296C4648B@macnews.de>
Message-ID: <AC72940D-3BEE-45CB-9402-4D4ED883DC5B@apple.com>

good suggestion, will do.  I"m also going to change the ZFS module  
such that if you give me a whole disk, I'll error out and  
unceremoniously complain at you to give me a partition :)

Noel

On Apr 10, 2008, at 10:16 PM, Ralf Bertling wrote:

> Hi,
> maybe step 2 should read
> 2) create your pool, but be careful not to state full disks but only  
> partitions.
> i.e. disk27s2 instead of disk27 because otherwise zpool will  
> overwrite the GUID-label and the disk will revert to a MBR/fdisk- 
> style partition map.
> ralf
> Am 10.04.2008 um 22:53 schrieb No?l Dellofano:
>> Ok, so I'll add this to the wiki too since it seems there is a lot  
>> of conflicting information out there :)
>>
>> So, the deal is you MUST use the diskutil command to partition your  
>> drive first as a ZFS disk BEFORE you create the pool.  I can't  
>> stress this enough.  Otherwise with the way the system works, all  
>> kinds of weird things can occur. Namely, if you fail to label the  
>> drive before creating the pool, then you're pool won't import on  
>> boot.  Also, your drives will and can be misidentified by diskutil,  
>> and therefore your pool or pieces of it may just not come up on  
>> boot either.
>>
>> If you want more detail read on:
>> So in Solaris, these two steps the labeling of the drive and the  
>> pool creation are done all at once, which I agree is just rad.   
>> However, because of the way Mac OSX works, diskutil needs to do the  
>> labeling of the drive.  As Linus I'm sure would agree, it is a  
>> "rampant layerying violation" for ZFS to call the diskutil code,  
>> cause they are above us in the stack (let alone just not a good  
>> idea anyway and would be disaster).  So, that leaves us with a two  
>> step process for now.
>> 1) use diskutil to label your drive for ZFS
>> 2) create your pool
>>
>> ZFS is still aware that if given the disk0s2 slice of a drive that  
>> it has the whole drive and thus all the goodness that comes with  
>> giving ZFS the whole drive will still occur and you'll be set.   
>> It's just two steps instead of one.
>> For more info on how the lables are layed out on the drive you can  
>> see this thread:
>>
>> http://lists.macosforge.org/pipermail/zfs-discuss/2008-March/000363.html
>>
>> also, there is no way to fix this once you've failed to do it.   
>> You'll have to zpool destroy the pool, wipe the drive, and then use  
>> diskutil to create the ZFS partition.  Sucks I know, but is the  
>> only way it works.  In the future hopefully we'll have this all in  
>> one fancy step, however for now, you need both :)
>>
>> let me know if you have any questions on this,
>> Noel
>>
>>
>> On Apr 8, 2008, at 12:45 PM, Ralf Bertling wrote:
>>
>>> Hi,
>>> not currently, unless you have a set of slightly larger disks  
>>> available.
>>> If you use the whole disks the data partitions are slightly larger  
>>> and right now ZFS is unable to replace devices by smaller ones, so  
>>> you have to back up your data, destroy the pool and start ove with  
>>> a new one..
>>> The sad part is that for Solaris, it is actually recommended to  
>>> use whole disks. It should be relatively simple to prevent this  
>>> from happening in the mac os x port.
>>> Tha ability to shrink pools will hopefully come some day (I could  
>>> not,however, find a ZFS roadmap anywhere on the net so far.
>>>
>>> Hope this helps,	ralf
>>> Am 08.04.2008 um 16:55 schrieb zfs-discuss-request@lists.macosforge.org 
>>> :
>>>> Hi Maarten,
>>>> i'm not sure. That was one of the reasons I joined the list. :)
>>>>
>>>> On Mon, Apr 7, 2008 at 11:36 PM, Maarten Billemont <lhunath@gmail.com 
>>>> > wrote:
>>>> Can this be done after creating a ZFS pool on the disk too  
>>>> without loosing the pool?
>>>>
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>

From ndellofano at apple.com  Fri Apr 11 14:59:28 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri Apr 11 14:59:08 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
In-Reply-To: <42E77539-C221-4486-B3FC-B40B2FB5DC34@electricteaparty.net>
References: <20080410215022.48B62243FCC@lists.macosforge.org>
	<42E77539-C221-4486-B3FC-B40B2FB5DC34@electricteaparty.net>
Message-ID: <E65E26D9-7EF0-432C-A6B5-B7541A2BF832@apple.com>

hmm, that's a tough one.

So "in theory" since you have a raidz i'd say you should be able to  
replace each disk one at a time and resilver each time in between.   
Because reformatting the disk, and the need for ZFS to get the disk2s2  
name instead of just disk2 (its stored in the label packed nvlist),  
you'll likely need to offline the device, reformat it and then issue a  
replace of the drive with itself.  I'm *very* hesitant to recommend  
this since I've never tried it ('zpool replace mypool disk2 disk2s2')  
and I'm really not quite sure how the system will react or what  it  
will do.  I'd recommend doing a snapshot of all your data and doing a  
'zfs send'  of  it to another machine/drive(HFS, ZFS whatever you  
got), though i realize you have a lot of data and I'm not sure if you  
have another machine you can store that much data on temporarily for  
backup.

If you have access to one drive (of the same size as the rest of your  
drives in your raidz) and you have space in your config to attach it  
to the system,  that would make your life a lot easier since you could  
just issue a straight up replace one drive at a time, reformatting and  
resilvering the replaced drives as you go.

Noel



On Apr 10, 2008, at 3:22 PM, Thornton Adrian wrote:

> Noel,
>
> All I can say is... OOPS!
>
> So is there any way I can fix my RAID-Z one disk at a time, removing  
> and fixing one disk at a time, resilvering as I go, or am I  
> completely SOL and need to find some way to get 2TB of data off the  
> pool and back on again? If the answer is "tough luck," would you be  
> able to explain why I would not be able to perform this slow and  
> laborious process rather than the expensive and unwieldy option of  
> buying expensive external drives for a complete re-build?
>
> Many thanks,
> Adrian
>
>> ------------------------------
>>
>> Message: 2
>> Date: Thu, 10 Apr 2008 13:53:44 -0700
>> From: No?l Dellofano <ndellofano@apple.com>
>> Subject: Re: [zfs-discuss] RAID-Z not re-mounting on reboot
>> To: Ralf Bertling <i_see@macnews.de>
>> Cc: zfs-discuss@lists.macosforge.org
>> Message-ID: <F9AD4709-A1BF-47B7-8AE4-D61DC743026C@apple.com>
>> Content-Type: text/plain; charset=US-ASCII; format=flowed; delsp=yes
>>
>> Ok, so I'll add this to the wiki too since it seems there is a lot of
>> conflicting information out there :)
>>
>> So, the deal is you MUST use the diskutil command to partition your
>> drive first as a ZFS disk BEFORE you create the pool.  I can't stress
>> this enough.  Otherwise with the way the system works, all kinds of
>> weird things can occur. Namely, if you fail to label the drive before
>> creating the pool, then you're pool won't import on boot.  Also, your
>> drives will and can be misidentified by diskutil, and therefore your
>> pool or pieces of it may just not come up on boot either.
>>
>> If you want more detail read on:
>> So in Solaris, these two steps the labeling of the drive and the pool
>> creation are done all at once, which I agree is just rad.  However,
>> because of the way Mac OSX works, diskutil needs to do the labeling  
>> of
>> the drive.  As Linus I'm sure would agree, it is a "rampant layerying
>> violation" for ZFS to call the diskutil code, cause they are above us
>> in the stack (let alone just not a good idea anyway and would be
>> disaster).  So, that leaves us with a two step process for now.
>> 1) use diskutil to label your drive for ZFS
>> 2) create your pool
>>
>> ZFS is still aware that if given the disk0s2 slice of a drive that it
>> has the whole drive and thus all the goodness that comes with giving
>> ZFS the whole drive will still occur and you'll be set.  It's just  
>> two
>> steps instead of one.
>> For more info on how the lables are layed out on the drive you can  
>> see
>> this thread:
>>
>> http://lists.macosforge.org/pipermail/zfs-discuss/2008-March/000363.html
>>
>> also, there is no way to fix this once you've failed to do it.   
>> You'll
>> have to zpool destroy the pool, wipe the drive, and then use diskutil
>> to create the ZFS partition.  Sucks I know, but is the only way it
>> works.  In the future hopefully we'll have this all in one fancy  
>> step,
>> however for now, you need both :)
>>
>> let me know if you have any questions on this,
>> Noel

From canadrian at electricteaparty.net  Fri Apr 11 15:17:34 2008
From: canadrian at electricteaparty.net (Thornton Adrian)
Date: Fri Apr 11 15:15:52 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
In-Reply-To: <E65E26D9-7EF0-432C-A6B5-B7541A2BF832@apple.com>
References: <20080410215022.48B62243FCC@lists.macosforge.org>
	<42E77539-C221-4486-B3FC-B40B2FB5DC34@electricteaparty.net>
	<E65E26D9-7EF0-432C-A6B5-B7541A2BF832@apple.com>
Message-ID: <4C082290-5C98-4750-924E-1C8694E8292A@electricteaparty.net>

Noel,

I am considering replacing all my 320gb drives with 750gb drives  
anyway, so this may be a good time to do it. My current SATA  
controller card can't seem to handle the 750gb drives, but as soon as  
I can find a compatible card that will, then i'll proceed with the  
process and let this group know what happens.

Cheers,
Adrian


On 11-Apr-08, at 3:59 PM, No?l Dellofano wrote:
> hmm, that's a tough one.
>
> So "in theory" since you have a raidz i'd say you should be able to  
> replace each disk one at a time and resilver each time in between.   
> Because reformatting the disk, and the need for ZFS to get the  
> disk2s2 name instead of just disk2 (its stored in the label packed  
> nvlist), you'll likely need to offline the device, reformat it and  
> then issue a replace of the drive with itself.  I'm *very* hesitant  
> to recommend this since I've never tried it ('zpool replace mypool  
> disk2 disk2s2') and I'm really not quite sure how the system will  
> react or what  it will do.  I'd recommend doing a snapshot of all  
> your data and doing a 'zfs send'  of  it to another machine/ 
> drive(HFS, ZFS whatever you got), though i realize you have a lot of  
> data and I'm not sure if you have another machine you can store that  
> much data on temporarily for backup.
>
> If you have access to one drive (of the same size as the rest of  
> your drives in your raidz) and you have space in your config to  
> attach it to the system,  that would make your life a lot easier  
> since you could just issue a straight up replace one drive at a  
> time, reformatting and resilvering the replaced drives as you go.
>
> Noel
>

From Jonathan.Edwards at Sun.COM  Fri Apr 11 15:17:32 2008
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Fri Apr 11 15:16:26 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
In-Reply-To: <AC72940D-3BEE-45CB-9402-4D4ED883DC5B@apple.com>
References: <20080408145543.2CBD4241AB3@lists.macosforge.org>
	<77DCF6BD-D149-485B-B647-4BA90F9FAAF1@macnews.de>
	<F9AD4709-A1BF-47B7-8AE4-D61DC743026C@apple.com>
	<E66332DA-D965-4FCA-A66C-723296C4648B@macnews.de>
	<AC72940D-3BEE-45CB-9402-4D4ED883DC5B@apple.com>
Message-ID: <0F73B716-E884-4688-8C0D-1359A6698DD1@sun.com>

actually - i've been using gpt directly to label the disks .. i don't  
see why you can't just make the same calls to check for existing  
labels and warn, but with a 'zpool create -f <pool> <disk>' simply do  
the equivalent of a 'gpt create -f <disk> ; gpt add -b 256 -i 2 -s  
<free sector size> -t <GUID> <disk>'

the zpool vdev creation may be somewhat obtuse given the (somewhat)  
broken EFI implementation in Solaris .. for example on a new disk  
presented to an SXDE 79b nevada build with a 'zpool create -f serenity  
c10t0d0' - i show the following from a "gpt show" on my mac:

# gpt show disk1
       start       size  index  contents
           0          1         PMBR
           1          1         Pri GPT header
           2          3         Pri GPT table
           5        251
         256  488380495      1  GPT part -  
6A898CC3-1DD2-11B2-99A6-080020736631
   488380751      16384      9  GPT part -  
6A945A3B-1DD2-11B2-99A6-080020736631
   488397135          3         Sec GPT table
   488397138         29
   488397167          1         Sec GPT header

instead of something like:
gpt show disk1
       start       size  index  contents
           0          1         PMBR
           1          1         Pri GPT header
           2         32         Pri GPT table
          34    5242880      1  GPT part - 48465300-0000-11AA- 
AA11-00306543ECAC
     5242914  483154221      2  GPT part - 48465300-0000-11AA- 
AA11-00306543ECAC
   488397135         32         Sec GPT table
   488397167          1         Sec GPT header

(one i did by hand with gpt) ..

on the sun EFI format label (which appears to be showing up in the mac  
zfs implementation as the same sort of label as i would get with a  
zpool create <vdev> on solaris) - i had put in an RFE at one point to  
get the sector start onto a 128KB boundary (so that's why you'll see  
the Sun EFI formatted label entry 1 start at 256 blocks) which can  
help with alignment issues on RAID sets (17KB is typically a bad place  
to start for stripe groups - you'll end up in R/M/W more often than  
not) .. the rest of it has more to do with some legacy sun device  
issues (like entry 9 reserved with 16384 sectors (8MB) that i still  
don't completely understand), and i really don't understand why we  
have a single sector we do for the primary GPT table instead of 32  
sectors for 256 entries like the EFI spec calls for (guess i need to  
file another RFE/Bug)

---
.je

On Apr 11, 2008, at 5:31 PM, No?l Dellofano wrote:
> good suggestion, will do.  I"m also going to change the ZFS module  
> such that if you give me a whole disk, I'll error out and  
> unceremoniously complain at you to give me a partition :)
>
> Noel
>
> On Apr 10, 2008, at 10:16 PM, Ralf Bertling wrote:
>
>> Hi,
>> maybe step 2 should read
>> 2) create your pool, but be careful not to state full disks but  
>> only partitions.
>> i.e. disk27s2 instead of disk27 because otherwise zpool will  
>> overwrite the GUID-label and the disk will revert to a MBR/fdisk- 
>> style partition map.
>> ralf
>> Am 10.04.2008 um 22:53 schrieb No?l Dellofano:
>>> Ok, so I'll add this to the wiki too since it seems there is a lot  
>>> of conflicting information out there :)
>>>
>>> So, the deal is you MUST use the diskutil command to partition  
>>> your drive first as a ZFS disk BEFORE you create the pool.  I  
>>> can't stress this enough.  Otherwise with the way the system  
>>> works, all kinds of weird things can occur. Namely, if you fail to  
>>> label the drive before creating the pool, then you're pool won't  
>>> import on boot.  Also, your drives will and can be misidentified  
>>> by diskutil, and therefore your pool or pieces of it may just not  
>>> come up on boot either.
>>>
>>> If you want more detail read on:
>>> So in Solaris, these two steps the labeling of the drive and the  
>>> pool creation are done all at once, which I agree is just rad.   
>>> However, because of the way Mac OSX works, diskutil needs to do  
>>> the labeling of the drive.  As Linus I'm sure would agree, it is a  
>>> "rampant layerying violation" for ZFS to call the diskutil code,  
>>> cause they are above us in the stack (let alone just not a good  
>>> idea anyway and would be disaster).  So, that leaves us with a two  
>>> step process for now.
>>> 1) use diskutil to label your drive for ZFS
>>> 2) create your pool
>>>
>>> ZFS is still aware that if given the disk0s2 slice of a drive that  
>>> it has the whole drive and thus all the goodness that comes with  
>>> giving ZFS the whole drive will still occur and you'll be set.   
>>> It's just two steps instead of one.
>>> For more info on how the lables are layed out on the drive you can  
>>> see this thread:
>>>
>>> http://lists.macosforge.org/pipermail/zfs-discuss/2008-March/000363.html
>>>
>>> also, there is no way to fix this once you've failed to do it.   
>>> You'll have to zpool destroy the pool, wipe the drive, and then  
>>> use diskutil to create the ZFS partition.  Sucks I know, but is  
>>> the only way it works.  In the future hopefully we'll have this  
>>> all in one fancy step, however for now, you need both :)
>>>
>>> let me know if you have any questions on this,
>>> Noel
>>>
>>>
>>> On Apr 8, 2008, at 12:45 PM, Ralf Bertling wrote:
>>>
>>>> Hi,
>>>> not currently, unless you have a set of slightly larger disks  
>>>> available.
>>>> If you use the whole disks the data partitions are slightly  
>>>> larger and right now ZFS is unable to replace devices by smaller  
>>>> ones, so you have to back up your data, destroy the pool and  
>>>> start ove with a new one..
>>>> The sad part is that for Solaris, it is actually recommended to  
>>>> use whole disks. It should be relatively simple to prevent this  
>>>> from happening in the mac os x port.
>>>> Tha ability to shrink pools will hopefully come some day (I could  
>>>> not,however, find a ZFS roadmap anywhere on the net so far.
>>>>
>>>> Hope this helps,	ralf
>>>> Am 08.04.2008 um 16:55 schrieb zfs-discuss-request@lists.macosforge.org 
>>>> :
>>>>> Hi Maarten,
>>>>> i'm not sure. That was one of the reasons I joined the list. :)
>>>>>
>>>>> On Mon, Apr 7, 2008 at 11:36 PM, Maarten Billemont <lhunath@gmail.com 
>>>>> > wrote:
>>>>> Can this be done after creating a ZFS pool on the disk too  
>>>>> without loosing the pool?
>>>>>
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From ndellofano at apple.com  Fri Apr 11 15:28:11 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri Apr 11 15:27:53 2008
Subject: [zfs-discuss] ZFS on an XServe RAID
In-Reply-To: <A30DC84845CE2241A2900B68D6111099F09CA6@D1VS1.D1.USA.NET>
References: <A30DC84845CE2241A2900B68D6111099F09CA6@D1VS1.D1.USA.NET>
Message-ID: <CBD3AF0E-1ACE-47F7-B7E3-F911C09582C3@apple.com>

Awesome!  So I'd definitely recommend converting all the disks to JBOD  
and letting ZFS do all the raid work since then we can do end to end  
checksums and can heal any corrupted blocks we detect.  As to the  
configuration, it really depends on what you want to do with the  
machine.  What do you use it for? are you more interested in  
performance or replication and safety?
How much space do you need to get out of the machine?

We have an Xraid here at work that I've configured, and the drives in  
it are really old so I don't trust them, hence I went for extreme  
replication (it's a server and extreme perf isn't dire).  I did a pool  
of two groups of raidz-2 (6 drives each) and then added a hotspare to  
each group.

Most generally I'd say you'd be good with configuring a pool with 2  
groups of raidz's (7 -250GB in one and 7- 700GB in the other), with  
that you'll get good performance with still some replication while  
only losing 950GB overall for replication cost (parity).  If you  
wanted to be more cautious, you could also do the same config but use  
raidz2 instead raidz so that you could withstand more disk failures,  
but you'll lose more storage to your parity.  There's a ton on  
configs, which really depend on your workload, preference, perfomance  
needs, and storage needs.  I'd recommend checking out the best  
practices guide which has a section on how to consider configuring  
your pool:

http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide

Roch Bourbonnais of Sun also has a blog on when or when not to use  
raidz:
http://blogs.sun.com/roch/entry/when_to_and_not_to

and please feel free to ask me as well if you have more questions  
about a config too.  It just really depends on what you need to use it  
for :)

Noel


On Apr 11, 2008, at 2:23 PM, Vaughn, Benjamin [WHQSE] wrote:

> Hello,
>
>   I currently have an XServe RAID box with all 14 drive slots filled  
> (left side is 250GB, right side is 700GB disks.)  In the future when  
> ZFS is more stable and enterprise friendly, how would be the best  
> way to deploy ZFS?  Convert all the disks to JBOD and have RAIDZ do  
> the raiding, or let the XServe RAID do RAID5 and put ZFS on that?
>
> Thanks for the info,
> Ben
>
> Benjamin Vaughn
> Project Manager - Computer Investigations and Systems Integrity
> United Airlines Corporate Security - OPCSE
> +1 847 700 9505 Office
> +1 630 329 7629 Mobile
> Ben.Vaughn@united.com
> P Please consider the environment before printing this e-mail.
>
>
>
>
>
>
>
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080411/5fec2eb0/attachment-0001.html
From William.Winnett at Sun.COM  Fri Apr 11 16:18:27 2008
From: William.Winnett at Sun.COM (Bill Winnett)
Date: Fri Apr 11 16:16:41 2008
Subject: [zfs-discuss] RAID-Z not re-mounting on reboot
Message-ID: <47FFF1C3.4080801@sun.com>

On Apr 11, 2008, at 5:31 PM, No?l Dellofano wrote:

> > good suggestion, will do.  I"m also going to change the ZFS module  
> > such that if you give me a whole disk, I'll error out and  
> > unceremoniously complain at you to give me a partition  :) 
I already have 3 1tb whole disks in use, I hope your new module does not 
prevent me from being able to
mount my disks.

-bill w.
From riscky at gmail.com  Fri Apr 11 22:14:42 2008
From: riscky at gmail.com (Riscky Abacus)
Date: Fri Apr 11 22:12:55 2008
Subject: [zfs-discuss] zpool status -v
In-Reply-To: <AB3828E3-B960-4E58-8B64-979317C3BD53@apple.com>
References: <17348603-B0C9-4B6E-AF13-3E2B16325BBC@spamfreemail.de>
	<AB3828E3-B960-4E58-8B64-979317C3BD53@apple.com>
Message-ID: <51cfc2260804112214wa5ff3c7wd6ad9e9ecf701b79@mail.gmail.com>

You know I've seen this countless times on my iMac. I have to zfs
"drives" and when one of these drives is attached zpool status throws
a kernal panic... I posted a message about this last month... if I
move this "drive" to another system and run zpool status the same
panic ensues... with the drive is also attached and I issue other
zpool commands sometimes I get strange results like tonight when I
exported my other pool the WindowServer suddenly spiked to 99% cpu
time for an extended amount of time (the drive held my iTMS files). A
few other times kernel_task runs at 65% until I reboot the system...
not sure if this just happens to happen then or if its related to
zpool.

Anyway the pool that cause this problem for me is actually a
collection of 4 usb flash drives connected to one usb hub to create on
large flash drive... oh and I scrubbed the zpool and found no
issues... the only thing I think that might cause this problem is I
have the the zpool root set to /src while the mount point is
/Volumes/pool

Here are two logs each from a different system:

Sat Apr  5 17:21:22 2008
panic(cpu 1 caller 0x001A8C8A): Kernel trap at 0x003618c1, type
14=page fault, registers:
CR0: 0x80010033, CR2: 0x0000000c, CR3: 0x00d07000, CR4: 0x000006e0
EAX: 0x00000000, EBX: 0x35913de8, ECX: 0x1f000000, EDX: 0x00000000
CR2: 0x0000000c, EBP: 0x35913d18, ESI: 0x00000200, EDI: 0x03a77000
EFL: 0x00010202, EIP: 0x003618c1, CS:  0x00000004, DS:  0x1000000c
Error code: 0x00000000

Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x35913b88 : 0x12b0f7 (0x4581f4 0x35913bbc 0x133230 0x0)
0x35913bd8 : 0x1a8c8a (0x461720 0x3618c1 0xe 0x460ed0)
0x35913cb8 : 0x19ece5 (0x35913cd0 0x0 0x35913d18 0x3618c1)
0x35913cc8 : 0x3618c1 (0xe 0x5630048 0x3591000c 0x20000c)
0x35913d18 : 0x3fb9fff8 (0x0 0xfe972958 0x1 0x0)
0x35913d38 : 0x3fba79dd (0x0 0x0 0x35913d88 0x1362ad)
0x35913d78 : 0x202bea (0x1f000000 0xcf1c5a20 0x3a77000 0x3)
0x35913db8 : 0x1f6039 (0x35913de8 0x246 0x35913e18 0x1da207)
0x35913e18 : 0x1ec2f6 (0x485a680 0xcf1c5a20 0x3a77000 0x3)
0x35913e78 : 0x3653a7 (0x4440c80 0xcf1c5a20 0x3a77000 0x35913f50)
0x35913e98 : 0x38bd28 (0x4440c80 0xcf1c5a20 0x3a77000 0x35913f50)
0x35913f78 : 0x3dcf13 (0x3e26530 0x4c3fee0 0x4c3ff24 0x0)
0x35913fc8 : 0x19f1c3 (0x4c44bd4 0x0 0x4 0x456b97c)
No mapping exists for frame pointer
Backtrace terminated-invalid frame pointer 0xbfffa7d8
      Kernel loadable modules in backtrace (with dependencies):
         com.apple.filesystems.zfs(8.0)@0x3fb44000->0x3fc0ffff

BSD process name corresponding to current thread: zpool

Mac OS version:
9C7010

Kernel version:
Darwin Kernel Version 9.2.2: Tue Mar  4 21:17:34 PST 2008;
root:xnu-1228.4.31~1/RELEASE_I386
System model name: iMac4,1 (Mac-F42786C8)


Mon Mar 10 00:06:34 2008
panic(cpu 1 caller 0x001A8C8A): Kernel trap at 0x00361855, type
14=page fault, registers:
CR0: 0x80010033, CR2: 0x0000000c, CR3: 0x00cf2000, CR4: 0x00000660
EAX: 0x00000000, EBX: 0x36f1fde8, ECX: 0x1f000000, EDX: 0x00000000
CR2: 0x0000000c, EBP: 0x36f1fd18, ESI: 0x00000200, EDI: 0x03b56000
EFL: 0x00010202, EIP: 0x00361855, CS:  0x00000008, DS:  0x03960010
Error code: 0x00000000

Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x36f1fb38 : 0x12b0f7 (0x458124 0x36f1fb6c 0x133230 0x0)
0x36f1fb88 : 0x1a8c8a (0x461650 0x361855 0xe 0x460e00)
0x36f1fc68 : 0x19ece5 (0x36f1fc80 0x3bc23c8 0x36f1fd18 0x361855)
0x36f1fc78 : 0x361855 (0xe 0x48 0x10 0x190010)
0x36f1fd18 : 0x35840ff8 (0x0 0x4ec63 0x3000 0x0)
0x36f1fd38 : 0x358489dd (0x0 0x0 0x36f1fd58 0x1a236f)
0x36f1fd78 : 0x202bea (0x1f000000 0xcf1c5a20 0x3b56000 0x3)
0x36f1fdb8 : 0x1f6039 (0x36f1fde8 0x246 0x36f1fe18 0x1da207)
0x36f1fe18 : 0x1ec2f6 (0x5a59e20 0xcf1c5a20 0x3b56000 0x3)
0x36f1fe78 : 0x36533b (0x43cb2b0 0xcf1c5a20 0x3b56000 0x36f1ff50)
0x36f1fe98 : 0x38bcbc (0x43cb2b0 0xcf1c5a20 0x3b56000 0x36f1ff50)
0x36f1ff78 : 0x3dcea7 (0x445d2b0 0x4b8a220 0x4b8a264 0x0)
0x36f1ffc8 : 0x19f1c3 (0x47d8040 0x0 0x1a20b5 0x47d8040)
No mapping exists for frame pointer
Backtrace terminated-invalid frame pointer 0xbfffa618
      Kernel loadable modules in backtrace (with dependencies):
         com.apple.filesystems.zfs(8.0)@0x357e5000->0x358b0fff

BSD process name corresponding to current thread: zpool

Mac OS version:
9C2015

Kernel version:
Darwin Kernel Version 9.2.1: Tue Feb  5 23:08:45 PST 2008;
root:xnu-1228.4.20~1/RELEASE_I386
System model name: MacBook4,1 (Mac-F22788A9)




On Wed, Apr 9, 2008 at 4:18 PM, No?l Dellofano <ndellofano@apple.com> wrote:
> yikes :(
>  What is your config?  Something huge?  what kind of system are you running
> on?  and is it only zpool status? or any zpool command freezes the system?
>
>  Noel
>
>
>
>  On Apr 9, 2008, at 9:39 AM, Franz Schmalzl wrote:
>
>
> > zpool status  -v gives me a complete system freeze....
> >
> > any ideas ?
> >
> >
> > regards
> >
> > franz schmalzl _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss@lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> >
>
>  _______________________________________________
>  zfs-discuss mailing list
>  zfs-discuss@lists.macosforge.org
>  http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
From mattsnow at gmail.com  Sat Apr 12 08:26:52 2008
From: mattsnow at gmail.com (Matt Snow)
Date: Sat Apr 12 08:25:06 2008
Subject: [zfs-discuss] zpool status -v
In-Reply-To: <51cfc2260804112214wa5ff3c7wd6ad9e9ecf701b79@mail.gmail.com>
References: <17348603-B0C9-4B6E-AF13-3E2B16325BBC@spamfreemail.de>
	<AB3828E3-B960-4E58-8B64-979317C3BD53@apple.com>
	<51cfc2260804112214wa5ff3c7wd6ad9e9ecf701b79@mail.gmail.com>
Message-ID: <6879ebc80804120826p556689bdk1667e027b20e958@mail.gmail.com>

I take it these disks are on USB or firewire? I have not run in to this yet,
but all of my disks are directly attached via SATA.

xos:~ msnow$ zpool status -v
  pool: export
 state: ONLINE
status: The pool is formatted using an older on-disk format.  The pool can
    still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
    pool will no longer be accessible on older software versions.
 scrub: none requested
config:

    NAME         STATE     READ WRITE CKSUM
    export       ONLINE       0     0     0
      raidz1     ONLINE       0     0     0
        disk0s2  ONLINE       0     0     0
        disk1s2  ONLINE       0     0     0
        disk3s2  ONLINE       0     0     0
        disk4s2  ONLINE       0     0     0
        disk5s2  ONLINE       0     0     0

errors: No known data errors
xos:~ msnow$ zfs list
NAME     USED  AVAIL  REFER  MOUNTPOINT
export   439G  1.36T   439G  /Volumes/export
xos:~ msnow$


On Fri, Apr 11, 2008 at 10:14 PM, Riscky Abacus <riscky@gmail.com> wrote:

> You know I've seen this countless times on my iMac. I have to zfs
> "drives" and when one of these drives is attached zpool status throws
> a kernal panic... I posted a message about this last month... if I
> move this "drive" to another system and run zpool status the same
> panic ensues... with the drive is also attached and I issue other
> zpool commands sometimes I get strange results like tonight when I
> exported my other pool the WindowServer suddenly spiked to 99% cpu
> time for an extended amount of time (the drive held my iTMS files). A
> few other times kernel_task runs at 65% until I reboot the system...
> not sure if this just happens to happen then or if its related to
> zpool.
>
> Anyway the pool that cause this problem for me is actually a
> collection of 4 usb flash drives connected to one usb hub to create on
> large flash drive... oh and I scrubbed the zpool and found no
> issues... the only thing I think that might cause this problem is I
> have the the zpool root set to /src while the mount point is
> /Volumes/pool
>
> Here are two logs each from a different system:
>
> Sat Apr  5 17:21:22 2008
> panic(cpu 1 caller 0x001A8C8A): Kernel trap at 0x003618c1, type
> 14=page fault, registers:
> CR0: 0x80010033, CR2: 0x0000000c, CR3: 0x00d07000, CR4: 0x000006e0
> EAX: 0x00000000, EBX: 0x35913de8, ECX: 0x1f000000, EDX: 0x00000000
> CR2: 0x0000000c, EBP: 0x35913d18, ESI: 0x00000200, EDI: 0x03a77000
> EFL: 0x00010202, EIP: 0x003618c1, CS:  0x00000004, DS:  0x1000000c
> Error code: 0x00000000
>
> Backtrace, Format - Frame : Return Address (4 potential args on stack)
> 0x35913b88 : 0x12b0f7 (0x4581f4 0x35913bbc 0x133230 0x0)
> 0x35913bd8 : 0x1a8c8a (0x461720 0x3618c1 0xe 0x460ed0)
> 0x35913cb8 : 0x19ece5 (0x35913cd0 0x0 0x35913d18 0x3618c1)
> 0x35913cc8 : 0x3618c1 (0xe 0x5630048 0x3591000c 0x20000c)
> 0x35913d18 : 0x3fb9fff8 (0x0 0xfe972958 0x1 0x0)
> 0x35913d38 : 0x3fba79dd (0x0 0x0 0x35913d88 0x1362ad)
> 0x35913d78 : 0x202bea (0x1f000000 0xcf1c5a20 0x3a77000 0x3)
> 0x35913db8 : 0x1f6039 (0x35913de8 0x246 0x35913e18 0x1da207)
> 0x35913e18 : 0x1ec2f6 (0x485a680 0xcf1c5a20 0x3a77000 0x3)
> 0x35913e78 : 0x3653a7 (0x4440c80 0xcf1c5a20 0x3a77000 0x35913f50)
> 0x35913e98 : 0x38bd28 (0x4440c80 0xcf1c5a20 0x3a77000 0x35913f50)
> 0x35913f78 : 0x3dcf13 (0x3e26530 0x4c3fee0 0x4c3ff24 0x0)
> 0x35913fc8 : 0x19f1c3 (0x4c44bd4 0x0 0x4 0x456b97c)
> No mapping exists for frame pointer
> Backtrace terminated-invalid frame pointer 0xbfffa7d8
>      Kernel loadable modules in backtrace (with dependencies):
>         com.apple.filesystems.zfs(8.0)@0x3fb44000->0x3fc0ffff
>
> BSD process name corresponding to current thread: zpool
>
> Mac OS version:
> 9C7010
>
> Kernel version:
> Darwin Kernel Version 9.2.2: Tue Mar  4 21:17:34 PST 2008;
> root:xnu-1228.4.31~1/RELEASE_I386
> System model name: iMac4,1 (Mac-F42786C8)
>
>
> Mon Mar 10 00:06:34 2008
> panic(cpu 1 caller 0x001A8C8A): Kernel trap at 0x00361855, type
> 14=page fault, registers:
> CR0: 0x80010033, CR2: 0x0000000c, CR3: 0x00cf2000, CR4: 0x00000660
> EAX: 0x00000000, EBX: 0x36f1fde8, ECX: 0x1f000000, EDX: 0x00000000
> CR2: 0x0000000c, EBP: 0x36f1fd18, ESI: 0x00000200, EDI: 0x03b56000
> EFL: 0x00010202, EIP: 0x00361855, CS:  0x00000008, DS:  0x03960010
> Error code: 0x00000000
>
> Backtrace, Format - Frame : Return Address (4 potential args on stack)
> 0x36f1fb38 : 0x12b0f7 (0x458124 0x36f1fb6c 0x133230 0x0)
> 0x36f1fb88 : 0x1a8c8a (0x461650 0x361855 0xe 0x460e00)
> 0x36f1fc68 : 0x19ece5 (0x36f1fc80 0x3bc23c8 0x36f1fd18 0x361855)
> 0x36f1fc78 : 0x361855 (0xe 0x48 0x10 0x190010)
> 0x36f1fd18 : 0x35840ff8 (0x0 0x4ec63 0x3000 0x0)
> 0x36f1fd38 : 0x358489dd (0x0 0x0 0x36f1fd58 0x1a236f)
> 0x36f1fd78 : 0x202bea (0x1f000000 0xcf1c5a20 0x3b56000 0x3)
> 0x36f1fdb8 : 0x1f6039 (0x36f1fde8 0x246 0x36f1fe18 0x1da207)
> 0x36f1fe18 : 0x1ec2f6 (0x5a59e20 0xcf1c5a20 0x3b56000 0x3)
> 0x36f1fe78 : 0x36533b (0x43cb2b0 0xcf1c5a20 0x3b56000 0x36f1ff50)
> 0x36f1fe98 : 0x38bcbc (0x43cb2b0 0xcf1c5a20 0x3b56000 0x36f1ff50)
> 0x36f1ff78 : 0x3dcea7 (0x445d2b0 0x4b8a220 0x4b8a264 0x0)
> 0x36f1ffc8 : 0x19f1c3 (0x47d8040 0x0 0x1a20b5 0x47d8040)
> No mapping exists for frame pointer
> Backtrace terminated-invalid frame pointer 0xbfffa618
>      Kernel loadable modules in backtrace (with dependencies):
>         com.apple.filesystems.zfs(8.0)@0x357e5000->0x358b0fff
>
> BSD process name corresponding to current thread: zpool
>
> Mac OS version:
> 9C2015
>
> Kernel version:
> Darwin Kernel Version 9.2.1: Tue Feb  5 23:08:45 PST 2008;
> root:xnu-1228.4.20~1/RELEASE_I386
> System model name: MacBook4,1 (Mac-F22788A9)
>
>
>
>
> On Wed, Apr 9, 2008 at 4:18 PM, No?l Dellofano <ndellofano@apple.com>
> wrote:
> > yikes :(
> >  What is your config?  Something huge?  what kind of system are you
> running
> > on?  and is it only zpool status? or any zpool command freezes the
> system?
> >
> >  Noel
> >
> >
> >
> >  On Apr 9, 2008, at 9:39 AM, Franz Schmalzl wrote:
> >
> >
> > > zpool status  -v gives me a complete system freeze....
> > >
> > > any ideas ?
> > >
> > >
> > > regards
> > >
> > > franz schmalzl _______________________________________________
> > > zfs-discuss mailing list
> > > zfs-discuss@lists.macosforge.org
> > > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> > >
> >
> >  _______________________________________________
> >  zfs-discuss mailing list
> >  zfs-discuss@lists.macosforge.org
> >  http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> >
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080412/5eb2404c/attachment-0001.html
From hanche at math.ntnu.no  Sun Apr 13 02:01:10 2008
From: hanche at math.ntnu.no (Harald Hanche-Olsen)
Date: Sun Apr 13 01:59:22 2008
Subject: [zfs-discuss] .zfs and rsync (Was: Status of ZFS on Mac OS X?)
Message-ID: <20080413.110110.12767332.hanche@math.ntnu.no>

Alex Blewitt wrote:

> Quick bit of feedback about .zfs - the 'rsync' program has an -C or
> --cvs-exclude, which sets an exclude list to ignore certain
> directories:
> 
>               The exclude list is initialized to:
> 
>               RCS SCCS CVS CVS.adm RCSLOG cvslog.* tags TAGS
>               .make.state .nse_depinfo *~ #* .#* ,* _$* *$ *.old
>               *.bak *.BAK *.orig *.rej .del-* *.a *.olb *.o *.obj
>               *.so *.exe *.Z *.elc *.ln core .svn/
> 
> I see that it's got .svn/ in there; I think a great addition would
> be .zfs/ so that when you're rsync'ing across multiple directories
> you don't get accidental snapshots carried with you.

This is not necessary. The .zfs directories shall by design be even
more hidden than the leading dot indicates: They shall not be visible
at all to a program that opens the parent directory and reads it. In
particular, ls -l and rsync won't ever see such directories, unless
you name them explicitly on the command line.

I am sure the rationale for this magic is precisely avoiding such
things as you mention, not only for rsync but for all sorts of backup
programs.

- Harald

PS. I am new to the list, so copied the quote above out of the webbed
mail archive. Hence the lack of proper references to the thread in the
mail headers. I apologize if this messes up threading (in the archive
at least).
From franzschmalzl at spamfreemail.de  Sun Apr 13 08:48:45 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sun Apr 13 08:47:01 2008
Subject: [zfs-discuss] rsync
Message-ID: <4AD242F1-0ACB-470B-AEE7-E5DF15A98E66@spamfreemail.de>

Hi list!

my system completely freezes when copying large directory structures  
to a raidz array...

is this a known issue ?

best regards

franz
From franzschmalzl at spamfreemail.de  Sun Apr 13 09:49:36 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sun Apr 13 09:47:51 2008
Subject: [zfs-discuss] zpool status -v
In-Reply-To: <AB3828E3-B960-4E58-8B64-979317C3BD53@apple.com>
References: <17348603-B0C9-4B6E-AF13-3E2B16325BBC@spamfreemail.de>
	<AB3828E3-B960-4E58-8B64-979317C3BD53@apple.com>
Message-ID: <0FAC3CFA-E63C-47FF-BDD6-E56A988D9C4E@spamfreemail.de>


On Apr 9, 2008, at 10:18 PM, No?l Dellofano wrote:
> yikes :(
> What is your config?  Something huge?  what kind of system are you  
> running on?  and is it only zpool status? or any zpool command  
> freezes the system?

sooo

not that huge... 3 drives with a total amount of 1,5TB of space
i use 10.5.3

it's only zpool status -v
zpool status works...

best regards

franz




>
>
> Noel
>
> On Apr 9, 2008, at 9:39 AM, Franz Schmalzl wrote:
>
>> zpool status  -v gives me a complete system freeze....
>>
>> any ideas ?
>>
>>
>> regards
>>
>> franz schmalzl _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

From info at martin-hauser.net  Sun Apr 13 09:55:57 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Sun Apr 13 09:54:48 2008
Subject: [zfs-discuss] zpool status -v
In-Reply-To: <0FAC3CFA-E63C-47FF-BDD6-E56A988D9C4E@spamfreemail.de>
References: <17348603-B0C9-4B6E-AF13-3E2B16325BBC@spamfreemail.de>
	<AB3828E3-B960-4E58-8B64-979317C3BD53@apple.com>
	<0FAC3CFA-E63C-47FF-BDD6-E56A988D9C4E@spamfreemail.de>
Message-ID: <4F1472CE-40A6-4338-8A9F-4F8B07D868AE@martin-hauser.net>

10.5.3 ? Latest version of 10.5 is 10.5.2 as far as I am aware of ?

On Apr 13, 2008, at 18:49 PM, Franz Schmalzl wrote:
>
> On Apr 9, 2008, at 10:18 PM, No?l Dellofano wrote:
>> yikes :(
>> What is your config?  Something huge?  what kind of system are you  
>> running on?  and is it only zpool status? or any zpool command  
>> freezes the system?
>
> sooo
>
> not that huge... 3 drives with a total amount of 1,5TB of space
> i use 10.5.3
>
> it's only zpool status -v
> zpool status works...
>
> best regards
>
> franz
>
>
>
>
>>
>>
>> Noel
>>
>> On Apr 9, 2008, at 9:39 AM, Franz Schmalzl wrote:
>>
>>> zpool status  -v gives me a complete system freeze....
>>>
>>> any ideas ?
>>>
>>>
>>> regards
>>>
>>> franz schmalzl _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080413/47e8c066/PGP.bin
From mattsnow at gmail.com  Sun Apr 13 10:49:20 2008
From: mattsnow at gmail.com (Matt Snow)
Date: Sun Apr 13 10:47:22 2008
Subject: [zfs-discuss] zpool status -v
In-Reply-To: <0FAC3CFA-E63C-47FF-BDD6-E56A988D9C4E@spamfreemail.de>
References: <17348603-B0C9-4B6E-AF13-3E2B16325BBC@spamfreemail.de>
	<AB3828E3-B960-4E58-8B64-979317C3BD53@apple.com>
	<0FAC3CFA-E63C-47FF-BDD6-E56A988D9C4E@spamfreemail.de>
Message-ID: <6879ebc80804131049m18c6687am182fc82394394c1b@mail.gmail.com>

Are you a beta tester for 10.5.3?


On Sun, Apr 13, 2008 at 9:49 AM, Franz Schmalzl <
franzschmalzl@spamfreemail.de> wrote:

>
> On Apr 9, 2008, at 10:18 PM, No?l Dellofano wrote:
>
> > yikes :(
> > What is your config?  Something huge?  what kind of system are you
> > running on?  and is it only zpool status? or any zpool command freezes the
> > system?
> >
>
> sooo
>
> not that huge... 3 drives with a total amount of 1,5TB of space
> i use 10.5.3
>
> it's only zpool status -v
> zpool status works...
>
> best regards
>
> franz
>
>
>
>
>
>
> >
> > Noel
> >
> > On Apr 9, 2008, at 9:39 AM, Franz Schmalzl wrote:
> >
> >  zpool status  -v gives me a complete system freeze....
> > >
> > > any ideas ?
> > >
> > >
> > > regards
> > >
> > > franz schmalzl _______________________________________________
> > > zfs-discuss mailing list
> > > zfs-discuss@lists.macosforge.org
> > > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> > >
> >
> >
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080413/497714c5/attachment.html
From franzschmalzl at spamfreemail.de  Sun Apr 13 11:31:42 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sun Apr 13 11:29:56 2008
Subject: [zfs-discuss] rsync
In-Reply-To: <57A74DA7-8A9A-437D-96DA-F5EFCC2DF33B@inius.com>
References: <4AD242F1-0ACB-470B-AEE7-E5DF15A98E66@spamfreemail.de>
	<57A74DA7-8A9A-437D-96DA-F5EFCC2DF33B@inius.com>
Message-ID: <6362A5FF-B693-412E-BF6D-9F7E683B75A7@spamfreemail.de>

i use the bundled  version..

maybe i should try the latest, but will this version still be patched  
to handel resource forks and hfs metadata and so on ?

franz


On Apr 13, 2008, at 7:23 PM, Kane Dijkman wrote:

> Are you using rsync 3 or the earlier version that comes with  
> Leopard? I had this problem too, until I downloaded the latest,  
> works fine now.
>
> Kane
>
>
>
> On Apr 13, 2008, at 8:48 AM, Franz Schmalzl wrote:
>> Hi list!
>>
>> my system completely freezes when copying large directory  
>> structures to a raidz array...
>>
>> is this a known issue ?
>>
>> best regards
>>
>> franz
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
>
> ---------------------------------------------------------------------------------------------
> "It said Windows 2000 or better on the box, so I bought a Mac"
>

From franzschmalzl at spamfreemail.de  Sun Apr 13 11:33:16 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sun Apr 13 11:31:32 2008
Subject: [zfs-discuss] zpool status -v
In-Reply-To: <6879ebc80804131049m18c6687am182fc82394394c1b@mail.gmail.com>
References: <17348603-B0C9-4B6E-AF13-3E2B16325BBC@spamfreemail.de>
	<AB3828E3-B960-4E58-8B64-979317C3BD53@apple.com>
	<0FAC3CFA-E63C-47FF-BDD6-E56A988D9C4E@spamfreemail.de>
	<6879ebc80804131049m18c6687am182fc82394394c1b@mail.gmail.com>
Message-ID: <9E3C699B-D524-4EF2-AF82-E0BAC00EC783@spamfreemail.de>

sorry guys

10.5.2 ;-) did not want to confuse you here




On Apr 13, 2008, at 7:49 PM, Matt Snow wrote:

> Are you a beta tester for 10.5.3?
>
>
> On Sun, Apr 13, 2008 at 9:49 AM, Franz Schmalzl <franzschmalzl@spamfreemail.de 
> > wrote:
>
> On Apr 9, 2008, at 10:18 PM, No?l Dellofano wrote:
> yikes :(
> What is your config?  Something huge?  what kind of system are you  
> running on?  and is it only zpool status? or any zpool command  
> freezes the system?
>
> sooo
>
> not that huge... 3 drives with a total amount of 1,5TB of space
> i use 10.5.3
>
> it's only zpool status -v
> zpool status works...
>
> best regards
>
> franz
>
>
>
>
>
>
>
> Noel
>
> On Apr 9, 2008, at 9:39 AM, Franz Schmalzl wrote:
>
> zpool status  -v gives me a complete system freeze....
>
> any ideas ?
>
>
> regards
>
> franz schmalzl _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080413/bb875c16/attachment-0001.html
From kane at inius.com  Sun Apr 13 12:55:56 2008
From: kane at inius.com (Kane Dijkman)
Date: Sun Apr 13 12:55:06 2008
Subject: [zfs-discuss] rsync
In-Reply-To: <6362A5FF-B693-412E-BF6D-9F7E683B75A7@spamfreemail.de>
References: <4AD242F1-0ACB-470B-AEE7-E5DF15A98E66@spamfreemail.de>
	<57A74DA7-8A9A-437D-96DA-F5EFCC2DF33B@inius.com>
	<6362A5FF-B693-412E-BF6D-9F7E683B75A7@spamfreemail.de>
Message-ID: <1B0327C3-19BC-40E8-A01B-35006D6F5505@inius.com>

Yeah it has -X for extended attributes:

  -X, --xattrs                preserve extended attributes

Kane


On Apr 13, 2008, at 11:31 AM, Franz Schmalzl wrote:
> i use the bundled  version..
>
> maybe i should try the latest, but will this version still be  
> patched to handel resource forks and hfs metadata and so on ?
>
> franz
>
>
> On Apr 13, 2008, at 7:23 PM, Kane Dijkman wrote:
>
>> Are you using rsync 3 or the earlier version that comes with  
>> Leopard? I had this problem too, until I downloaded the latest,  
>> works fine now.
>>
>> Kane
>>
>>
>>
>> On Apr 13, 2008, at 8:48 AM, Franz Schmalzl wrote:
>>> Hi list!
>>>
>>> my system completely freezes when copying large directory  
>>> structures to a raidz array...
>>>
>>> is this a known issue ?
>>>
>>> best regards
>>>
>>> franz
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>>
>> ---------------------------------------------------------------------------------------------
>> "It said Windows 2000 or better on the box, so I bought a Mac"
>>
>


---------------------------------------------------------------------------------------------
A computer without a Microsoft operating system is like a dog without  
bricks tied to its head.

From david299792 at googlemail.com  Mon Apr 14 05:06:28 2008
From: david299792 at googlemail.com (David Ritchie)
Date: Mon Apr 14 05:04:33 2008
Subject: [zfs-discuss] Panic
Message-ID: <EE4E85B2-A136-409F-A0AB-C8EC1A2AC6CA@googlemail.com>

Hi,

This might be a known issue, but in case it's not here's a crash  
report from a kernel panic I just had. I've had this happen once or  
twice before:-


Mon Apr 14 13:00:29 2008
panic(cpu 2 caller 0x0101D539): "zfs: vmem_alloc couldn't alloc 131072  
bytes\n"@/Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/ 
zfs_kext/zfs/zfs_context.c:668
Backtrace, Format - Frame : Return Address (4 potential args on stack)
0xbed2f908 : 0x12b0f7 (0x4581f4 0xbed2f93c 0x133230 0x0)
0xbed2f958 : 0x101d539 (0x1077720 0x20000 0x20000 0x101c1ed)
0xbed2f988 : 0x1037117 (0x108b264 0x20000 0x0 0x100)
0xbed2f9e8 : 0xfd3f09 (0xb7aca00 0x0 0xb388230 0x0)
0xbed2fa08 : 0xfde136 (0x20000 0x0 0x1084310 0x0)
0xbed2fa88 : 0xfde989 (0xb7ac200 0x0 0xbed2fab8 0x8e)
0xbed2fac8 : 0xfe1222 (0xbaee000 0x20000 0xb9b732d8 0x0)
0xbed2fb58 : 0xfe71a1 (0xb9b732d8 0x1fd24de0 0x2 0x0)
0xbed2fba8 : 0xfe72c4 (0x8e 0x0 0x0 0x1)
0xbed2fc48 : 0xfe77ae (0x10000 0x0 0x1 0x11c0000)
0xbed2fca8 : 0xfc4216 (0x18600880 0xca 0x0 0x11c0000)
0xbed2fd78 : 0x1f6131 (0xbed2fdac 0x246 0xbed2fdd8 0x1da207)
0xbed2fdd8 : 0x1ec98f (0x1232a510 0xbed2febc 0x1 0xbed2ff54)
0xbed2fe68 : 0x38b3ca (0xb5417f0 0xbed2febc 0x1 0xbed2ff54)
0xbed2ff18 : 0x38b53b (0xbed2ff54 0xb5417f0 0x24546000 0x0)
0xbed2ff78 : 0x3dcf13 (0x1f870a00 0x1b34f9c0 0x1b34fa04 0x0)
	Backtrace continues...
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0xfc1000->0x108cfff

BSD process name corresponding to current thread: vmware-vmx

Mac OS version:
9C7010

Kernel version:
Darwin Kernel Version 9.2.2: Tue Mar  4 21:17:34 PST 2008;  
root:xnu-1228.4.31~1/RELEASE_I386
System model name: MacPro1,1 (Mac-F4208DC8)

From Ben.Vaughn at united.com  Mon Apr 14 06:39:15 2008
From: Ben.Vaughn at united.com (Vaughn, Benjamin [WHQSE])
Date: Mon Apr 14 06:37:26 2008
Subject: [zfs-discuss] ZFS on an XServe RAID
In-Reply-To: <CBD3AF0E-1ACE-47F7-B7E3-F911C09582C3@apple.com>
References: <A30DC84845CE2241A2900B68D6111099F09CA6@D1VS1.D1.USA.NET>
	<CBD3AF0E-1ACE-47F7-B7E3-F911C09582C3@apple.com>
Message-ID: <A30DC84845CE2241A2900B68D6111099F09EFF@D1VS1.D1.USA.NET>

Noel,
 
    We use the array as a storage medium for an electronic discovery archiving application (indexing hard drives, etc.)  The left side is a MySQL database, only 125GB of 1.25TB is used, and the right side is raw file storage (up to 60M distinct files) taking up anywhere from 2TB to 4TB.  My main complaint right now is that turning on HFS journaling destroys the performance of the array, but turning it off causes sometimes catastrophic filesystem corruptions -- I'm dealing with one of those now, in fact.  I'm looking for additional performance and stability for this most important system.
 
Thanks,
Ben
 

Benjamin Vaughn

Project Manager - Computer Investigations and Systems Integrity

United Airlines Corporate Security - OPCSE

+1 847 700 9505 Office

+1 630 329 7629 Mobile

Ben.Vaughn@united.com <mailto:Ben.Vaughn@united.com> 

P Please consider the environment before printing this e-mail.

 

 

 

 

 

 

 

 

 

 

________________________________

From: No?l Dellofano [mailto:ndellofano@apple.com] 
Sent: Friday, April 11, 2008 17:28
To: Vaughn, Benjamin [WHQSE]
Cc: zfs-discuss@lists.macosforge.org
Subject: Re: [zfs-discuss] ZFS on an XServe RAID


Awesome!  So I'd definitely recommend converting all the disks to JBOD and letting ZFS do all the raid work since then we can do end to end checksums and can heal any corrupted blocks we detect.  As to the configuration, it really depends on what you want to do with the machine.  What do you use it for? are you more interested in performance or replication and safety? 
How much space do you need to get out of the machine?

We have an Xraid here at work that I've configured, and the drives in it are really old so I don't trust them, hence I went for extreme replication (it's a server and extreme perf isn't dire).  I did a pool of two groups of raidz-2 (6 drives each) and then added a hotspare to each group. 

Most generally I'd say you'd be good with configuring a pool with 2 groups of raidz's (7 -250GB in one and 7- 700GB in the other), with that you'll get good performance with still some replication while only losing 950GB overall for replication cost (parity).  If you wanted to be more cautious, you could also do the same config but use raidz2 instead raidz so that you could withstand more disk failures, but you'll lose more storage to your parity.  There's a ton on configs, which really depend on your workload, preference, perfomance needs, and storage needs.  I'd recommend checking out the best practices guide which has a section on how to consider configuring your pool:

http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide

Roch Bourbonnais of Sun also has a blog on when or when not to use raidz:
http://blogs.sun.com/roch/entry/when_to_and_not_to


and please feel free to ask me as well if you have more questions about a config too.  It just really depends on what you need to use it for :)

Noel 


On Apr 11, 2008, at 2:23 PM, Vaughn, Benjamin [WHQSE] wrote:


	
	Hello,
	 
	  I currently have an XServe RAID box with all 14 drive slots filled (left side is 250GB, right side is 700GB disks.)  In the future when ZFS is more stable and enterprise friendly, how would be the best way to deploy ZFS?  Convert all the disks to JBOD and have RAIDZ do the raiding, or let the XServe RAID do RAID5 and put ZFS on that?
	 
	Thanks for the info,
	Ben
	 
Benjamin Vaughn
Project Manager - Computer Investigations and Systems Integrity
United Airlines Corporate Security - OPCSE
+1 847 700 9505 Office
+1 630 329 7629 Mobile
Ben.Vaughn@united.com <mailto:Ben.Vaughn@united.com> 
P Please consider the environment before printing this e-mail.
	
	
	 
	_______________________________________________
	zfs-discuss mailing list
	zfs-discuss@lists.macosforge.org
	http://lists.macosforge.org/mailman/listinfo/zfs-discuss
	


-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080414/ad42145a/attachment-0001.html
From ndellofano at apple.com  Mon Apr 14 13:24:00 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon Apr 14 13:23:27 2008
Subject: [zfs-discuss] Panic
In-Reply-To: <EE4E85B2-A136-409F-A0AB-C8EC1A2AC6CA@googlemail.com>
References: <EE4E85B2-A136-409F-A0AB-C8EC1A2AC6CA@googlemail.com>
Message-ID: <74B6C6E3-3A16-46C0-BA14-76BA53C332D3@apple.com>

yep, this is a known problem.  I'm assuming you were rsyncing,  
ditto'ing, tar'ing, etc.,  something from/to a zfs pool?  Basically  
the kernel map is getting blown because the userland tools are trying  
to stash everything in memory while traversing the hierarchy.
Updating to rsync 3 should fix your issue.  You can find more info on  
how to do this on this thread:

http://lists.macosforge.org/pipermail/zfs-discuss/2008-January/000035.html


Noel

On Apr 14, 2008, at 5:06 AM, David Ritchie wrote:

> Hi,
>
> This might be a known issue, but in case it's not here's a crash  
> report from a kernel panic I just had. I've had this happen once or  
> twice before:-
>
>
> Mon Apr 14 13:00:29 2008
> panic(cpu 2 caller 0x0101D539): "zfs: vmem_alloc couldn't alloc  
> 131072 bytes\n"@/Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/ 
> zfs-102A/zfs_kext/zfs/zfs_context.c:668
> Backtrace, Format - Frame : Return Address (4 potential args on stack)
> 0xbed2f908 : 0x12b0f7 (0x4581f4 0xbed2f93c 0x133230 0x0)
> 0xbed2f958 : 0x101d539 (0x1077720 0x20000 0x20000 0x101c1ed)
> 0xbed2f988 : 0x1037117 (0x108b264 0x20000 0x0 0x100)
> 0xbed2f9e8 : 0xfd3f09 (0xb7aca00 0x0 0xb388230 0x0)
> 0xbed2fa08 : 0xfde136 (0x20000 0x0 0x1084310 0x0)
> 0xbed2fa88 : 0xfde989 (0xb7ac200 0x0 0xbed2fab8 0x8e)
> 0xbed2fac8 : 0xfe1222 (0xbaee000 0x20000 0xb9b732d8 0x0)
> 0xbed2fb58 : 0xfe71a1 (0xb9b732d8 0x1fd24de0 0x2 0x0)
> 0xbed2fba8 : 0xfe72c4 (0x8e 0x0 0x0 0x1)
> 0xbed2fc48 : 0xfe77ae (0x10000 0x0 0x1 0x11c0000)
> 0xbed2fca8 : 0xfc4216 (0x18600880 0xca 0x0 0x11c0000)
> 0xbed2fd78 : 0x1f6131 (0xbed2fdac 0x246 0xbed2fdd8 0x1da207)
> 0xbed2fdd8 : 0x1ec98f (0x1232a510 0xbed2febc 0x1 0xbed2ff54)
> 0xbed2fe68 : 0x38b3ca (0xb5417f0 0xbed2febc 0x1 0xbed2ff54)
> 0xbed2ff18 : 0x38b53b (0xbed2ff54 0xb5417f0 0x24546000 0x0)
> 0xbed2ff78 : 0x3dcf13 (0x1f870a00 0x1b34f9c0 0x1b34fa04 0x0)
> 	Backtrace continues...
>      Kernel loadable modules in backtrace (with dependencies):
>         com.apple.filesystems.zfs(8.0)@0xfc1000->0x108cfff
>
> BSD process name corresponding to current thread: vmware-vmx
>
> Mac OS version:
> 9C7010
>
> Kernel version:
> Darwin Kernel Version 9.2.2: Tue Mar  4 21:17:34 PST 2008;  
> root:xnu-1228.4.31~1/RELEASE_I386
> System model name: MacPro1,1 (Mac-F4208DC8)
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From chris.shuman at gmail.com  Tue Apr 15 18:24:28 2008
From: chris.shuman at gmail.com (Chris Shuman)
Date: Tue Apr 15 18:22:24 2008
Subject: [zfs-discuss] 3 x 750 GB or 4 x 500 GB
Message-ID: <E07B4731-8BFF-46B9-8899-A7501F83F49E@gmail.com>

I have been testing ZFS on my Mac with some spare disks I had lying  
around, but I am now ready to make the jump to all home folders on  
ZFS. My question is whether or not it would make a difference in  
performance or reliability building a raidz from 4 x 500 GB drives vs.  
3 x 750 GB drives?

750 GB 7200 w/32 MB cache vs 500 GB 7200 w/16MB cache.

and. . .Seagate Barracuda vs. Samsung Spinpoint F1. Should either of  
these be avoided?

Thanks,

Chris Shuman

From canadrian at electricteaparty.net  Tue Apr 15 18:35:37 2008
From: canadrian at electricteaparty.net (Thornton Adrian)
Date: Tue Apr 15 18:33:38 2008
Subject: [zfs-discuss] RAIDZ continuous panic after zpool replace
Message-ID: <4AFB341E-A3A8-4E23-AFBE-9753C01AE4A7@electricteaparty.net>

I'm having a big problem here. I was replacing the drives in my RAIDZ  
with bigger ones, and in the midst of a resilver the machine panicked.  
I rebooted, and now it panics shortly after the login screen every  
time I reboot the machine. "Screw it," I thought, and moved the disks  
over to a Solaris 10 box to repair the array. Unfortunately, no matter  
what I do the Solaris box won't recognize my SiI 3114 card so I can't  
get it to see all the drives. The only way to get OS X to boot  
properly is in safe mode or by detaching some of the drives. In safe  
mode, zfs doesn't function.

Is there any way I can STOP it from doing whatever it's trying to do  
(resilver? scrub?) and figure out what the problem is? Or at least how  
to I find a log of the kernel panic so I can post it for your perusal?  
This is utterly aggravating because 7 of the 8 drives in the RAIDZ  
should be perfectly fine, and my data should be intact. I just can't  
get the bloody thing to stop panicking if I have all the drives  
attached.

Thanks,
Adrian
From canadrian at electricteaparty.net  Tue Apr 15 19:37:05 2008
From: canadrian at electricteaparty.net (Thornton Adrian)
Date: Tue Apr 15 19:34:57 2008
Subject: [zfs-discuss] Re: RAIDZ continuous panic after zpool replace
In-Reply-To: <4AFB341E-A3A8-4E23-AFBE-9753C01AE4A7@electricteaparty.net>
References: <4AFB341E-A3A8-4E23-AFBE-9753C01AE4A7@electricteaparty.net>
Message-ID: <CB635C30-EE86-4CC4-8A47-C31194F696BE@electricteaparty.net>

I managed to boot in single-user mode and a zpool import showed the  
pool with one faulted device, just as it should. I thought maybe I  
could do a zpool import -f poolname followed rapidly by a zpool  
destroy poolname to clear any tasks, and then import -f again, but I  
got a panic. Luckily it wrote the details to the screen this time, so  
it's time for me to type...

panic(cpu 1 caller 0x37046B71): "[ZFS]: assertion failed in /Volumes/ 
pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/zfs_kext/zfs/dmu.c  
line 437: 0 == dmu_buf_hold_array(os, object, offset, size, FALSE,  
FTAG, &numbufs, &dbp)"@Volumes/pixie_d
Debugger called: <panic>
Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x391b7b78 : 0x12b0f7 (0x4581f4 0x391b7bac 0x133230 0x0)
0x391b7bc8 : 0x37046b71 (0x370b2e0c 0x370b29e4 0x1b5 0x370b2dbc)
0x391b7c58 : 0x3702efe9 (0x511fd18 0x1ec 0x0 0xe938)
0x391b7d28 : 0x3702b9c9 (0x515ce20 0x1 0c515cc2c 0c511fd18)
0x391b7db8 : 0x370730b7 (0x515cc00 0x25ca1f 0x0 0x0)
0x391b7e08 : 0x3706508f (0x487f800 0x25ca1f 0x0 0x0)
0x391b7f58 : 0x3704e22a (0x4a3d800 0x25ca1f 0x0 0x1369fb)
0x391b7fc8 : 0x19eadc (0x511bc00 0x0 0x1a20b5 0x51bcba0)
Backtrace terminate-invalid frame pointer 0
	Kernel loadable modules in backtrace (with dependencies):
		com.apple.filesystems.zfs(8.0)@0x3701d000->0x370e8fff

BSD process name corresponding to current thread: kernel_task

Mac OS version:
9C7010


On 15-Apr-08, at 7:35 PM, Thornton Adrian wrote:
> I'm having a big problem here. I was replacing the drives in my  
> RAIDZ with bigger ones, and in the midst of a resilver the machine  
> panicked. I rebooted, and now it panics shortly after the login  
> screen every time I reboot the machine. "Screw it," I thought, and  
> moved the disks over to a Solaris 10 box to repair the array.  
> Unfortunately, no matter what I do the Solaris box won't recognize  
> my SiI 3114 card so I can't get it to see all the drives. The only  
> way to get OS X to boot properly is in safe mode or by detaching  
> some of the drives. In safe mode, zfs doesn't function.
>
> Is there any way I can STOP it from doing whatever it's trying to do  
> (resilver? scrub?) and figure out what the problem is? Or at least  
> how to I find a log of the kernel panic so I can post it for your  
> perusal? This is utterly aggravating because 7 of the 8 drives in  
> the RAIDZ should be perfectly fine, and my data should be intact. I  
> just can't get the bloody thing to stop panicking if I have all the  
> drives attached.
>
> Thanks,
> Adrian

From canadrian at electricteaparty.net  Tue Apr 15 23:42:39 2008
From: canadrian at electricteaparty.net (Thornton Adrian)
Date: Tue Apr 15 23:40:30 2008
Subject: [zfs-discuss] Re: RAIDZ continuous panic after zpool replace
In-Reply-To: <CB635C30-EE86-4CC4-8A47-C31194F696BE@electricteaparty.net>
References: <4AFB341E-A3A8-4E23-AFBE-9753C01AE4A7@electricteaparty.net>
	<CB635C30-EE86-4CC4-8A47-C31194F696BE@electricteaparty.net>
Message-ID: <B2B4DD52-E3A7-4C3F-9CC8-0F6CD55E00A9@electricteaparty.net>

Turns out a Belenix LiveCD recognizes my SATA card and sees my pool.  
But it's not all good news. For some reason, though Belenix shows  
seven out of eight devices online and happy, it still claims it can't  
import the pool. Here's the zpool import:

pool: Archives
id: 16750878087681595004
state: UNAVAIL
status: One or more devices are missing from the system.
action: The pool cannot be imported. Attach the missing devices and  
try again.
see: http://www.sun.com/msg/ZFS-8000-3C
config:

	Archives		UNAVAIL	insufficient replicas
		raidz1	UNAVAIL	corrupted data
		c8d1p0	ONLINE
		c2t1d0p0	ONLINE
	`	c7d1p0	ONLINE
		c2t0d0p0	ONLINE
		c8d0s1	ONLINE
		c4d0s1	ONLINE
		636893339691769212	UNAVAIL	cannot open
		c5d0s1	ONLINE

Looks fine, right? (Well, aside from the fact is should say DEGRADED  
not UNAVAIL.) One missing disk in an 8-disk raidz1 pool. But zpool  
import -f Archives says:

cannot import 'Archives': invalid vdev configuration

So... what's going on then? Any clues?

Cheers,
Adrian

On 15-Apr-08, at 8:37 PM, Thornton Adrian wrote:
> I managed to boot in single-user mode and a zpool import showed the  
> pool with one faulted device, just as it should. I thought maybe I  
> could do a zpool import -f poolname followed rapidly by a zpool  
> destroy poolname to clear any tasks, and then import -f again, but I  
> got a panic. Luckily it wrote the details to the screen this time,  
> so it's time for me to type...
>
> panic(cpu 1 caller 0x37046B71): "[ZFS]: assertion failed in /Volumes/ 
> pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/zfs_kext/zfs/dmu.c  
> line 437: 0 == dmu_buf_hold_array(os, object, offset, size, FALSE,  
> FTAG, &numbufs, &dbp)"@Volumes/pixie_d
> Debugger called: <panic>
> Backtrace, Format - Frame : Return Address (4 potential args on stack)
> 0x391b7b78 : 0x12b0f7 (0x4581f4 0x391b7bac 0x133230 0x0)
> 0x391b7bc8 : 0x37046b71 (0x370b2e0c 0x370b29e4 0x1b5 0x370b2dbc)
> 0x391b7c58 : 0x3702efe9 (0x511fd18 0x1ec 0x0 0xe938)
> 0x391b7d28 : 0x3702b9c9 (0x515ce20 0x1 0c515cc2c 0c511fd18)
> 0x391b7db8 : 0x370730b7 (0x515cc00 0x25ca1f 0x0 0x0)
> 0x391b7e08 : 0x3706508f (0x487f800 0x25ca1f 0x0 0x0)
> 0x391b7f58 : 0x3704e22a (0x4a3d800 0x25ca1f 0x0 0x1369fb)
> 0x391b7fc8 : 0x19eadc (0x511bc00 0x0 0x1a20b5 0x51bcba0)
> Backtrace terminate-invalid frame pointer 0
> 	Kernel loadable modules in backtrace (with dependencies):
> 		com.apple.filesystems.zfs(8.0)@0x3701d000->0x370e8fff
>
> BSD process name corresponding to current thread: kernel_task
>
> Mac OS version:
> 9C7010
>
>
> On 15-Apr-08, at 7:35 PM, Thornton Adrian wrote:
>> I'm having a big problem here. I was replacing the drives in my  
>> RAIDZ with bigger ones, and in the midst of a resilver the machine  
>> panicked. I rebooted, and now it panics shortly after the login  
>> screen every time I reboot the machine. "Screw it," I thought, and  
>> moved the disks over to a Solaris 10 box to repair the array.  
>> Unfortunately, no matter what I do the Solaris box won't recognize  
>> my SiI 3114 card so I can't get it to see all the drives. The only  
>> way to get OS X to boot properly is in safe mode or by detaching  
>> some of the drives. In safe mode, zfs doesn't function.
>>
>> Is there any way I can STOP it from doing whatever it's trying to  
>> do (resilver? scrub?) and figure out what the problem is? Or at  
>> least how to I find a log of the kernel panic so I can post it for  
>> your perusal? This is utterly aggravating because 7 of the 8 drives  
>> in the RAIDZ should be perfectly fine, and my data should be  
>> intact. I just can't get the bloody thing to stop panicking if I  
>> have all the drives attached.
>>
>> Thanks,
>> Adrian
>

From franzschmalzl at spamfreemail.de  Wed Apr 16 02:45:12 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Wed Apr 16 02:43:15 2008
Subject: [zfs-discuss] ram
Message-ID: <81234A4F-43CC-4419-84C3-94521481D945@spamfreemail.de>

for odd reasons zfs started to fill up my ram...

the last 10 times i used my array osx claimed to have no free ram  
left...

1.8 GB show up as "reserved"

rysc crashes when ram is full, if coping to my raidz array...


ideas ?

regards

franz
From Jonathan.Edwards at Sun.COM  Wed Apr 16 03:55:41 2008
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Wed Apr 16 03:57:32 2008
Subject: [zfs-discuss] Re: RAIDZ continuous panic after zpool replace
In-Reply-To: <B2B4DD52-E3A7-4C3F-9CC8-0F6CD55E00A9@electricteaparty.net>
References: <4AFB341E-A3A8-4E23-AFBE-9753C01AE4A7@electricteaparty.net>
	<CB635C30-EE86-4CC4-8A47-C31194F696BE@electricteaparty.net>
	<B2B4DD52-E3A7-4C3F-9CC8-0F6CD55E00A9@electricteaparty.net>
Message-ID: <20EED4DC-63EC-475A-B683-CEAE60DC7E9B@sun.com>

in this case what you're seeing is the inability to get the config  
information off one of the disks (it's telling you the device  
identifier it can't find) .. possibly a corrupt uberblock .. if you  
were simply replacing disks i'm thinking you should have seen 9  
devices during the replace - so while i'm not sure what devices you're  
replacing with other devices i'm guessing there's another device  
somewhere that might be missing from the config as you may not have  
enough valid data to reliably bring the pool around .. you're in a  
sticky situation, and i've noticed that zdb isn't distributed with the  
macosforge port (which can be helpful for diagnosing label problems)

on a side note - there's been known problems with the Si3114  
controller and data corruption - you may want to consider a different  
controller and get your data off this hardware (see: http://opensolaris.org/jive/thread.jspa?messageID=196275) 
  if you can at this point ..

on another side note .. the SXDE should get you newer bits than  
whatever solaris build you were trying to import the pool into ..  
belenix i believe is around nv72 (8/2007) .. the last SXDE is going to  
be nv79b (01/2008), and the current nevada build should be nv86 or  
nv87 (04/2008)

On Apr 16, 2008, at 2:42 AM, Thornton Adrian wrote:

> Turns out a Belenix LiveCD recognizes my SATA card and sees my pool.  
> But it's not all good news. For some reason, though Belenix shows  
> seven out of eight devices online and happy, it still claims it  
> can't import the pool. Here's the zpool import:
>
> pool: Archives
> id: 16750878087681595004
> state: UNAVAIL
> status: One or more devices are missing from the system.
> action: The pool cannot be imported. Attach the missing devices and  
> try again.
> see: http://www.sun.com/msg/ZFS-8000-3C
> config:
>
> 	Archives		UNAVAIL	insufficient replicas
> 		raidz1	UNAVAIL	corrupted data
> 		c8d1p0	ONLINE
> 		c2t1d0p0	ONLINE
> 	`	c7d1p0	ONLINE
> 		c2t0d0p0	ONLINE
> 		c8d0s1	ONLINE
> 		c4d0s1	ONLINE
> 		636893339691769212	UNAVAIL	cannot open
> 		c5d0s1	ONLINE
>
> Looks fine, right? (Well, aside from the fact is should say DEGRADED  
> not UNAVAIL.) One missing disk in an 8-disk raidz1 pool. But zpool  
> import -f Archives says:
>
> cannot import 'Archives': invalid vdev configuration
>
> So... what's going on then? Any clues?
>
> Cheers,
> Adrian
>
> On 15-Apr-08, at 8:37 PM, Thornton Adrian wrote:
>> I managed to boot in single-user mode and a zpool import showed the  
>> pool with one faulted device, just as it should. I thought maybe I  
>> could do a zpool import -f poolname followed rapidly by a zpool  
>> destroy poolname to clear any tasks, and then import -f again, but  
>> I got a panic. Luckily it wrote the details to the screen this  
>> time, so it's time for me to type...
>>
>> panic(cpu 1 caller 0x37046B71): "[ZFS]: assertion failed in / 
>> Volumes/pixie_dust/home/ndellofano/zfs-work/wiki/zfs-102A/zfs_kext/ 
>> zfs/dmu.c line 437: 0 == dmu_buf_hold_array(os, object, offset,  
>> size, FALSE, FTAG, &numbufs, &dbp)"@Volumes/pixie_d
>> Debugger called: <panic>
>> Backtrace, Format - Frame : Return Address (4 potential args on  
>> stack)
>> 0x391b7b78 : 0x12b0f7 (0x4581f4 0x391b7bac 0x133230 0x0)
>> 0x391b7bc8 : 0x37046b71 (0x370b2e0c 0x370b29e4 0x1b5 0x370b2dbc)
>> 0x391b7c58 : 0x3702efe9 (0x511fd18 0x1ec 0x0 0xe938)
>> 0x391b7d28 : 0x3702b9c9 (0x515ce20 0x1 0c515cc2c 0c511fd18)
>> 0x391b7db8 : 0x370730b7 (0x515cc00 0x25ca1f 0x0 0x0)
>> 0x391b7e08 : 0x3706508f (0x487f800 0x25ca1f 0x0 0x0)
>> 0x391b7f58 : 0x3704e22a (0x4a3d800 0x25ca1f 0x0 0x1369fb)
>> 0x391b7fc8 : 0x19eadc (0x511bc00 0x0 0x1a20b5 0x51bcba0)
>> Backtrace terminate-invalid frame pointer 0
>> 	Kernel loadable modules in backtrace (with dependencies):
>> 		com.apple.filesystems.zfs(8.0)@0x3701d000->0x370e8fff
>>
>> BSD process name corresponding to current thread: kernel_task
>>
>> Mac OS version:
>> 9C7010
>>
>>
>> On 15-Apr-08, at 7:35 PM, Thornton Adrian wrote:
>>> I'm having a big problem here. I was replacing the drives in my  
>>> RAIDZ with bigger ones, and in the midst of a resilver the machine  
>>> panicked. I rebooted, and now it panics shortly after the login  
>>> screen every time I reboot the machine. "Screw it," I thought, and  
>>> moved the disks over to a Solaris 10 box to repair the array.  
>>> Unfortunately, no matter what I do the Solaris box won't recognize  
>>> my SiI 3114 card so I can't get it to see all the drives. The only  
>>> way to get OS X to boot properly is in safe mode or by detaching  
>>> some of the drives. In safe mode, zfs doesn't function.
>>>
>>> Is there any way I can STOP it from doing whatever it's trying to  
>>> do (resilver? scrub?) and figure out what the problem is? Or at  
>>> least how to I find a log of the kernel panic so I can post it for  
>>> your perusal? This is utterly aggravating because 7 of the 8  
>>> drives in the RAIDZ should be perfectly fine, and my data should  
>>> be intact. I just can't get the bloody thing to stop panicking if  
>>> I have all the drives attached.
>>>
>>> Thanks,
>>> Adrian
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From Jonathan.Edwards at Sun.COM  Wed Apr 16 04:00:53 2008
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Wed Apr 16 04:00:53 2008
Subject: [zfs-discuss] ram
In-Reply-To: <81234A4F-43CC-4419-84C3-94521481D945@spamfreemail.de>
References: <81234A4F-43CC-4419-84C3-94521481D945@spamfreemail.de>
Message-ID: <FFFAC0A5-1D88-42C4-9B01-8303A115E332@sun.com>

the ARC (adaptive cache) will consume as much ram as it can when the  
pool is active - it's supposed to react to memory pressure and resize  
itself, but some apps don't push .. in solaris we can limit this with  
a couple kernel tunables (which can be helpful as you also set a fixed  
limit), but i'm not sure what the equivalent params are on the mac

On Apr 16, 2008, at 5:45 AM, Franz Schmalzl wrote:

> for odd reasons zfs started to fill up my ram...
>
> the last 10 times i used my array osx claimed to have no free ram  
> left...
>
> 1.8 GB show up as "reserved"
>
> rysc crashes when ram is full, if coping to my raidz array...
>
>
> ideas ?
>
> regards
>
> franz
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From i_see at macnews.de  Wed Apr 16 09:07:36 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Wed Apr 16 09:05:32 2008
Subject: [zfs-discuss] Re: RAIDZ continuous panic after zpool replace
In-Reply-To: <20080416105414.C04602482A9@lists.macosforge.org>
References: <20080416105414.C04602482A9@lists.macosforge.org>
Message-ID: <594289BC-F559-4F15-BF9C-6E262AA64969@macnews.de>

Hi you might try booting with whatever drive is missing in the list  
below detached.
Probably that will bring your pool back up in "degraded" state. If you  
can, that would be a good time to evacuate your data to other drives.
Do NOT re-attach the drive with the current version as that will  
probably return you to the point of PANIC.
You might try backing up and then zeroing the bad uberblock data on  
the device to stop the panics, but then probably the drive will not be  
recognized by ZFS.
dd and the archives of this lists may be your friend.
Maybe No?l also has some helpful advice. She will probably also like  
to reproduce your situation the first and last MB of each device and  
the output of diskutil -list might be helpful.
Hope this helps,
	ralf
Am 16.04.2008 um 12:54 schrieb zfs-discuss-request@lists.macosforge.org:
> Re: RAIDZ continuous panic after zpool replace
>
>
> Turns out a Belenix LiveCD recognizes my SATA card and sees my pool.  
> But it's not all good news. For some reason, though Belenix shows  
> seven out of eight devices online and happy, it still claims it  
> can't import the pool. Here's the zpool import:
>
> pool: Archives
> id: 16750878087681595004
> state: UNAVAIL
> status: One or more devices are missing from the system.
> action: The pool cannot be imported. Attach the missing devices and  
> try again.
> see: http://www.sun.com/msg/ZFS-8000-3C
> config:
>
> 	Archives		UNAVAIL	insufficient replicas
> 		raidz1	UNAVAIL	corrupted data
> 		c8d1p0	ONLINE
> 		c2t1d0p0	ONLINE
> 	`	c7d1p0	ONLINE
> 		c2t0d0p0	ONLINE
> 		c8d0s1	ONLINE
> 		c4d0s1	ONLINE
> 		636893339691769212	UNAVAIL	cannot open
> 		c5d0s1	ONLINE
>
> Looks fine, right? (Well, aside from the fact is should say DEGRADED  
> not UNAVAIL.) One missing disk in an 8-disk raidz1 pool. But zpool  
> import -f Archives says:
>
> cannot import 'Archives': invalid vdev configuration
>
> So... what's going on then? Any clues?
>
> Cheers,
> Adrian

From i_see at macnews.de  Wed Apr 16 09:18:07 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Wed Apr 16 09:16:02 2008
Subject: [zfs-discuss] Re: 3 x 750 GB or 4 x 500 GB
In-Reply-To: <20080416105414.C04602482A9@lists.macosforge.org>
References: <20080416105414.C04602482A9@lists.macosforge.org>
Message-ID: <AC781598-E9C7-4F62-BD68-42683B9CC9CC@macnews.de>

Hi Chris,
I don't know about the influence of cache size on ZFS, but in general  
the number of spindels is an important factor to disk array  
performance for sequential reads and writes. Since ZFS will almost  
always write sequentially, this would offer a slightly better  
performance with more smaller disks.
Small random reads might be a bit better on smaller vdevs, but this  
could change in future versions of ZFS IMHO and is not a very typical  
workload.
Reliability is a bit better with less drives in a vdev, but then  
again, if you really care, you use 3-way mirroring or raidz2 with a  
hot spare. the effect of devoting an extra disk to redundancy is MUCH  
bigger then the difference in probablility of two out of three disks  
failing compared to two out of four disks failing.
If you want to be reasonably save, use cron or some alternative to  
issue a zpool scrub on a regular basis, so you do actually detect  
problems while they can still be fixed. If you do that and replace  
disks that show regular problems the odds are good you will not live  
long enough to see data loss.

Hope this helps,
ralf
Am 16.04.2008 um 12:54 schrieb zfs-discuss-request@lists.macosforge.org:
>
>
> I have been testing ZFS on my Mac with some spare disks I had lying  
> around, but I am now ready to make the jump to all home folders on  
> ZFS. My question is whether or not it would make a difference in  
> performance or reliability building a raidz from 4 x 500 GB drives  
> vs. 3 x 750 GB drives?
>
> 750 GB 7200 w/32 MB cache vs 500 GB 7200 w/16MB cache.
>
> and. . .Seagate Barracuda vs. Samsung Spinpoint F1. Should either of  
> these be avoided?
>
> Thanks,
>
> Chris Shuman

From ndellofano at apple.com  Wed Apr 16 11:37:53 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed Apr 16 11:36:18 2008
Subject: [zfs-discuss] ZFS new bits on the wiki
Message-ID: <76AE368E-F506-4FFB-822D-F09856A43E1C@apple.com>

Hey everyone,

Thanks for you patience!  Just a heads up that I posted new bits on  
the wiki today.  The latest drop is zfs-111.  Fixes in this rev are:

<rdar://problem/4873926> make mmap'd pages for zfs play nice
<rdar://problem/5290357> copyfile(3) fails on symlinks on ZFS
<rdar://problem/5687660> zfs send won't die
<rdar://problem/5659137> No negative namecache for ZFS
<rdar://problem/5285605> zfs should tune the number of threads it  
initially generates
<rdar://problem/5653782> zio_checksum_sha256() incorrect if buffer  
size not a multiple of 64
<rdar://problem/5727711> zfs panic when swap file resides on zfs
<rdar://problem/5735151> Fix zfs build targets
<rdar://problem/5756450> need to fix Leopard only build target due to  
symbol change
<rdar://problem/5795082> ZFS should remove its kqueue support
<rdar://problem/5853667> define MAX_UPL_TRANSFER for open source builds


I've also put all this on the website and I've also left zfs-102A up  
at the bottom under 'Past Revisions' incase anyone was dependent on  
that for anything.
Things to note:
- This build *is* mmap coherent, so your mmap programs should be much  
happier and working.
- the build process has been changed. With the fix of 5795082, kqueues  
are no longer needed in ZFS since they've moved up to the VFS layer.   
So all you need to do to build is choose the "Leopard_Release" config  
under the build menu and you're good to go.  No more flags to mess with.

let me know if you have any problems with the website or downloading  
the bits.

thanks!
Noel
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080416/c18b5143/attachment.html
From jbsnyder at gmail.com  Wed Apr 16 12:16:47 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Wed Apr 16 12:14:37 2008
Subject: [zfs-discuss] ZFS new bits on the wiki
In-Reply-To: <76AE368E-F506-4FFB-822D-F09856A43E1C@apple.com>
References: <76AE368E-F506-4FFB-822D-F09856A43E1C@apple.com>
Message-ID: <33644d3c0804161216v672d609ck90167a17b11626ef@mail.gmail.com>

Hi -

Thanks for the fresh bits!

A couple minor notes (haven't really dug into extensive testing):

1) Install instructions reference "Release" directory, should be
adjusted to for the "Leopard_Release" path they reside within in the
tarball.
2) Firefox addon install issue still present (though it sounds like
something in xulrunner needs to be fixed to resolve that issue)

Another few questions:
Anything in particular that should be banged on in this release aside
from the noted fixes?  Is there anything else new we should check out?

Also, is there a workaround for the piping/fifo issue aside from using fifos?

Is using swap on a zfs volume advisable?

Is there a generalized approach for making a minimal HFS partition and
having everything else on ZFS, similar to what one can do on FreeBSD
(http://wiki.freebsd.org/ZFSOnRoot)?

Again, thanks for the new release.  My first-five-minutes impressions
are that it works as well as the previous release.  As before, I'll be
sure to report any experiences :-)
On Wed, Apr 16, 2008 at 1:37 PM, No?l Dellofano <ndellofano@apple.com> wrote:
> Hey everyone,
>
> Thanks for you patience!  Just a heads up that I posted new bits on the wiki
> today.  The latest drop is zfs-111.  Fixes in this rev are:
>
> <rdar://problem/4873926> make mmap'd pages for zfs play nice
> <rdar://problem/5290357> copyfile(3) fails on symlinks on ZFS
> <rdar://problem/5687660> zfs send won't die
> <rdar://problem/5659137> No negative namecache for ZFS
> <rdar://problem/5285605> zfs should tune the number of threads it initially
> generates
> <rdar://problem/5653782> zio_checksum_sha256() incorrect if buffer size not
> a multiple of 64
> <rdar://problem/5727711> zfs panic when swap file resides on zfs
> <rdar://problem/5735151> Fix zfs build targets
> <rdar://problem/5756450> need to fix Leopard only build target due to symbol
> change
> <rdar://problem/5795082> ZFS should remove its kqueue support
> <rdar://problem/5853667> define MAX_UPL_TRANSFER for open source builds
>
>
> I've also put all this on the website and I've also left zfs-102A up at the
> bottom under 'Past Revisions' incase anyone was dependent on that for
> anything.
> Things to note:
> - This build *is* mmap coherent, so your mmap programs should be much
> happier and working.
> - the build process has been changed. With the fix of 5795082, kqueues are
> no longer needed in ZFS since they've moved up to the VFS layer.  So all you
> need to do to build is choose the "Leopard_Release" config under the build
> menu and you're good to go.  No more flags to mess with.
>
> let me know if you have any problems with the website or downloading the
> bits.
>
> thanks!
> Noel
>
> _______________________________________________
>  zfs-discuss mailing list
>  zfs-discuss@lists.macosforge.org
>  http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
From alex.blewitt at gmail.com  Wed Apr 16 12:24:04 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Wed Apr 16 12:24:06 2008
Subject: [zfs-discuss] ZFS new bits on the wiki
In-Reply-To: <76AE368E-F506-4FFB-822D-F09856A43E1C@apple.com>
References: <76AE368E-F506-4FFB-822D-F09856A43E1C@apple.com>
Message-ID: <AD911E8A-EB22-4750-94E9-5EC85B469162@gmail.com>

Great stuff! Will try it out and get back to you with anything else I  
find. Will also put together an installer pig for ease of use by others.

Alex

Sent from my iPhone

On 16 Apr 2008, at 19:37, No?l Dellofano <ndellofano@apple.com> wrote:

> Hey everyone,
>
> Thanks for you patience!  Just a heads up that I posted new bits on  
> the wiki today.  The latest drop is zfs-111.  Fixes in this rev are:
>
> <rdar://problem/4873926> make mmap'd pages for zfs play nice
> <rdar://problem/5290357> copyfile(3) fails on symlinks on ZFS
> <rdar://problem/5687660> zfs send won't die
> <rdar://problem/5659137> No negative namecache for ZFS
> <rdar://problem/5285605> zfs should tune the number of threads it  
> initially generates
> <rdar://problem/5653782> zio_checksum_sha256() incorrect if buffer  
> size not a multiple of 64
> <rdar://problem/5727711> zfs panic when swap file resides on zfs
> <rdar://problem/5735151> Fix zfs build targets
> <rdar://problem/5756450> need to fix Leopard only build target due  
> to symbol change
> <rdar://problem/5795082> ZFS should remove its kqueue support
> <rdar://problem/5853667> define MAX_UPL_TRANSFER for open source  
> builds
>
>
> I've also put all this on the website and I've also left zfs-102A up  
> at the bottom under 'Past Revisions' incase anyone was dependent on  
> that for anything.
> Things to note:
> - This build *is* mmap coherent, so your mmap programs should be  
> much happier and working.
> - the build process has been changed. With the fix of 5795082,  
> kqueues are no longer needed in ZFS since they've moved up to the  
> VFS layer.  So all you need to do to build is choose the  
> "Leopard_Release" config under the build menu and you're good to  
> go.  No more flags to mess with.
>
> let me know if you have any problems with the website or downloading  
> the bits.
>
> thanks!
> Noe
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080416/0dcd0207/attachment-0001.html
From info at martin-hauser.net  Wed Apr 16 12:50:01 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Wed Apr 16 12:48:30 2008
Subject: [zfs-discuss] ZFS new bits on the wiki
In-Reply-To: <33644d3c0804161216v672d609ck90167a17b11626ef@mail.gmail.com>
References: <76AE368E-F506-4FFB-822D-F09856A43E1C@apple.com>
	<33644d3c0804161216v672d609ck90167a17b11626ef@mail.gmail.com>
Message-ID: <D694A2AB-E51E-47FA-8CE5-6F0300CA1AF5@martin-hauser.net>

Hello Everyone,

I'm fresh off and running from this new version of zfs. It seems to  
look quite nice. I would also want to know how to do a minimal HFS+  
and rest zfs or at least home dir on ZFS thing... I've not found any  
doku on that scenarios anywhere yet.

As far as I can tell, the upgrade was nearly seamless. Only problem  
was a kernel panick on kextunload zfs.kext even though all pools were  
exported.

mmap coherent means now that 99.9% of all programs that made problems  
before should work fine now, doesn't it?

Keep up the great work!

kind regards

Martin

-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080416/5cdfda87/PGP.bin
From ndellofano at apple.com  Wed Apr 16 13:27:01 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Wed Apr 16 13:26:31 2008
Subject: [zfs-discuss] ZFS new bits on the wiki
In-Reply-To: <D694A2AB-E51E-47FA-8CE5-6F0300CA1AF5@martin-hauser.net>
References: <76AE368E-F506-4FFB-822D-F09856A43E1C@apple.com>
	<33644d3c0804161216v672d609ck90167a17b11626ef@mail.gmail.com>
	<D694A2AB-E51E-47FA-8CE5-6F0300CA1AF5@martin-hauser.net>
Message-ID: <449BEB67-7283-4509-9309-213EAA734E12@apple.com>

>
> As far as I can tell, the upgrade was nearly seamless. Only problem  
> was a kernel panick on kextunload zfs.kext even though all pools  
> were exported.

This is a known and annoying problem that I have a bug open on but  
haven't been able to get to since it's rather low on the stack.  Hence  
rebooting is your only refuge when you change the kext.

> mmap coherent means now that 99.9% of all programs that made  
> problems before should work fine now, doesn't it?

If it was an application that relies on mmap then yes.  This still  
won't fix programs or apps that have case insensitivity issues or  
Finder issues, but those are still being worked on.

> Keep up the great work!

thanks!!! :)

Noel

On Apr 16, 2008, at 12:50 PM, Martin Hauser wrote:

> Hello Everyone,
>
> I'm fresh off and running from this new version of zfs. It seems to  
> look quite nice. I would also want to know how to do a minimal HFS+  
> and rest zfs or at least home dir on ZFS thing... I've not found any  
> doku on that scenarios anywhere yet.
>
> As far as I can tell, the upgrade was nearly seamless. Only problem  
> was a kernel panick on kextunload zfs.kext even though all pools  
> were exported.
>
> mmap coherent means now that 99.9% of all programs that made  
> problems before should work fine now, doesn't it?
>
> Keep up the great work!
>
> kind regards
>
> Martin
>

From bwaters at nrao.edu  Wed Apr 16 13:38:12 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Wed Apr 16 13:37:18 2008
Subject: [zfs-discuss] ZFS new bits on the wiki
In-Reply-To: <76AE368E-F506-4FFB-822D-F09856A43E1C@apple.com>
References: <76AE368E-F506-4FFB-822D-F09856A43E1C@apple.com>
Message-ID: <AC74F639-061B-426E-A9D1-D9C7024E5A03@nrao.edu>

Yay, new bits!

I installed the binary by using the instructions on the Wiki.. "sudo  
cp zfs /usr/sbin/zfs" etc.

This causes a problem, because the files were owned by me  
(bwaters:staff) and not root:wheel.

MAKE SURE THE NEW BITS ARE OWNED BY root:wheel BEFORE YOU INSTALL THEM!


If it's too late, and you need to recover from this, here's how I did  
it:


On reboot, Leopard marked everything zfs with com.apple.quarantine  
xattr and refused to load it.

Since my home directory is now on the ZFS volume, things got  
interesting! :-)

1) I logged into the console by entering ">console" as the username.

2) Logged in as me; the console login allowed me to log in even though  
it could not find my home directory.

3) sudo -i  to "log in" as root and cd to /var/root

4) find /usr/sbin/z{pool,fs} /System/Library/Extensions/zfs.kext / 
System/Library/Filesystems/zfs.fs | xargs xattr -d com.apple.quarantine

5) find /usr/sbin/z{pool,fs} /System/Library/Extensions/zfs.kext / 
System/Library/Filesystems/zfs.fs | xargs chown root:wheel

6) reboot




On Apr 16, 2008, at 12:37 PM, No?l Dellofano wrote:
> Hey everyone,
>
> Thanks for you patience!  Just a heads up that I posted new bits on  
> the wiki today.  The latest drop is zfs-111.  Fixes in this rev are:
>
> <rdar://problem/4873926> make mmap'd pages for zfs play nice
> <rdar://problem/5290357> copyfile(3) fails on symlinks on ZFS
> <rdar://problem/5687660> zfs send won't die
> <rdar://problem/5659137> No negative namecache for ZFS
> <rdar://problem/5285605> zfs should tune the number of threads it  
> initially generates
> <rdar://problem/5653782> zio_checksum_sha256() incorrect if buffer  
> size not a multiple of 64
> <rdar://problem/5727711> zfs panic when swap file resides on zfs
> <rdar://problem/5735151> Fix zfs build targets
> <rdar://problem/5756450> need to fix Leopard only build target due  
> to symbol change
> <rdar://problem/5795082> ZFS should remove its kqueue support
> <rdar://problem/5853667> define MAX_UPL_TRANSFER for open source  
> builds
>
>
> I've also put all this on the website and I've also left zfs-102A up  
> at the bottom under 'Past Revisions' incase anyone was dependent on  
> that for anything.
> Things to note:
> - This build *is* mmap coherent, so your mmap programs should be  
> much happier and working.
> - the build process has been changed. With the fix of 5795082,  
> kqueues are no longer needed in ZFS since they've moved up to the  
> VFS layer.  So all you need to do to build is choose the  
> "Leopard_Release" config under the build menu and you're good to  
> go.  No more flags to mess with.
>
> let me know if you have any problems with the website or downloading  
> the bits.
>
> thanks!
> Noel
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From q0rban at gmail.com  Wed Apr 16 15:03:40 2008
From: q0rban at gmail.com (james sansbury)
Date: Wed Apr 16 15:01:30 2008
Subject: [zfs-discuss] Re: Timing out
In-Reply-To: <eb3e47ad0801240613i4161ee03x8957a9e3812748b5@mail.gmail.com>
References: <eb3e47ad0801240613i4161ee03x8957a9e3812748b5@mail.gmail.com>
Message-ID: <eb3e47ad0804161503o11b6abd6l46f980df6cb12164@mail.gmail.com>

Still occuring in the latest build (ZFS 103)... :(

On Thu, Jan 24, 2008 at 10:13 AM, james sansbury <q0rban@gmail.com> wrote:

> ***This is a repost of a previous message posted in the wrong thread.
> Please reply to this post.  Apologies.***
>
> First, some details as to what I'm working with.  I have a raid-z pool
> that I created on Solaris 10, then upgraded to OpenSolaris Nevada (upgraded
> the pool as well), and then just today took the plunge and exported it to
> try and import into Leopard (ZFS 102a).
>
> Everything seemed to go very smoothly, until I needed to change ownership
> on some very large folders (300+GB).  A *chown -Rv user folder* seemed to
> work fine for about 2 minutes and then it hung.  I tried to break out of the
> process.  Nothing.  I tried to open another terminal and force unmount the
> pool.  Nothing.  I tried to eject the drives in Disk Utility.  Nothing.  So
> then after waiting another 10 minutes or so I just unplugged the drives.
> Nothing.
>
> I tried to shutdown and it wouldn't shut down so I just powered off,
> booted up, tried everything again with all the same results, except that
> this time I tried plugging back in the drives and letting it sit.  By doing
> this I was at least able to get a kernel panic out of it! ;) [I now realize
> from another post that this kernel panic is most likely due to unplugging
> the drives hot, so I won't be reposting the kernel panic info.]
>
> I am also able to replicate all of this in finder with pretty much any
> process (copy, move, etc.), provided the process takes long enough.
>
> I may try this again today, leave the drives alone, and just let it sit to
> see if I can get some sort of dump/panic; something to work with at least.
>
> -James
>
>


-- 

"A society in which consumption has to be
artificially stimulated in order to keep production
going is a society founded on trash and waste,
and such a society is a house built upon sand."

-- Dorothy Sayers
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080416/785f3c02/attachment.html
From chris.shuman at gmail.com  Wed Apr 16 20:57:35 2008
From: chris.shuman at gmail.com (Chris Shuman)
Date: Wed Apr 16 20:55:39 2008
Subject: [zfs-discuss] Version Verification
Message-ID: <0F13B6D2-5204-4BA5-8DB6-EEE4C4606DEE@gmail.com>

Might be a stupid question, but how do I verify that I have version  
111 installed?

Chris
From lopez.on.the.lists at yellowspace.net  Thu Apr 17 13:05:33 2008
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Thu Apr 17 13:04:19 2008
Subject: [zfs-discuss] ZFS new bits on the wiki
In-Reply-To: <76AE368E-F506-4FFB-822D-F09856A43E1C@apple.com>
References: <76AE368E-F506-4FFB-822D-F09856A43E1C@apple.com>
Message-ID: <5120F6F8-9679-4F31-BC9A-88781BCE3E9E@yellowspace.net>

On 16.04.2008, at 20:37, No?l Dellofano wrote:

> Hey everyone,
>
> Thanks for you patience!  Just a heads up that I posted new bits on  
> the wiki today.  The latest drop is zfs-111.  Fixes in this rev are:

Cool! Just formatted my new MacBook Pro drive and moving everything  
but the usual suspects for now (OS, /Applications, /Users) to my zfs  
pool.
I'm moving to zfs: my home (hoping a symlink to .username will suffice  
for File Vault to believe me), as well as /opt/local, my parallels  
virtual machines, Movies, Downloads and Music. Hope it keeps its  
promises!

BTW, First bugs/annoyances encountered:

I partitioned the drive with diskutil (while booting via firewire from  
another), using

# diskutil partitionDisk disk0 4 GPTFormat  "Journaled HFS+" 70G Boot  
ZFS 200G ZetaByte UFS 13G FreeBSD "MS-DOS FAT32" Windows

As you see I named my Zpool "ZetaByte" in the first run.
Then I decided to rename it as well as change the size of the  
partitions, so I issued:

# zfs umount -f ZetaByte
# zpool destroy ZetaByte
# diskutil umountDisk disk0
# dd if=/dev/zero bs=1m of=/dev/disk0 count=300

Then:

# diskutil partitionDisk disk0 4 GPTFormat  "Journaled HFS+" 80G Boot  
ZFS 190G Zorro UFS 13G FreeBSD "MS-DOS FAT32" Windows

After this the Finder kept on showing the old name, ZetaByte instead  
of Zorro - also zpool list showed the same and zfs list.
Don't know if this is a diskutil, Finder or zfs bug. Workaround was to  
reboot between the wiping and the next partitionDisk.

Another annoying thing is that filesystems created in the pool, when  
double clicked in the Finder, show up as the pool (in the title bar of  
the Finder).

Other than that, copying stuff around is lightning fast right now.  
Might be the new disk, a Hitachi 320GB, 5400RPM 2.5", but hey, Zfs is  
holding up with it with really nice 50MB/s peaks on my MBP! :-)


Keep it up and running! :-)


Lorenzo




From ndellofano at apple.com  Thu Apr 17 13:45:36 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Thu Apr 17 13:45:00 2008
Subject: [zfs-discuss] Version Verification
In-Reply-To: <0F13B6D2-5204-4BA5-8DB6-EEE4C4606DEE@gmail.com>
References: <0F13B6D2-5204-4BA5-8DB6-EEE4C4606DEE@gmail.com>
Message-ID: <B9171DE4-5811-4EA5-900F-A89E8B84B75C@apple.com>

Actually not a stupid question at all.  We currently don't have a  
pretty way to do this, but the quick and dirty way would be to get to  
your nearest terminal window and do:

#strings   /System/Library/Extensions/zfs.kext/Contents/MacOS/zfs

which will spit out a bunch of strings at you from the executable,  
which will say what version you're running.  For example:

/Volumes/pixie_dust/home/ndellofano/Wiki/zfs-111/zfs_kext/zfs/ 
zfs_vfsops.c
"[ZFS]: assertion failed in %s line %d: %s"@/Volumes/pixie_dust/home/ 
ndellofano/Wiki/zfs-111/zfs_kext/zfs/zfs_vfsops.c:195


Noel

On Apr 16, 2008, at 8:57 PM, Chris Shuman wrote:

> Might be a stupid question, but how do I verify that I have version  
> 111 installed?
>
> Chris
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From Jonathan.Edwards at Sun.COM  Thu Apr 17 19:01:25 2008
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Thu Apr 17 18:59:26 2008
Subject: [zfs-discuss] Version Verification
In-Reply-To: <B9171DE4-5811-4EA5-900F-A89E8B84B75C@apple.com>
References: <0F13B6D2-5204-4BA5-8DB6-EEE4C4606DEE@gmail.com>
	<B9171DE4-5811-4EA5-900F-A89E8B84B75C@apple.com>
Message-ID: <BC5E04C7-6812-4BA3-B1FD-C6671BF7B934@sun.com>

should probably update the Info.plist for a new CFBundleVersion to  
have it show up in kextstat .. but for md5 checksums on 111 i show:

$ openssl md5 /System/Library/Extensions/zfs.kext/Contents/MacOS/zfs
MD5(/System/Library/Extensions/zfs.kext/Contents/MacOS/zfs)=  
1eb60fb0207b07480859c77b5c38fe3d
$ openssl md5 /System/Library/Filesystems/zfs.fs/zfs.util
MD5(/System/Library/Filesystems/zfs.fs/zfs.util)=  
28ca89154c61b75f7ea6348a6065e874
$ openssl md5 /usr/sbin/zfs
MD5(/usr/sbin/zfs)= c38cf2f456453aab2d6816dbee5cc43b
$ openssl md5 /usr/sbin/zpool
MD5(/usr/sbin/zpool)= 299926f8822f4c28d761bd5ad0af54c0
$ openssl md5 /usr/lib/libzfs.dylib
MD5(/usr/lib/libzfs.dylib)= c83af97e487feffea96036cb0138eb6a
$ openssl md5 /usr/sbin/zoink
MD5(/usr/sbin/zoink)= 77c835f93942af1e230d88ccd9edd350

instead of:
$ openssl md5 /System/Library/Extensions/zfs.kext.102A/Contents/MacOS/ 
zfs
MD5(/System/Library/Extensions/zfs.kext.102A/Contents/MacOS/zfs)=  
4e7a07febe62f6d608deb5ad24461f0f
$ openssl md5 /System/Library/Filesystems/zfs.fs.102A/zfs.util
MD5(/System/Library/Filesystems/zfs.fs.102A/zfs.util)=  
c46e790ca50b71a4e4bda700ae6737c2
$ openssl md5 /usr/sbin/zfs.102A
MD5(/usr/sbin/zfs.102A)= d068d3d12df63e7c32caf360363aea5e
$ openssl md5 /usr/sbin/zpool.102A
MD5(/usr/sbin/zpool.102A)= 375e2f60f6e9fd43abf6c943f9b3db49
$ openssl md5 /usr/lib/libzfs.dylib.102A
MD5(/usr/lib/libzfs.dylib.102A)= 66e5703fcf58d55fbbb708c42caecc89
$ openssl md5 /usr/sbin/zoink.102A
MD5(/usr/sbin/zoink.102A)= 04c2c5bd92bdc5a26ce00e69435156d8

---
.je

On Apr 17, 2008, at 4:45 PM, No?l Dellofano wrote:

> Actually not a stupid question at all.  We currently don't have a  
> pretty way to do this, but the quick and dirty way would be to get  
> to your nearest terminal window and do:
>
> #strings   /System/Library/Extensions/zfs.kext/Contents/MacOS/zfs
>
> which will spit out a bunch of strings at you from the executable,  
> which will say what version you're running.  For example:
>
> /Volumes/pixie_dust/home/ndellofano/Wiki/zfs-111/zfs_kext/zfs/ 
> zfs_vfsops.c
> "[ZFS]: assertion failed in %s line %d: %s"@/Volumes/pixie_dust/home/ 
> ndellofano/Wiki/zfs-111/zfs_kext/zfs/zfs_vfsops.c:195
>
>
> Noel
>
> On Apr 16, 2008, at 8:57 PM, Chris Shuman wrote:
>
>> Might be a stupid question, but how do I verify that I have version  
>> 111 installed?
>>
>> Chris
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From cdl at asgaard.org  Fri Apr 18 00:15:07 2008
From: cdl at asgaard.org (Christopher LILJENSTOLPE)
Date: Fri Apr 18 00:14:48 2008
Subject: [zfs-discuss] ZFS new bits on the wiki
In-Reply-To: <AC74F639-061B-426E-A9D1-D9C7024E5A03@nrao.edu>
References: <76AE368E-F506-4FFB-822D-F09856A43E1C@apple.com>
	<AC74F639-061B-426E-A9D1-D9C7024E5A03@nrao.edu>
Message-ID: <51231C1E-F7F5-4848-9305-A47AB6180F38@asgaard.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

A good precaution when frobbing a Unix machine.  Have an account that  
relies on NOTHING that is not shipping bog standard stuff, and that  
can get to root status (via sudo or what-have-you).  In the case of  
home directories on "enhanced file systems" I would recommend that  
that bail-out account has a homedir set to /tmp, or some such....

	Chris

On 16 Apr 2008, at 13.38, Boyd Waters wrote:

> Yay, new bits!
>
> I installed the binary by using the instructions on the Wiki.. "sudo  
> cp zfs /usr/sbin/zfs" etc.
>
> This causes a problem, because the files were owned by me  
> (bwaters:staff) and not root:wheel.
>
> MAKE SURE THE NEW BITS ARE OWNED BY root:wheel BEFORE YOU INSTALL  
> THEM!
>
>
> If it's too late, and you need to recover from this, here's how I  
> did it:
>
>
> On reboot, Leopard marked everything zfs with com.apple.quarantine  
> xattr and refused to load it.
>
> Since my home directory is now on the ZFS volume, things got  
> interesting! :-)
>
> 1) I logged into the console by entering ">console" as the username.
>
> 2) Logged in as me; the console login allowed me to log in even  
> though it could not find my home directory.
>
> 3) sudo -i  to "log in" as root and cd to /var/root
>
> 4) find /usr/sbin/z{pool,fs} /System/Library/Extensions/zfs.kext / 
> System/Library/Filesystems/zfs.fs | xargs xattr -d  
> com.apple.quarantine
>
> 5) find /usr/sbin/z{pool,fs} /System/Library/Extensions/zfs.kext / 
> System/Library/Filesystems/zfs.fs | xargs chown root:wheel
>
> 6) reboot
>
>
>
>
> On Apr 16, 2008, at 12:37 PM, No?l Dellofano wrote:
>> Hey everyone,
>>
>> Thanks for you patience!  Just a heads up that I posted new bits on  
>> the wiki today.  The latest drop is zfs-111.  Fixes in this rev are:
>>
>> <rdar://problem/4873926> make mmap'd pages for zfs play nice
>> <rdar://problem/5290357> copyfile(3) fails on symlinks on ZFS
>> <rdar://problem/5687660> zfs send won't die
>> <rdar://problem/5659137> No negative namecache for ZFS
>> <rdar://problem/5285605> zfs should tune the number of threads it  
>> initially generates
>> <rdar://problem/5653782> zio_checksum_sha256() incorrect if buffer  
>> size not a multiple of 64
>> <rdar://problem/5727711> zfs panic when swap file resides on zfs
>> <rdar://problem/5735151> Fix zfs build targets
>> <rdar://problem/5756450> need to fix Leopard only build target due  
>> to symbol change
>> <rdar://problem/5795082> ZFS should remove its kqueue support
>> <rdar://problem/5853667> define MAX_UPL_TRANSFER for open source  
>> builds
>>
>>
>> I've also put all this on the website and I've also left zfs-102A  
>> up at the bottom under 'Past Revisions' incase anyone was dependent  
>> on that for anything.
>> Things to note:
>> - This build *is* mmap coherent, so your mmap programs should be  
>> much happier and working.
>> - the build process has been changed. With the fix of 5795082,  
>> kqueues are no longer needed in ZFS since they've moved up to the  
>> VFS layer.  So all you need to do to build is choose the  
>> "Leopard_Release" config under the build menu and you're good to  
>> go.  No more flags to mess with.
>>
>> let me know if you have any problems with the website or  
>> downloading the bits.
>>
>> thanks!
>> Noel
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

- ---
???
Check my PGP key here:
http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B




-----BEGIN PGP SIGNATURE-----

iQEcBAEBAgAGBQJICEp7AAoJEGmx2Mt/+Iw/ncsH/1zA3xr0nENGi2Ti3Ozs67GO
fkwZjc/DdwL40OrLWv5N6/msJLKTEoBRZesaE/qz7TKhdbeJ63TWW3zOvj9jDdOC
27Ps0j1ht2HxGMGL8PAMNCUar1DzhKmek96wJqJLKMiqmmenp53+ftXq+fNDuMJI
kRN+u8mALOj8azq1wZSGmtGNgyrcvFr+w9AGmcA/wTcMflZgufHTH4zlbAqi/eEc
/vxpRHp5ctRbNpvcPJ845sZc9aNI4PTTvL+knFFEWA/e6GuiPh1U3mBp3AT/n8E6
PRe0a7tFEDhaIxP6fJg2UDBa30Y7HtYpV7ww0lqX3j8d8wA8YtczNdCgOC5728o=
=M3Pr
-----END PGP SIGNATURE-----
From lists at loveturtle.net  Fri Apr 18 06:30:22 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Fri Apr 18 06:28:10 2008
Subject: [zfs-discuss] ZFS new bits on the wiki
In-Reply-To: <51231C1E-F7F5-4848-9305-A47AB6180F38@asgaard.org>
References: <76AE368E-F506-4FFB-822D-F09856A43E1C@apple.com>	<AC74F639-061B-426E-A9D1-D9C7024E5A03@nrao.edu>
	<51231C1E-F7F5-4848-9305-A47AB6180F38@asgaard.org>
Message-ID: <4808A26E.8090606@loveturtle.net>

I think that account already exists, it's called "root" :-P
It has root access and a home dir in /var/root!

Cheers!

Christopher LILJENSTOLPE wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> A good precaution when frobbing a Unix machine.  Have an account that 
> relies on NOTHING that is not shipping bog standard stuff, and that 
> can get to root status (via sudo or what-have-you).  In the case of 
> home directories on "enhanced file systems" I would recommend that 
> that bail-out account has a homedir set to /tmp, or some such....
>
>     Chris
>
> On 16 Apr 2008, at 13.38, Boyd Waters wrote:
>
>> Yay, new bits!
>>
>> I installed the binary by using the instructions on the Wiki.. "sudo 
>> cp zfs /usr/sbin/zfs" etc.
>>
>> This causes a problem, because the files were owned by me 
>> (bwaters:staff) and not root:wheel.
>>
>> MAKE SURE THE NEW BITS ARE OWNED BY root:wheel BEFORE YOU INSTALL THEM!
>>
>>
>> If it's too late, and you need to recover from this, here's how I did 
>> it:
>>
>>
>> On reboot, Leopard marked everything zfs with com.apple.quarantine 
>> xattr and refused to load it.
>>
>> Since my home directory is now on the ZFS volume, things got 
>> interesting! :-)
>>
>> 1) I logged into the console by entering ">console" as the username.
>>
>> 2) Logged in as me; the console login allowed me to log in even 
>> though it could not find my home directory.
>>
>> 3) sudo -i  to "log in" as root and cd to /var/root
>>
>> 4) find /usr/sbin/z{pool,fs} /System/Library/Extensions/zfs.kext 
>> /System/Library/Filesystems/zfs.fs | xargs xattr -d com.apple.quarantine
>>
>> 5) find /usr/sbin/z{pool,fs} /System/Library/Extensions/zfs.kext 
>> /System/Library/Filesystems/zfs.fs | xargs chown root:wheel
>>
>> 6) reboot
>>
>>
>>
>>
>> On Apr 16, 2008, at 12:37 PM, No?l Dellofano wrote:
>>> Hey everyone,
>>>
>>> Thanks for you patience!  Just a heads up that I posted new bits on 
>>> the wiki today.  The latest drop is zfs-111.  Fixes in this rev are:
>>>
>>> <rdar://problem/4873926> make mmap'd pages for zfs play nice
>>> <rdar://problem/5290357> copyfile(3) fails on symlinks on ZFS
>>> <rdar://problem/5687660> zfs send won't die
>>> <rdar://problem/5659137> No negative namecache for ZFS
>>> <rdar://problem/5285605> zfs should tune the number of threads it 
>>> initially generates
>>> <rdar://problem/5653782> zio_checksum_sha256() incorrect if buffer 
>>> size not a multiple of 64
>>> <rdar://problem/5727711> zfs panic when swap file resides on zfs
>>> <rdar://problem/5735151> Fix zfs build targets
>>> <rdar://problem/5756450> need to fix Leopard only build target due 
>>> to symbol change
>>> <rdar://problem/5795082> ZFS should remove its kqueue support
>>> <rdar://problem/5853667> define MAX_UPL_TRANSFER for open source builds
>>>
>>>
>>> I've also put all this on the website and I've also left zfs-102A up 
>>> at the bottom under 'Past Revisions' incase anyone was dependent on 
>>> that for anything.
>>> Things to note:
>>> - This build *is* mmap coherent, so your mmap programs should be 
>>> much happier and working.
>>> - the build process has been changed. With the fix of 5795082, 
>>> kqueues are no longer needed in ZFS since they've moved up to the 
>>> VFS layer.  So all you need to do to build is choose the 
>>> "Leopard_Release" config under the build menu and you're good to 
>>> go.  No more flags to mess with.
>>>
>>> let me know if you have any problems with the website or downloading 
>>> the bits.
>>>
>>> thanks!
>>> Noel
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>
> - ---
> ???
> Check my PGP key here:
> http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B
>
>
>
>
> -----BEGIN PGP SIGNATURE-----
>
> iQEcBAEBAgAGBQJICEp7AAoJEGmx2Mt/+Iw/ncsH/1zA3xr0nENGi2Ti3Ozs67GO
> fkwZjc/DdwL40OrLWv5N6/msJLKTEoBRZesaE/qz7TKhdbeJ63TWW3zOvj9jDdOC
> 27Ps0j1ht2HxGMGL8PAMNCUar1DzhKmek96wJqJLKMiqmmenp53+ftXq+fNDuMJI
> kRN+u8mALOj8azq1wZSGmtGNgyrcvFr+w9AGmcA/wTcMflZgufHTH4zlbAqi/eEc
> /vxpRHp5ctRbNpvcPJ845sZc9aNI4PTTvL+knFFEWA/e6GuiPh1U3mBp3AT/n8E6
> PRe0a7tFEDhaIxP6fJg2UDBa30Y7HtYpV7ww0lqX3j8d8wA8YtczNdCgOC5728o=
> =M3Pr
> -----END PGP SIGNATURE-----
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From jeroenvandeven at gmail.com  Fri Apr 18 06:33:14 2008
From: jeroenvandeven at gmail.com (Jeroen van de Ven)
Date: Fri Apr 18 06:30:57 2008
Subject: [zfs-discuss] Uninstalling ZFS
Message-ID: <b99b378d0804180633m1ea91e6dre7e12297b73a80de@mail.gmail.com>

Hi everyone,

Lately I've started getting very frequent crashes related to zfs, and
at least for the weekend I need my mbp to not crash. I've moved all my
stuff off the zfs pool (which, by the way, fortunately, always
survived the crashes), and I would like to completely remove ZFS or
revert to the way it is by default on Leopard, and I can't seem to
find any information on how to accomplish this. Any help would be very
much appreciated.

Best regards,
Jeroen
From info at martin-hauser.net  Fri Apr 18 06:36:51 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Fri Apr 18 06:34:43 2008
Subject: [zfs-discuss] Uninstalling ZFS
In-Reply-To: <b99b378d0804180633m1ea91e6dre7e12297b73a80de@mail.gmail.com>
References: <b99b378d0804180633m1ea91e6dre7e12297b73a80de@mail.gmail.com>
Message-ID: <DF79307A-E2AF-4AD9-B3F5-8FD198634C66@martin-hauser.net>

Hello,

This are just theoretical thoughts, let someone else confirm before  
executing:

1. The kernel extension can savely be removed from the installation  
directory as the readonly extension is called zfs.readonly.kext
2. The userspace utilities are probably gone (did they even exist ? )
3. Same counts for zfs.fs ...

If you do not need readonly zfs support you are probably fine with  
just getting rid of zfs.kext (the userspace utilities won't get in  
your way).


On Apr 18, 2008, at 15:33 PM, Jeroen van de Ven wrote:

> Hi everyone,
>
> Lately I've started getting very frequent crashes related to zfs, and
> at least for the weekend I need my mbp to not crash. I've moved all my
> stuff off the zfs pool (which, by the way, fortunately, always
> survived the crashes), and I would like to completely remove ZFS or
> revert to the way it is by default on Leopard, and I can't seem to
> find any information on how to accomplish this. Any help would be very
> much appreciated.
>
> Best regards,
> Jeroen
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080418/40338d8a/PGP.bin
From quension at mac.com  Fri Apr 18 07:06:25 2008
From: quension at mac.com (Trevor Talbot)
Date: Fri Apr 18 07:04:13 2008
Subject: [zfs-discuss] Uninstalling ZFS
In-Reply-To: <b99b378d0804180633m1ea91e6dre7e12297b73a80de@mail.gmail.com>
References: <b99b378d0804180633m1ea91e6dre7e12297b73a80de@mail.gmail.com>
Message-ID: <BFC8DA17-9370-46DE-8B92-9C1298C19EE4@mac.com>

On Apr 18, 2008, at 6:33 AM, Jeroen van de Ven wrote:

> Lately I've started getting very frequent crashes related to zfs,  
> and at least for the weekend I need my mbp to not crash. I've moved  
> all my stuff off the zfs pool (which, by the way, fortunately,  
> always survived the crashes), and I would like to completely remove  
> ZFS or revert to the way it is by default on Leopard, and I can't  
> seem to find any information on how to accomplish this. Any help  
> would be very much appreciated.

To remove the kernel bits of ZFS, trash these:

/System/Library/Filesystems/zfs.fs
/System/Library/Extensions/zfs.kext
/System/Library/Extensions/zfs.readonly.kext

and restart.


To revert to default Leopard, trash only this:

/System/Library/Extensions/zfs.kext

and replace these with the backups you made before installing:

/System/Library/Filesystems/zfs.fs
/usr/lib/libzfs.dylib
/usr/sbin/zfs
/usr/sbin/zpool

From zorg at sogeeky.net  Fri Apr 18 08:05:42 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Fri Apr 18 08:03:36 2008
Subject: [zfs-discuss] ZFS new bits on the wiki
In-Reply-To: <4808A26E.8090606@loveturtle.net>
References: <76AE368E-F506-4FFB-822D-F09856A43E1C@apple.com>
	<AC74F639-061B-426E-A9D1-D9C7024E5A03@nrao.edu>
	<51231C1E-F7F5-4848-9305-A47AB6180F38@asgaard.org>
	<4808A26E.8090606@loveturtle.net>
Message-ID: <D757959E-E445-4C88-98DB-DEEEE718F655@sogeeky.net>

Granted, but for security purposes it is often considered bad to have  
a login capable root. OS X doesn't, out of the box.

On Apr 18, 2008, at 6:30 AM, Dillon Kass <lists@loveturtle.net> wrote:

> I think that account already exists, it's called "root" :-P
> It has root access and a home dir in /var/root!
>
> Cheers!
>
> Christopher LILJENSTOLPE wrote:
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> A good precaution when frobbing a Unix machine.  Have an account  
>> that relies on NOTHING that is not shipping bog standard stuff, and  
>> that can get to root status (via sudo or what-have-you).  In the  
>> case of home directories on "enhanced file systems" I would  
>> recommend that that bail-out account has a homedir set to /tmp, or  
>> some such....
>>
>>    Chris
>>
>> On 16 Apr 2008, at 13.38, Boyd Waters wrote:
>>
>>> Yay, new bits!
>>>
>>> I installed the binary by using the instructions on the Wiki..  
>>> "sudo cp zfs /usr/sbin/zfs" etc.
>>>
>>> This causes a problem, because the files were owned by me  
>>> (bwaters:staff) and not root:wheel.
>>>
>>> MAKE SURE THE NEW BITS ARE OWNED BY root:wheel BEFORE YOU INSTALL  
>>> THEM!
>>>
>>>
>>> If it's too late, and you need to recover from this, here's how I  
>>> did it:
>>>
>>>
>>> On reboot, Leopard marked everything zfs with com.apple.quarantine  
>>> xattr and refused to load it.
>>>
>>> Since my home directory is now on the ZFS volume, things got  
>>> interesting! :-)
>>>
>>> 1) I logged into the console by entering ">console" as the username.
>>>
>>> 2) Logged in as me; the console login allowed me to log in even  
>>> though it could not find my home directory.
>>>
>>> 3) sudo -i  to "log in" as root and cd to /var/root
>>>
>>> 4) find /usr/sbin/z{pool,fs} /System/Library/Extensions/zfs.kext / 
>>> System/Library/Filesystems/zfs.fs | xargs xattr -d  
>>> com.apple.quarantine
>>>
>>> 5) find /usr/sbin/z{pool,fs} /System/Library/Extensions/zfs.kext / 
>>> System/Library/Filesystems/zfs.fs | xargs chown root:wheel
>>>
>>> 6) reboot
>>>
>>>
>>>
>>>
>>> On Apr 16, 2008, at 12:37 PM, No?l Dellofano wrote:
>>>> Hey everyone,
>>>>
>>>> Thanks for you patience!  Just a heads up that I posted new bits  
>>>> on the wiki today.  The latest drop is zfs-111.  Fixes in this  
>>>> rev are:
>>>>
>>>> <rdar://problem/4873926> make mmap'd pages for zfs play nice
>>>> <rdar://problem/5290357> copyfile(3) fails on symlinks on ZFS
>>>> <rdar://problem/5687660> zfs send won't die
>>>> <rdar://problem/5659137> No negative namecache for ZFS
>>>> <rdar://problem/5285605> zfs should tune the number of threads it  
>>>> initially generates
>>>> <rdar://problem/5653782> zio_checksum_sha256() incorrect if  
>>>> buffer size not a multiple of 64
>>>> <rdar://problem/5727711> zfs panic when swap file resides on zfs
>>>> <rdar://problem/5735151> Fix zfs build targets
>>>> <rdar://problem/5756450> need to fix Leopard only build target  
>>>> due to symbol change
>>>> <rdar://problem/5795082> ZFS should remove its kqueue support
>>>> <rdar://problem/5853667> define MAX_UPL_TRANSFER for open source  
>>>> builds
>>>>
>>>>
>>>> I've also put all this on the website and I've also left zfs-102A  
>>>> up at the bottom under 'Past Revisions' incase anyone was  
>>>> dependent on that for anything.
>>>> Things to note:
>>>> - This build *is* mmap coherent, so your mmap programs should be  
>>>> much happier and working.
>>>> - the build process has been changed. With the fix of 5795082,  
>>>> kqueues are no longer needed in ZFS since they've moved up to the  
>>>> VFS layer.  So all you need to do to build is choose the  
>>>> "Leopard_Release" config under the build menu and you're good to  
>>>> go.  No more flags to mess with.
>>>>
>>>> let me know if you have any problems with the website or  
>>>> downloading the bits.
>>>>
>>>> thanks!
>>>> Noel
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>>
>> - ---
>> ???
>> Check my PGP key here:
>> http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B
>>
>>
>>
>>
>> -----BEGIN PGP SIGNATURE-----
>>
>> iQEcBAEBAgAGBQJICEp7AAoJEGmx2Mt/+Iw/ncsH/1zA3xr0nENGi2Ti3Ozs67GO
>> fkwZjc/DdwL40OrLWv5N6/msJLKTEoBRZesaE/qz7TKhdbeJ63TWW3zOvj9jDdOC
>> 27Ps0j1ht2HxGMGL8PAMNCUar1DzhKmek96wJqJLKMiqmmenp53+ftXq+fNDuMJI
>> kRN+u8mALOj8azq1wZSGmtGNgyrcvFr+w9AGmcA/wTcMflZgufHTH4zlbAqi/eEc
>> /vxpRHp5ctRbNpvcPJ845sZc9aNI4PTTvL+knFFEWA/e6GuiPh1U3mBp3AT/n8E6
>> PRe0a7tFEDhaIxP6fJg2UDBa30Y7HtYpV7ww0lqX3j8d8wA8YtczNdCgOC5728o=
>> =M3Pr
>> -----END PGP SIGNATURE-----
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
From cdl at asgaard.org  Fri Apr 18 11:18:30 2008
From: cdl at asgaard.org (Christopher LILJENSTOLPE)
Date: Fri Apr 18 11:16:55 2008
Subject: [zfs-discuss] ZFS new bits on the wiki
In-Reply-To: <4808A26E.8090606@loveturtle.net>
References: <76AE368E-F506-4FFB-822D-F09856A43E1C@apple.com>	<AC74F639-061B-426E-A9D1-D9C7024E5A03@nrao.edu>
	<51231C1E-F7F5-4848-9305-A47AB6180F38@asgaard.org>
	<4808A26E.8090606@loveturtle.net>
Message-ID: <2FA9281C-ACD3-4091-A694-951BA63A82F5@asgaard.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

if you've enabled login for it..  By default it's not enabled for  
login (it has no password).  I usually have another "test" account  
that has sudo priv, but isn't root.



	Chris

On 18 Apr 2008, at 06.30, Dillon Kass wrote:

> I think that account already exists, it's called "root" :-P
> It has root access and a home dir in /var/root!
>
> Cheers!
>
> Christopher LILJENSTOLPE wrote:
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> A good precaution when frobbing a Unix machine.  Have an account  
>> that relies on NOTHING that is not shipping bog standard stuff, and  
>> that can get to root status (via sudo or what-have-you).  In the  
>> case of home directories on "enhanced file systems" I would  
>> recommend that that bail-out account has a homedir set to /tmp, or  
>> some such....
>>
>>    Chris
>>
>> On 16 Apr 2008, at 13.38, Boyd Waters wrote:
>>
>>> Yay, new bits!
>>>
>>> I installed the binary by using the instructions on the Wiki..  
>>> "sudo cp zfs /usr/sbin/zfs" etc.
>>>
>>> This causes a problem, because the files were owned by me  
>>> (bwaters:staff) and not root:wheel.
>>>
>>> MAKE SURE THE NEW BITS ARE OWNED BY root:wheel BEFORE YOU INSTALL  
>>> THEM!
>>>
>>>
>>> If it's too late, and you need to recover from this, here's how I  
>>> did it:
>>>
>>>
>>> On reboot, Leopard marked everything zfs with com.apple.quarantine  
>>> xattr and refused to load it.
>>>
>>> Since my home directory is now on the ZFS volume, things got  
>>> interesting! :-)
>>>
>>> 1) I logged into the console by entering ">console" as the username.
>>>
>>> 2) Logged in as me; the console login allowed me to log in even  
>>> though it could not find my home directory.
>>>
>>> 3) sudo -i  to "log in" as root and cd to /var/root
>>>
>>> 4) find /usr/sbin/z{pool,fs} /System/Library/Extensions/zfs.kext / 
>>> System/Library/Filesystems/zfs.fs | xargs xattr -d  
>>> com.apple.quarantine
>>>
>>> 5) find /usr/sbin/z{pool,fs} /System/Library/Extensions/zfs.kext / 
>>> System/Library/Filesystems/zfs.fs | xargs chown root:wheel
>>>
>>> 6) reboot
>>>
>>>
>>>
>>>
>>> On Apr 16, 2008, at 12:37 PM, No?l Dellofano wrote:
>>>> Hey everyone,
>>>>
>>>> Thanks for you patience!  Just a heads up that I posted new bits  
>>>> on the wiki today.  The latest drop is zfs-111.  Fixes in this  
>>>> rev are:
>>>>
>>>> <rdar://problem/4873926> make mmap'd pages for zfs play nice
>>>> <rdar://problem/5290357> copyfile(3) fails on symlinks on ZFS
>>>> <rdar://problem/5687660> zfs send won't die
>>>> <rdar://problem/5659137> No negative namecache for ZFS
>>>> <rdar://problem/5285605> zfs should tune the number of threads it  
>>>> initially generates
>>>> <rdar://problem/5653782> zio_checksum_sha256() incorrect if  
>>>> buffer size not a multiple of 64
>>>> <rdar://problem/5727711> zfs panic when swap file resides on zfs
>>>> <rdar://problem/5735151> Fix zfs build targets
>>>> <rdar://problem/5756450> need to fix Leopard only build target  
>>>> due to symbol change
>>>> <rdar://problem/5795082> ZFS should remove its kqueue support
>>>> <rdar://problem/5853667> define MAX_UPL_TRANSFER for open source  
>>>> builds
>>>>
>>>>
>>>> I've also put all this on the website and I've also left zfs-102A  
>>>> up at the bottom under 'Past Revisions' incase anyone was  
>>>> dependent on that for anything.
>>>> Things to note:
>>>> - This build *is* mmap coherent, so your mmap programs should be  
>>>> much happier and working.
>>>> - the build process has been changed. With the fix of 5795082,  
>>>> kqueues are no longer needed in ZFS since they've moved up to the  
>>>> VFS layer.  So all you need to do to build is choose the  
>>>> "Leopard_Release" config under the build menu and you're good to  
>>>> go.  No more flags to mess with.
>>>>
>>>> let me know if you have any problems with the website or  
>>>> downloading the bits.
>>>>
>>>> thanks!
>>>> Noel
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>>
>> - ---
>> ???
>> Check my PGP key here:
>> http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B
>>
>>
>>
>>
>> -----BEGIN PGP SIGNATURE-----
>>
>> iQEcBAEBAgAGBQJICEp7AAoJEGmx2Mt/+Iw/ncsH/1zA3xr0nENGi2Ti3Ozs67GO
>> fkwZjc/DdwL40OrLWv5N6/msJLKTEoBRZesaE/qz7TKhdbeJ63TWW3zOvj9jDdOC
>> 27Ps0j1ht2HxGMGL8PAMNCUar1DzhKmek96wJqJLKMiqmmenp53+ftXq+fNDuMJI
>> kRN+u8mALOj8azq1wZSGmtGNgyrcvFr+w9AGmcA/wTcMflZgufHTH4zlbAqi/eEc
>> /vxpRHp5ctRbNpvcPJ845sZc9aNI4PTTvL+knFFEWA/e6GuiPh1U3mBp3AT/n8E6
>> PRe0a7tFEDhaIxP6fJg2UDBa30Y7HtYpV7ww0lqX3j8d8wA8YtczNdCgOC5728o=
>> =M3Pr
>> -----END PGP SIGNATURE-----
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

- ---
???
Check my PGP key here:
http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B




-----BEGIN PGP SIGNATURE-----

iQEcBAEBAgAGBQJICOX2AAoJEGmx2Mt/+Iw/UX4H/jx9GeoD6PlNjZd/P1kUI+Ap
nJJ9DFJoZZDieFMWkaZ+37YHWyi3CF0/gMVZMzf64nXgkRdmnz6x3cDQKqt3mDnk
oFxYGyjgzeVuPGgGvNhx3Usvypl6a08f9umkWTXiMg1alduq23tQJOFbBxcviyxN
xDDgzXrANB3ibwVoMCmon0nJR4ptxhzrRgiO21hWOSt5MNue5KktdzbRdqOqrC71
4IX00jjDMN0f7IE7F6ICS3LYZ7YclrPZiiJ98eXC1J6s0BXzmkJkXgPQyFvgS0Hx
WfSVshokH+ES/pglT8x5FLyo7w3LgXP3/h72PUjLK5K0UP0gJY8A+0AKtG534mU=
=GHfh
-----END PGP SIGNATURE-----
From ndellofano at apple.com  Fri Apr 18 11:48:57 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri Apr 18 11:48:07 2008
Subject: [zfs-discuss] ZFS new bits on the wiki
In-Reply-To: <33644d3c0804161216v672d609ck90167a17b11626ef@mail.gmail.com>
References: <76AE368E-F506-4FFB-822D-F09856A43E1C@apple.com>
	<33644d3c0804161216v672d609ck90167a17b11626ef@mail.gmail.com>
Message-ID: <A325BEA1-7C0C-4B15-BD60-1D97FC1E0566@apple.com>

> 1) Install instructions reference "Release" directory, should be
> adjusted to for the "Leopard_Release" path they reside within in the
> tarball.

Fixed.  Thanks for finding this :)


> 2) Firefox addon install issue still present (though it sounds like
> something in xulrunner needs to be fixed to resolve that issue)

So the Firefox addon issue is due to the fact that we (ZFS) don't  
support vop_preallocate, hence why in your debugging you were seeing  
45, ENOTSUP.  vop_preallocate doesn't even make sense for ZFS  
really.   Ideally, it would be cool if I could contact the Firefox  
people and ask them not to rely on vop_preallocate returning success,  
just make the write call outright.  I can try and fake something up  
but I"m not sure if that will make Firefox really happy...


> Anything in particular that should be banged on in this release aside
> from the noted fixes?  Is there anything else new we should check out?

Not really, the new mmap coherency is really the most interesting  
thing in this drop that you could bang on.  Assuming you have programs  
that rely on mmap coherency.


> Is using swap on a zfs volume advisable?

Sun discourages it in their systems because /swap is a separate  
partition. However, in Mac OS X swap is just a file that resides on  
the root filesystem. Since swap is just a file, having it on zfs (by  
mounting a zfs file system over "/var/vm") should work for the vm swap  
file.  AFAIK, swap is only accessed by page in/out not read/write.


>
>
> Is there a generalized approach for making a minimal HFS partition and
> having everything else on ZFS, similar to what one can do on FreeBSD

There's no generalized approach yet really.  ZFS boot is still being  
worked on.  If you want to go "do it yourself" style, then for  
testing, we are able to mount zfs file systems over most of the  
directories in "/" ( /Applications, /System, /Library, /Users, etc).

So you could strip some of these down on HFS+ (especially / 
Applications, /Users to the minimum), perhaps looking at the Mac OSX  
install DVD as an example for the minimums.


Noel


On Apr 16, 2008, at 12:16 PM, James Snyder wrote:

> Hi -
>
> Thanks for the fresh bits!
>
> A couple minor notes (haven't really dug into extensive testing):
>
> 1) Install instructions reference "Release" directory, should be
> adjusted to for the "Leopard_Release" path they reside within in the
> tarball.
> 2) Firefox addon install issue still present (though it sounds like
> something in xulrunner needs to be fixed to resolve that issue)
>
> Another few questions:
> Anything in particular that should be banged on in this release aside
> from the noted fixes?  Is there anything else new we should check out?
>
> Also, is there a workaround for the piping/fifo issue aside from  
> using fifos?
>
> Is using swap on a zfs volume advisable?
>
> Is there a generalized approach for making a minimal HFS partition and
> having everything else on ZFS, similar to what one can do on FreeBSD
> (http://wiki.freebsd.org/ZFSOnRoot)?
>
> Again, thanks for the new release.  My first-five-minutes impressions
> are that it works as well as the previous release.  As before, I'll be
> sure to report any experiences :-)
> On Wed, Apr 16, 2008 at 1:37 PM, No?l Dellofano  
> <ndellofano@apple.com> wrote:
>> Hey everyone,
>>
>> Thanks for you patience!  Just a heads up that I posted new bits on  
>> the wiki
>> today.  The latest drop is zfs-111.  Fixes in this rev are:
>>
>> <rdar://problem/4873926> make mmap'd pages for zfs play nice
>> <rdar://problem/5290357> copyfile(3) fails on symlinks on ZFS
>> <rdar://problem/5687660> zfs send won't die
>> <rdar://problem/5659137> No negative namecache for ZFS
>> <rdar://problem/5285605> zfs should tune the number of threads it  
>> initially
>> generates
>> <rdar://problem/5653782> zio_checksum_sha256() incorrect if buffer  
>> size not
>> a multiple of 64
>> <rdar://problem/5727711> zfs panic when swap file resides on zfs
>> <rdar://problem/5735151> Fix zfs build targets
>> <rdar://problem/5756450> need to fix Leopard only build target due  
>> to symbol
>> change
>> <rdar://problem/5795082> ZFS should remove its kqueue support
>> <rdar://problem/5853667> define MAX_UPL_TRANSFER for open source  
>> builds
>>
>>
>> I've also put all this on the website and I've also left zfs-102A  
>> up at the
>> bottom under 'Past Revisions' incase anyone was dependent on that for
>> anything.
>> Things to note:
>> - This build *is* mmap coherent, so your mmap programs should be much
>> happier and working.
>> - the build process has been changed. With the fix of 5795082,  
>> kqueues are
>> no longer needed in ZFS since they've moved up to the VFS layer.   
>> So all you
>> need to do to build is choose the "Leopard_Release" config under  
>> the build
>> menu and you're good to go.  No more flags to mess with.
>>
>> let me know if you have any problems with the website or  
>> downloading the
>> bits.
>>
>> thanks!
>> Noel
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>>
>
>
>
> -- 
> James Snyder
> Biomedical Engineering
> Northwestern University
> jbsnyder@gmail.com

From ndellofano at apple.com  Fri Apr 18 15:48:42 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri Apr 18 15:47:38 2008
Subject: [zfs-discuss] Re: Timing out
In-Reply-To: <eb3e47ad0804161503o11b6abd6l46f980df6cb12164@mail.gmail.com>
References: <eb3e47ad0801240613i4161ee03x8957a9e3812748b5@mail.gmail.com>
	<eb3e47ad0804161503o11b6abd6l46f980df6cb12164@mail.gmail.com>
Message-ID: <CFD48AF6-8CD4-423B-8A91-495F68635032@apple.com>

Yeah I think we still have some weirdness happening on large  
directories it seems :(  I've filed

<rdar://problem/5875044> chown -R on very large folders(300+GB) hangs

  to track this guy.  I'll try and see if I can repro...

Thanks!!
Noel

On Apr 16, 2008, at 3:03 PM, james sansbury wrote:

> Still occuring in the latest build (ZFS 103)... :(
>
> On Thu, Jan 24, 2008 at 10:13 AM, james sansbury <q0rban@gmail.com>  
> wrote:
> ***This is a repost of a previous message posted in the wrong  
> thread.  Please reply to this post.  Apologies.***
>
> First, some details as to what I'm working with.  I have a raid-z  
> pool that I created on Solaris 10, then upgraded to OpenSolaris  
> Nevada (upgraded the pool as well), and then just today took the  
> plunge and exported it to try and import into Leopard (ZFS 102a).
>
> Everything seemed to go very smoothly, until I needed to change  
> ownership on some very large folders (300+GB).  A chown -Rv user  
> folder seemed to work fine for about 2 minutes and then it hung.  I  
> tried to break out of the process.  Nothing.  I tried to open  
> another terminal and force unmount the pool.  Nothing.  I tried to  
> eject the drives in Disk Utility.  Nothing.  So then after waiting  
> another 10 minutes or so I just unplugged the drives.  Nothing.
>
> I tried to shutdown and it wouldn't shut down so I just powered off,  
> booted up, tried everything again with all the same results, except  
> that this time I tried plugging back in the drives and letting it  
> sit.  By doing this I was at least able to get a kernel panic out of  
> it! ;) [I now realize from another post that this kernel panic is  
> most likely due to unplugging the drives hot, so I won't be  
> reposting the kernel panic info.]
>
> I am also able to replicate all of this in finder with pretty much  
> any process (copy, move, etc.), provided the process takes long  
> enough.
>
> I may try this again today, leave the drives alone, and just let it  
> sit to see if I can get some sort of dump/panic; something to work  
> with at least.
>
> -James
>
>
>
>
> -- 
>
> "A society in which consumption has to be
> artificially stimulated in order to keep production
> going is a society founded on trash and waste,
> and such a society is a house built upon sand."
>
> -- Dorothy Sayers _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080418/a10e52f1/attachment-0001.html
From i_see at macnews.de  Fri Apr 18 22:39:33 2008
From: i_see at macnews.de (Ralf Bertling)
Date: Fri Apr 18 22:37:13 2008
Subject: [zfs-discuss] ZFS on Darwin
Message-ID: <CCD56436-909B-4CF2-898B-731335C64203@macnews.de>

Hi list,
I am going to build a (file-)server from used hardware (and probably  
some new disks) and wondered if the Leopard-ZFS implementation will  
also work with Darwin, as I will not be able to buy a shiny Mac Pro or  
XServe for the NPO that will utilize the file-server and doubt Apple  
will fund us ;-/

Alternatives are obviously ZFS/FUSE/Linux, Solaris or ZFS/FreeBSD (if  
the hardware is supported) or using another filesystem (but I don't  
want to do that).

Has anyone tried ZFS on Darwin yet?

Cheers,
	ralf
From mattsnow at gmail.com  Fri Apr 18 23:10:23 2008
From: mattsnow at gmail.com (Matt Snow)
Date: Fri Apr 18 23:08:02 2008
Subject: [zfs-discuss] ZFS on Darwin
In-Reply-To: <CCD56436-909B-4CF2-898B-731335C64203@macnews.de>
References: <CCD56436-909B-4CF2-898B-731335C64203@macnews.de>
Message-ID: <6879ebc80804182310o5935cda1o6bd5f76c46159008@mail.gmail.com>

In theory it should work just fine as long as you have decently supported
disk controllers. Noel, or some one who is an actual developer(not me) could
give you a definitive answer though on this.
I actually played around with OS X on a PC before getting a mac pro and the
ZFS extentions/binaries worked fine with an Intel Intel ICH7 based south
bridge and a Rosewill RC-211(si3132 PCI-E). i'm still using the Rosewill
card with out any problems in a raidz config.

I would stay clear of Fuse:ZFS, it is slow, unstable, and locks up very
easily. If you are not comfortable with Solaris then give FreeBSD a whirl
otherwise you really can not go wrong with Solaris. Just stay away from the
Si3114 based SATA cards as they've been known to cause data corruption. And
for CPU try to make sure you get a 64-bit cpu for more optimal ZFS
performance. it works on 32-bit but not as well.

..Matt

On Fri, Apr 18, 2008 at 10:39 PM, Ralf Bertling <i_see@macnews.de> wrote:

> Hi list,
> I am going to build a (file-)server from used hardware (and probably some
> new disks) and wondered if the Leopard-ZFS implementation will also work
> with Darwin, as I will not be able to buy a shiny Mac Pro or XServe for the
> NPO that will utilize the file-server and doubt Apple will fund us ;-/
>
> Alternatives are obviously ZFS/FUSE/Linux, Solaris or ZFS/FreeBSD (if the
> hardware is supported) or using another filesystem (but I don't want to do
> that).
>
> Has anyone tried ZFS on Darwin yet?
>
> Cheers,
>        ralf
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080418/b6053e1b/attachment.html
From cdl at asgaard.org  Fri Apr 18 18:32:30 2008
From: cdl at asgaard.org (Christopher LILJENSTOLPE)
Date: Sat Apr 19 00:09:01 2008
Subject: [zfs-discuss] Zpools went away - completely
In-Reply-To: <b99b378d0804180633m1ea91e6dre7e12297b73a80de@mail.gmail.com>
References: <b99b378d0804180633m1ea91e6dre7e12297b73a80de@mail.gmail.com>
Message-ID: <18D5A528-EFDE-4D0D-B87C-541AD2BD8978@asgaard.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Gretings,

	I just upgraded to the new release from 102A and had one 3x1TB raidz  
pool on my 10.5 PPC server.  After I removed the files/dirs that 110  
would over-write, I dropped the new 110 build files in, and rebooted.   
When the system came back, no pools or filesystems built on them  
mounted, and a zpool list or zfs list showed no pools found.

	Fighting back some panic, I ripped out the 110 load, and pulled the  
102A files down and re-installed them.  Rebooted, and still, no pools.

	I have things backed up, but I'm "concerned" about pools that just  
vanish in the night....

	Chris

-----BEGIN PGP SIGNATURE-----

iQEcBAEBAgAGBQJICUuuAAoJEGmx2Mt/+Iw/fKQH/if+dB97wiDMS/mxtU2nc6vy
lMFoH7r5Wc8zHf6cX8aEWebxSDSnloA9K1V1nZBuXc7Vi2OJRxiyiI0/uLF+MPqO
APh5evK/23iVfN/BUXY5elrPrkBnK53SV8Mf9QOi1kjM1WwH3YLy2mwjbl57f1zA
Hm+ea5NW7hl69CXrwHQmJK/qkPYyQnOU3Tf4Fns79m+dzyeHnd9gowd6NY0mduOH
jX0w76mrFR8vGSslk1aVrHtUVCgyOsgcmZzBk/HfdnF0caNGdvN1POL3LRexWCRq
3EpeRlDQbxNfEmPMzhiVstGxsnlzRlYp8Bz+Pc/T2Z8WCFu8+dY7CrbdIBH36rw=
=6qFq
-----END PGP SIGNATURE-----
From zorg at sogeeky.net  Sat Apr 19 00:37:03 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Sat Apr 19 00:34:51 2008
Subject: [zfs-discuss] Zpools went away - completely
In-Reply-To: <18D5A528-EFDE-4D0D-B87C-541AD2BD8978@asgaard.org>
References: <b99b378d0804180633m1ea91e6dre7e12297b73a80de@mail.gmail.com>
	<18D5A528-EFDE-4D0D-B87C-541AD2BD8978@asgaard.org>
Message-ID: <FBDF4E38-8F13-4E64-BED2-EBDDBA109322@sogeeky.net>

Did you chown the files after dropping them in?  I could see where  
that may cause what you're seeing. I had no problems.

On Apr 18, 2008, at 6:32 PM, Christopher LILJENSTOLPE  
<cdl@asgaard.org> wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Gretings,
>
>    I just upgraded to the new release from 102A and had one 3x1TB  
> raidz pool on my 10.5 PPC server.  After I removed the files/dirs  
> that 110 would over-write, I dropped the new 110 build files in, and  
> rebooted.  When the system came back, no pools or filesystems built  
> on them mounted, and a zpool list or zfs list showed no pools found.
>
>    Fighting back some panic, I ripped out the 110 load, and pulled  
> the 102A files down and re-installed them.  Rebooted, and still, no  
> pools.
>
>    I have things backed up, but I'm "concerned" about pools that  
> just vanish in the night....
>
>    Chris
>
> -----BEGIN PGP SIGNATURE-----
>
> iQEcBAEBAgAGBQJICUuuAAoJEGmx2Mt/+Iw/fKQH/if+dB97wiDMS/mxtU2nc6vy
> lMFoH7r5Wc8zHf6cX8aEWebxSDSnloA9K1V1nZBuXc7Vi2OJRxiyiI0/uLF+MPqO
> APh5evK/23iVfN/BUXY5elrPrkBnK53SV8Mf9QOi1kjM1WwH3YLy2mwjbl57f1zA
> Hm+ea5NW7hl69CXrwHQmJK/qkPYyQnOU3Tf4Fns79m+dzyeHnd9gowd6NY0mduOH
> jX0w76mrFR8vGSslk1aVrHtUVCgyOsgcmZzBk/HfdnF0caNGdvN1POL3LRexWCRq
> 3EpeRlDQbxNfEmPMzhiVstGxsnlzRlYp8Bz+Pc/T2Z8WCFu8+dY7CrbdIBH36rw=
> =6qFq
> -----END PGP SIGNATURE-----
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
From zfs at hessmann.de  Sat Apr 19 01:40:42 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Sat Apr 19 01:38:29 2008
Subject: [zfs-discuss] scrub on RAID.Z starts over again after ~30%
Message-ID: <629B0DD0-AF26-4E08-9B85-9C357DA961F1@hessmann.de>

Good morning,


just out of curiosity I started a zpool scrub on my RAID.Z (3x 750GB).

After around 8 hours, this was the actual status:

====
bash-3.2$ zpool status
   pool: tank
  state: ONLINE
  scrub: scrub in progress, 28.28% done, 9h43m to go
config:

         NAME         STATE     READ WRITE CKSUM
         tank         ONLINE       0     0     0
           raidz1     ONLINE       0     0     0
             disk1s2  ONLINE       0     0     0
             disk2s2  ONLINE       0     0     0
             disk3s2  ONLINE       0     0     0

errors: No known data errors
====

About 20 minutes later, without any user interaction:

====
bash-3.2$ zpool status
   pool: tank
  state: ONLINE
  scrub: scrub in progress, 0.17% done, 38h19m to go
config:

         NAME         STATE     READ WRITE CKSUM
         tank         ONLINE       0     0     0
           raidz1     ONLINE       0     0     0
             disk1s2  ONLINE       0     0     0
             disk2s2  ONLINE       0     0     0
             disk3s2  ONLINE       0     0     0

errors: No known data errors
====

Huh?

I stopped the scrub and started it again, with the same results.

I searched in system.log, but couldn't find any error message that  
could be linked to ZFS.

Any other hints?


Best regards,

Christian

From phil_w at uk-businessdirectory.co.uk  Sat Apr 19 02:26:54 2008
From: phil_w at uk-businessdirectory.co.uk (Philip Worrall)
Date: Sat Apr 19 02:24:59 2008
Subject: [zfs-discuss] zfs 2xUSB2 disk mirrored hang
Message-ID: <45C1732A-5884-469B-847D-F83AAFAF3C30@uk-businessdirectory.co.uk>

Hi

Ive been playing with ZFS and bought to 250GB USB2 disks to use with it.

zpool create tank mirror /dev/disk3 /dev/disk4

zfs set compression=on tank

cp -R archievedFiles /Volumes/tank

then after a few GB of copying the hard drives stop and in dmesg I find.


USBF:	26455.368	[0x2b4b300] The IOUSBFamily is having trouble  
enumerating a USB device that has been plugged in.  It will keep  
retrying.  (Port 3 of hub @ location: 0xfd000000)
USBF:	26459.  3	[0x2b4b300] The IOUSBFamily was not able to enumerate  
a device.
zfs_context_init: footprint.maximum=357913941,  
footprint.target=102711296
kobj_open_file: "/etc/zfs/zpool.cache", err 0 from vnode_open
zfs_module_start: memory footprint 1283968 (kalloc 1283968, kernel 0)


The system doesn't lock up but the copy and scrub processes just hang.

I've not unplugged or otherwise done anything to the two USB2 disks  
plugged in. They are both new and are self powered.

Any suggestions?

phil worrall
From cdl at asgaard.org  Sat Apr 19 08:14:23 2008
From: cdl at asgaard.org (Christopher LILJENSTOLPE)
Date: Sat Apr 19 08:12:08 2008
Subject: [zfs-discuss] Zpools went away - completely
In-Reply-To: <FBDF4E38-8F13-4E64-BED2-EBDDBA109322@sogeeky.net>
References: <b99b378d0804180633m1ea91e6dre7e12297b73a80de@mail.gmail.com>
	<18D5A528-EFDE-4D0D-B87C-541AD2BD8978@asgaard.org>
	<FBDF4E38-8F13-4E64-BED2-EBDDBA109322@sogeeky.net>
Message-ID: <8F9B6204-0D8F-46AE-B074-8AC2A2C17C5F@asgaard.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Yup, I did.

	Chris

On 19 Apr 2008, at 00.37, Mr. Zorg wrote:

> Did you chown the files after dropping them in?  I could see where  
> that may cause what you're seeing. I had no problems.
>
> On Apr 18, 2008, at 6:32 PM, Christopher LILJENSTOLPE  
> <cdl@asgaard.org> wrote:
>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> Gretings,
>>
>>   I just upgraded to the new release from 102A and had one 3x1TB  
>> raidz pool on my 10.5 PPC server.  After I removed the files/dirs  
>> that 110 would over-write, I dropped the new 110 build files in,  
>> and rebooted.  When the system came back, no pools or filesystems  
>> built on them mounted, and a zpool list or zfs list showed no pools  
>> found.
>>
>>   Fighting back some panic, I ripped out the 110 load, and pulled  
>> the 102A files down and re-installed them.  Rebooted, and still, no  
>> pools.
>>
>>   I have things backed up, but I'm "concerned" about pools that  
>> just vanish in the night....
>>
>>   Chris
>>
>> -----BEGIN PGP SIGNATURE-----
>>
>> iQEcBAEBAgAGBQJICUuuAAoJEGmx2Mt/+Iw/fKQH/if+dB97wiDMS/mxtU2nc6vy
>> lMFoH7r5Wc8zHf6cX8aEWebxSDSnloA9K1V1nZBuXc7Vi2OJRxiyiI0/uLF+MPqO
>> APh5evK/23iVfN/BUXY5elrPrkBnK53SV8Mf9QOi1kjM1WwH3YLy2mwjbl57f1zA
>> Hm+ea5NW7hl69CXrwHQmJK/qkPYyQnOU3Tf4Fns79m+dzyeHnd9gowd6NY0mduOH
>> jX0w76mrFR8vGSslk1aVrHtUVCgyOsgcmZzBk/HfdnF0caNGdvN1POL3LRexWCRq
>> 3EpeRlDQbxNfEmPMzhiVstGxsnlzRlYp8Bz+Pc/T2Z8WCFu8+dY7CrbdIBH36rw=
>> =6qFq
>> -----END PGP SIGNATURE-----
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

- ---
???
Check my PGP key here:
http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B




-----BEGIN PGP SIGNATURE-----

iQEcBAEBAgAGBQJICgxPAAoJEGmx2Mt/+Iw/RM8H/1oTx7lloI3CZ35db2AewYKD
E8tePxsDZitrdzK0pw4w1HNEAgnSy1+avBE+0p89eIdO6WFcSzf5kHiQEXcmie6m
1x1yiu7m4UzNZDwvFtCU+Uo/E4msMjyvjDXhjsaxEUsO2pDXq7v3BV3D8SaA5zn8
GXJtq/Paa8MpJK5sgKeskHrbIbVJkvnLZI8BrMUO2ZNWxe18d6PBafpXpjCLE8rC
h2NSY0U+1a6GWXj465O8PgRCbdd+tFBzq29M1HymZIgxFzMIPFKli7zvB1uNEx9E
3rkUPofzIr+lcBlQif8j084cW5t8EhyEo+v/9fHo4MEsRrudRhd3m+xztceIuiA=
=b/WE
-----END PGP SIGNATURE-----
From zorg at sogeeky.net  Sat Apr 19 09:32:59 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Sat Apr 19 09:30:46 2008
Subject: [zfs-discuss] zfs 2xUSB2 disk mirrored hang
In-Reply-To: <45C1732A-5884-469B-847D-F83AAFAF3C30@uk-businessdirectory.co.uk>
References: <45C1732A-5884-469B-847D-F83AAFAF3C30@uk-businessdirectory.co.uk>
Message-ID: <E225BF66-2A4C-4CB9-8433-39FB076EFC18@sogeeky.net>

I don't believe compression is functional yet. Additionally, there are  
known issues copying or operating on large directories. Try breaking  
your copy into smaller chunks.

On Apr 19, 2008, at 2:26 AM, Philip Worrall <phil_w@uk-businessdirectory.co.uk 
 > wrote:

> Hi
>
> Ive been playing with ZFS and bought to 250GB USB2 disks to use with  
> it.
>
> zpool create tank mirror /dev/disk3 /dev/disk4
>
> zfs set compression=on tank
>
> cp -R archievedFiles /Volumes/tank
>
> then after a few GB of copying the hard drives stop and in dmesg I  
> find.
>
>
> USBF:    26455.368    [0x2b4b300] The IOUSBFamily is having trouble  
> enumerating a USB device that has been plugged in.  It will keep  
> retrying.  (Port 3 of hub @ location: 0xfd000000)
> USBF:    26459.  3    [0x2b4b300] The IOUSBFamily was not able to  
> enumerate a device.
> zfs_context_init: footprint.maximum=357913941,  
> footprint.target=102711296
> kobj_open_file: "/etc/zfs/zpool.cache", err 0 from vnode_open
> zfs_module_start: memory footprint 1283968 (kalloc 1283968, kernel 0)
>
>
> The system doesn't lock up but the copy and scrub processes just hang.
>
> I've not unplugged or otherwise done anything to the two USB2 disks  
> plugged in. They are both new and are self powered.
>
> Any suggestions?
>
> phil worrall
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
From quension at mac.com  Sat Apr 19 09:49:19 2008
From: quension at mac.com (Trevor Talbot)
Date: Sat Apr 19 09:46:57 2008
Subject: [zfs-discuss] zfs 2xUSB2 disk mirrored hang
In-Reply-To: <E225BF66-2A4C-4CB9-8433-39FB076EFC18@sogeeky.net>
References: <45C1732A-5884-469B-847D-F83AAFAF3C30@uk-businessdirectory.co.uk>
	<E225BF66-2A4C-4CB9-8433-39FB076EFC18@sogeeky.net>
Message-ID: <7569052A-798B-4808-A059-C2D7650AE931@mac.com>

On Apr 19, 2008, at 9:32 AM, Mr. Zorg wrote:

> I don't believe compression is functional yet.

Compression seems to work; I'm using it with copies=2 to good effect  
on a flakey drive.

From canadrian at electricteaparty.net  Sat Apr 19 11:49:24 2008
From: canadrian at electricteaparty.net (Adrian Thornton)
Date: Sat Apr 19 11:47:02 2008
Subject: [zfs-discuss] RE: Time machine with ZFS backend
Message-ID: <14AB7A2B-58DF-4A5C-8D14-A06DEDBE25CB@electricteaparty.net>

Hey there, just wondering if anyone has anything else to say about the  
below-described method. Also, just curious why Time Machine would not  
work simply with a writable disk image on the ZFS pool, without going  
to all the AFS/SSH trouble described below? If Time Machine has a  
properly formatted disk image, why would it even care what filesystem  
that disk image is stored on?

Thanks,
Adrian

> [zfs-discuss] Time machine with ZFS backend
> Franz Schmalzl franzschmalzl at spamfreemail.de
> Sat Mar 22 06:17:42 PDT 2008
>
> Hi list!
>
> I just managed to get Time machine working on my external raidz...
> it's a bit dirty, but works
>
> Steps:
>
> Create a zfs volume
> Share over the network via afp
> open up ssh tunnel to localhost, since afp won't let you connect to
> localhost
> (ssh user at localhost -L randomportnumber:localhost:548 )
> Command+K, apf://localhost:randomportnumber)
>
> (( of course randomportnumber hast to be a random port number ;), like
> 1234 )
>
> Your "network" share should get mounted  and you can use it for time
> machine witch creates i'ts sparsebundles...
>
> p.s.
>
> don't forget to apply
>
> defaults write com.apple.systempreferences
> TMShowUnsupportedNetworkVolumes 1
>
>
> regards
>
> franz schmalzl

From Jonathan.Edwards at Sun.COM  Sat Apr 19 13:44:13 2008
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Sat Apr 19 13:41:57 2008
Subject: [zfs-discuss] Zpools went away - completely
In-Reply-To: <8F9B6204-0D8F-46AE-B074-8AC2A2C17C5F@asgaard.org>
References: <b99b378d0804180633m1ea91e6dre7e12297b73a80de@mail.gmail.com>
	<18D5A528-EFDE-4D0D-B87C-541AD2BD8978@asgaard.org>
	<FBDF4E38-8F13-4E64-BED2-EBDDBA109322@sogeeky.net>
	<8F9B6204-0D8F-46AE-B074-8AC2A2C17C5F@asgaard.org>
Message-ID: <F1670B78-0C92-41F3-8A10-BEB9D8E0FA0B@sun.com>

the cache file might have disappeared (/etc/zfs/zpool.cache)

can you import them with a zpool import <pool>?

On Apr 19, 2008, at 11:14 AM, Christopher LILJENSTOLPE wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Yup, I did.
>
> 	Chris
>
> On 19 Apr 2008, at 00.37, Mr. Zorg wrote:
>
>> Did you chown the files after dropping them in?  I could see where  
>> that may cause what you're seeing. I had no problems.
>>
>> On Apr 18, 2008, at 6:32 PM, Christopher LILJENSTOLPE <cdl@asgaard.org 
>> > wrote:
>>
>>> -----BEGIN PGP SIGNED MESSAGE-----
>>> Hash: SHA1
>>>
>>> Gretings,
>>>
>>>  I just upgraded to the new release from 102A and had one 3x1TB  
>>> raidz pool on my 10.5 PPC server.  After I removed the files/dirs  
>>> that 110 would over-write, I dropped the new 110 build files in,  
>>> and rebooted.  When the system came back, no pools or filesystems  
>>> built on them mounted, and a zpool list or zfs list showed no  
>>> pools found.
>>>
>>>  Fighting back some panic, I ripped out the 110 load, and pulled  
>>> the 102A files down and re-installed them.  Rebooted, and still,  
>>> no pools.
>>>
>>>  I have things backed up, but I'm "concerned" about pools that  
>>> just vanish in the night....
>>>
>>>  Chris
>>>
>>> -----BEGIN PGP SIGNATURE-----
>>>
>>> iQEcBAEBAgAGBQJICUuuAAoJEGmx2Mt/+Iw/fKQH/if+dB97wiDMS/mxtU2nc6vy
>>> lMFoH7r5Wc8zHf6cX8aEWebxSDSnloA9K1V1nZBuXc7Vi2OJRxiyiI0/uLF+MPqO
>>> APh5evK/23iVfN/BUXY5elrPrkBnK53SV8Mf9QOi1kjM1WwH3YLy2mwjbl57f1zA
>>> Hm+ea5NW7hl69CXrwHQmJK/qkPYyQnOU3Tf4Fns79m+dzyeHnd9gowd6NY0mduOH
>>> jX0w76mrFR8vGSslk1aVrHtUVCgyOsgcmZzBk/HfdnF0caNGdvN1POL3LRexWCRq
>>> 3EpeRlDQbxNfEmPMzhiVstGxsnlzRlYp8Bz+Pc/T2Z8WCFu8+dY7CrbdIBH36rw=
>>> =6qFq
>>> -----END PGP SIGNATURE-----
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>
> - ---
> ???
> Check my PGP key here:
> http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B
>
>
>
>
> -----BEGIN PGP SIGNATURE-----
>
> iQEcBAEBAgAGBQJICgxPAAoJEGmx2Mt/+Iw/RM8H/1oTx7lloI3CZ35db2AewYKD
> E8tePxsDZitrdzK0pw4w1HNEAgnSy1+avBE+0p89eIdO6WFcSzf5kHiQEXcmie6m
> 1x1yiu7m4UzNZDwvFtCU+Uo/E4msMjyvjDXhjsaxEUsO2pDXq7v3BV3D8SaA5zn8
> GXJtq/Paa8MpJK5sgKeskHrbIbVJkvnLZI8BrMUO2ZNWxe18d6PBafpXpjCLE8rC
> h2NSY0U+1a6GWXj465O8PgRCbdd+tFBzq29M1HymZIgxFzMIPFKli7zvB1uNEx9E
> 3rkUPofzIr+lcBlQif8j084cW5t8EhyEo+v/9fHo4MEsRrudRhd3m+xztceIuiA=
> =b/WE
> -----END PGP SIGNATURE-----
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From zfs-discuss at openhealth.org  Sat Apr 19 22:16:27 2008
From: zfs-discuss at openhealth.org (Jonathan Borden)
Date: Sat Apr 19 22:14:11 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <46949EA3-6ACA-4813-A410-65727242C689@asgaard.org>
References: <47E33231.40107@jrv.org>
	<CFA57BDA-2C10-4CD5-B899-D425AD960427@gmail.com>
	<11DDD05B-E100-4115-A5A7-07224EECE11A@martin-hauser.net>
	<47FA08D7.2010901@jrv.org>
	<C433CDE9-FEAF-4D7D-B8A3-7D5136FF96F1@martin-hauser.net>
	<3EF46CF4-B83D-4864-A572-EF86E5AF1539@asgaard.org>
	<E34E4AD4-A863-4F52-BEA6-27FC4CE2EE2B@martin-hauser.net>
	<C9D41D73-DEC3-4288-A905-4D5689481C8A@asgaard.org>
	<47FE7D0F.4050503@loveturtle.net>
	<46949EA3-6ACA-4813-A410-65727242C689@asgaard.org>
Message-ID: <7a0bc8420804192216p327c8362se11afa9cbae80785@mail.gmail.com>

I have the same problem with Handbrake, also EyeTV editing/compation
and VLC transcoding...

This seems to be a general issue with ZFS and programs that
edit/transcode/export large video streams...

I seems to me that working with these types of files would cover a
large percentage of the people who are using raid and OS X...

Jonathan
From franzschmalzl at spamfreemail.de  Sun Apr 20 00:03:15 2008
From: franzschmalzl at spamfreemail.de (Franz Schmalzl)
Date: Sun Apr 20 00:01:01 2008
Subject: [zfs-discuss] RE: Time machine with ZFS backend
In-Reply-To: <14AB7A2B-58DF-4A5C-8D14-A06DEDBE25CB@electricteaparty.net>
References: <14AB7A2B-58DF-4A5C-8D14-A06DEDBE25CB@electricteaparty.net>
Message-ID: <5D6879E9-C1DD-40C6-8536-A79FFB615306@spamfreemail.de>


On Apr 19, 2008, at 8:49 PM, Adrian Thornton wrote:

> Hey there, just wondering if anyone has anything else to say about  
> the below-described method. Also, just curious why Time Machine  
> would not work simply with a writable disk image on the ZFS pool,  
> without going to all the AFS/SSH trouble described below? If Time  
> Machine has a properly formatted disk image, why would it even care  
> what filesystem that disk image is stored on?

Time machine just won't let you choose a local sparsbundle.... that's  
why the afp/ssh thing is needed....


>
>
> Thanks,
> Adrian
>
>> [zfs-discuss] Time machine with ZFS backend
>> Franz Schmalzl franzschmalzl at spamfreemail.de
>> Sat Mar 22 06:17:42 PDT 2008
>>
>> Hi list!
>>
>> I just managed to get Time machine working on my external raidz...
>> it's a bit dirty, but works
>>
>> Steps:
>>
>> Create a zfs volume
>> Share over the network via afp
>> open up ssh tunnel to localhost, since afp won't let you connect to
>> localhost
>> (ssh user at localhost -L randomportnumber:localhost:548 )
>> Command+K, apf://localhost:randomportnumber)
>>
>> (( of course randomportnumber hast to be a random port number ;),  
>> like
>> 1234 )
>>
>> Your "network" share should get mounted  and you can use it for time
>> machine witch creates i'ts sparsebundles...
>>
>> p.s.
>>
>> don't forget to apply
>>
>> defaults write com.apple.systempreferences
>> TMShowUnsupportedNetworkVolumes 1
>>
>>
>> regards
>>
>> franz schmalzl
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From zfs at hessmann.de  Sun Apr 20 00:22:11 2008
From: zfs at hessmann.de (=?ISO-8859-1?Q?Christian_He=DFmann?=)
Date: Sun Apr 20 00:21:12 2008
Subject: [zfs-discuss] Re: Time machine with ZFS backend
In-Reply-To: <5D6879E9-C1DD-40C6-8536-A79FFB615306@spamfreemail.de>
References: <14AB7A2B-58DF-4A5C-8D14-A06DEDBE25CB@electricteaparty.net>
	<5D6879E9-C1DD-40C6-8536-A79FFB615306@spamfreemail.de>
Message-ID: <867F0104-4003-461B-B7B5-C800D1548B39@hessmann.de>

Good morning,


On 20.04.2008, at 09:03, Franz Schmalzl wrote:
>
> On Apr 19, 2008, at 8:49 PM, Adrian Thornton wrote:
>
>> Hey there, just wondering if anyone has anything else to say about  
>> the below-described method. Also, just curious why Time Machine  
>> would not work simply with a writable disk image on the ZFS pool,  
>> without going to all the AFS/SSH trouble described below? If Time  
>> Machine has a properly formatted disk image, why would it even care  
>> what filesystem that disk image is stored on?
> Time machine just won't let you choose a local sparsbundle....  
> that's why the afp/ssh thing is needed....

I was thinking about using TM on my RAID.Z, as well, but in the end  
decided against it, because I couldn't find a way to make use of the  
advantages ZFS gives me, especially with block based snapshots.

My backup strategy (for 6 Macs) is now a simple perl script that uses  
rsync to copy the home directory of each computer into its file system  
on the RAID.Z (via ssh, 3 computers remote via ADSL). It overwrites or  
deletes every file which has changed or is no longer in the source,  
but makes a snapshot after every run.

The script is scheduled to run every 3 hours, and most of the time, it  
even transmits huge files like the Parallels HD image (~15GB) -  
nonetheless, neither the transmission takes a long time (thanks to  
rsync) nor does it use a lot of space (thanks to ZFS) - most snapshots  
are within 100MB (considering the home dirs are about 60GB each).

I am not yet ready to put the home directory on my notebooks on ZFS,  
but this (for me) is the next best thing.


-- 
Best regards,

Christian
From cdl at asgaard.org  Sun Apr 20 00:46:08 2008
From: cdl at asgaard.org (Christopher LILJENSTOLPE)
Date: Sun Apr 20 00:43:52 2008
Subject: [zfs-discuss] Zpools went away - completely
In-Reply-To: <F1670B78-0C92-41F3-8A10-BEB9D8E0FA0B@sun.com>
References: <b99b378d0804180633m1ea91e6dre7e12297b73a80de@mail.gmail.com>
	<18D5A528-EFDE-4D0D-B87C-541AD2BD8978@asgaard.org>
	<FBDF4E38-8F13-4E64-BED2-EBDDBA109322@sogeeky.net>
	<8F9B6204-0D8F-46AE-B074-8AC2A2C17C5F@asgaard.org>
	<F1670B78-0C92-41F3-8A10-BEB9D8E0FA0B@sun.com>
Message-ID: <95F78A0A-4EA1-491A-A26E-B64E182014D5@asgaard.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

I tried to do an import, but I can try again.  Not at home now, it  
will have to wait until Monday night.  I'll look and report then.

	Chris

On 19 Apr 2008, at 13.44, Jonathan Edwards wrote:

> the cache file might have disappeared (/etc/zfs/zpool.cache)
>
> can you import them with a zpool import <pool>?
>
> On Apr 19, 2008, at 11:14 AM, Christopher LILJENSTOLPE wrote:
>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> Yup, I did.
>>
>> 	Chris
>>
>> On 19 Apr 2008, at 00.37, Mr. Zorg wrote:
>>
>>> Did you chown the files after dropping them in?  I could see where  
>>> that may cause what you're seeing. I had no problems.
>>>
>>> On Apr 18, 2008, at 6:32 PM, Christopher LILJENSTOLPE <cdl@asgaard.org 
>>> > wrote:
>>>
>>>> -----BEGIN PGP SIGNED MESSAGE-----
>>>> Hash: SHA1
>>>>
>>>> Gretings,
>>>>
>>>> I just upgraded to the new release from 102A and had one 3x1TB  
>>>> raidz pool on my 10.5 PPC server.  After I removed the files/dirs  
>>>> that 110 would over-write, I dropped the new 110 build files in,  
>>>> and rebooted.  When the system came back, no pools or filesystems  
>>>> built on them mounted, and a zpool list or zfs list showed no  
>>>> pools found.
>>>>
>>>> Fighting back some panic, I ripped out the 110 load, and pulled  
>>>> the 102A files down and re-installed them.  Rebooted, and still,  
>>>> no pools.
>>>>
>>>> I have things backed up, but I'm "concerned" about pools that  
>>>> just vanish in the night....
>>>>
>>>> Chris
>>>>
>>>> -----BEGIN PGP SIGNATURE-----
>>>>
>>>> iQEcBAEBAgAGBQJICUuuAAoJEGmx2Mt/+Iw/fKQH/if+dB97wiDMS/mxtU2nc6vy
>>>> lMFoH7r5Wc8zHf6cX8aEWebxSDSnloA9K1V1nZBuXc7Vi2OJRxiyiI0/uLF+MPqO
>>>> APh5evK/23iVfN/BUXY5elrPrkBnK53SV8Mf9QOi1kjM1WwH3YLy2mwjbl57f1zA
>>>> Hm+ea5NW7hl69CXrwHQmJK/qkPYyQnOU3Tf4Fns79m+dzyeHnd9gowd6NY0mduOH
>>>> jX0w76mrFR8vGSslk1aVrHtUVCgyOsgcmZzBk/HfdnF0caNGdvN1POL3LRexWCRq
>>>> 3EpeRlDQbxNfEmPMzhiVstGxsnlzRlYp8Bz+Pc/T2Z8WCFu8+dY7CrbdIBH36rw=
>>>> =6qFq
>>>> -----END PGP SIGNATURE-----
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>>
>> - ---
>> ???
>> Check my PGP key here:
>> http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B
>>
>>
>>
>>
>> -----BEGIN PGP SIGNATURE-----
>>
>> iQEcBAEBAgAGBQJICgxPAAoJEGmx2Mt/+Iw/RM8H/1oTx7lloI3CZ35db2AewYKD
>> E8tePxsDZitrdzK0pw4w1HNEAgnSy1+avBE+0p89eIdO6WFcSzf5kHiQEXcmie6m
>> 1x1yiu7m4UzNZDwvFtCU+Uo/E4msMjyvjDXhjsaxEUsO2pDXq7v3BV3D8SaA5zn8
>> GXJtq/Paa8MpJK5sgKeskHrbIbVJkvnLZI8BrMUO2ZNWxe18d6PBafpXpjCLE8rC
>> h2NSY0U+1a6GWXj465O8PgRCbdd+tFBzq29M1HymZIgxFzMIPFKli7zvB1uNEx9E
>> 3rkUPofzIr+lcBlQif8j084cW5t8EhyEo+v/9fHo4MEsRrudRhd3m+xztceIuiA=
>> =b/WE
>> -----END PGP SIGNATURE-----
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
>

- ---
???
Check my PGP key here:
http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B




-----BEGIN PGP SIGNATURE-----

iQEcBAEBAgAGBQJICvTAAAoJEGmx2Mt/+Iw/K8IH/1ocupTSCZ2M+NE61pCv5Yzf
O4DcBmvpxTjoYMXlIK2em1blZYeJzjF4GsNZi/K6e00QLirLrYp5WUOzNXbOr0zP
+FP2EPL4BtTESjNwQ7XEMs71z0J4bo5teRM7Y7c7HDKBh3lJSF83RAiwBKE+iJM5
SyzePK+NRpVPZHImhU0EmM1x+sFgmeGkO0XEVqbuHye61FALjI1kHVJnGwZdUJ0e
lyM4pM87IZxQa/Mgt+iKDzbQRrlIP+FllCZnPmh9qOGqldoLmTFf/uuczkEGeXRc
pNomLCqNNo7OrwKoNoaV3tSH/F25/8UDKDdBEUrz800HCwE/bUSpF0NxV+RewrA=
=VwsQ
-----END PGP SIGNATURE-----
From jeff at jeffawaddell.com  Sun Apr 20 03:08:48 2008
From: jeff at jeffawaddell.com (Jeff Waddell)
Date: Sun Apr 20 03:06:31 2008
Subject: [zfs-discuss] Re: zfs-discuss Digest, Vol 4, Issue 28
In-Reply-To: <20080420074357.3176D24B7F8@lists.macosforge.org>
References: <20080420074357.3176D24B7F8@lists.macosforge.org>
Message-ID: <b58016cd0804200308h67bf66d4rea3dced218dfc413@mail.gmail.com>

Christian,
Your backup strategy sounds very similar to what I would like to implement,
which is rsyncing my laptop out to a ZFS backup server, and then using
snapshots for incrementals. Out of curiosity, how are you handling snapshot
management? Do you have a script destroy snapshots after a certain period of
time, or are you doing it manually? As I'm not programmatically adept enough
to create my own scripts, I've been scouring the internet looking for a
script that would accomplish this, similar to Tim Foster's excellent
Automatic Snapshot Service for Solaris or Chris Gerhard's Rolling
Incremental Backup and ZFS cleanup scripts (see links below), although I
haven't found one that is compatible with OS X. Would you mind discussing a
little more about your scripts, and possibly sharing them with the list? I'm
sure others may find them helpful as well!

http://blogs.sun.com/timf/entry/zfs_automatic_snapshots_0_10
http://blogs.sun.com/chrisg/entry/cleaning_up_zfs_snapshots
http://blogs.sun.com/chrisg/entry/rolling_incremental_backups

For those looking for a Time Machine-like backup utility for ZFS, check out
Tim Foster's Automatic Backup Service. It automatically creates ZFS backups
to a USB drive upon connection to the system. Granted, it's built for
Solaris and won't work on OS X, but I think something similar (if missing
the GUI) could be accomplished through various scripts (
http://blogs.sun.com/timf/entry/zfs_automatic_backup_0_1)

-Jeff Waddell

On Sun, Apr 20, 2008 at 12:43 AM, <zfs-discuss-request@lists.macosforge.org>
wrote:

>
> Date: Sun, 20 Apr 2008 09:22:11 +0200
> From: Christian He?mann <zfs@hessmann.de>
> Subject: [zfs-discuss] Re: Time machine with ZFS backend
> To: ZFS on OSX mailing list mailing list
>        <zfs-discuss@lists.macosforge.org>
> Message-ID: <867F0104-4003-461B-B7B5-C800D1548B39@hessmann.de>
>
> Good morning,
>
> I was thinking about using TM on my RAID.Z, as well, but in the end
> decided against it, because I couldn't find a way to make use of the
> advantages ZFS gives me, especially with block based snapshots.
>
> My backup strategy (for 6 Macs) is now a simple perl script that uses
> rsync to copy the home directory of each computer into its file system
> on the RAID.Z (via ssh, 3 computers remote via ADSL). It overwrites or
> deletes every file which has changed or is no longer in the source,
> but makes a snapshot after every run.
>
> The script is scheduled to run every 3 hours, and most of the time, it
> even transmits huge files like the Parallels HD image (~15GB) -
> nonetheless, neither the transmission takes a long time (thanks to
> rsync) nor does it use a lot of space (thanks to ZFS) - most snapshots
> are within 100MB (considering the home dirs are about 60GB each).
>
> I am not yet ready to put the home directory on my notebooks on ZFS,
> but this (for me) is the next best thing.
>
> --
> Best regards,
>
> Christian
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080420/0ff2696e/attachment.html
From alex.blewitt at gmail.com  Sun Apr 20 05:34:09 2008
From: alex.blewitt at gmail.com (Alex Blewitt)
Date: Sun Apr 20 05:34:02 2008
Subject: [zfs-discuss] Handbrake on ZFS
In-Reply-To: <7a0bc8420804192216p327c8362se11afa9cbae80785@mail.gmail.com>
References: <47E33231.40107@jrv.org>
	<CFA57BDA-2C10-4CD5-B899-D425AD960427@gmail.com>
	<11DDD05B-E100-4115-A5A7-07224EECE11A@martin-hauser.net>
	<47FA08D7.2010901@jrv.org>
	<C433CDE9-FEAF-4D7D-B8A3-7D5136FF96F1@martin-hauser.net>
	<3EF46CF4-B83D-4864-A572-EF86E5AF1539@asgaard.org>
	<E34E4AD4-A863-4F52-BEA6-27FC4CE2EE2B@martin-hauser.net>
	<C9D41D73-DEC3-4288-A905-4D5689481C8A@asgaard.org>
	<47FE7D0F.4050503@loveturtle.net>
	<46949EA3-6ACA-4813-A410-65727242C689@asgaard.org>
	<7a0bc8420804192216p327c8362se11afa9cbae80785@mail.gmail.com>
Message-ID: <8CD6CB90-E15E-4D3F-A09C-70CB1611B3A4@gmail.com>

Is it likely that these programs ate ones that use mmap to punt large  
amounts of data in and out? Is there any difference with the 111 build?

Alex

Sent from my iPhone

On 20 Apr 2008, at 06:16, "Jonathan Borden" <zfs- 
discuss@openhealth.org> wrote:

> I have the same problem with Handbrake, also EyeTV editing/compation
> and VLC transcoding...
>
> This seems to be a general issue with ZFS and programs that
> edit/transcode/export large video streams...
>
> I seems to me that working with these types of files would cover a
> large percentage of the people who are using raid and OS X...
>
> Jonathan
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
From lopez.on.the.lists at yellowspace.net  Mon Apr 21 09:59:39 2008
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Mon Apr 21 10:09:21 2008
Subject: [zfs-discuss] zpool not imported automatically
Message-ID: <5B67FCD8-2D39-47EC-8B66-A3F6FE81CF2A@yellowspace.net>


Hi to everyone,

Have a problem with my newly created ZFS pool.
It won't import it automatically at boot.

This is almost probably related to my installation of rEFIt,
letting it make an MBR PM out of my GPT PM, and/or installing
FreeBSD 7.0-RELEASE on a separate partition of my MacBook Pro
(as it didn't happen before).

However, it's nasty and I can't find a good readon for it ;)

Here's my feedback.

ENVIRONMENT:
MacBook Pro 2,2 (2.1 GHz Core2Duo), 4GB RAM, 320GB HD.
Mac OS X 10.5.2
zfs-111 (newly downloaded the binaries from macosforge)

DESCRIPTION:
ZFS pools are no more imported automatically at boot (they used to)

CONFIGURATION:
I have one Zpool made out of one partition (disk0s3).

BEHAVIOR:

Upon boot, zpool does not import the pool automatically, as it
used to do. Here's what happens in the console as root:

# zpool list
kernel[0]: zfs_context_init: footprint.maximum=2147483647,  
footprint.target=102711296
kernel[0]: kobj_open_file: "/etc/zfs/zpool.cache", err 0 from vnode_open
kernel[0]: fs_module_start: memory footprint 4822912 (kalloc 4822912,  
kernel 0)
no pools available

# zpool import
   pool: Zorro
     id: 5221204521612907677
  state: ONLINE
status: The pool is formatted using an older on-disk version.
action: The pool can be imported using its name or numeric identifier,  
though
	some features will not be available without an explicit 'zpool  
upgrade'.
config:

	Zorro       ONLINE
	  disk0s3   ONLINE

# zpool import Zorro
cannot import 'Zorro': pool may be in use on another system
use '-f' to import anyway
# zpool import -f Zorro

-> this succeeds

Note: I tried a zpool upgrade, just for fun, as it is stated above.
But it didn't help, because zfs told the version is already up to date
and there is no reason to upgrade (which is true, I guess, as the
pool was created with the same software)

HISTORY OF ACTIONS:

What I did between when it worked and when it stopped working:

- I installed rEFIt (http://refit.sf.net/ )
- I had rEFIt make an MBR partition table which is a copy of my GPT  
Table
   (the ZFS partition is noted as "Solaris" partition)
- I installed FreeBSD 7.0-RELEASE amd64 on the partition after it,
   And then couldn't resist letting him have a look (just a "look"  
with zfs import)
   to see if it could recognize the ZFS partition (it did, yet warned  
about different
   protocol version, so I _did not_ import it: FreeBSD uses version 6,  
OSX version 8).
- Afterwards I tried deleting the /etc/zfs directory, no results so far.

Relevant system.log entries:
Apr 21 16:06:38 zeta kernel[0]: zfs_context_init:  
footprint.maximum=2147483647, footprint.target=102711296
Apr 21 16:06:38 zeta kernel[0]: kobj_open_file: "/etc/zfs/ 
zpool.cache", err 0 from vnode_open
Apr 21 16:06:38 zeta kernel[0]: zfs_module_start: memory footprint  
4822912 (kalloc 4822912, kernel 0)
Apr 21 16:07:04 zeta fseventsd[29]: log dir: /Volumes/Zorro/.fseventsd  
getting new uuid: E8D57080-37BC-4ABE-9655-F6D1D3B8D551
Apr 21 16:07:10 zeta login[132]: DEAD_PROCESS: 132 console

Upon shutdown (all volumes shown are part of the pool):
Apr 21 16:04:56 zeta kextd[10]: error reconsidering volume /Downloads
Apr 21 16:04:56 zeta kextd[10]: error reconsidering volume /Games
Apr 21 16:04:56 zeta kextd[10]: error reconsidering volume /Movies
Apr 21 16:04:56 zeta kextd[10]: error reconsidering volume /Music
Apr 21 16:04:56 zeta kextd[10]: error reconsidering volume /VirtualPCs
Apr 21 16:04:56 zeta shutdown[253]: SHUTDOWN_TIME: 1208786696 245286
Apr 21 16:04:56 zeta kextd[10]: error reconsidering volume /Volumes/ 
Zorro
Apr 21 16:04:56 zeta shutdown[253]: reboot by admin:

Any Ideas of what might be causing this and how to resolve it?
It's not _that_ mac-like to go into >console at every boot just to
import the zpool and then log in with my real user ;)


Thanx for reading,


Regards,

Lorenzo

From quension at mac.com  Mon Apr 21 10:51:17 2008
From: quension at mac.com (Trevor Talbot)
Date: Mon Apr 21 10:48:57 2008
Subject: [zfs-discuss] zpool not imported automatically
In-Reply-To: <5B67FCD8-2D39-47EC-8B66-A3F6FE81CF2A@yellowspace.net>
References: <5B67FCD8-2D39-47EC-8B66-A3F6FE81CF2A@yellowspace.net>
Message-ID: <D37F7A2B-AB36-4F97-B64F-E0716665A658@mac.com>

On Apr 21, 2008, at 9:59 AM, Lorenzo Perone wrote:

> Have a problem with my newly created ZFS pool.
> It won't import it automatically at boot.

> # zpool import
>  pool: Zorro
>    id: 5221204521612907677
> state: ONLINE
> status: The pool is formatted using an older on-disk version.
> action: The pool can be imported using its name or numeric  
> identifier, though
> 	some features will not be available without an explicit 'zpool  
> upgrade'.
> config:
>
> 	Zorro       ONLINE
> 	  disk0s3   ONLINE

> What I did between when it worked and when it stopped working:
>
> - I installed rEFIt (http://refit.sf.net/ )
> - I had rEFIt make an MBR partition table which is a copy of my GPT  
> Table
>  (the ZFS partition is noted as "Solaris" partition)

It was probably this. As the wiki says, only GPT is supported. What does
     diskutil list disk0
show now?

From millerdc at fusion.gat.com  Mon Apr 21 13:58:44 2008
From: millerdc at fusion.gat.com (David Miller)
Date: Mon Apr 21 14:04:56 2008
Subject: [zfs-discuss] ZFS on Mac OS X in production use?
Message-ID: <EBD66233-0050-4D81-9017-300417D6BCA4@fusion.gat.com>

It would appear there are a few people on this list using ZFS+Leopard  
in production. Is ZFS under Mac OS X really ready to be used for  
production file storage and backups?

I also had a question about pools. is a pool the total size of storage  
you have in the pool? I know you can't grow a raidz by adding another  
disk but could you add multiple raidz's to a pool? for example could I  
do this

zpool create tank raidz disk2 disk3 disk4

Then later add another raidz to the tank pool to increase the storage  
of that filesystem? if so could I also add mirrors to the pool?

Thanks
David.
From zorg at sogeeky.net  Mon Apr 21 14:12:54 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Mon Apr 21 14:31:18 2008
Subject: [zfs-discuss] ZFS on Mac OS X in production use?
In-Reply-To: <EBD66233-0050-4D81-9017-300417D6BCA4@fusion.gat.com>
References: <EBD66233-0050-4D81-9017-300417D6BCA4@fusion.gat.com>
Message-ID: <32EE4F43-1D19-4AA1-891A-DDF698E5D20F@sogeeky.net>

I think the fact that the read/write drivers are still beta should be  
answer enough to your first question. That said, some people are using  
it in "production" environments, but its suitability for that depends  
greatly on your intended usage and whether or not any of the known  
limitations are important to you.

As for growing the pool. Yes, you've got it correct. You can add more  
disks, raidz, or mirrors to a pool just fine. But as you say, you  
can't grow a raidz.

On Apr 21, 2008, at 1:58 PM, David Miller <millerdc@fusion.gat.com>  
wrote:

> It would appear there are a few people on this list using ZFS 
> +Leopard in production. Is ZFS under Mac OS X really ready to be  
> used for production file storage and backups?
>
> I also had a question about pools. is a pool the total size of  
> storage you have in the pool? I know you can't grow a raidz by  
> adding another disk but could you add multiple raidz's to a pool?  
> for example could I do this
>
> zpool create tank raidz disk2 disk3 disk4
>
> Then later add another raidz to the tank pool to increase the  
> storage of that filesystem? if so could I also add mirrors to the  
> pool?
>
> Thanks
> David.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
From canadrian at electricteaparty.net  Mon Apr 21 15:01:32 2008
From: canadrian at electricteaparty.net (Adrian Thornton)
Date: Mon Apr 21 15:02:27 2008
Subject: [zfs-discuss] ZFS on Mac OS X in production use?
In-Reply-To: <32EE4F43-1D19-4AA1-891A-DDF698E5D20F@sogeeky.net>
References: <EBD66233-0050-4D81-9017-300417D6BCA4@fusion.gat.com>
	<32EE4F43-1D19-4AA1-891A-DDF698E5D20F@sogeeky.net>
Message-ID: <DC597AFE-B67E-497A-96DB-BB55AB7C6EBD@electricteaparty.net>

I can also attest to being able to grow a raidz by replacing all of  
the disks with bigger disks.  i.e. you can increase raidz capacity by  
replacing four existing 320gb disks with four 750gb disks, but you can  
not increase capacity by adding a fifth disk to a four-disk raidz.

On Apr 21, 2008, at 15:12 , Mr. Zorg wrote:

> I think the fact that the read/write drivers are still beta should  
> be answer enough to your first question. That said, some people are  
> using it in "production" environments, but its suitability for that  
> depends greatly on your intended usage and whether or not any of the  
> known limitations are important to you.
>
> As for growing the pool. Yes, you've got it correct. You can add  
> more disks, raidz, or mirrors to a pool just fine. But as you say,  
> you can't grow a raidz.
>
> On Apr 21, 2008, at 1:58 PM, David Miller <millerdc@fusion.gat.com>  
> wrote:
>
>> It would appear there are a few people on this list using ZFS 
>> +Leopard in production. Is ZFS under Mac OS X really ready to be  
>> used for production file storage and backups?
>>
>> I also had a question about pools. is a pool the total size of  
>> storage you have in the pool? I know you can't grow a raidz by  
>> adding another disk but could you add multiple raidz's to a pool?  
>> for example could I do this
>>
>> zpool create tank raidz disk2 disk3 disk4
>>
>> Then later add another raidz to the tank pool to increase the  
>> storage of that filesystem? if so could I also add mirrors to the  
>> pool?
>>
>> Thanks
>> David.
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

From zorg at sogeeky.net  Mon Apr 21 15:41:11 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Mon Apr 21 16:06:08 2008
Subject: [zfs-discuss] ZFS on Mac OS X in production use?
In-Reply-To: <DC597AFE-B67E-497A-96DB-BB55AB7C6EBD@electricteaparty.net>
References: <EBD66233-0050-4D81-9017-300417D6BCA4@fusion.gat.com>
	<32EE4F43-1D19-4AA1-891A-DDF698E5D20F@sogeeky.net>
	<DC597AFE-B67E-497A-96DB-BB55AB7C6EBD@electricteaparty.net>
Message-ID: <A66DA2EC-DE24-4AD5-9848-37A429F110FD@sogeeky.net>

Good to know!

On Apr 21, 2008, at 3:01 PM, Adrian Thornton <canadrian@electricteaparty.net 
 > wrote:

> I can also attest to being able to grow a raidz by replacing all of  
> the disks with bigger disks.  i.e. you can increase raidz capacity  
> by replacing four existing 320gb disks with four 750gb disks, but  
> you can not increase capacity by adding a fifth disk to a four-disk  
> raidz.
>
> On Apr 21, 2008, at 15:12 , Mr. Zorg wrote:
>
>> I think the fact that the read/write drivers are still beta should  
>> be answer enough to your first question. That said, some people are  
>> using it in "production" environments, but its suitability for that  
>> depends greatly on your intended usage and whether or not any of  
>> the known limitations are important to you.
>>
>> As for growing the pool. Yes, you've got it correct. You can add  
>> more disks, raidz, or mirrors to a pool just fine. But as you say,  
>> you can't grow a raidz.
>>
>> On Apr 21, 2008, at 1:58 PM, David Miller <millerdc@fusion.gat.com>  
>> wrote:
>>
>>> It would appear there are a few people on this list using ZFS 
>>> +Leopard in production. Is ZFS under Mac OS X really ready to be  
>>> used for production file storage and backups?
>>>
>>> I also had a question about pools. is a pool the total size of  
>>> storage you have in the pool? I know you can't grow a raidz by  
>>> adding another disk but could you add multiple raidz's to a pool?  
>>> for example could I do this
>>>
>>> zpool create tank raidz disk2 disk3 disk4
>>>
>>> Then later add another raidz to the tank pool to increase the  
>>> storage of that filesystem? if so could I also add mirrors to the  
>>> pool?
>>>
>>> Thanks
>>> David.
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
From lopez.on.the.lists at yellowspace.net  Mon Apr 21 18:14:55 2008
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Mon Apr 21 18:13:11 2008
Subject: [zfs-discuss] zpool not imported automatically
In-Reply-To: <D37F7A2B-AB36-4F97-B64F-E0716665A658@mac.com>
References: <5B67FCD8-2D39-47EC-8B66-A3F6FE81CF2A@yellowspace.net>
	<D37F7A2B-AB36-4F97-B64F-E0716665A658@mac.com>
Message-ID: <ECC742D3-4E9A-4BE9-AE69-2FF043B5BF54@yellowspace.net>

An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080422/538869dd/attachment.html
From jbsnyder at gmail.com  Tue Apr 22 06:31:51 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Tue Apr 22 06:29:16 2008
Subject: [zfs-discuss] weird zfs thread synchronization issue?
Message-ID: <33644d3c0804220631ife68103y48fd9b5cda3baaa3@mail.gmail.com>

SUMMARY
During the following situations:
git reset --hard <revision>
git-svn clone ... (on a large svn repository)
(sometimes followed by)
rm -rf <some directory> (after just working with a lot of files, like
one of the above commands)

The listed git commands sometimes fail, sometimes succeed, citing
sometimes missing files.  With rm -rf, shortly after the operation an
ls or other file operation will sometimes show certain
directories/files just operated on and sometimes not.  This may or may
not stabilize in the subsequent seconds.  This behavior leaves me with
the sense that the file operations are talking to different threads
associated with ZFS and that they're not all on the "same page"

STEPS TO REPRODUCE
1. Find a rather large svn repository, with lots of revisions and
files (or make one).  The repo I'm working with has lots of binary
files in it and the db for svn is around 9 gigabytes with 4000
revisions or so.
2. Install latest git w/ svn support from macports. (sudo port install git +svn)
3. do a git-svn clone on your repository
4. With the current version of git-svn, it should try and repack files
at different stages during the clone process if you have enough
revisions, you will likely see a failure after this point when it
tries to pick up and check out the next increment from svn.
5. Quickly look for the files it says it cannot find in the .git
directory of your partial clone, you should find that sometimes it is
there and sometimes not.

RESULTS
Files associated with recent large batches of operations are there/not
there with subsequent "ls" listings.  If no new userland operations
are pending on the file, the file should either be there or not there,
not in limbo.

REGRESSION
I haven't checked this with 102A or other revs, just 111.

NOTES
Running current 111 release on a MacBook, with a single pool on a
single partition.

-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
From info at martin-hauser.net  Tue Apr 22 06:36:26 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Tue Apr 22 06:33:58 2008
Subject: [zfs-discuss] weird zfs thread synchronization issue?
In-Reply-To: <33644d3c0804220631ife68103y48fd9b5cda3baaa3@mail.gmail.com>
References: <33644d3c0804220631ife68103y48fd9b5cda3baaa3@mail.gmail.com>
Message-ID: <9DACCDE5-F405-4C92-BF22-17EF9DD27333@martin-hauser.net>

Hi there,

more on that issue,

I've got some problems with it as well, but mostly due to git not  
catching commits immediatlly as such. For example:

1. do some work
2. git add somefile
3. git commit -m "some commit message"
4. git status

you'll notice that the file you added for commiting is still listed

For me, it was easy enough to 'work around' just by doing a git status  
before every index file related operation (just a small alias in  
my .zshrc), might be work-around-ing your problem also James?

I'd be much prefering the issue to be fixed though.

Martin

On Apr 22, 2008, at 15:31 PM, James Snyder wrote:

> SUMMARY
> During the following situations:
> git reset --hard <revision>
> git-svn clone ... (on a large svn repository)
> (sometimes followed by)
> rm -rf <some directory> (after just working with a lot of files, like
> one of the above commands)
>
> The listed git commands sometimes fail, sometimes succeed, citing
> sometimes missing files.  With rm -rf, shortly after the operation an
> ls or other file operation will sometimes show certain
> directories/files just operated on and sometimes not.  This may or may
> not stabilize in the subsequent seconds.  This behavior leaves me with
> the sense that the file operations are talking to different threads
> associated with ZFS and that they're not all on the "same page"
>
> STEPS TO REPRODUCE
> 1. Find a rather large svn repository, with lots of revisions and
> files (or make one).  The repo I'm working with has lots of binary
> files in it and the db for svn is around 9 gigabytes with 4000
> revisions or so.
> 2. Install latest git w/ svn support from macports. (sudo port  
> install git +svn)
> 3. do a git-svn clone on your repository
> 4. With the current version of git-svn, it should try and repack files
> at different stages during the clone process if you have enough
> revisions, you will likely see a failure after this point when it
> tries to pick up and check out the next increment from svn.
> 5. Quickly look for the files it says it cannot find in the .git
> directory of your partial clone, you should find that sometimes it is
> there and sometimes not.
>
> RESULTS
> Files associated with recent large batches of operations are there/not
> there with subsequent "ls" listings.  If no new userland operations
> are pending on the file, the file should either be there or not there,
> not in limbo.
>
> REGRESSION
> I haven't checked this with 102A or other revs, just 111.
>
> NOTES
> Running current 111 release on a MacBook, with a single pool on a
> single partition.
>
> -- 
> James Snyder
> Biomedical Engineering
> Northwestern University
> jbsnyder@gmail.com
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss

-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080422/905bfeee/PGP.bin
From jbsnyder at gmail.com  Tue Apr 22 06:49:16 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Tue Apr 22 06:46:42 2008
Subject: [zfs-discuss] weird zfs thread synchronization issue?
In-Reply-To: <9DACCDE5-F405-4C92-BF22-17EF9DD27333@martin-hauser.net>
References: <33644d3c0804220631ife68103y48fd9b5cda3baaa3@mail.gmail.com>
	<9DACCDE5-F405-4C92-BF22-17EF9DD27333@martin-hauser.net>
Message-ID: <33644d3c0804220649r507d81b7u3a6a8f42064e2c2a@mail.gmail.com>

I'll give that a try.  I'm not 100% sure that that will resolve the
problem though.

Does that seem to work consistently or do you find yourself having to
sometimes do things multiple times?

I'm not sure what git is doing behind the scenes during a status
check.  Is it refreshing the index somehow that obviates a little of
this limbo file state issue?

I haven't checked entirely whether this happens for creating, deleting
and modifying, but if it happens on modifications too, that could lead
to some messy issues.

Example: Say, I was working with an sqlite database, made some
changes, unlocked things, and subsequently another process hit the
database and made changes but got the old version of the file and
worked with that.

On Tue, Apr 22, 2008 at 8:36 AM, Martin Hauser <info@martin-hauser.net> wrote:
> Hi there,
>
>  more on that issue,
>
>  I've got some problems with it as well, but mostly due to git not catching
> commits immediatlly as such. For example:
>
>  1. do some work
>  2. git add somefile
>  3. git commit -m "some commit message"
>  4. git status
>
>  you'll notice that the file you added for commiting is still listed
>
>  For me, it was easy enough to 'work around' just by doing a git status
> before every index file related operation (just a small alias in my .zshrc),
> might be work-around-ing your problem also James?
>
>  I'd be much prefering the issue to be fixed though.
>
>  Martin
>
>
>
>  On Apr 22, 2008, at 15:31 PM, James Snyder wrote:
>
>
> >
> >
> >
> > SUMMARY
> > During the following situations:
> > git reset --hard <revision>
> > git-svn clone ... (on a large svn repository)
> > (sometimes followed by)
> > rm -rf <some directory> (after just working with a lot of files, like
> > one of the above commands)
> >
> > The listed git commands sometimes fail, sometimes succeed, citing
> > sometimes missing files.  With rm -rf, shortly after the operation an
> > ls or other file operation will sometimes show certain
> > directories/files just operated on and sometimes not.  This may or may
> > not stabilize in the subsequent seconds.  This behavior leaves me with
> > the sense that the file operations are talking to different threads
> > associated with ZFS and that they're not all on the "same page"
> >
> > STEPS TO REPRODUCE
> > 1. Find a rather large svn repository, with lots of revisions and
> > files (or make one).  The repo I'm working with has lots of binary
> > files in it and the db for svn is around 9 gigabytes with 4000
> > revisions or so.
> > 2. Install latest git w/ svn support from macports. (sudo port install git
> +svn)
> > 3. do a git-svn clone on your repository
> > 4. With the current version of git-svn, it should try and repack files
> > at different stages during the clone process if you have enough
> > revisions, you will likely see a failure after this point when it
> > tries to pick up and check out the next increment from svn.
> > 5. Quickly look for the files it says it cannot find in the .git
> > directory of your partial clone, you should find that sometimes it is
> > there and sometimes not.
> >
> > RESULTS
> > Files associated with recent large batches of operations are there/not
> > there with subsequent "ls" listings.  If no new userland operations
> > are pending on the file, the file should either be there or not there,
> > not in limbo.
> >
> > REGRESSION
> > I haven't checked this with 102A or other revs, just 111.
> >
> > NOTES
> > Running current 111 release on a MacBook, with a single pool on a
> > single partition.
> >
> > --
> > James Snyder
> > Biomedical Engineering
> > Northwestern University
> > jbsnyder@gmail.com
> > _______________________________________________
> > zfs-discuss mailing list
> > zfs-discuss@lists.macosforge.org
> > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> >
>
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
From info at martin-hauser.net  Tue Apr 22 07:20:23 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Tue Apr 22 07:17:55 2008
Subject: [zfs-discuss] weird zfs thread synchronization issue?
In-Reply-To: <33644d3c0804220649r507d81b7u3a6a8f42064e2c2a@mail.gmail.com>
References: <33644d3c0804220631ife68103y48fd9b5cda3baaa3@mail.gmail.com>
	<9DACCDE5-F405-4C92-BF22-17EF9DD27333@martin-hauser.net>
	<33644d3c0804220649r507d81b7u3a6a8f42064e2c2a@mail.gmail.com>
Message-ID: <9E99B62A-A904-4F96-A832-4C2DA31FDB38@martin-hauser.net>

Everything seemed pretty consistent till now. The only thing I've ever  
notice that is all git status related things (so the listing of the  
files in git-commit, the commiting itself works fine). I would assume  
that the git's index file is treated special somehow and doing a git  
diff rewrites that file....

Let me know if you find out more.

Martin

On Apr 22, 2008, at 15:49 PM, James Snyder wrote:

> I'll give that a try.  I'm not 100% sure that that will resolve the
> problem though.
>
> Does that seem to work consistently or do you find yourself having to
> sometimes do things multiple times?
>
> I'm not sure what git is doing behind the scenes during a status
> check.  Is it refreshing the index somehow that obviates a little of
> this limbo file state issue?
>
> I haven't checked entirely whether this happens for creating, deleting
> and modifying, but if it happens on modifications too, that could lead
> to some messy issues.
>
> Example: Say, I was working with an sqlite database, made some
> changes, unlocked things, and subsequently another process hit the
> database and made changes but got the old version of the file and
> worked with that.
>
> On Tue, Apr 22, 2008 at 8:36 AM, Martin Hauser <info@martin- 
> hauser.net> wrote:
>> Hi there,
>>
>> more on that issue,
>>
>> I've got some problems with it as well, but mostly due to git not  
>> catching
>> commits immediatlly as such. For example:
>>
>> 1. do some work
>> 2. git add somefile
>> 3. git commit -m "some commit message"
>> 4. git status
>>
>> you'll notice that the file you added for commiting is still listed
>>
>> For me, it was easy enough to 'work around' just by doing a git  
>> status
>> before every index file related operation (just a small alias in  
>> my .zshrc),
>> might be work-around-ing your problem also James?
>>
>> I'd be much prefering the issue to be fixed though.
>>
>> Martin
>>
>>
>>
>> On Apr 22, 2008, at 15:31 PM, James Snyder wrote:
>>
>>
>>>
>>>
>>>
>>> SUMMARY
>>> During the following situations:
>>> git reset --hard <revision>
>>> git-svn clone ... (on a large svn repository)
>>> (sometimes followed by)
>>> rm -rf <some directory> (after just working with a lot of files,  
>>> like
>>> one of the above commands)
>>>
>>> The listed git commands sometimes fail, sometimes succeed, citing
>>> sometimes missing files.  With rm -rf, shortly after the operation  
>>> an
>>> ls or other file operation will sometimes show certain
>>> directories/files just operated on and sometimes not.  This may or  
>>> may
>>> not stabilize in the subsequent seconds.  This behavior leaves me  
>>> with
>>> the sense that the file operations are talking to different threads
>>> associated with ZFS and that they're not all on the "same page"
>>>
>>> STEPS TO REPRODUCE
>>> 1. Find a rather large svn repository, with lots of revisions and
>>> files (or make one).  The repo I'm working with has lots of binary
>>> files in it and the db for svn is around 9 gigabytes with 4000
>>> revisions or so.
>>> 2. Install latest git w/ svn support from macports. (sudo port  
>>> install git
>> +svn)
>>> 3. do a git-svn clone on your repository
>>> 4. With the current version of git-svn, it should try and repack  
>>> files
>>> at different stages during the clone process if you have enough
>>> revisions, you will likely see a failure after this point when it
>>> tries to pick up and check out the next increment from svn.
>>> 5. Quickly look for the files it says it cannot find in the .git
>>> directory of your partial clone, you should find that sometimes it  
>>> is
>>> there and sometimes not.
>>>
>>> RESULTS
>>> Files associated with recent large batches of operations are there/ 
>>> not
>>> there with subsequent "ls" listings.  If no new userland operations
>>> are pending on the file, the file should either be there or not  
>>> there,
>>> not in limbo.
>>>
>>> REGRESSION
>>> I haven't checked this with 102A or other revs, just 111.
>>>
>>> NOTES
>>> Running current 111 release on a MacBook, with a single pool on a
>>> single partition.
>>>
>>> --
>>> James Snyder
>>> Biomedical Engineering
>>> Northwestern University
>>> jbsnyder@gmail.com
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>>
>>
>
>
>
> -- 
> James Snyder
> Biomedical Engineering
> Northwestern University
> jbsnyder@gmail.com

-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080422/a296d073/PGP.bin
From jbsnyder at gmail.com  Tue Apr 22 09:15:01 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Tue Apr 22 09:12:26 2008
Subject: [zfs-discuss] weird zfs thread synchronization issue?
In-Reply-To: <9E99B62A-A904-4F96-A832-4C2DA31FDB38@martin-hauser.net>
References: <33644d3c0804220631ife68103y48fd9b5cda3baaa3@mail.gmail.com>
	<9DACCDE5-F405-4C92-BF22-17EF9DD27333@martin-hauser.net>
	<33644d3c0804220649r507d81b7u3a6a8f42064e2c2a@mail.gmail.com>
	<9E99B62A-A904-4F96-A832-4C2DA31FDB38@martin-hauser.net>
Message-ID: <33644d3c0804220915j6f497719l9b8fbd515dd4c2b0@mail.gmail.com>

Might I ask if you're passing any special options to git status, or
how you're doing your alias?

If I run the attached (somewhat ugly) script, I intermittently get
things like the following, usually within the first 5 runs or so:
# On branch master
# Changed but not updated:
#   (use "git add <file>..." to update what will be committed)
#
#	modified:   testme.txt
#
no changes added to commit (use "git add" and/or "git commit -a")
error: 'testme.txt' has local modifications
(use --cached to keep the file, or -f to force removal)


I can't get the same to happen when I try this on HFS+.

git_test.sh
#!/bin/zsh

# anything more than the following, indicates a problem:
# # On branch master
# nothing to commit (working directory clean)
# rm 'testme.txt'

for i in {1..10}
do
	echo "RUN: $i"
	DIRNAME=`dd if=/dev/urandom bs=30 count=1 | md5` &>/dev/null
	mkdir $DIRNAME
	cd $DIRNAME
	git init &>/dev/null
	echo hi > there &>/dev/null
	git add there &>/dev/null
	git commit -m 'Small repo' &>/dev/null
	dd if=/dev/urandom of=testme.txt count=10 bs=1m &>/dev/null
	git add testme.txt &>/dev/null
	git commit -m 'Add big binary file' &>/dev/null
	sleep 1
	git status; git rm testme.txt
	git commit -m 'delete binary file' &>/dev/null
	cd ..
	rm -rf $DIRNAME
	echo "\n"
done


On Tue, Apr 22, 2008 at 9:20 AM, Martin Hauser <info@martin-hauser.net> wrote:
> Everything seemed pretty consistent till now. The only thing I've ever
> notice that is all git status related things (so the listing of the files in
> git-commit, the commiting itself works fine). I would assume that the git's
> index file is treated special somehow and doing a git diff rewrites that
> file....
>
>  Let me know if you find out more.
>
>  Martin
>
>
>
>  On Apr 22, 2008, at 15:49 PM, James Snyder wrote:
>
>
> > I'll give that a try.  I'm not 100% sure that that will resolve the
> > problem though.
> >
> > Does that seem to work consistently or do you find yourself having to
> > sometimes do things multiple times?
> >
> > I'm not sure what git is doing behind the scenes during a status
> > check.  Is it refreshing the index somehow that obviates a little of
> > this limbo file state issue?
> >
> > I haven't checked entirely whether this happens for creating, deleting
> > and modifying, but if it happens on modifications too, that could lead
> > to some messy issues.
> >
> > Example: Say, I was working with an sqlite database, made some
> > changes, unlocked things, and subsequently another process hit the
> > database and made changes but got the old version of the file and
> > worked with that.
> >
> > On Tue, Apr 22, 2008 at 8:36 AM, Martin Hauser <info@martin-hauser.net>
> wrote:
> >
> > > Hi there,
> > >
> > > more on that issue,
> > >
> > > I've got some problems with it as well, but mostly due to git not
> catching
> > > commits immediatlly as such. For example:
> > >
> > > 1. do some work
> > > 2. git add somefile
> > > 3. git commit -m "some commit message"
> > > 4. git status
> > >
> > > you'll notice that the file you added for commiting is still listed
> > >
> > > For me, it was easy enough to 'work around' just by doing a git status
> > > before every index file related operation (just a small alias in my
> .zshrc),
> > > might be work-around-ing your problem also James?
> > >
> > > I'd be much prefering the issue to be fixed though.
> > >
> > > Martin
> > >
> > >
> > >
> > > On Apr 22, 2008, at 15:31 PM, James Snyder wrote:
> > >
> > >
> > >
> > > >
> > > >
> > > >
> > > > SUMMARY
> > > > During the following situations:
> > > > git reset --hard <revision>
> > > > git-svn clone ... (on a large svn repository)
> > > > (sometimes followed by)
> > > > rm -rf <some directory> (after just working with a lot of files, like
> > > > one of the above commands)
> > > >
> > > > The listed git commands sometimes fail, sometimes succeed, citing
> > > > sometimes missing files.  With rm -rf, shortly after the operation an
> > > > ls or other file operation will sometimes show certain
> > > > directories/files just operated on and sometimes not.  This may or may
> > > > not stabilize in the subsequent seconds.  This behavior leaves me with
> > > > the sense that the file operations are talking to different threads
> > > > associated with ZFS and that they're not all on the "same page"
> > > >
> > > > STEPS TO REPRODUCE
> > > > 1. Find a rather large svn repository, with lots of revisions and
> > > > files (or make one).  The repo I'm working with has lots of binary
> > > > files in it and the db for svn is around 9 gigabytes with 4000
> > > > revisions or so.
> > > > 2. Install latest git w/ svn support from macports. (sudo port install
> git
> > > >
> > > +svn)
> > >
> > > > 3. do a git-svn clone on your repository
> > > > 4. With the current version of git-svn, it should try and repack files
> > > > at different stages during the clone process if you have enough
> > > > revisions, you will likely see a failure after this point when it
> > > > tries to pick up and check out the next increment from svn.
> > > > 5. Quickly look for the files it says it cannot find in the .git
> > > > directory of your partial clone, you should find that sometimes it is
> > > > there and sometimes not.
> > > >
> > > > RESULTS
> > > > Files associated with recent large batches of operations are there/not
> > > > there with subsequent "ls" listings.  If no new userland operations
> > > > are pending on the file, the file should either be there or not there,
> > > > not in limbo.
> > > >
> > > > REGRESSION
> > > > I haven't checked this with 102A or other revs, just 111.
> > > >
> > > > NOTES
> > > > Running current 111 release on a MacBook, with a single pool on a
> > > > single partition.
> > > >
> > > > --
> > > > James Snyder
> > > > Biomedical Engineering
> > > > Northwestern University
> > > > jbsnyder@gmail.com
> > > > _______________________________________________
> > > > zfs-discuss mailing list
> > > > zfs-discuss@lists.macosforge.org
> > > > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> > > >
> > > >
> > >
> > >
> > >
> >
> >
> >
> > --
> > James Snyder
> > Biomedical Engineering
> > Northwestern University
> > jbsnyder@gmail.com
> >
>
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
From jbsnyder at gmail.com  Tue Apr 22 10:30:05 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Tue Apr 22 10:27:36 2008
Subject: [zfs-discuss] weird zfs thread synchronization issue?
In-Reply-To: <33644d3c0804220915j6f497719l9b8fbd515dd4c2b0@mail.gmail.com>
References: <33644d3c0804220631ife68103y48fd9b5cda3baaa3@mail.gmail.com>
	<9DACCDE5-F405-4C92-BF22-17EF9DD27333@martin-hauser.net>
	<33644d3c0804220649r507d81b7u3a6a8f42064e2c2a@mail.gmail.com>
	<9E99B62A-A904-4F96-A832-4C2DA31FDB38@martin-hauser.net>
	<33644d3c0804220915j6f497719l9b8fbd515dd4c2b0@mail.gmail.com>
Message-ID: <33644d3c0804221030o66c453e3y79e7c791d783f97a@mail.gmail.com>

More weirdness...

sample repo that is broken on zfs, not on hfs+...
http://fanplastic.org/files/weird_repo.tgz


The linked repo (just has one file with random data in it), gives me
this for git status on HFS+:
# On branch master
nothing to commit (working directory clean)

and this on ZFS:
# On branch master
# Changed but not updated:
#   (use "git add <file>..." to update what will be committed)
#
#	modified:   testingfile.txt
#

any ideas?  Try moving the unzipped dir between zfs and hfs+...
git-status -a gets things right on zfs, but git-status does not

On Tue, Apr 22, 2008 at 11:15 AM, James Snyder <jbsnyder@gmail.com> wrote:
> Might I ask if you're passing any special options to git status, or
>  how you're doing your alias?
>
>  If I run the attached (somewhat ugly) script, I intermittently get
>  things like the following, usually within the first 5 runs or so:
>  # On branch master
>  # Changed but not updated:
>  #   (use "git add <file>..." to update what will be committed)
>  #
>  #       modified:   testme.txt
>  #
>  no changes added to commit (use "git add" and/or "git commit -a")
>  error: 'testme.txt' has local modifications
>  (use --cached to keep the file, or -f to force removal)
>
>
>  I can't get the same to happen when I try this on HFS+.
>
>  git_test.sh
>  #!/bin/zsh
>
>  # anything more than the following, indicates a problem:
>  # # On branch master
>  # nothing to commit (working directory clean)
>  # rm 'testme.txt'
>
>  for i in {1..10}
>  do
>         echo "RUN: $i"
>         DIRNAME=`dd if=/dev/urandom bs=30 count=1 | md5` &>/dev/null
>         mkdir $DIRNAME
>         cd $DIRNAME
>         git init &>/dev/null
>         echo hi > there &>/dev/null
>         git add there &>/dev/null
>         git commit -m 'Small repo' &>/dev/null
>         dd if=/dev/urandom of=testme.txt count=10 bs=1m &>/dev/null
>         git add testme.txt &>/dev/null
>         git commit -m 'Add big binary file' &>/dev/null
>         sleep 1
>         git status; git rm testme.txt
>         git commit -m 'delete binary file' &>/dev/null
>         cd ..
>         rm -rf $DIRNAME
>         echo "\n"
>  done
>
>
>
>
>  On Tue, Apr 22, 2008 at 9:20 AM, Martin Hauser <info@martin-hauser.net> wrote:
>  > Everything seemed pretty consistent till now. The only thing I've ever
>  > notice that is all git status related things (so the listing of the files in
>  > git-commit, the commiting itself works fine). I would assume that the git's
>  > index file is treated special somehow and doing a git diff rewrites that
>  > file....
>  >
>  >  Let me know if you find out more.
>  >
>  >  Martin
>  >
>  >
>  >
>  >  On Apr 22, 2008, at 15:49 PM, James Snyder wrote:
>  >
>  >
>  > > I'll give that a try.  I'm not 100% sure that that will resolve the
>  > > problem though.
>  > >
>  > > Does that seem to work consistently or do you find yourself having to
>  > > sometimes do things multiple times?
>  > >
>  > > I'm not sure what git is doing behind the scenes during a status
>  > > check.  Is it refreshing the index somehow that obviates a little of
>  > > this limbo file state issue?
>  > >
>  > > I haven't checked entirely whether this happens for creating, deleting
>  > > and modifying, but if it happens on modifications too, that could lead
>  > > to some messy issues.
>  > >
>  > > Example: Say, I was working with an sqlite database, made some
>  > > changes, unlocked things, and subsequently another process hit the
>  > > database and made changes but got the old version of the file and
>  > > worked with that.
>  > >
>  > > On Tue, Apr 22, 2008 at 8:36 AM, Martin Hauser <info@martin-hauser.net>
>  > wrote:
>  > >
>  > > > Hi there,
>  > > >
>  > > > more on that issue,
>  > > >
>  > > > I've got some problems with it as well, but mostly due to git not
>  > catching
>  > > > commits immediatlly as such. For example:
>  > > >
>  > > > 1. do some work
>  > > > 2. git add somefile
>  > > > 3. git commit -m "some commit message"
>  > > > 4. git status
>  > > >
>  > > > you'll notice that the file you added for commiting is still listed
>  > > >
>  > > > For me, it was easy enough to 'work around' just by doing a git status
>  > > > before every index file related operation (just a small alias in my
>  > .zshrc),
>  > > > might be work-around-ing your problem also James?
>  > > >
>  > > > I'd be much prefering the issue to be fixed though.
>  > > >
>  > > > Martin
>  > > >
>  > > >
>  > > >
>  > > > On Apr 22, 2008, at 15:31 PM, James Snyder wrote:
>  > > >
>  > > >
>  > > >
>  > > > >
>  > > > >
>  > > > >
>  > > > > SUMMARY
>  > > > > During the following situations:
>  > > > > git reset --hard <revision>
>  > > > > git-svn clone ... (on a large svn repository)
>  > > > > (sometimes followed by)
>  > > > > rm -rf <some directory> (after just working with a lot of files, like
>  > > > > one of the above commands)
>  > > > >
>  > > > > The listed git commands sometimes fail, sometimes succeed, citing
>  > > > > sometimes missing files.  With rm -rf, shortly after the operation an
>  > > > > ls or other file operation will sometimes show certain
>  > > > > directories/files just operated on and sometimes not.  This may or may
>  > > > > not stabilize in the subsequent seconds.  This behavior leaves me with
>  > > > > the sense that the file operations are talking to different threads
>  > > > > associated with ZFS and that they're not all on the "same page"
>  > > > >
>  > > > > STEPS TO REPRODUCE
>  > > > > 1. Find a rather large svn repository, with lots of revisions and
>  > > > > files (or make one).  The repo I'm working with has lots of binary
>  > > > > files in it and the db for svn is around 9 gigabytes with 4000
>  > > > > revisions or so.
>  > > > > 2. Install latest git w/ svn support from macports. (sudo port install
>  > git
>  > > > >
>  > > > +svn)
>  > > >
>  > > > > 3. do a git-svn clone on your repository
>  > > > > 4. With the current version of git-svn, it should try and repack files
>  > > > > at different stages during the clone process if you have enough
>  > > > > revisions, you will likely see a failure after this point when it
>  > > > > tries to pick up and check out the next increment from svn.
>  > > > > 5. Quickly look for the files it says it cannot find in the .git
>  > > > > directory of your partial clone, you should find that sometimes it is
>  > > > > there and sometimes not.
>  > > > >
>  > > > > RESULTS
>  > > > > Files associated with recent large batches of operations are there/not
>  > > > > there with subsequent "ls" listings.  If no new userland operations
>  > > > > are pending on the file, the file should either be there or not there,
>  > > > > not in limbo.
>  > > > >
>  > > > > REGRESSION
>  > > > > I haven't checked this with 102A or other revs, just 111.
>  > > > >
>  > > > > NOTES
>  > > > > Running current 111 release on a MacBook, with a single pool on a
>  > > > > single partition.
>  > > > >
>  > > > > --
>  > > > > James Snyder
>  > > > > Biomedical Engineering
>  > > > > Northwestern University
>  > > > > jbsnyder@gmail.com
>  > > > > _______________________________________________
>  > > > > zfs-discuss mailing list
>  > > > > zfs-discuss@lists.macosforge.org
>  > > > > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>  > > > >
>  > > > >
>  > > >
>  > > >
>  > > >
>  > >
>  > >
>  > >
>  > > --
>  > > James Snyder
>  > > Biomedical Engineering
>  > > Northwestern University
>  > > jbsnyder@gmail.com
>  > >
>  >
>  >
>
>
>
>  --
>
>
> James Snyder
>  Biomedical Engineering
>  Northwestern University
>  jbsnyder@gmail.com
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
From jbsnyder at gmail.com  Tue Apr 22 18:10:13 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Tue Apr 22 18:07:40 2008
Subject: [zfs-discuss] weird zfs thread synchronization issue?
In-Reply-To: <33644d3c0804221030o66c453e3y79e7c791d783f97a@mail.gmail.com>
References: <33644d3c0804220631ife68103y48fd9b5cda3baaa3@mail.gmail.com>
	<9DACCDE5-F405-4C92-BF22-17EF9DD27333@martin-hauser.net>
	<33644d3c0804220649r507d81b7u3a6a8f42064e2c2a@mail.gmail.com>
	<9E99B62A-A904-4F96-A832-4C2DA31FDB38@martin-hauser.net>
	<33644d3c0804220915j6f497719l9b8fbd515dd4c2b0@mail.gmail.com>
	<33644d3c0804221030o66c453e3y79e7c791d783f97a@mail.gmail.com>
Message-ID: <33644d3c0804221810t36b8e3f5mcce4701368faf9c0@mail.gmail.com>

It looks like Git 1.5.5.1 handles the situation a little better.  The
repositories that get into the state of the one I zipped up appear to
function again on ZFS.  I wonder if this has something to do with some
of the changes in lstat behavior in the 1.5.5 revision?

That said, there's definitely still something going on.  I've had
builds in MacPorts get hung up on similar "files in limbo" issues.
Sometimes waiting a bit helps, sometimes re-rm'ing files seems to do
the trick.  A reboot also helped in one case where something got stuck
and I couldn't get a limbo file to be properly deleted.  I suppose
this doesn't have anything to do with mmap stuff, but it does sound
like some sort of caching issue?  Perhaps associated with the arc or
the zil?  Is there any way to manually flush things to disk or force
things to sync up aside from "sync"?

I don't see any external tunables like on freebsd for zfs, but is
there a way to disable the zil, prefetch or other useful but
not-quite-essential aspects of ZFS on Mac OS X?

On Tue, Apr 22, 2008 at 12:30 PM, James Snyder <jbsnyder@gmail.com> wrote:
> More weirdness...
>
>  sample repo that is broken on zfs, not on hfs+...
>  http://fanplastic.org/files/weird_repo.tgz
>
>
>  The linked repo (just has one file with random data in it), gives me
>  this for git status on HFS+:
>
> # On branch master
>  nothing to commit (working directory clean)
>
>  and this on ZFS:
>
> # On branch master
>  # Changed but not updated:
>  #   (use "git add <file>..." to update what will be committed)
>  #
>  #       modified:   testingfile.txt
>  #
>
>  any ideas?  Try moving the unzipped dir between zfs and hfs+...
>  git-status -a gets things right on zfs, but git-status does not
>
>
>
>  On Tue, Apr 22, 2008 at 11:15 AM, James Snyder <jbsnyder@gmail.com> wrote:
>  > Might I ask if you're passing any special options to git status, or
>  >  how you're doing your alias?
>  >
>  >  If I run the attached (somewhat ugly) script, I intermittently get
>  >  things like the following, usually within the first 5 runs or so:
>  >  # On branch master
>  >  # Changed but not updated:
>  >  #   (use "git add <file>..." to update what will be committed)
>  >  #
>  >  #       modified:   testme.txt
>  >  #
>  >  no changes added to commit (use "git add" and/or "git commit -a")
>  >  error: 'testme.txt' has local modifications
>  >  (use --cached to keep the file, or -f to force removal)
>  >
>  >
>  >  I can't get the same to happen when I try this on HFS+.
>  >
>  >  git_test.sh
>  >  #!/bin/zsh
>  >
>  >  # anything more than the following, indicates a problem:
>  >  # # On branch master
>  >  # nothing to commit (working directory clean)
>  >  # rm 'testme.txt'
>  >
>  >  for i in {1..10}
>  >  do
>  >         echo "RUN: $i"
>  >         DIRNAME=`dd if=/dev/urandom bs=30 count=1 | md5` &>/dev/null
>  >         mkdir $DIRNAME
>  >         cd $DIRNAME
>  >         git init &>/dev/null
>  >         echo hi > there &>/dev/null
>  >         git add there &>/dev/null
>  >         git commit -m 'Small repo' &>/dev/null
>  >         dd if=/dev/urandom of=testme.txt count=10 bs=1m &>/dev/null
>  >         git add testme.txt &>/dev/null
>  >         git commit -m 'Add big binary file' &>/dev/null
>  >         sleep 1
>  >         git status; git rm testme.txt
>  >         git commit -m 'delete binary file' &>/dev/null
>  >         cd ..
>  >         rm -rf $DIRNAME
>  >         echo "\n"
>  >  done
>  >
>  >
>  >
>  >
>  >  On Tue, Apr 22, 2008 at 9:20 AM, Martin Hauser <info@martin-hauser.net> wrote:
>  >  > Everything seemed pretty consistent till now. The only thing I've ever
>  >  > notice that is all git status related things (so the listing of the files in
>  >  > git-commit, the commiting itself works fine). I would assume that the git's
>  >  > index file is treated special somehow and doing a git diff rewrites that
>  >  > file....
>  >  >
>  >  >  Let me know if you find out more.
>  >  >
>  >  >  Martin
>  >  >
>  >  >
>  >  >
>  >  >  On Apr 22, 2008, at 15:49 PM, James Snyder wrote:
>  >  >
>  >  >
>  >  > > I'll give that a try.  I'm not 100% sure that that will resolve the
>  >  > > problem though.
>  >  > >
>  >  > > Does that seem to work consistently or do you find yourself having to
>  >  > > sometimes do things multiple times?
>  >  > >
>  >  > > I'm not sure what git is doing behind the scenes during a status
>  >  > > check.  Is it refreshing the index somehow that obviates a little of
>  >  > > this limbo file state issue?
>  >  > >
>  >  > > I haven't checked entirely whether this happens for creating, deleting
>  >  > > and modifying, but if it happens on modifications too, that could lead
>  >  > > to some messy issues.
>  >  > >
>  >  > > Example: Say, I was working with an sqlite database, made some
>  >  > > changes, unlocked things, and subsequently another process hit the
>  >  > > database and made changes but got the old version of the file and
>  >  > > worked with that.
>  >  > >
>  >  > > On Tue, Apr 22, 2008 at 8:36 AM, Martin Hauser <info@martin-hauser.net>
>  >  > wrote:
>  >  > >
>  >  > > > Hi there,
>  >  > > >
>  >  > > > more on that issue,
>  >  > > >
>  >  > > > I've got some problems with it as well, but mostly due to git not
>  >  > catching
>  >  > > > commits immediatlly as such. For example:
>  >  > > >
>  >  > > > 1. do some work
>  >  > > > 2. git add somefile
>  >  > > > 3. git commit -m "some commit message"
>  >  > > > 4. git status
>  >  > > >
>  >  > > > you'll notice that the file you added for commiting is still listed
>  >  > > >
>  >  > > > For me, it was easy enough to 'work around' just by doing a git status
>  >  > > > before every index file related operation (just a small alias in my
>  >  > .zshrc),
>  >  > > > might be work-around-ing your problem also James?
>  >  > > >
>  >  > > > I'd be much prefering the issue to be fixed though.
>  >  > > >
>  >  > > > Martin
>  >  > > >
>  >  > > >
>  >  > > >
>  >  > > > On Apr 22, 2008, at 15:31 PM, James Snyder wrote:
>  >  > > >
>  >  > > >
>  >  > > >
>  >  > > > >
>  >  > > > >
>  >  > > > >
>  >  > > > > SUMMARY
>  >  > > > > During the following situations:
>  >  > > > > git reset --hard <revision>
>  >  > > > > git-svn clone ... (on a large svn repository)
>  >  > > > > (sometimes followed by)
>  >  > > > > rm -rf <some directory> (after just working with a lot of files, like
>  >  > > > > one of the above commands)
>  >  > > > >
>  >  > > > > The listed git commands sometimes fail, sometimes succeed, citing
>  >  > > > > sometimes missing files.  With rm -rf, shortly after the operation an
>  >  > > > > ls or other file operation will sometimes show certain
>  >  > > > > directories/files just operated on and sometimes not.  This may or may
>  >  > > > > not stabilize in the subsequent seconds.  This behavior leaves me with
>  >  > > > > the sense that the file operations are talking to different threads
>  >  > > > > associated with ZFS and that they're not all on the "same page"
>  >  > > > >
>  >  > > > > STEPS TO REPRODUCE
>  >  > > > > 1. Find a rather large svn repository, with lots of revisions and
>  >  > > > > files (or make one).  The repo I'm working with has lots of binary
>  >  > > > > files in it and the db for svn is around 9 gigabytes with 4000
>  >  > > > > revisions or so.
>  >  > > > > 2. Install latest git w/ svn support from macports. (sudo port install
>  >  > git
>  >  > > > >
>  >  > > > +svn)
>  >  > > >
>  >  > > > > 3. do a git-svn clone on your repository
>  >  > > > > 4. With the current version of git-svn, it should try and repack files
>  >  > > > > at different stages during the clone process if you have enough
>  >  > > > > revisions, you will likely see a failure after this point when it
>  >  > > > > tries to pick up and check out the next increment from svn.
>  >  > > > > 5. Quickly look for the files it says it cannot find in the .git
>  >  > > > > directory of your partial clone, you should find that sometimes it is
>  >  > > > > there and sometimes not.
>  >  > > > >
>  >  > > > > RESULTS
>  >  > > > > Files associated with recent large batches of operations are there/not
>  >  > > > > there with subsequent "ls" listings.  If no new userland operations
>  >  > > > > are pending on the file, the file should either be there or not there,
>  >  > > > > not in limbo.
>  >  > > > >
>  >  > > > > REGRESSION
>  >  > > > > I haven't checked this with 102A or other revs, just 111.
>  >  > > > >
>  >  > > > > NOTES
>  >  > > > > Running current 111 release on a MacBook, with a single pool on a
>  >  > > > > single partition.
>  >  > > > >
>  >  > > > > --
>  >  > > > > James Snyder
>  >  > > > > Biomedical Engineering
>  >  > > > > Northwestern University
>  >  > > > > jbsnyder@gmail.com
>  >  > > > > _______________________________________________
>  >  > > > > zfs-discuss mailing list
>  >  > > > > zfs-discuss@lists.macosforge.org
>  >  > > > > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>  >  > > > >
>  >  > > > >
>  >  > > >
>  >  > > >
>  >  > > >
>  >  > >
>  >  > >
>  >  > >
>  >  > > --
>  >  > > James Snyder
>  >  > > Biomedical Engineering
>  >  > > Northwestern University
>  >  > > jbsnyder@gmail.com
>  >  > >
>  >  >
>  >  >
>  >
>  >
>  >
>  >  --
>  >
>  >
>  > James Snyder
>  >  Biomedical Engineering
>  >  Northwestern University
>  >  jbsnyder@gmail.com
>  >
>
>
>
>  --
>
>
> James Snyder
>  Biomedical Engineering
>  Northwestern University
>  jbsnyder@gmail.com
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
From jbsnyder at gmail.com  Tue Apr 22 20:55:15 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Tue Apr 22 20:52:39 2008
Subject: [zfs-discuss] weird zfs thread synchronization issue? (perhaps
	mmap is the culprit?)
Message-ID: <33644d3c0804222055t29a6293fsa6f23fefc8dc5aff@mail.gmail.com>

I know I've already thrown out a fair amount of data here, but I've
played with this some more.

Building git with NO_MMAP enabled (which, as one might guess) disables
the use of direct mmap syscalls in the git code, and seems to work
around this problem.  I no longer get any errors on the git rm
command.

Side note: Weirdly, there are still mmap calls that show up when I
dtruss  the "git rm testme.txt" from the script I posted.  At the same
time, collecting stack traces for mmap system calls turns up nothing
in Instruments, where before without NO_MMAP disabled it logged a
number of calls. Perhaps mmap is getting called through something
else?

If you would like to try this yourself, and you built git from
macports, you can replace the following file:

/opt/local/var/macports/sources/rsync.macports.org/release/ports/devel/git-core/files/patch-Makefile.diff
--- Makefile.orig   2008-04-22 22:25:26.000000000 -0500
+++ Makefile    2008-04-22 22:26:20.000000000 -0500
@@ -164,8 +164,6 @@

 # CFLAGS and LDFLAGS are for the users to override from the command line.

-CFLAGS = -g -O2 -Wall
-LDFLAGS =
 ALL_CFLAGS = $(CFLAGS)
 ALL_LDFLAGS = $(LDFLAGS)
 STRIP ?= strip
@@ -682,6 +680,7 @@
            BASIC_LDFLAGS += -L/opt/local/lib
        endif
    endif
+   NO_MMAP = YesPlease
 endif

 ifdef NO_R_TO_GCC_LINKER
@@ -689,7 +688,7 @@
    # the runtime dynamic library path.
    CC_LD_DYNPATH = -Wl,-rpath=
 else
-   CC_LD_DYNPATH = -R
+   CC_LD_DYNPATH = -L
 endif

 ifdef NO_CURL

and also patch the git-core Portfile with this diff:
/opt/local/var/macports/sources/rsync.macports.org/release/ports/devel/git-core/Portfile
--- Portfile.orig	2008-04-22 22:48:33.000000000 -0500
+++ Portfile	2008-04-22 22:49:12.000000000 -0500
@@ -3,7 +3,7 @@
 PortSystem        1.0

 name              git-core
-version           1.5.4.5
+version           1.5.5.1
 description       The stupid content tracker.
 long_description  A stupid (but extremely fast) directory content manager. \
                   It doesn't do a whole lot, but what it _does_ do is track \
@@ -18,8 +18,8 @@
 distfiles         git-${version}${extract.suffix} \
                   git-manpages-${version}${extract.suffix}

-checksums    git-${version}${extract.suffix} sha1
69c4b904f13b72f57405393d54f33831c9cfad8f \
-             git-manpages-${version}${extract.suffix} sha1
659b5217b342b757a01603f61bd90a4d60f7e681
+checksums    git-${version}${extract.suffix} sha1
a450cd02cbfe8e18311816a9568bcd6d10d6cf52 \
+             git-manpages-${version}${extract.suffix} sha1
e062f2eb9e46546616e9cc62ecc4bcd485d30d9a

 depends_run  port:openssh port:rsync port:perl5.8 port:p5-error
 depends_lib  port:curl port:zlib port:openssl port:expat port:libiconv
@@ -74,7 +74,7 @@

 variant doc description {Install HTML and plaintext documentation} {
     distfiles-append    git-htmldocs-${version}${extract.suffix}
-    checksums-append    git-htmldocs-${version}${extract.suffix} sha1
7087b54af84b8d91fec8d9a48c8f746047d3cd6a
+    checksums-append    git-htmldocs-${version}${extract.suffix} sha1
56ae3fa2e5a7c64177af5122876f5fe063800b40
 }

 variant gitweb description {Install gitweb.cgi} {

So far, so good.

On Tue, Apr 22, 2008 at 8:10 PM, James Snyder <jbsnyder@gmail.com> wrote:
> It looks like Git 1.5.5.1 handles the situation a little better.  The
>  repositories that get into the state of the one I zipped up appear to
>  function again on ZFS.  I wonder if this has something to do with some
>  of the changes in lstat behavior in the 1.5.5 revision?
>
>  That said, there's definitely still something going on.  I've had
>  builds in MacPorts get hung up on similar "files in limbo" issues.
>  Sometimes waiting a bit helps, sometimes re-rm'ing files seems to do
>  the trick.  A reboot also helped in one case where something got stuck
>  and I couldn't get a limbo file to be properly deleted.  I suppose
>  this doesn't have anything to do with mmap stuff, but it does sound
>  like some sort of caching issue?  Perhaps associated with the arc or
>  the zil?  Is there any way to manually flush things to disk or force
>  things to sync up aside from "sync"?
>
>  I don't see any external tunables like on freebsd for zfs, but is
>  there a way to disable the zil, prefetch or other useful but
>  not-quite-essential aspects of ZFS on Mac OS X?
>
>
>
>  On Tue, Apr 22, 2008 at 12:30 PM, James Snyder <jbsnyder@gmail.com> wrote:
>  > More weirdness...
>  >
>  >  sample repo that is broken on zfs, not on hfs+...
>  >  http://fanplastic.org/files/weird_repo.tgz
>  >
>  >
>  >  The linked repo (just has one file with random data in it), gives me
>  >  this for git status on HFS+:
>  >
>  > # On branch master
>  >  nothing to commit (working directory clean)
>  >
>  >  and this on ZFS:
>  >
>  > # On branch master
>  >  # Changed but not updated:
>  >  #   (use "git add <file>..." to update what will be committed)
>  >  #
>  >  #       modified:   testingfile.txt
>  >  #
>  >
>  >  any ideas?  Try moving the unzipped dir between zfs and hfs+...
>  >  git-status -a gets things right on zfs, but git-status does not
>  >
>  >
>  >
>  >  On Tue, Apr 22, 2008 at 11:15 AM, James Snyder <jbsnyder@gmail.com> wrote:
>  >  > Might I ask if you're passing any special options to git status, or
>  >  >  how you're doing your alias?
>  >  >
>  >  >  If I run the attached (somewhat ugly) script, I intermittently get
>  >  >  things like the following, usually within the first 5 runs or so:
>  >  >  # On branch master
>  >  >  # Changed but not updated:
>  >  >  #   (use "git add <file>..." to update what will be committed)
>  >  >  #
>  >  >  #       modified:   testme.txt
>  >  >  #
>  >  >  no changes added to commit (use "git add" and/or "git commit -a")
>  >  >  error: 'testme.txt' has local modifications
>  >  >  (use --cached to keep the file, or -f to force removal)
>  >  >
>  >  >
>  >  >  I can't get the same to happen when I try this on HFS+.
>  >  >
>  >  >  git_test.sh
>  >  >  #!/bin/zsh
>  >  >
>  >  >  # anything more than the following, indicates a problem:
>  >  >  # # On branch master
>  >  >  # nothing to commit (working directory clean)
>  >  >  # rm 'testme.txt'
>  >  >
>  >  >  for i in {1..10}
>  >  >  do
>  >  >         echo "RUN: $i"
>  >  >         DIRNAME=`dd if=/dev/urandom bs=30 count=1 | md5` &>/dev/null
>  >  >         mkdir $DIRNAME
>  >  >         cd $DIRNAME
>  >  >         git init &>/dev/null
>  >  >         echo hi > there &>/dev/null
>  >  >         git add there &>/dev/null
>  >  >         git commit -m 'Small repo' &>/dev/null
>  >  >         dd if=/dev/urandom of=testme.txt count=10 bs=1m &>/dev/null
>  >  >         git add testme.txt &>/dev/null
>  >  >         git commit -m 'Add big binary file' &>/dev/null
>  >  >         sleep 1
>  >  >         git status; git rm testme.txt
>  >  >         git commit -m 'delete binary file' &>/dev/null
>  >  >         cd ..
>  >  >         rm -rf $DIRNAME
>  >  >         echo "\n"
>  >  >  done
>  >  >
>  >  >
>  >  >
>  >  >
>  >  >  On Tue, Apr 22, 2008 at 9:20 AM, Martin Hauser <info@martin-hauser.net> wrote:
>  >  >  > Everything seemed pretty consistent till now. The only thing I've ever
>  >  >  > notice that is all git status related things (so the listing of the files in
>  >  >  > git-commit, the commiting itself works fine). I would assume that the git's
>  >  >  > index file is treated special somehow and doing a git diff rewrites that
>  >  >  > file....
>  >  >  >
>  >  >  >  Let me know if you find out more.
>  >  >  >
>  >  >  >  Martin
>  >  >  >
>  >  >  >
>  >  >  >
>  >  >  >  On Apr 22, 2008, at 15:49 PM, James Snyder wrote:
>  >  >  >
>  >  >  >
>  >  >  > > I'll give that a try.  I'm not 100% sure that that will resolve the
>  >  >  > > problem though.
>  >  >  > >
>  >  >  > > Does that seem to work consistently or do you find yourself having to
>  >  >  > > sometimes do things multiple times?
>  >  >  > >
>  >  >  > > I'm not sure what git is doing behind the scenes during a status
>  >  >  > > check.  Is it refreshing the index somehow that obviates a little of
>  >  >  > > this limbo file state issue?
>  >  >  > >
>  >  >  > > I haven't checked entirely whether this happens for creating, deleting
>  >  >  > > and modifying, but if it happens on modifications too, that could lead
>  >  >  > > to some messy issues.
>  >  >  > >
>  >  >  > > Example: Say, I was working with an sqlite database, made some
>  >  >  > > changes, unlocked things, and subsequently another process hit the
>  >  >  > > database and made changes but got the old version of the file and
>  >  >  > > worked with that.
>  >  >  > >
>  >  >  > > On Tue, Apr 22, 2008 at 8:36 AM, Martin Hauser <info@martin-hauser.net>
>  >  >  > wrote:
>  >  >  > >
>  >  >  > > > Hi there,
>  >  >  > > >
>  >  >  > > > more on that issue,
>  >  >  > > >
>  >  >  > > > I've got some problems with it as well, but mostly due to git not
>  >  >  > catching
>  >  >  > > > commits immediatlly as such. For example:
>  >  >  > > >
>  >  >  > > > 1. do some work
>  >  >  > > > 2. git add somefile
>  >  >  > > > 3. git commit -m "some commit message"
>  >  >  > > > 4. git status
>  >  >  > > >
>  >  >  > > > you'll notice that the file you added for commiting is still listed
>  >  >  > > >
>  >  >  > > > For me, it was easy enough to 'work around' just by doing a git status
>  >  >  > > > before every index file related operation (just a small alias in my
>  >  >  > .zshrc),
>  >  >  > > > might be work-around-ing your problem also James?
>  >  >  > > >
>  >  >  > > > I'd be much prefering the issue to be fixed though.
>  >  >  > > >
>  >  >  > > > Martin
>  >  >  > > >
>  >  >  > > >
>  >  >  > > >
>  >  >  > > > On Apr 22, 2008, at 15:31 PM, James Snyder wrote:
>  >  >  > > >
>  >  >  > > >
>  >  >  > > >
>  >  >  > > > >
>  >  >  > > > >
>  >  >  > > > >
>  >  >  > > > > SUMMARY
>  >  >  > > > > During the following situations:
>  >  >  > > > > git reset --hard <revision>
>  >  >  > > > > git-svn clone ... (on a large svn repository)
>  >  >  > > > > (sometimes followed by)
>  >  >  > > > > rm -rf <some directory> (after just working with a lot of files, like
>  >  >  > > > > one of the above commands)
>  >  >  > > > >
>  >  >  > > > > The listed git commands sometimes fail, sometimes succeed, citing
>  >  >  > > > > sometimes missing files.  With rm -rf, shortly after the operation an
>  >  >  > > > > ls or other file operation will sometimes show certain
>  >  >  > > > > directories/files just operated on and sometimes not.  This may or may
>  >  >  > > > > not stabilize in the subsequent seconds.  This behavior leaves me with
>  >  >  > > > > the sense that the file operations are talking to different threads
>  >  >  > > > > associated with ZFS and that they're not all on the "same page"
>  >  >  > > > >
>  >  >  > > > > STEPS TO REPRODUCE
>  >  >  > > > > 1. Find a rather large svn repository, with lots of revisions and
>  >  >  > > > > files (or make one).  The repo I'm working with has lots of binary
>  >  >  > > > > files in it and the db for svn is around 9 gigabytes with 4000
>  >  >  > > > > revisions or so.
>  >  >  > > > > 2. Install latest git w/ svn support from macports. (sudo port install
>  >  >  > git
>  >  >  > > > >
>  >  >  > > > +svn)
>  >  >  > > >
>  >  >  > > > > 3. do a git-svn clone on your repository
>  >  >  > > > > 4. With the current version of git-svn, it should try and repack files
>  >  >  > > > > at different stages during the clone process if you have enough
>  >  >  > > > > revisions, you will likely see a failure after this point when it
>  >  >  > > > > tries to pick up and check out the next increment from svn.
>  >  >  > > > > 5. Quickly look for the files it says it cannot find in the .git
>  >  >  > > > > directory of your partial clone, you should find that sometimes it is
>  >  >  > > > > there and sometimes not.
>  >  >  > > > >
>  >  >  > > > > RESULTS
>  >  >  > > > > Files associated with recent large batches of operations are there/not
>  >  >  > > > > there with subsequent "ls" listings.  If no new userland operations
>  >  >  > > > > are pending on the file, the file should either be there or not there,
>  >  >  > > > > not in limbo.
>  >  >  > > > >
>  >  >  > > > > REGRESSION
>  >  >  > > > > I haven't checked this with 102A or other revs, just 111.
>  >  >  > > > >
>  >  >  > > > > NOTES
>  >  >  > > > > Running current 111 release on a MacBook, with a single pool on a
>  >  >  > > > > single partition.
>  >  >  > > > >
>  >  >  > > > > --
>  >  >  > > > > James Snyder
>  >  >  > > > > Biomedical Engineering
>  >  >  > > > > Northwestern University
>  >  >  > > > > jbsnyder@gmail.com
>  >  >  > > > > _______________________________________________
>  >  >  > > > > zfs-discuss mailing list
>  >  >  > > > > zfs-discuss@lists.macosforge.org
>  >  >  > > > > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>  >  >  > > > >
>  >  >  > > > >
>  >  >  > > >
>  >  >  > > >
>  >  >  > > >
>  >  >  > >
>  >  >  > >
>  >  >  > >
>  >  >  > > --
>  >  >  > > James Snyder
>  >  >  > > Biomedical Engineering
>  >  >  > > Northwestern University
>  >  >  > > jbsnyder@gmail.com
>  >  >  > >
>  >  >  >
>  >  >  >
>  >  >
>  >  >
>  >  >
>  >  >  --
>  >  >
>  >  >
>  >  > James Snyder
>  >  >  Biomedical Engineering
>  >  >  Northwestern University
>  >  >  jbsnyder@gmail.com
>  >  >
>  >
>  >
>  >
>  >  --
>  >
>  >
>  > James Snyder
>  >  Biomedical Engineering
>  >  Northwestern University
>  >  jbsnyder@gmail.com
>  >
>
>
>
>  --
>
>
> James Snyder
>  Biomedical Engineering
>  Northwestern University
>  jbsnyder@gmail.com
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
From info at martin-hauser.net  Wed Apr 23 00:20:45 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Wed Apr 23 00:18:15 2008
Subject: [zfs-discuss] weird zfs thread synchronization issue?
In-Reply-To: <33644d3c0804221810t36b8e3f5mcce4701368faf9c0@mail.gmail.com>
References: <33644d3c0804220631ife68103y48fd9b5cda3baaa3@mail.gmail.com>
	<9DACCDE5-F405-4C92-BF22-17EF9DD27333@martin-hauser.net>
	<33644d3c0804220649r507d81b7u3a6a8f42064e2c2a@mail.gmail.com>
	<9E99B62A-A904-4F96-A832-4C2DA31FDB38@martin-hauser.net>
	<33644d3c0804220915j6f497719l9b8fbd515dd4c2b0@mail.gmail.com>
	<33644d3c0804221030o66c453e3y79e7c791d783f97a@mail.gmail.com>
	<33644d3c0804221810t36b8e3f5mcce4701368faf9c0@mail.gmail.com>
Message-ID: <B50F7BDD-FBA4-4581-A8AF-3899C49BD8AE@martin-hauser.net>

Hello James,

sorry wasn't reading any mail since my last reply yesterday afternoon

I've been using the following trick to get my git working correctly:

git()
{
     if [[ "$1" = "status" ]]; then
         /Users/mh/Library/Gentoo/usr/bin/git diff 1>/dev/null
     elif [[ "$1" = "add" ]]; then
         /Users/mh/Library/Gentoo/usr/bin/git diff 1>/dev/null
     fi
     /Users/mh/Library/Gentoo/usr/bin/git $*
}

as you might are noticing I'm using the git from gentoo/alt prefix  
installation, not the one from macports.

About your repository:

20029:0][9:17][mh@SnowFlake:Projects/Private/weird_repo/]% git status
# On branch master
nothing to commit (working directory clean)

Try the above ZSH (not BASH!) function (or manually doing a git diff  
before every git status, maybe even before a git commit if you are  
using the version that calles an editor (i Prefer using -m )).

let me know if you need further tweaks.

[20030:0][9:17][mh@SnowFlake:Projects/Private/weird_repo/]% git -- 
version
git version 1.5.4.5

Martin

On Apr 23, 2008, at 03:10 AM, James Snyder wrote:

> It looks like Git 1.5.5.1 handles the situation a little better.  The
> repositories that get into the state of the one I zipped up appear to
> function again on ZFS.  I wonder if this has something to do with some
> of the changes in lstat behavior in the 1.5.5 revision?
>
> That said, there's definitely still something going on.  I've had
> builds in MacPorts get hung up on similar "files in limbo" issues.
> Sometimes waiting a bit helps, sometimes re-rm'ing files seems to do
> the trick.  A reboot also helped in one case where something got stuck
> and I couldn't get a limbo file to be properly deleted.  I suppose
> this doesn't have anything to do with mmap stuff, but it does sound
> like some sort of caching issue?  Perhaps associated with the arc or
> the zil?  Is there any way to manually flush things to disk or force
> things to sync up aside from "sync"?
>
> I don't see any external tunables like on freebsd for zfs, but is
> there a way to disable the zil, prefetch or other useful but
> not-quite-essential aspects of ZFS on Mac OS X?
>
> On Tue, Apr 22, 2008 at 12:30 PM, James Snyder <jbsnyder@gmail.com>  
> wrote:
>> More weirdness...
>>
>> sample repo that is broken on zfs, not on hfs+...
>> http://fanplastic.org/files/weird_repo.tgz
>>
>>
>> The linked repo (just has one file with random data in it), gives me
>> this for git status on HFS+:
>>
>> # On branch master
>> nothing to commit (working directory clean)
>>
>> and this on ZFS:
>>
>> # On branch master
>> # Changed but not updated:
>> #   (use "git add <file>..." to update what will be committed)
>> #
>> #       modified:   testingfile.txt
>> #
>>
>> any ideas?  Try moving the unzipped dir between zfs and hfs+...
>> git-status -a gets things right on zfs, but git-status does not
>>
>>
>>
>> On Tue, Apr 22, 2008 at 11:15 AM, James Snyder <jbsnyder@gmail.com>  
>> wrote:
>>> Might I ask if you're passing any special options to git status, or
>>> how you're doing your alias?
>>>
>>> If I run the attached (somewhat ugly) script, I intermittently get
>>> things like the following, usually within the first 5 runs or so:
>>> # On branch master
>>> # Changed but not updated:
>>> #   (use "git add <file>..." to update what will be committed)
>>> #
>>> #       modified:   testme.txt
>>> #
>>> no changes added to commit (use "git add" and/or "git commit -a")
>>> error: 'testme.txt' has local modifications
>>> (use --cached to keep the file, or -f to force removal)
>>>
>>>
>>> I can't get the same to happen when I try this on HFS+.
>>>
>>> git_test.sh
>>> #!/bin/zsh
>>>
>>> # anything more than the following, indicates a problem:
>>> # # On branch master
>>> # nothing to commit (working directory clean)
>>> # rm 'testme.txt'
>>>
>>> for i in {1..10}
>>> do
>>>        echo "RUN: $i"
>>>        DIRNAME=`dd if=/dev/urandom bs=30 count=1 | md5` &>/dev/null
>>>        mkdir $DIRNAME
>>>        cd $DIRNAME
>>>        git init &>/dev/null
>>>        echo hi > there &>/dev/null
>>>        git add there &>/dev/null
>>>        git commit -m 'Small repo' &>/dev/null
>>>        dd if=/dev/urandom of=testme.txt count=10 bs=1m &>/dev/null
>>>        git add testme.txt &>/dev/null
>>>        git commit -m 'Add big binary file' &>/dev/null
>>>        sleep 1
>>>        git status; git rm testme.txt
>>>        git commit -m 'delete binary file' &>/dev/null
>>>        cd ..
>>>        rm -rf $DIRNAME
>>>        echo "\n"
>>> done
>>>
>>>
>>>
>>>
>>> On Tue, Apr 22, 2008 at 9:20 AM, Martin Hauser <info@martin-hauser.net 
>>> > wrote:
>>>> Everything seemed pretty consistent till now. The only thing I've  
>>>> ever
>>>> notice that is all git status related things (so the listing of  
>>>> the files in
>>>> git-commit, the commiting itself works fine). I would assume that  
>>>> the git's
>>>> index file is treated special somehow and doing a git diff  
>>>> rewrites that
>>>> file....
>>>>
>>>> Let me know if you find out more.
>>>>
>>>> Martin
>>>>
>>>>
>>>>
>>>> On Apr 22, 2008, at 15:49 PM, James Snyder wrote:
>>>>
>>>>
>>>>> I'll give that a try.  I'm not 100% sure that that will resolve  
>>>>> the
>>>>> problem though.
>>>>>
>>>>> Does that seem to work consistently or do you find yourself  
>>>>> having to
>>>>> sometimes do things multiple times?
>>>>>
>>>>> I'm not sure what git is doing behind the scenes during a status
>>>>> check.  Is it refreshing the index somehow that obviates a  
>>>>> little of
>>>>> this limbo file state issue?
>>>>>
>>>>> I haven't checked entirely whether this happens for creating,  
>>>>> deleting
>>>>> and modifying, but if it happens on modifications too, that  
>>>>> could lead
>>>>> to some messy issues.
>>>>>
>>>>> Example: Say, I was working with an sqlite database, made some
>>>>> changes, unlocked things, and subsequently another process hit the
>>>>> database and made changes but got the old version of the file and
>>>>> worked with that.
>>>>>
>>>>> On Tue, Apr 22, 2008 at 8:36 AM, Martin Hauser <info@martin-hauser.net 
>>>>> >
>>>> wrote:
>>>>>
>>>>>> Hi there,
>>>>>>
>>>>>> more on that issue,
>>>>>>
>>>>>> I've got some problems with it as well, but mostly due to git not
>>>> catching
>>>>>> commits immediatlly as such. For example:
>>>>>>
>>>>>> 1. do some work
>>>>>> 2. git add somefile
>>>>>> 3. git commit -m "some commit message"
>>>>>> 4. git status
>>>>>>
>>>>>> you'll notice that the file you added for commiting is still  
>>>>>> listed
>>>>>>
>>>>>> For me, it was easy enough to 'work around' just by doing a git  
>>>>>> status
>>>>>> before every index file related operation (just a small alias  
>>>>>> in my
>>>> .zshrc),
>>>>>> might be work-around-ing your problem also James?
>>>>>>
>>>>>> I'd be much prefering the issue to be fixed though.
>>>>>>
>>>>>> Martin
>>>>>>
>>>>>>
>>>>>>
>>>>>> On Apr 22, 2008, at 15:31 PM, James Snyder wrote:
>>>>>>
>>>>>>
>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> SUMMARY
>>>>>>> During the following situations:
>>>>>>> git reset --hard <revision>
>>>>>>> git-svn clone ... (on a large svn repository)
>>>>>>> (sometimes followed by)
>>>>>>> rm -rf <some directory> (after just working with a lot of  
>>>>>>> files, like
>>>>>>> one of the above commands)
>>>>>>>
>>>>>>> The listed git commands sometimes fail, sometimes succeed,  
>>>>>>> citing
>>>>>>> sometimes missing files.  With rm -rf, shortly after the  
>>>>>>> operation an
>>>>>>> ls or other file operation will sometimes show certain
>>>>>>> directories/files just operated on and sometimes not.  This  
>>>>>>> may or may
>>>>>>> not stabilize in the subsequent seconds.  This behavior leaves  
>>>>>>> me with
>>>>>>> the sense that the file operations are talking to different  
>>>>>>> threads
>>>>>>> associated with ZFS and that they're not all on the "same page"
>>>>>>>
>>>>>>> STEPS TO REPRODUCE
>>>>>>> 1. Find a rather large svn repository, with lots of revisions  
>>>>>>> and
>>>>>>> files (or make one).  The repo I'm working with has lots of  
>>>>>>> binary
>>>>>>> files in it and the db for svn is around 9 gigabytes with 4000
>>>>>>> revisions or so.
>>>>>>> 2. Install latest git w/ svn support from macports. (sudo port  
>>>>>>> install
>>>> git
>>>>>>>
>>>>>> +svn)
>>>>>>
>>>>>>> 3. do a git-svn clone on your repository
>>>>>>> 4. With the current version of git-svn, it should try and  
>>>>>>> repack files
>>>>>>> at different stages during the clone process if you have enough
>>>>>>> revisions, you will likely see a failure after this point when  
>>>>>>> it
>>>>>>> tries to pick up and check out the next increment from svn.
>>>>>>> 5. Quickly look for the files it says it cannot find in the .git
>>>>>>> directory of your partial clone, you should find that  
>>>>>>> sometimes it is
>>>>>>> there and sometimes not.
>>>>>>>
>>>>>>> RESULTS
>>>>>>> Files associated with recent large batches of operations are  
>>>>>>> there/not
>>>>>>> there with subsequent "ls" listings.  If no new userland  
>>>>>>> operations
>>>>>>> are pending on the file, the file should either be there or  
>>>>>>> not there,
>>>>>>> not in limbo.
>>>>>>>
>>>>>>> REGRESSION
>>>>>>> I haven't checked this with 102A or other revs, just 111.
>>>>>>>
>>>>>>> NOTES
>>>>>>> Running current 111 release on a MacBook, with a single pool  
>>>>>>> on a
>>>>>>> single partition.
>>>>>>>
>>>>>>> --
>>>>>>> James Snyder
>>>>>>> Biomedical Engineering
>>>>>>> Northwestern University
>>>>>>> jbsnyder@gmail.com
>>>>>>> _______________________________________________
>>>>>>> zfs-discuss mailing list
>>>>>>> zfs-discuss@lists.macosforge.org
>>>>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>>>>>
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> James Snyder
>>>>> Biomedical Engineering
>>>>> Northwestern University
>>>>> jbsnyder@gmail.com
>>>>>
>>>>
>>>>
>>>
>>>
>>>
>>> --
>>>
>>>
>>> James Snyder
>>> Biomedical Engineering
>>> Northwestern University
>>> jbsnyder@gmail.com
>>>
>>
>>
>>
>> --
>>
>>
>> James Snyder
>> Biomedical Engineering
>> Northwestern University
>> jbsnyder@gmail.com
>>
>
>
>
> -- 
> James Snyder
> Biomedical Engineering
> Northwestern University
> jbsnyder@gmail.com

-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080423/c4df706b/PGP-0001.bin
From info at martin-hauser.net  Wed Apr 23 00:24:26 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Wed Apr 23 00:22:00 2008
Subject: [zfs-discuss] weird zfs thread synchronization issue? (perhaps
	mmap is the culprit?)
In-Reply-To: <33644d3c0804222055t29a6293fsa6f23fefc8dc5aff@mail.gmail.com>
References: <33644d3c0804222055t29a6293fsa6f23fefc8dc5aff@mail.gmail.com>
Message-ID: <420D0A27-82E1-494F-B990-E107B6AB3B24@martin-hauser.net>

Interesting that those things still happen on zfs, as far as I  
remember this version should not have any problems with MMAP?

Git was doing some weird behaviour in the past. If I have some free  
time (right now I am at work and can' t play around with git that  
much) I'll dig into the Gentoo repository and find out whether they  
build my version with or without mmap support.

Martin

On Apr 23, 2008, at 05:55 AM, James Snyder wrote:

> I know I've already thrown out a fair amount of data here, but I've
> played with this some more.
>
> Building git with NO_MMAP enabled (which, as one might guess) disables
> the use of direct mmap syscalls in the git code, and seems to work
> around this problem.  I no longer get any errors on the git rm
> command.
>
> Side note: Weirdly, there are still mmap calls that show up when I
> dtruss  the "git rm testme.txt" from the script I posted.  At the same
> time, collecting stack traces for mmap system calls turns up nothing
> in Instruments, where before without NO_MMAP disabled it logged a
> number of calls. Perhaps mmap is getting called through something
> else?
>
> If you would like to try this yourself, and you built git from
> macports, you can replace the following file:
>
> /opt/local/var/macports/sources/rsync.macports.org/release/ports/ 
> devel/git-core/files/patch-Makefile.diff
> --- Makefile.orig   2008-04-22 22:25:26.000000000 -0500
> +++ Makefile    2008-04-22 22:26:20.000000000 -0500
> @@ -164,8 +164,6 @@
>
> # CFLAGS and LDFLAGS are for the users to override from the command  
> line.
>
> -CFLAGS = -g -O2 -Wall
> -LDFLAGS =
> ALL_CFLAGS = $(CFLAGS)
> ALL_LDFLAGS = $(LDFLAGS)
> STRIP ?= strip
> @@ -682,6 +680,7 @@
>            BASIC_LDFLAGS += -L/opt/local/lib
>        endif
>    endif
> +   NO_MMAP = YesPlease
> endif
>
> ifdef NO_R_TO_GCC_LINKER
> @@ -689,7 +688,7 @@
>    # the runtime dynamic library path.
>    CC_LD_DYNPATH = -Wl,-rpath=
> else
> -   CC_LD_DYNPATH = -R
> +   CC_LD_DYNPATH = -L
> endif
>
> ifdef NO_CURL
>
> and also patch the git-core Portfile with this diff:
> /opt/local/var/macports/sources/rsync.macports.org/release/ports/ 
> devel/git-core/Portfile
> --- Portfile.orig	2008-04-22 22:48:33.000000000 -0500
> +++ Portfile	2008-04-22 22:49:12.000000000 -0500
> @@ -3,7 +3,7 @@
> PortSystem        1.0
>
> name              git-core
> -version           1.5.4.5
> +version           1.5.5.1
> description       The stupid content tracker.
> long_description  A stupid (but extremely fast) directory content  
> manager. \
>                   It doesn't do a whole lot, but what it _does_ do  
> is track \
> @@ -18,8 +18,8 @@
> distfiles         git-${version}${extract.suffix} \
>                   git-manpages-${version}${extract.suffix}
>
> -checksums    git-${version}${extract.suffix} sha1
> 69c4b904f13b72f57405393d54f33831c9cfad8f \
> -             git-manpages-${version}${extract.suffix} sha1
> 659b5217b342b757a01603f61bd90a4d60f7e681
> +checksums    git-${version}${extract.suffix} sha1
> a450cd02cbfe8e18311816a9568bcd6d10d6cf52 \
> +             git-manpages-${version}${extract.suffix} sha1
> e062f2eb9e46546616e9cc62ecc4bcd485d30d9a
>
> depends_run  port:openssh port:rsync port:perl5.8 port:p5-error
> depends_lib  port:curl port:zlib port:openssl port:expat port:libiconv
> @@ -74,7 +74,7 @@
>
> variant doc description {Install HTML and plaintext documentation} {
>     distfiles-append    git-htmldocs-${version}${extract.suffix}
> -    checksums-append    git-htmldocs-${version}${extract.suffix} sha1
> 7087b54af84b8d91fec8d9a48c8f746047d3cd6a
> +    checksums-append    git-htmldocs-${version}${extract.suffix} sha1
> 56ae3fa2e5a7c64177af5122876f5fe063800b40
> }
>
> variant gitweb description {Install gitweb.cgi} {
>
> So far, so good.
>
> On Tue, Apr 22, 2008 at 8:10 PM, James Snyder <jbsnyder@gmail.com>  
> wrote:
>> It looks like Git 1.5.5.1 handles the situation a little better.  The
>> repositories that get into the state of the one I zipped up appear to
>> function again on ZFS.  I wonder if this has something to do with  
>> some
>> of the changes in lstat behavior in the 1.5.5 revision?
>>
>> That said, there's definitely still something going on.  I've had
>> builds in MacPorts get hung up on similar "files in limbo" issues.
>> Sometimes waiting a bit helps, sometimes re-rm'ing files seems to do
>> the trick.  A reboot also helped in one case where something got  
>> stuck
>> and I couldn't get a limbo file to be properly deleted.  I suppose
>> this doesn't have anything to do with mmap stuff, but it does sound
>> like some sort of caching issue?  Perhaps associated with the arc or
>> the zil?  Is there any way to manually flush things to disk or force
>> things to sync up aside from "sync"?
>>
>> I don't see any external tunables like on freebsd for zfs, but is
>> there a way to disable the zil, prefetch or other useful but
>> not-quite-essential aspects of ZFS on Mac OS X?
>>
>>
>>
>> On Tue, Apr 22, 2008 at 12:30 PM, James Snyder <jbsnyder@gmail.com>  
>> wrote:
>>> More weirdness...
>>>
>>> sample repo that is broken on zfs, not on hfs+...
>>> http://fanplastic.org/files/weird_repo.tgz
>>>
>>>
>>> The linked repo (just has one file with random data in it), gives me
>>> this for git status on HFS+:
>>>
>>> # On branch master
>>> nothing to commit (working directory clean)
>>>
>>> and this on ZFS:
>>>
>>> # On branch master
>>> # Changed but not updated:
>>> #   (use "git add <file>..." to update what will be committed)
>>> #
>>> #       modified:   testingfile.txt
>>> #
>>>
>>> any ideas?  Try moving the unzipped dir between zfs and hfs+...
>>> git-status -a gets things right on zfs, but git-status does not
>>>
>>>
>>>
>>> On Tue, Apr 22, 2008 at 11:15 AM, James Snyder  
>>> <jbsnyder@gmail.com> wrote:
>>>> Might I ask if you're passing any special options to git status, or
>>>> how you're doing your alias?
>>>>
>>>> If I run the attached (somewhat ugly) script, I intermittently get
>>>> things like the following, usually within the first 5 runs or so:
>>>> # On branch master
>>>> # Changed but not updated:
>>>> #   (use "git add <file>..." to update what will be committed)
>>>> #
>>>> #       modified:   testme.txt
>>>> #
>>>> no changes added to commit (use "git add" and/or "git commit -a")
>>>> error: 'testme.txt' has local modifications
>>>> (use --cached to keep the file, or -f to force removal)
>>>>
>>>>
>>>> I can't get the same to happen when I try this on HFS+.
>>>>
>>>> git_test.sh
>>>> #!/bin/zsh
>>>>
>>>> # anything more than the following, indicates a problem:
>>>> # # On branch master
>>>> # nothing to commit (working directory clean)
>>>> # rm 'testme.txt'
>>>>
>>>> for i in {1..10}
>>>> do
>>>>        echo "RUN: $i"
>>>>        DIRNAME=`dd if=/dev/urandom bs=30 count=1 | md5` &>/dev/null
>>>>        mkdir $DIRNAME
>>>>        cd $DIRNAME
>>>>        git init &>/dev/null
>>>>        echo hi > there &>/dev/null
>>>>        git add there &>/dev/null
>>>>        git commit -m 'Small repo' &>/dev/null
>>>>        dd if=/dev/urandom of=testme.txt count=10 bs=1m &>/dev/null
>>>>        git add testme.txt &>/dev/null
>>>>        git commit -m 'Add big binary file' &>/dev/null
>>>>        sleep 1
>>>>        git status; git rm testme.txt
>>>>        git commit -m 'delete binary file' &>/dev/null
>>>>        cd ..
>>>>        rm -rf $DIRNAME
>>>>        echo "\n"
>>>> done
>>>>
>>>>
>>>>
>>>>
>>>> On Tue, Apr 22, 2008 at 9:20 AM, Martin Hauser <info@martin-hauser.net 
>>>> > wrote:
>>>>> Everything seemed pretty consistent till now. The only thing  
>>>>> I've ever
>>>>> notice that is all git status related things (so the listing of  
>>>>> the files in
>>>>> git-commit, the commiting itself works fine). I would assume  
>>>>> that the git's
>>>>> index file is treated special somehow and doing a git diff  
>>>>> rewrites that
>>>>> file....
>>>>>
>>>>> Let me know if you find out more.
>>>>>
>>>>> Martin
>>>>>
>>>>>
>>>>>
>>>>> On Apr 22, 2008, at 15:49 PM, James Snyder wrote:
>>>>>
>>>>>
>>>>>> I'll give that a try.  I'm not 100% sure that that will resolve  
>>>>>> the
>>>>>> problem though.
>>>>>>
>>>>>> Does that seem to work consistently or do you find yourself  
>>>>>> having to
>>>>>> sometimes do things multiple times?
>>>>>>
>>>>>> I'm not sure what git is doing behind the scenes during a status
>>>>>> check.  Is it refreshing the index somehow that obviates a  
>>>>>> little of
>>>>>> this limbo file state issue?
>>>>>>
>>>>>> I haven't checked entirely whether this happens for creating,  
>>>>>> deleting
>>>>>> and modifying, but if it happens on modifications too, that  
>>>>>> could lead
>>>>>> to some messy issues.
>>>>>>
>>>>>> Example: Say, I was working with an sqlite database, made some
>>>>>> changes, unlocked things, and subsequently another process hit  
>>>>>> the
>>>>>> database and made changes but got the old version of the file and
>>>>>> worked with that.
>>>>>>
>>>>>> On Tue, Apr 22, 2008 at 8:36 AM, Martin Hauser <info@martin-hauser.net 
>>>>>> >
>>>>> wrote:
>>>>>>
>>>>>>> Hi there,
>>>>>>>
>>>>>>> more on that issue,
>>>>>>>
>>>>>>> I've got some problems with it as well, but mostly due to git  
>>>>>>> not
>>>>> catching
>>>>>>> commits immediatlly as such. For example:
>>>>>>>
>>>>>>> 1. do some work
>>>>>>> 2. git add somefile
>>>>>>> 3. git commit -m "some commit message"
>>>>>>> 4. git status
>>>>>>>
>>>>>>> you'll notice that the file you added for commiting is still  
>>>>>>> listed
>>>>>>>
>>>>>>> For me, it was easy enough to 'work around' just by doing a  
>>>>>>> git status
>>>>>>> before every index file related operation (just a small alias  
>>>>>>> in my
>>>>> .zshrc),
>>>>>>> might be work-around-ing your problem also James?
>>>>>>>
>>>>>>> I'd be much prefering the issue to be fixed though.
>>>>>>>
>>>>>>> Martin
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On Apr 22, 2008, at 15:31 PM, James Snyder wrote:
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> SUMMARY
>>>>>>>> During the following situations:
>>>>>>>> git reset --hard <revision>
>>>>>>>> git-svn clone ... (on a large svn repository)
>>>>>>>> (sometimes followed by)
>>>>>>>> rm -rf <some directory> (after just working with a lot of  
>>>>>>>> files, like
>>>>>>>> one of the above commands)
>>>>>>>>
>>>>>>>> The listed git commands sometimes fail, sometimes succeed,  
>>>>>>>> citing
>>>>>>>> sometimes missing files.  With rm -rf, shortly after the  
>>>>>>>> operation an
>>>>>>>> ls or other file operation will sometimes show certain
>>>>>>>> directories/files just operated on and sometimes not.  This  
>>>>>>>> may or may
>>>>>>>> not stabilize in the subsequent seconds.  This behavior  
>>>>>>>> leaves me with
>>>>>>>> the sense that the file operations are talking to different  
>>>>>>>> threads
>>>>>>>> associated with ZFS and that they're not all on the "same page"
>>>>>>>>
>>>>>>>> STEPS TO REPRODUCE
>>>>>>>> 1. Find a rather large svn repository, with lots of revisions  
>>>>>>>> and
>>>>>>>> files (or make one).  The repo I'm working with has lots of  
>>>>>>>> binary
>>>>>>>> files in it and the db for svn is around 9 gigabytes with 4000
>>>>>>>> revisions or so.
>>>>>>>> 2. Install latest git w/ svn support from macports. (sudo  
>>>>>>>> port install
>>>>> git
>>>>>>>>
>>>>>>> +svn)
>>>>>>>
>>>>>>>> 3. do a git-svn clone on your repository
>>>>>>>> 4. With the current version of git-svn, it should try and  
>>>>>>>> repack files
>>>>>>>> at different stages during the clone process if you have enough
>>>>>>>> revisions, you will likely see a failure after this point  
>>>>>>>> when it
>>>>>>>> tries to pick up and check out the next increment from svn.
>>>>>>>> 5. Quickly look for the files it says it cannot find in  
>>>>>>>> the .git
>>>>>>>> directory of your partial clone, you should find that  
>>>>>>>> sometimes it is
>>>>>>>> there and sometimes not.
>>>>>>>>
>>>>>>>> RESULTS
>>>>>>>> Files associated with recent large batches of operations are  
>>>>>>>> there/not
>>>>>>>> there with subsequent "ls" listings.  If no new userland  
>>>>>>>> operations
>>>>>>>> are pending on the file, the file should either be there or  
>>>>>>>> not there,
>>>>>>>> not in limbo.
>>>>>>>>
>>>>>>>> REGRESSION
>>>>>>>> I haven't checked this with 102A or other revs, just 111.
>>>>>>>>
>>>>>>>> NOTES
>>>>>>>> Running current 111 release on a MacBook, with a single pool  
>>>>>>>> on a
>>>>>>>> single partition.
>>>>>>>>
>>>>>>>> --
>>>>>>>> James Snyder
>>>>>>>> Biomedical Engineering
>>>>>>>> Northwestern University
>>>>>>>> jbsnyder@gmail.com
>>>>>>>> _______________________________________________
>>>>>>>> zfs-discuss mailing list
>>>>>>>> zfs-discuss@lists.macosforge.org
>>>>>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> James Snyder
>>>>>> Biomedical Engineering
>>>>>> Northwestern University
>>>>>> jbsnyder@gmail.com
>>>>>>
>>>>>
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>>
>>>>
>>>> James Snyder
>>>> Biomedical Engineering
>>>> Northwestern University
>>>> jbsnyder@gmail.com
>>>>
>>>
>>>
>>>
>>> --
>>>
>>>
>>> James Snyder
>>> Biomedical Engineering
>>> Northwestern University
>>> jbsnyder@gmail.com
>>>
>>
>>
>>
>> --
>>
>>
>> James Snyder
>> Biomedical Engineering
>> Northwestern University
>> jbsnyder@gmail.com
>>
>
>
>
> -- 
> James Snyder
> Biomedical Engineering
> Northwestern University
> jbsnyder@gmail.com

-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080423/366112aa/PGP.bin
From cdl at asgaard.org  Wed Apr 23 08:06:47 2008
From: cdl at asgaard.org (Christopher LILJENSTOLPE)
Date: Wed Apr 23 08:04:17 2008
Subject: [zfs-discuss] Zpools went away - completely
In-Reply-To: <F1670B78-0C92-41F3-8A10-BEB9D8E0FA0B@sun.com>
References: <b99b378d0804180633m1ea91e6dre7e12297b73a80de@mail.gmail.com>
	<18D5A528-EFDE-4D0D-B87C-541AD2BD8978@asgaard.org>
	<FBDF4E38-8F13-4E64-BED2-EBDDBA109322@sogeeky.net>
	<8F9B6204-0D8F-46AE-B074-8AC2A2C17C5F@asgaard.org>
	<F1670B78-0C92-41F3-8A10-BEB9D8E0FA0B@sun.com>
Message-ID: <3E6FDC69-FBE4-4F65-8DFB-5571BD38492C@asgaard.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Greetings,

	There is a zpool.cache file there, and I can't import with an import.

	Chris

On 19 Apr 2008, at 13.44, Jonathan Edwards wrote:

> the cache file might have disappeared (/etc/zfs/zpool.cache)
>
> can you import them with a zpool import <pool>?
>
> On Apr 19, 2008, at 11:14 AM, Christopher LILJENSTOLPE wrote:
>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> Yup, I did.
>>
>> 	Chris
>>
>> On 19 Apr 2008, at 00.37, Mr. Zorg wrote:
>>
>>> Did you chown the files after dropping them in?  I could see where  
>>> that may cause what you're seeing. I had no problems.
>>>
>>> On Apr 18, 2008, at 6:32 PM, Christopher LILJENSTOLPE <cdl@asgaard.org 
>>> > wrote:
>>>
>>>> -----BEGIN PGP SIGNED MESSAGE-----
>>>> Hash: SHA1
>>>>
>>>> Gretings,
>>>>
>>>> I just upgraded to the new release from 102A and had one 3x1TB  
>>>> raidz pool on my 10.5 PPC server.  After I removed the files/dirs  
>>>> that 110 would over-write, I dropped the new 110 build files in,  
>>>> and rebooted.  When the system came back, no pools or filesystems  
>>>> built on them mounted, and a zpool list or zfs list showed no  
>>>> pools found.
>>>>
>>>> Fighting back some panic, I ripped out the 110 load, and pulled  
>>>> the 102A files down and re-installed them.  Rebooted, and still,  
>>>> no pools.
>>>>
>>>> I have things backed up, but I'm "concerned" about pools that  
>>>> just vanish in the night....
>>>>
>>>> Chris
>>>>
>>>> -----BEGIN PGP SIGNATURE-----
>>>>
>>>> iQEcBAEBAgAGBQJICUuuAAoJEGmx2Mt/+Iw/fKQH/if+dB97wiDMS/mxtU2nc6vy
>>>> lMFoH7r5Wc8zHf6cX8aEWebxSDSnloA9K1V1nZBuXc7Vi2OJRxiyiI0/uLF+MPqO
>>>> APh5evK/23iVfN/BUXY5elrPrkBnK53SV8Mf9QOi1kjM1WwH3YLy2mwjbl57f1zA
>>>> Hm+ea5NW7hl69CXrwHQmJK/qkPYyQnOU3Tf4Fns79m+dzyeHnd9gowd6NY0mduOH
>>>> jX0w76mrFR8vGSslk1aVrHtUVCgyOsgcmZzBk/HfdnF0caNGdvN1POL3LRexWCRq
>>>> 3EpeRlDQbxNfEmPMzhiVstGxsnlzRlYp8Bz+Pc/T2Z8WCFu8+dY7CrbdIBH36rw=
>>>> =6qFq
>>>> -----END PGP SIGNATURE-----
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>>
>> - ---
>> ???
>> Check my PGP key here:
>> http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B
>>
>>
>>
>>
>> -----BEGIN PGP SIGNATURE-----
>>
>> iQEcBAEBAgAGBQJICgxPAAoJEGmx2Mt/+Iw/RM8H/1oTx7lloI3CZ35db2AewYKD
>> E8tePxsDZitrdzK0pw4w1HNEAgnSy1+avBE+0p89eIdO6WFcSzf5kHiQEXcmie6m
>> 1x1yiu7m4UzNZDwvFtCU+Uo/E4msMjyvjDXhjsaxEUsO2pDXq7v3BV3D8SaA5zn8
>> GXJtq/Paa8MpJK5sgKeskHrbIbVJkvnLZI8BrMUO2ZNWxe18d6PBafpXpjCLE8rC
>> h2NSY0U+1a6GWXj465O8PgRCbdd+tFBzq29M1HymZIgxFzMIPFKli7zvB1uNEx9E
>> 3rkUPofzIr+lcBlQif8j084cW5t8EhyEo+v/9fHo4MEsRrudRhd3m+xztceIuiA=
>> =b/WE
>> -----END PGP SIGNATURE-----
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>
>

- ---
???
Check my PGP key here:
http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B




-----BEGIN PGP SIGNATURE-----

iQEcBAEBAgAGBQJID1CIAAoJEGmx2Mt/+Iw/4xAH+QG6dP6GvVJ04JBdH0w6gQ0A
IhJsSCiVHHVWhA6N1x5OO3mPf+kBRAP3IOegQHEKOQbdbM6zZL4vZEZkYDXm6sAh
6286HJwRLQMVflfg4ZFAYKKYq7f+HoyQW8Hq3zd954ZMmd8eeFlTj6eMvurkjSzW
/fEwTSKxQklOFi2Hktis1csp03mNTt8r1Md94QbGSXO4+Ul6Nwm7722np91aebkU
6S88Zf38cesl9TMpxQFbFThR8rsfPS04w+8UZZsp5gdqpwd3oEQKSGED/48w1D3I
uWfBtPwtyxYxNv5AyJmHbaK7zYo8CBVCjVx1zSX8PwctiWVtDMs3IgEHusX99Cs=
=AuTp
-----END PGP SIGNATURE-----
From jbsnyder at gmail.com  Wed Apr 23 09:09:01 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Wed Apr 23 09:06:21 2008
Subject: [zfs-discuss] weird zfs thread synchronization issue? (perhaps
	mmap is the culprit?)
In-Reply-To: <420D0A27-82E1-494F-B990-E107B6AB3B24@martin-hauser.net>
References: <33644d3c0804222055t29a6293fsa6f23fefc8dc5aff@mail.gmail.com>
	<420D0A27-82E1-494F-B990-E107B6AB3B24@martin-hauser.net>
Message-ID: <33644d3c0804230909k66b85244j88d34c3143552979@mail.gmail.com>

Hmm..  How well does the gentoo repo work on Leopard?  I seem to
recall taking note of it some time ago, but the integration seemed a
little lacking.

I'll give your script wrapper a try although between disabling mmap
and updating git, it seems a little easier to deal with weirdness.

I wish there were a way to get a little more debugging info from zfs
on what might be going on in specific situations, but I don't have the
courage to hack the source with only one Mac, which happens to depend
on ZFS :-)  I suppose the sysctls or similar could be extended to make
available such information.

No?l: is there a better way to be able to debug what's going on here
on the user end?  I noticed that the dtrace probes have been adjusted
slightly in the latest rev.  Is there a way to tap into those?  I
would have far fewer qualms about recompiling with just some probes
added as opposed to hacking more substantially leading to a situation
that could break my FS :-)



On Wed, Apr 23, 2008 at 2:24 AM, Martin Hauser <info@martin-hauser.net> wrote:
> Interesting that those things still happen on zfs, as far as I remember this
> version should not have any problems with MMAP?
>
>  Git was doing some weird behaviour in the past. If I have some free time
> (right now I am at work and can' t play around with git that much) I'll dig
> into the Gentoo repository and find out whether they build my version with
> or without mmap support.
>
>  Martin
>
>
>
>  On Apr 23, 2008, at 05:55 AM, James Snyder wrote:
>
>
> > I know I've already thrown out a fair amount of data here, but I've
> > played with this some more.
> >
> > Building git with NO_MMAP enabled (which, as one might guess) disables
> > the use of direct mmap syscalls in the git code, and seems to work
> > around this problem.  I no longer get any errors on the git rm
> > command.
> >
> > Side note: Weirdly, there are still mmap calls that show up when I
> > dtruss  the "git rm testme.txt" from the script I posted.  At the same
> > time, collecting stack traces for mmap system calls turns up nothing
> > in Instruments, where before without NO_MMAP disabled it logged a
> > number of calls. Perhaps mmap is getting called through something
> > else?
> >
> > If you would like to try this yourself, and you built git from
> > macports, you can replace the following file:
> >
> >
> /opt/local/var/macports/sources/rsync.macports.org/release/ports/devel/git-core/files/patch-Makefile.diff
> > --- Makefile.orig   2008-04-22 22:25:26.000000000 -0500
> > +++ Makefile    2008-04-22 22:26:20.000000000 -0500
> > @@ -164,8 +164,6 @@
> >
> > # CFLAGS and LDFLAGS are for the users to override from the command line.
> >
> > -CFLAGS = -g -O2 -Wall
> > -LDFLAGS =
> > ALL_CFLAGS = $(CFLAGS)
> > ALL_LDFLAGS = $(LDFLAGS)
> > STRIP ?= strip
> > @@ -682,6 +680,7 @@
> >           BASIC_LDFLAGS += -L/opt/local/lib
> >       endif
> >   endif
> > +   NO_MMAP = YesPlease
> > endif
> >
> > ifdef NO_R_TO_GCC_LINKER
> > @@ -689,7 +688,7 @@
> >   # the runtime dynamic library path.
> >   CC_LD_DYNPATH = -Wl,-rpath=
> > else
> > -   CC_LD_DYNPATH = -R
> > +   CC_LD_DYNPATH = -L
> > endif
> >
> > ifdef NO_CURL
> >
> > and also patch the git-core Portfile with this diff:
> >
> /opt/local/var/macports/sources/rsync.macports.org/release/ports/devel/git-core/Portfile
> > --- Portfile.orig       2008-04-22 22:48:33.000000000 -0500
> > +++ Portfile    2008-04-22 22:49:12.000000000 -0500
> > @@ -3,7 +3,7 @@
> > PortSystem        1.0
> >
> > name              git-core
> > -version           1.5.4.5
> > +version           1.5.5.1
> > description       The stupid content tracker.
> > long_description  A stupid (but extremely fast) directory content manager.
> \
> >                  It doesn't do a whole lot, but what it _does_ do is track
> \
> > @@ -18,8 +18,8 @@
> > distfiles         git-${version}${extract.suffix} \
> >                  git-manpages-${version}${extract.suffix}
> >
> > -checksums    git-${version}${extract.suffix} sha1
> > 69c4b904f13b72f57405393d54f33831c9cfad8f \
> > -             git-manpages-${version}${extract.suffix} sha1
> > 659b5217b342b757a01603f61bd90a4d60f7e681
> > +checksums    git-${version}${extract.suffix} sha1
> > a450cd02cbfe8e18311816a9568bcd6d10d6cf52 \
> > +             git-manpages-${version}${extract.suffix} sha1
> > e062f2eb9e46546616e9cc62ecc4bcd485d30d9a
> >
> > depends_run  port:openssh port:rsync port:perl5.8 port:p5-error
> > depends_lib  port:curl port:zlib port:openssl port:expat port:libiconv
> > @@ -74,7 +74,7 @@
> >
> > variant doc description {Install HTML and plaintext documentation} {
> >    distfiles-append    git-htmldocs-${version}${extract.suffix}
> > -    checksums-append    git-htmldocs-${version}${extract.suffix} sha1
> > 7087b54af84b8d91fec8d9a48c8f746047d3cd6a
> > +    checksums-append    git-htmldocs-${version}${extract.suffix} sha1
> > 56ae3fa2e5a7c64177af5122876f5fe063800b40
> > }
> >
> > variant gitweb description {Install gitweb.cgi} {
> >
> > So far, so good.
> >
> > On Tue, Apr 22, 2008 at 8:10 PM, James Snyder <jbsnyder@gmail.com> wrote:
> >
> > > It looks like Git 1.5.5.1 handles the situation a little better.  The
> > > repositories that get into the state of the one I zipped up appear to
> > > function again on ZFS.  I wonder if this has something to do with some
> > > of the changes in lstat behavior in the 1.5.5 revision?
> > >
> > > That said, there's definitely still something going on.  I've had
> > > builds in MacPorts get hung up on similar "files in limbo" issues.
> > > Sometimes waiting a bit helps, sometimes re-rm'ing files seems to do
> > > the trick.  A reboot also helped in one case where something got stuck
> > > and I couldn't get a limbo file to be properly deleted.  I suppose
> > > this doesn't have anything to do with mmap stuff, but it does sound
> > > like some sort of caching issue?  Perhaps associated with the arc or
> > > the zil?  Is there any way to manually flush things to disk or force
> > > things to sync up aside from "sync"?
> > >
> > > I don't see any external tunables like on freebsd for zfs, but is
> > > there a way to disable the zil, prefetch or other useful but
> > > not-quite-essential aspects of ZFS on Mac OS X?
> > >
> > >
> > >
> > > On Tue, Apr 22, 2008 at 12:30 PM, James Snyder <jbsnyder@gmail.com>
> wrote:
> > >
> > > > More weirdness...
> > > >
> > > > sample repo that is broken on zfs, not on hfs+...
> > > > http://fanplastic.org/files/weird_repo.tgz
> > > >
> > > >
> > > > The linked repo (just has one file with random data in it), gives me
> > > > this for git status on HFS+:
> > > >
> > > > # On branch master
> > > > nothing to commit (working directory clean)
> > > >
> > > > and this on ZFS:
> > > >
> > > > # On branch master
> > > > # Changed but not updated:
> > > > #   (use "git add <file>..." to update what will be committed)
> > > > #
> > > > #       modified:   testingfile.txt
> > > > #
> > > >
> > > > any ideas?  Try moving the unzipped dir between zfs and hfs+...
> > > > git-status -a gets things right on zfs, but git-status does not
> > > >
> > > >
> > > >
> > > > On Tue, Apr 22, 2008 at 11:15 AM, James Snyder <jbsnyder@gmail.com>
> wrote:
> > > >
> > > > > Might I ask if you're passing any special options to git status, or
> > > > > how you're doing your alias?
> > > > >
> > > > > If I run the attached (somewhat ugly) script, I intermittently get
> > > > > things like the following, usually within the first 5 runs or so:
> > > > > # On branch master
> > > > > # Changed but not updated:
> > > > > #   (use "git add <file>..." to update what will be committed)
> > > > > #
> > > > > #       modified:   testme.txt
> > > > > #
> > > > > no changes added to commit (use "git add" and/or "git commit -a")
> > > > > error: 'testme.txt' has local modifications
> > > > > (use --cached to keep the file, or -f to force removal)
> > > > >
> > > > >
> > > > > I can't get the same to happen when I try this on HFS+.
> > > > >
> > > > > git_test.sh
> > > > > #!/bin/zsh
> > > > >
> > > > > # anything more than the following, indicates a problem:
> > > > > # # On branch master
> > > > > # nothing to commit (working directory clean)
> > > > > # rm 'testme.txt'
> > > > >
> > > > > for i in {1..10}
> > > > > do
> > > > >       echo "RUN: $i"
> > > > >       DIRNAME=`dd if=/dev/urandom bs=30 count=1 | md5` &>/dev/null
> > > > >       mkdir $DIRNAME
> > > > >       cd $DIRNAME
> > > > >       git init &>/dev/null
> > > > >       echo hi > there &>/dev/null
> > > > >       git add there &>/dev/null
> > > > >       git commit -m 'Small repo' &>/dev/null
> > > > >       dd if=/dev/urandom of=testme.txt count=10 bs=1m &>/dev/null
> > > > >       git add testme.txt &>/dev/null
> > > > >       git commit -m 'Add big binary file' &>/dev/null
> > > > >       sleep 1
> > > > >       git status; git rm testme.txt
> > > > >       git commit -m 'delete binary file' &>/dev/null
> > > > >       cd ..
> > > > >       rm -rf $DIRNAME
> > > > >       echo "\n"
> > > > > done
> > > > >
> > > > >
> > > > >
> > > > >
> > > > > On Tue, Apr 22, 2008 at 9:20 AM, Martin Hauser
> <info@martin-hauser.net> wrote:
> > > > >
> > > > > > Everything seemed pretty consistent till now. The only thing I've
> ever
> > > > > > notice that is all git status related things (so the listing of
> the files in
> > > > > > git-commit, the commiting itself works fine). I would assume that
> the git's
> > > > > > index file is treated special somehow and doing a git diff
> rewrites that
> > > > > > file....
> > > > > >
> > > > > > Let me know if you find out more.
> > > > > >
> > > > > > Martin
> > > > > >
> > > > > >
> > > > > >
> > > > > > On Apr 22, 2008, at 15:49 PM, James Snyder wrote:
> > > > > >
> > > > > >
> > > > > >
> > > > > > > I'll give that a try.  I'm not 100% sure that that will resolve
> the
> > > > > > > problem though.
> > > > > > >
> > > > > > > Does that seem to work consistently or do you find yourself
> having to
> > > > > > > sometimes do things multiple times?
> > > > > > >
> > > > > > > I'm not sure what git is doing behind the scenes during a status
> > > > > > > check.  Is it refreshing the index somehow that obviates a
> little of
> > > > > > > this limbo file state issue?
> > > > > > >
> > > > > > > I haven't checked entirely whether this happens for creating,
> deleting
> > > > > > > and modifying, but if it happens on modifications too, that
> could lead
> > > > > > > to some messy issues.
> > > > > > >
> > > > > > > Example: Say, I was working with an sqlite database, made some
> > > > > > > changes, unlocked things, and subsequently another process hit
> the
> > > > > > > database and made changes but got the old version of the file
> and
> > > > > > > worked with that.
> > > > > > >
> > > > > > > On Tue, Apr 22, 2008 at 8:36 AM, Martin Hauser
> <info@martin-hauser.net>
> > > > > > >
> > > > > > wrote:
> > > > > >
> > > > > > >
> > > > > > >
> > > > > > > > Hi there,
> > > > > > > >
> > > > > > > > more on that issue,
> > > > > > > >
> > > > > > > > I've got some problems with it as well, but mostly due to git
> not
> > > > > > > >
> > > > > > >
> > > > > > catching
> > > > > >
> > > > > > >
> > > > > > > > commits immediatlly as such. For example:
> > > > > > > >
> > > > > > > > 1. do some work
> > > > > > > > 2. git add somefile
> > > > > > > > 3. git commit -m "some commit message"
> > > > > > > > 4. git status
> > > > > > > >
> > > > > > > > you'll notice that the file you added for commiting is still
> listed
> > > > > > > >
> > > > > > > > For me, it was easy enough to 'work around' just by doing a
> git status
> > > > > > > > before every index file related operation (just a small alias
> in my
> > > > > > > >
> > > > > > >
> > > > > > .zshrc),
> > > > > >
> > > > > > >
> > > > > > > > might be work-around-ing your problem also James?
> > > > > > > >
> > > > > > > > I'd be much prefering the issue to be fixed though.
> > > > > > > >
> > > > > > > > Martin
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > > On Apr 22, 2008, at 15:31 PM, James Snyder wrote:
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > > SUMMARY
> > > > > > > > > During the following situations:
> > > > > > > > > git reset --hard <revision>
> > > > > > > > > git-svn clone ... (on a large svn repository)
> > > > > > > > > (sometimes followed by)
> > > > > > > > > rm -rf <some directory> (after just working with a lot of
> files, like
> > > > > > > > > one of the above commands)
> > > > > > > > >
> > > > > > > > > The listed git commands sometimes fail, sometimes succeed,
> citing
> > > > > > > > > sometimes missing files.  With rm -rf, shortly after the
> operation an
> > > > > > > > > ls or other file operation will sometimes show certain
> > > > > > > > > directories/files just operated on and sometimes not.  This
> may or may
> > > > > > > > > not stabilize in the subsequent seconds.  This behavior
> leaves me with
> > > > > > > > > the sense that the file operations are talking to different
> threads
> > > > > > > > > associated with ZFS and that they're not all on the "same
> page"
> > > > > > > > >
> > > > > > > > > STEPS TO REPRODUCE
> > > > > > > > > 1. Find a rather large svn repository, with lots of
> revisions and
> > > > > > > > > files (or make one).  The repo I'm working with has lots of
> binary
> > > > > > > > > files in it and the db for svn is around 9 gigabytes with
> 4000
> > > > > > > > > revisions or so.
> > > > > > > > > 2. Install latest git w/ svn support from macports. (sudo
> port install
> > > > > > > > >
> > > > > > > >
> > > > > > >
> > > > > > git
> > > > > >
> > > > > > >
> > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > > +svn)
> > > > > > > >
> > > > > > > >
> > > > > > > > > 3. do a git-svn clone on your repository
> > > > > > > > > 4. With the current version of git-svn, it should try and
> repack files
> > > > > > > > > at different stages during the clone process if you have
> enough
> > > > > > > > > revisions, you will likely see a failure after this point
> when it
> > > > > > > > > tries to pick up and check out the next increment from svn.
> > > > > > > > > 5. Quickly look for the files it says it cannot find in the
> .git
> > > > > > > > > directory of your partial clone, you should find that
> sometimes it is
> > > > > > > > > there and sometimes not.
> > > > > > > > >
> > > > > > > > > RESULTS
> > > > > > > > > Files associated with recent large batches of operations are
> there/not
> > > > > > > > > there with subsequent "ls" listings.  If no new userland
> operations
> > > > > > > > > are pending on the file, the file should either be there or
> not there,
> > > > > > > > > not in limbo.
> > > > > > > > >
> > > > > > > > > REGRESSION
> > > > > > > > > I haven't checked this with 102A or other revs, just 111.
> > > > > > > > >
> > > > > > > > > NOTES
> > > > > > > > > Running current 111 release on a MacBook, with a single pool
> on a
> > > > > > > > > single partition.
> > > > > > > > >
> > > > > > > > > --
> > > > > > > > > James Snyder
> > > > > > > > > Biomedical Engineering
> > > > > > > > > Northwestern University
> > > > > > > > > jbsnyder@gmail.com
> > > > > > > > > _______________________________________________
> > > > > > > > > zfs-discuss mailing list
> > > > > > > > > zfs-discuss@lists.macosforge.org
> > > > > > > > > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
> > > > > > > > >
> > > > > > > > >
> > > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > >
> > > > > > >
> > > > > > >
> > > > > > > --
> > > > > > > James Snyder
> > > > > > > Biomedical Engineering
> > > > > > > Northwestern University
> > > > > > > jbsnyder@gmail.com
> > > > > > >
> > > > > > >
> > > > > >
> > > > > >
> > > > > >
> > > > >
> > > > >
> > > > >
> > > > > --
> > > > >
> > > > >
> > > > > James Snyder
> > > > > Biomedical Engineering
> > > > > Northwestern University
> > > > > jbsnyder@gmail.com
> > > > >
> > > > >
> > > >
> > > >
> > > >
> > > > --
> > > >
> > > >
> > > > James Snyder
> > > > Biomedical Engineering
> > > > Northwestern University
> > > > jbsnyder@gmail.com
> > > >
> > > >
> > >
> > >
> > >
> > > --
> > >
> > >
> > > James Snyder
> > > Biomedical Engineering
> > > Northwestern University
> > > jbsnyder@gmail.com
> > >
> > >
> >
> >
> >
> > --
> > James Snyder
> > Biomedical Engineering
> > Northwestern University
> > jbsnyder@gmail.com
> >
>
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
From cdl at asgaard.org  Wed Apr 23 17:21:21 2008
From: cdl at asgaard.org (Christopher LILJENSTOLPE)
Date: Wed Apr 23 17:18:41 2008
Subject: [zfs-discuss] Zpools went away - completely
In-Reply-To: <3E6FDC69-FBE4-4F65-8DFB-5571BD38492C@asgaard.org>
References: <b99b378d0804180633m1ea91e6dre7e12297b73a80de@mail.gmail.com>
	<18D5A528-EFDE-4D0D-B87C-541AD2BD8978@asgaard.org>
	<FBDF4E38-8F13-4E64-BED2-EBDDBA109322@sogeeky.net>
	<8F9B6204-0D8F-46AE-B074-8AC2A2C17C5F@asgaard.org>
	<F1670B78-0C92-41F3-8A10-BEB9D8E0FA0B@sun.com>
	<3E6FDC69-FBE4-4F65-8DFB-5571BD38492C@asgaard.org>
Message-ID: <222C0048-308F-438A-BC9D-B2C282040BBC@asgaard.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Greetings,

	Ok, I completely blew away everything but the read-only stuff, and  
then installed 111.  It now works, and the pools came back.  Something  
got queered by the upgrade.  Not sure what happened.

	Chris

On 23 Apr 2008, at 08.06, Christopher LILJENSTOLPE wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> Greetings,
>
> 	There is a zpool.cache file there, and I can't import with an import.
>
> 	Chris
>
> On 19 Apr 2008, at 13.44, Jonathan Edwards wrote:
>
>> the cache file might have disappeared (/etc/zfs/zpool.cache)
>>
>> can you import them with a zpool import <pool>?
>>
>> On Apr 19, 2008, at 11:14 AM, Christopher LILJENSTOLPE wrote:
>>
>>> -----BEGIN PGP SIGNED MESSAGE-----
>>> Hash: SHA1
>>>
>>> Yup, I did.
>>>
>>> 	Chris
>>>
>>> On 19 Apr 2008, at 00.37, Mr. Zorg wrote:
>>>
>>>> Did you chown the files after dropping them in?  I could see  
>>>> where that may cause what you're seeing. I had no problems.
>>>>
>>>> On Apr 18, 2008, at 6:32 PM, Christopher LILJENSTOLPE <cdl@asgaard.org 
>>>> > wrote:
>>>>
>>>>> -----BEGIN PGP SIGNED MESSAGE-----
>>>>> Hash: SHA1
>>>>>
>>>>> Gretings,
>>>>>
>>>>> I just upgraded to the new release from 102A and had one 3x1TB  
>>>>> raidz pool on my 10.5 PPC server.  After I removed the files/ 
>>>>> dirs that 110 would over-write, I dropped the new 110 build  
>>>>> files in, and rebooted.  When the system came back, no pools or  
>>>>> filesystems built on them mounted, and a zpool list or zfs list  
>>>>> showed no pools found.
>>>>>
>>>>> Fighting back some panic, I ripped out the 110 load, and pulled  
>>>>> the 102A files down and re-installed them.  Rebooted, and still,  
>>>>> no pools.
>>>>>
>>>>> I have things backed up, but I'm "concerned" about pools that  
>>>>> just vanish in the night....
>>>>>
>>>>> Chris
>>>>>
>>>>> -----BEGIN PGP SIGNATURE-----
>>>>>
>>>>> iQEcBAEBAgAGBQJICUuuAAoJEGmx2Mt/+Iw/fKQH/if+dB97wiDMS/mxtU2nc6vy
>>>>> lMFoH7r5Wc8zHf6cX8aEWebxSDSnloA9K1V1nZBuXc7Vi2OJRxiyiI0/uLF+MPqO
>>>>> APh5evK/23iVfN/BUXY5elrPrkBnK53SV8Mf9QOi1kjM1WwH3YLy2mwjbl57f1zA
>>>>> Hm+ea5NW7hl69CXrwHQmJK/qkPYyQnOU3Tf4Fns79m+dzyeHnd9gowd6NY0mduOH
>>>>> jX0w76mrFR8vGSslk1aVrHtUVCgyOsgcmZzBk/HfdnF0caNGdvN1POL3LRexWCRq
>>>>> 3EpeRlDQbxNfEmPMzhiVstGxsnlzRlYp8Bz+Pc/T2Z8WCFu8+dY7CrbdIBH36rw=
>>>>> =6qFq
>>>>> -----END PGP SIGNATURE-----
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss@lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>>
>>>
>>> - ---
>>> ???
>>> Check my PGP key here:
>>> http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B
>>>
>>>
>>>
>>>
>>> -----BEGIN PGP SIGNATURE-----
>>>
>>> iQEcBAEBAgAGBQJICgxPAAoJEGmx2Mt/+Iw/RM8H/1oTx7lloI3CZ35db2AewYKD
>>> E8tePxsDZitrdzK0pw4w1HNEAgnSy1+avBE+0p89eIdO6WFcSzf5kHiQEXcmie6m
>>> 1x1yiu7m4UzNZDwvFtCU+Uo/E4msMjyvjDXhjsaxEUsO2pDXq7v3BV3D8SaA5zn8
>>> GXJtq/Paa8MpJK5sgKeskHrbIbVJkvnLZI8BrMUO2ZNWxe18d6PBafpXpjCLE8rC
>>> h2NSY0U+1a6GWXj465O8PgRCbdd+tFBzq29M1HymZIgxFzMIPFKli7zvB1uNEx9E
>>> 3rkUPofzIr+lcBlQif8j084cW5t8EhyEo+v/9fHo4MEsRrudRhd3m+xztceIuiA=
>>> =b/WE
>>> -----END PGP SIGNATURE-----
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>>
>
> - ---
> ???
> Check my PGP key here:
> http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B
>
>
>
>
> -----BEGIN PGP SIGNATURE-----
>
> iQEcBAEBAgAGBQJID1CIAAoJEGmx2Mt/+Iw/4xAH+QG6dP6GvVJ04JBdH0w6gQ0A
> IhJsSCiVHHVWhA6N1x5OO3mPf+kBRAP3IOegQHEKOQbdbM6zZL4vZEZkYDXm6sAh
> 6286HJwRLQMVflfg4ZFAYKKYq7f+HoyQW8Hq3zd954ZMmd8eeFlTj6eMvurkjSzW
> /fEwTSKxQklOFi2Hktis1csp03mNTt8r1Md94QbGSXO4+Ul6Nwm7722np91aebkU
> 6S88Zf38cesl9TMpxQFbFThR8rsfPS04w+8UZZsp5gdqpwd3oEQKSGED/48w1D3I
> uWfBtPwtyxYxNv5AyJmHbaK7zYo8CBVCjVx1zSX8PwctiWVtDMs3IgEHusX99Cs=
> =AuTp
> -----END PGP SIGNATURE-----
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss@lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

- ---
???
Check my PGP key here:
http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B




-----BEGIN PGP SIGNATURE-----

iQEcBAEBAgAGBQJID9KBAAoJEGmx2Mt/+Iw/pQAH/RicvZWURoWJHz4yvlcFphY5
qb0Kc69f60v03swneruELgqhXhsKwz7iollKhEdaZNzhY1HlgZYHmt7jAkhSZaPP
9ylSBMy/awL1eFLAX1qZ0LL8x5hooE1EaY4kmtr1KNfbBOkwVBDjX5eL/RmPKo23
QQOyAfwxT0DGNAyBcqyaP2PDHdx1b5kTWnutGRG3x0t+HDl1M2qrCVwKU2EsswIm
g9SBdk6HOT6nu0otuL1QmccH3yhLmu/Tij3HYq6xXHnY8U3E+QXZZafAWIKVqgE7
j+lVov5ygGGUgOi35N++DhcJ+DpM7tan/HVga4B+yX6voMOzkAWukV2GQFPKjlA=
=C+iz
-----END PGP SIGNATURE-----
From jbsnyder at gmail.com  Thu Apr 24 12:21:00 2008
From: jbsnyder at gmail.com (James Snyder)
Date: Thu Apr 24 12:18:14 2008
Subject: [zfs-discuss] weird zfs thread synchronization issue? (perhaps
	mmap is the culprit?)
In-Reply-To: <33644d3c0804230909k66b85244j88d34c3143552979@mail.gmail.com>
References: <33644d3c0804222055t29a6293fsa6f23fefc8dc5aff@mail.gmail.com>
	<420D0A27-82E1-494F-B990-E107B6AB3B24@martin-hauser.net>
	<33644d3c0804230909k66b85244j88d34c3143552979@mail.gmail.com>
Message-ID: <33644d3c0804241221q23ffbec3l92e5e8398fa0e92d@mail.gmail.com>

Yeah, I seem to be no longer having any real blocking issues with git
with NO_MMAP set.  I'm also not using the zfs functions you wrote.

Previously large checkouts + garbage collections would blow up, now
they seem to be fine.

On Wed, Apr 23, 2008 at 11:09 AM, James Snyder <jbsnyder@gmail.com> wrote:
> Hmm..  How well does the gentoo repo work on Leopard?  I seem to
>  recall taking note of it some time ago, but the integration seemed a
>  little lacking.
>
>  I'll give your script wrapper a try although between disabling mmap
>  and updating git, it seems a little easier to deal with weirdness.
>
>  I wish there were a way to get a little more debugging info from zfs
>  on what might be going on in specific situations, but I don't have the
>  courage to hack the source with only one Mac, which happens to depend
>  on ZFS :-)  I suppose the sysctls or similar could be extended to make
>  available such information.
>
>  No?l: is there a better way to be able to debug what's going on here
>  on the user end?  I noticed that the dtrace probes have been adjusted
>  slightly in the latest rev.  Is there a way to tap into those?  I
>  would have far fewer qualms about recompiling with just some probes
>  added as opposed to hacking more substantially leading to a situation
>  that could break my FS :-)
>
>
>
>
>
>  On Wed, Apr 23, 2008 at 2:24 AM, Martin Hauser <info@martin-hauser.net> wrote:
>  > Interesting that those things still happen on zfs, as far as I remember this
>  > version should not have any problems with MMAP?
>  >
>  >  Git was doing some weird behaviour in the past. If I have some free time
>  > (right now I am at work and can' t play around with git that much) I'll dig
>  > into the Gentoo repository and find out whether they build my version with
>  > or without mmap support.
>  >
>  >  Martin
>  >
>  >
>  >
>  >  On Apr 23, 2008, at 05:55 AM, James Snyder wrote:
>  >
>  >
>  > > I know I've already thrown out a fair amount of data here, but I've
>  > > played with this some more.
>  > >
>  > > Building git with NO_MMAP enabled (which, as one might guess) disables
>  > > the use of direct mmap syscalls in the git code, and seems to work
>  > > around this problem.  I no longer get any errors on the git rm
>  > > command.
>  > >
>  > > Side note: Weirdly, there are still mmap calls that show up when I
>  > > dtruss  the "git rm testme.txt" from the script I posted.  At the same
>  > > time, collecting stack traces for mmap system calls turns up nothing
>  > > in Instruments, where before without NO_MMAP disabled it logged a
>  > > number of calls. Perhaps mmap is getting called through something
>  > > else?
>  > >
>  > > If you would like to try this yourself, and you built git from
>  > > macports, you can replace the following file:
>  > >
>  > >
>  > /opt/local/var/macports/sources/rsync.macports.org/release/ports/devel/git-core/files/patch-Makefile.diff
>  > > --- Makefile.orig   2008-04-22 22:25:26.000000000 -0500
>  > > +++ Makefile    2008-04-22 22:26:20.000000000 -0500
>  > > @@ -164,8 +164,6 @@
>  > >
>  > > # CFLAGS and LDFLAGS are for the users to override from the command line.
>  > >
>  > > -CFLAGS = -g -O2 -Wall
>  > > -LDFLAGS =
>  > > ALL_CFLAGS = $(CFLAGS)
>  > > ALL_LDFLAGS = $(LDFLAGS)
>  > > STRIP ?= strip
>  > > @@ -682,6 +680,7 @@
>  > >           BASIC_LDFLAGS += -L/opt/local/lib
>  > >       endif
>  > >   endif
>  > > +   NO_MMAP = YesPlease
>  > > endif
>  > >
>  > > ifdef NO_R_TO_GCC_LINKER
>  > > @@ -689,7 +688,7 @@
>  > >   # the runtime dynamic library path.
>  > >   CC_LD_DYNPATH = -Wl,-rpath=
>  > > else
>  > > -   CC_LD_DYNPATH = -R
>  > > +   CC_LD_DYNPATH = -L
>  > > endif
>  > >
>  > > ifdef NO_CURL
>  > >
>  > > and also patch the git-core Portfile with this diff:
>  > >
>  > /opt/local/var/macports/sources/rsync.macports.org/release/ports/devel/git-core/Portfile
>  > > --- Portfile.orig       2008-04-22 22:48:33.000000000 -0500
>  > > +++ Portfile    2008-04-22 22:49:12.000000000 -0500
>  > > @@ -3,7 +3,7 @@
>  > > PortSystem        1.0
>  > >
>  > > name              git-core
>  > > -version           1.5.4.5
>  > > +version           1.5.5.1
>  > > description       The stupid content tracker.
>  > > long_description  A stupid (but extremely fast) directory content manager.
>  > \
>  > >                  It doesn't do a whole lot, but what it _does_ do is track
>  > \
>  > > @@ -18,8 +18,8 @@
>  > > distfiles         git-${version}${extract.suffix} \
>  > >                  git-manpages-${version}${extract.suffix}
>  > >
>  > > -checksums    git-${version}${extract.suffix} sha1
>  > > 69c4b904f13b72f57405393d54f33831c9cfad8f \
>  > > -             git-manpages-${version}${extract.suffix} sha1
>  > > 659b5217b342b757a01603f61bd90a4d60f7e681
>  > > +checksums    git-${version}${extract.suffix} sha1
>  > > a450cd02cbfe8e18311816a9568bcd6d10d6cf52 \
>  > > +             git-manpages-${version}${extract.suffix} sha1
>  > > e062f2eb9e46546616e9cc62ecc4bcd485d30d9a
>  > >
>  > > depends_run  port:openssh port:rsync port:perl5.8 port:p5-error
>  > > depends_lib  port:curl port:zlib port:openssl port:expat port:libiconv
>  > > @@ -74,7 +74,7 @@
>  > >
>  > > variant doc description {Install HTML and plaintext documentation} {
>  > >    distfiles-append    git-htmldocs-${version}${extract.suffix}
>  > > -    checksums-append    git-htmldocs-${version}${extract.suffix} sha1
>  > > 7087b54af84b8d91fec8d9a48c8f746047d3cd6a
>  > > +    checksums-append    git-htmldocs-${version}${extract.suffix} sha1
>  > > 56ae3fa2e5a7c64177af5122876f5fe063800b40
>  > > }
>  > >
>  > > variant gitweb description {Install gitweb.cgi} {
>  > >
>  > > So far, so good.
>  > >
>  > > On Tue, Apr 22, 2008 at 8:10 PM, James Snyder <jbsnyder@gmail.com> wrote:
>  > >
>  > > > It looks like Git 1.5.5.1 handles the situation a little better.  The
>  > > > repositories that get into the state of the one I zipped up appear to
>  > > > function again on ZFS.  I wonder if this has something to do with some
>  > > > of the changes in lstat behavior in the 1.5.5 revision?
>  > > >
>  > > > That said, there's definitely still something going on.  I've had
>  > > > builds in MacPorts get hung up on similar "files in limbo" issues.
>  > > > Sometimes waiting a bit helps, sometimes re-rm'ing files seems to do
>  > > > the trick.  A reboot also helped in one case where something got stuck
>  > > > and I couldn't get a limbo file to be properly deleted.  I suppose
>  > > > this doesn't have anything to do with mmap stuff, but it does sound
>  > > > like some sort of caching issue?  Perhaps associated with the arc or
>  > > > the zil?  Is there any way to manually flush things to disk or force
>  > > > things to sync up aside from "sync"?
>  > > >
>  > > > I don't see any external tunables like on freebsd for zfs, but is
>  > > > there a way to disable the zil, prefetch or other useful but
>  > > > not-quite-essential aspects of ZFS on Mac OS X?
>  > > >
>  > > >
>  > > >
>  > > > On Tue, Apr 22, 2008 at 12:30 PM, James Snyder <jbsnyder@gmail.com>
>  > wrote:
>  > > >
>  > > > > More weirdness...
>  > > > >
>  > > > > sample repo that is broken on zfs, not on hfs+...
>  > > > > http://fanplastic.org/files/weird_repo.tgz
>  > > > >
>  > > > >
>  > > > > The linked repo (just has one file with random data in it), gives me
>  > > > > this for git status on HFS+:
>  > > > >
>  > > > > # On branch master
>  > > > > nothing to commit (working directory clean)
>  > > > >
>  > > > > and this on ZFS:
>  > > > >
>  > > > > # On branch master
>  > > > > # Changed but not updated:
>  > > > > #   (use "git add <file>..." to update what will be committed)
>  > > > > #
>  > > > > #       modified:   testingfile.txt
>  > > > > #
>  > > > >
>  > > > > any ideas?  Try moving the unzipped dir between zfs and hfs+...
>  > > > > git-status -a gets things right on zfs, but git-status does not
>  > > > >
>  > > > >
>  > > > >
>  > > > > On Tue, Apr 22, 2008 at 11:15 AM, James Snyder <jbsnyder@gmail.com>
>  > wrote:
>  > > > >
>  > > > > > Might I ask if you're passing any special options to git status, or
>  > > > > > how you're doing your alias?
>  > > > > >
>  > > > > > If I run the attached (somewhat ugly) script, I intermittently get
>  > > > > > things like the following, usually within the first 5 runs or so:
>  > > > > > # On branch master
>  > > > > > # Changed but not updated:
>  > > > > > #   (use "git add <file>..." to update what will be committed)
>  > > > > > #
>  > > > > > #       modified:   testme.txt
>  > > > > > #
>  > > > > > no changes added to commit (use "git add" and/or "git commit -a")
>  > > > > > error: 'testme.txt' has local modifications
>  > > > > > (use --cached to keep the file, or -f to force removal)
>  > > > > >
>  > > > > >
>  > > > > > I can't get the same to happen when I try this on HFS+.
>  > > > > >
>  > > > > > git_test.sh
>  > > > > > #!/bin/zsh
>  > > > > >
>  > > > > > # anything more than the following, indicates a problem:
>  > > > > > # # On branch master
>  > > > > > # nothing to commit (working directory clean)
>  > > > > > # rm 'testme.txt'
>  > > > > >
>  > > > > > for i in {1..10}
>  > > > > > do
>  > > > > >       echo "RUN: $i"
>  > > > > >       DIRNAME=`dd if=/dev/urandom bs=30 count=1 | md5` &>/dev/null
>  > > > > >       mkdir $DIRNAME
>  > > > > >       cd $DIRNAME
>  > > > > >       git init &>/dev/null
>  > > > > >       echo hi > there &>/dev/null
>  > > > > >       git add there &>/dev/null
>  > > > > >       git commit -m 'Small repo' &>/dev/null
>  > > > > >       dd if=/dev/urandom of=testme.txt count=10 bs=1m &>/dev/null
>  > > > > >       git add testme.txt &>/dev/null
>  > > > > >       git commit -m 'Add big binary file' &>/dev/null
>  > > > > >       sleep 1
>  > > > > >       git status; git rm testme.txt
>  > > > > >       git commit -m 'delete binary file' &>/dev/null
>  > > > > >       cd ..
>  > > > > >       rm -rf $DIRNAME
>  > > > > >       echo "\n"
>  > > > > > done
>  > > > > >
>  > > > > >
>  > > > > >
>  > > > > >
>  > > > > > On Tue, Apr 22, 2008 at 9:20 AM, Martin Hauser
>  > <info@martin-hauser.net> wrote:
>  > > > > >
>  > > > > > > Everything seemed pretty consistent till now. The only thing I've
>  > ever
>  > > > > > > notice that is all git status related things (so the listing of
>  > the files in
>  > > > > > > git-commit, the commiting itself works fine). I would assume that
>  > the git's
>  > > > > > > index file is treated special somehow and doing a git diff
>  > rewrites that
>  > > > > > > file....
>  > > > > > >
>  > > > > > > Let me know if you find out more.
>  > > > > > >
>  > > > > > > Martin
>  > > > > > >
>  > > > > > >
>  > > > > > >
>  > > > > > > On Apr 22, 2008, at 15:49 PM, James Snyder wrote:
>  > > > > > >
>  > > > > > >
>  > > > > > >
>  > > > > > > > I'll give that a try.  I'm not 100% sure that that will resolve
>  > the
>  > > > > > > > problem though.
>  > > > > > > >
>  > > > > > > > Does that seem to work consistently or do you find yourself
>  > having to
>  > > > > > > > sometimes do things multiple times?
>  > > > > > > >
>  > > > > > > > I'm not sure what git is doing behind the scenes during a status
>  > > > > > > > check.  Is it refreshing the index somehow that obviates a
>  > little of
>  > > > > > > > this limbo file state issue?
>  > > > > > > >
>  > > > > > > > I haven't checked entirely whether this happens for creating,
>  > deleting
>  > > > > > > > and modifying, but if it happens on modifications too, that
>  > could lead
>  > > > > > > > to some messy issues.
>  > > > > > > >
>  > > > > > > > Example: Say, I was working with an sqlite database, made some
>  > > > > > > > changes, unlocked things, and subsequently another process hit
>  > the
>  > > > > > > > database and made changes but got the old version of the file
>  > and
>  > > > > > > > worked with that.
>  > > > > > > >
>  > > > > > > > On Tue, Apr 22, 2008 at 8:36 AM, Martin Hauser
>  > <info@martin-hauser.net>
>  > > > > > > >
>  > > > > > > wrote:
>  > > > > > >
>  > > > > > > >
>  > > > > > > >
>  > > > > > > > > Hi there,
>  > > > > > > > >
>  > > > > > > > > more on that issue,
>  > > > > > > > >
>  > > > > > > > > I've got some problems with it as well, but mostly due to git
>  > not
>  > > > > > > > >
>  > > > > > > >
>  > > > > > > catching
>  > > > > > >
>  > > > > > > >
>  > > > > > > > > commits immediatlly as such. For example:
>  > > > > > > > >
>  > > > > > > > > 1. do some work
>  > > > > > > > > 2. git add somefile
>  > > > > > > > > 3. git commit -m "some commit message"
>  > > > > > > > > 4. git status
>  > > > > > > > >
>  > > > > > > > > you'll notice that the file you added for commiting is still
>  > listed
>  > > > > > > > >
>  > > > > > > > > For me, it was easy enough to 'work around' just by doing a
>  > git status
>  > > > > > > > > before every index file related operation (just a small alias
>  > in my
>  > > > > > > > >
>  > > > > > > >
>  > > > > > > .zshrc),
>  > > > > > >
>  > > > > > > >
>  > > > > > > > > might be work-around-ing your problem also James?
>  > > > > > > > >
>  > > > > > > > > I'd be much prefering the issue to be fixed though.
>  > > > > > > > >
>  > > > > > > > > Martin
>  > > > > > > > >
>  > > > > > > > >
>  > > > > > > > >
>  > > > > > > > > On Apr 22, 2008, at 15:31 PM, James Snyder wrote:
>  > > > > > > > >
>  > > > > > > > >
>  > > > > > > > >
>  > > > > > > > >
>  > > > > > > > > >
>  > > > > > > > > >
>  > > > > > > > > >
>  > > > > > > > > > SUMMARY
>  > > > > > > > > > During the following situations:
>  > > > > > > > > > git reset --hard <revision>
>  > > > > > > > > > git-svn clone ... (on a large svn repository)
>  > > > > > > > > > (sometimes followed by)
>  > > > > > > > > > rm -rf <some directory> (after just working with a lot of
>  > files, like
>  > > > > > > > > > one of the above commands)
>  > > > > > > > > >
>  > > > > > > > > > The listed git commands sometimes fail, sometimes succeed,
>  > citing
>  > > > > > > > > > sometimes missing files.  With rm -rf, shortly after the
>  > operation an
>  > > > > > > > > > ls or other file operation will sometimes show certain
>  > > > > > > > > > directories/files just operated on and sometimes not.  This
>  > may or may
>  > > > > > > > > > not stabilize in the subsequent seconds.  This behavior
>  > leaves me with
>  > > > > > > > > > the sense that the file operations are talking to different
>  > threads
>  > > > > > > > > > associated with ZFS and that they're not all on the "same
>  > page"
>  > > > > > > > > >
>  > > > > > > > > > STEPS TO REPRODUCE
>  > > > > > > > > > 1. Find a rather large svn repository, with lots of
>  > revisions and
>  > > > > > > > > > files (or make one).  The repo I'm working with has lots of
>  > binary
>  > > > > > > > > > files in it and the db for svn is around 9 gigabytes with
>  > 4000
>  > > > > > > > > > revisions or so.
>  > > > > > > > > > 2. Install latest git w/ svn support from macports. (sudo
>  > port install
>  > > > > > > > > >
>  > > > > > > > >
>  > > > > > > >
>  > > > > > > git
>  > > > > > >
>  > > > > > > >
>  > > > > > > > >
>  > > > > > > > > >
>  > > > > > > > > >
>  > > > > > > > > +svn)
>  > > > > > > > >
>  > > > > > > > >
>  > > > > > > > > > 3. do a git-svn clone on your repository
>  > > > > > > > > > 4. With the current version of git-svn, it should try and
>  > repack files
>  > > > > > > > > > at different stages during the clone process if you have
>  > enough
>  > > > > > > > > > revisions, you will likely see a failure after this point
>  > when it
>  > > > > > > > > > tries to pick up and check out the next increment from svn.
>  > > > > > > > > > 5. Quickly look for the files it says it cannot find in the
>  > .git
>  > > > > > > > > > directory of your partial clone, you should find that
>  > sometimes it is
>  > > > > > > > > > there and sometimes not.
>  > > > > > > > > >
>  > > > > > > > > > RESULTS
>  > > > > > > > > > Files associated with recent large batches of operations are
>  > there/not
>  > > > > > > > > > there with subsequent "ls" listings.  If no new userland
>  > operations
>  > > > > > > > > > are pending on the file, the file should either be there or
>  > not there,
>  > > > > > > > > > not in limbo.
>  > > > > > > > > >
>  > > > > > > > > > REGRESSION
>  > > > > > > > > > I haven't checked this with 102A or other revs, just 111.
>  > > > > > > > > >
>  > > > > > > > > > NOTES
>  > > > > > > > > > Running current 111 release on a MacBook, with a single pool
>  > on a
>  > > > > > > > > > single partition.
>  > > > > > > > > >
>  > > > > > > > > > --
>  > > > > > > > > > James Snyder
>  > > > > > > > > > Biomedical Engineering
>  > > > > > > > > > Northwestern University
>  > > > > > > > > > jbsnyder@gmail.com
>  > > > > > > > > > _______________________________________________
>  > > > > > > > > > zfs-discuss mailing list
>  > > > > > > > > > zfs-discuss@lists.macosforge.org
>  > > > > > > > > > http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>  > > > > > > > > >
>  > > > > > > > > >
>  > > > > > > > > >
>  > > > > > > > >
>  > > > > > > > >
>  > > > > > > > >
>  > > > > > > > >
>  > > > > > > >
>  > > > > > > >
>  > > > > > > >
>  > > > > > > > --
>  > > > > > > > James Snyder
>  > > > > > > > Biomedical Engineering
>  > > > > > > > Northwestern University
>  > > > > > > > jbsnyder@gmail.com
>  > > > > > > >
>  > > > > > > >
>  > > > > > >
>  > > > > > >
>  > > > > > >
>  > > > > >
>  > > > > >
>  > > > > >
>  > > > > > --
>  > > > > >
>  > > > > >
>  > > > > > James Snyder
>  > > > > > Biomedical Engineering
>  > > > > > Northwestern University
>  > > > > > jbsnyder@gmail.com
>  > > > > >
>  > > > > >
>  > > > >
>  > > > >
>  > > > >
>  > > > > --
>  > > > >
>  > > > >
>  > > > > James Snyder
>  > > > > Biomedical Engineering
>  > > > > Northwestern University
>  > > > > jbsnyder@gmail.com
>  > > > >
>  > > > >
>  > > >
>  > > >
>  > > >
>  > > > --
>  > > >
>  > > >
>  > > > James Snyder
>  > > > Biomedical Engineering
>  > > > Northwestern University
>  > > > jbsnyder@gmail.com
>  > > >
>  > > >
>  > >
>  > >
>  > >
>  > > --
>  > > James Snyder
>  > > Biomedical Engineering
>  > > Northwestern University
>  > > jbsnyder@gmail.com
>  > >
>  >
>  >
>
>
>
>  --
>
>
> James Snyder
>  Biomedical Engineering
>  Northwestern University
>  jbsnyder@gmail.com
>



-- 
James Snyder
Biomedical Engineering
Northwestern University
jbsnyder@gmail.com
From canadrian at electricteaparty.net  Fri Apr 25 11:02:15 2008
From: canadrian at electricteaparty.net (Adrian Thornton)
Date: Fri Apr 25 10:59:29 2008
Subject: [zfs-discuss] RE: Time machine with ZFS backend
In-Reply-To: <5D6879E9-C1DD-40C6-8536-A79FFB615306@spamfreemail.de>
References: <14AB7A2B-58DF-4A5C-8D14-A06DEDBE25CB@electricteaparty.net>
	<5D6879E9-C1DD-40C6-8536-A79FFB615306@spamfreemail.de>
Message-ID: <938A6F82-AE62-425D-8D1B-9D13E260C11E@electricteaparty.net>

I tried this method, but now Time Machine is complaining it couldn't  
create the backup disk image. Any idea why this might be happening?
Thanks,
Adrian

On Apr 20, 2008, at 01:03 , Franz Schmalzl wrote:

>
> On Apr 19, 2008, at 8:49 PM, Adrian Thornton wrote:
>
>> Hey there, just wondering if anyone has anything else to say about  
>> the below-described method. Also, just curious why Time Machine  
>> would not work simply with a writable disk image on the ZFS pool,  
>> without going to all the AFS/SSH trouble described below? If Time  
>> Machine has a properly formatted disk image, why would it even care  
>> what filesystem that disk image is stored on?
>
> Time machine just won't let you choose a local sparsbundle....  
> that's why the afp/ssh thing is needed....
>
>
>>
>>
>> Thanks,
>> Adrian
>>
>>> [zfs-discuss] Time machine with ZFS backend
>>> Franz Schmalzl franzschmalzl at spamfreemail.de
>>> Sat Mar 22 06:17:42 PDT 2008
>>>
>>> Hi list!
>>>
>>> I just managed to get Time machine working on my external raidz...
>>> it's a bit dirty, but works
>>>
>>> Steps:
>>>
>>> Create a zfs volume
>>> Share over the network via afp
>>> open up ssh tunnel to localhost, since afp won't let you connect to
>>> localhost
>>> (ssh user at localhost -L randomportnumber:localhost:548 )
>>> Command+K, apf://localhost:randomportnumber)
>>>
>>> (( of course randomportnumber hast to be a random port number ;),  
>>> like
>>> 1234 )
>>>
>>> Your "network" share should get mounted  and you can use it for time
>>> machine witch creates i'ts sparsebundles...
>>>
>>> p.s.
>>>
>>> don't forget to apply
>>>
>>> defaults write com.apple.systempreferences
>>> TMShowUnsupportedNetworkVolumes 1
>>>
>>>
>>> regards
>>>
>>> franz schmalzl
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss@lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>

From bwaters at nrao.edu  Fri Apr 25 11:04:26 2008
From: bwaters at nrao.edu (Boyd Waters)
Date: Fri Apr 25 11:02:31 2008
Subject: [zfs-discuss] Re: [OG #289023] OmniGraffle Pro 5.0
In-Reply-To: <rt-3.6.6-289023-6081148-6.11.3334894187962@omnigroup.com>
References: <RT-Ticket-289023@omnigroup.com>
	<AAC1A520-0170-44E3-9D44-912C8898A732@nrao.edu>
	<rt-3.6.6-21021-1209144893-1005.289023-0-0@omnigroup.com>
	<rt-3.6.6-289023-6081038-6.0.229647992134758@omnigroup.com>
	<28A0494B-86F2-4B4D-A3FD-B82C0D4CB664@nrao.edu>
	<rt-3.6.6-21217-1209145999-611.289023-0-0@omnigroup.com>
	<rt-3.6.6-21566-1209146125-161.289023-0-0@omnigroup.com>
	<rt-3.6.6-289023-6081148-6.11.3334894187962@omnigroup.com>
Message-ID: <A5405337-DC0E-476F-AD27-923482706A1C@nrao.edu>

In general, everything works.

ZFS has omigod support for extended attributes, and I generally have  
no trouble saving things.

    $ ls -ld@  vision-enrollment.graffle
    drwxr-xr-x@ 3 bwaters  staff  6 Apr 25 11:27 vision- 
enrollment.graffle
	com.apple.FinderInfo	32


Weird! I can double-click to open the file, make a trivial change, but  
OmniGraffle doesn't want to save it: "The document could not be saved."

So I go to File..Save As... and make a trivial change to the file  
name, save it in the SAME directory. It saves *fine*.

Make a trivial change to *that* copy, try to save with File/Save, and  
OG says "no can do".


What's going on?




On Apr 25, 2008, at 11:55 AM, OmniGraffle Support wrote:

> Boyd-
>
> Aha, suddenly I'm wondering if this test version of ZFS can handle  
> file packages -- Do you have trouble normally saving files to it  
> when done so by hand?
>
> Joel Page
> Product Manager, OmniGraffle
> The Omni Group
>
>> [Boyd Waters - Fri Apr 25 10:53:19 2008]:
>>
>> Thanks, Joel!
>>
>> I didn't give you the whole story, of course...
>>
>> My home directory is on a crazy test version of ZFS, it's hosted on  
>> an
>> external ZFS RAID at the moment.
>> http://trac.macosforge.org/projects/zfs/wiki/
>>
>> The most recent version is supposed to be mmap-coherent, so I don't
>> know what the problem is.
>>
>> But if I enable "automatic backups", then every 2 minutes I get a
>> modal document sheet that says the backup cannot be saved because the
>> file isn't found.
>>
>> Can you think of a log that I can poke in?
>>
>>
>> Thanks!
>>
>>
>> On Apr 25, 2008, at 11:34 AM, OmniGraffle Support wrote:
>>
>>> Boyd-
>>>
>>> Interesting, I shouldn't think that the location of your home
>>> directory wouldn't have any bearing on the autosave/backup, as that
>>> backup gets created in the same directory as the OmniGraffle file.
>>>
>>> We'll poke around this a bit, but it stills sounds to me like it
>>> shouldn't have any effect.
>>>
>>> Thanks for the feedback,
>>>
>>> Joel Page
>>> Product Manager, OmniGraffle
>>> The Omni Group
>>>
>>>> [Boyd Waters - Fri Apr 25 10:13:29 2008]:
>>>>
>>>> Automatic Backup does NOT work if your $HOME is not /Users/username
>>>>
>>>>
>>>>
>>>>
>>>
>>
>>
>>
>>
>>   - boyd
>>
>>
>> Boyd Waters
>> Scientific Programmer
>> National Radio Astronomy Observatory
>> Socorro, New Mexico
>> http://www.aoc.nrao.edu/~bwaters
>>
>>
>>
>

From franzschmalzl at spamfreemail.de  Fri Apr 25 13:29:54 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Fri Apr 25 13:27:15 2008
Subject: [zfs-discuss] RE: Time machine with ZFS backend
In-Reply-To: <938A6F82-AE62-425D-8D1B-9D13E260C11E@electricteaparty.net>
References: <14AB7A2B-58DF-4A5C-8D14-A06DEDBE25CB@electricteaparty.net>
	<5D6879E9-C1DD-40C6-8536-A79FFB615306@spamfreemail.de>
	<938A6F82-AE62-425D-8D1B-9D13E260C11E@electricteaparty.net>
Message-ID: <C72E059B-A80F-4C97-BA60-E4C975510B7E@spamfreemail.de>


On Apr 25, 2008, at 8:02 PM, Adrian Thornton wrote:

> I tried this method, but now Time Machine is complaining it couldn't  
> create the backup disk image. Any idea why this might be happening?

jap, diskutil at the moment does not manage to create sparsebundles on  
zfs volumes
so when starting TM go to the volume you selected, write down the name  
or make a screenshot ( be quick as the sparsbundle disappears) then  
open up diskutility create the sparsebundle yourself and name it the  
exact name TM did ( create a sparsbundle using GPT)

save it to your desktop or something and copy it over to your zfs  
volume manually..

this way it works

regards

franz

>
> Thanks,
> Adrian
>
> On Apr 20, 2008, at 01:03 , Franz Schmalzl wrote:
>
>>
>> On Apr 19, 2008, at 8:49 PM, Adrian Thornton wrote:
>>
>>> Hey there, just wondering if anyone has anything else to say about  
>>> the below-described method. Also, just curious why Time Machine  
>>> would not work simply with a writable disk image on the ZFS pool,  
>>> without going to all the AFS/SSH trouble described below? If Time  
>>> Machine has a properly formatted disk image, why would it even  
>>> care what filesystem that disk image is stored on?
>>
>> Time machine just won't let you choose a local sparsbundle....  
>> that's why the afp/ssh thing is needed....
>>
>>
>>>
>>>
>>> Thanks,
>>> Adrian
>>>
>>>> [zfs-discuss] Time machine with ZFS backend
>>>> Franz Schmalzl franzschmalzl at spamfreemail.de
>>>> Sat Mar 22 06:17:42 PDT 2008
>>>>
>>>> Hi list!
>>>>
>>>> I just managed to get Time machine working on my external raidz...
>>>> it's a bit dirty, but works
>>>>
>>>> Steps:
>>>>
>>>> Create a zfs volume
>>>> Share over the network via afp
>>>> open up ssh tunnel to localhost, since afp won't let you connect to
>>>> localhost
>>>> (ssh user at localhost -L randomportnumber:localhost:548 )
>>>> Command+K, apf://localhost:randomportnumber)
>>>>
>>>> (( of course randomportnumber hast to be a random port number ;),  
>>>> like
>>>> 1234 )
>>>>
>>>> Your "network" share should get mounted  and you can use it for  
>>>> time
>>>> machine witch creates i'ts sparsebundles...
>>>>
>>>> p.s.
>>>>
>>>> don't forget to apply
>>>>
>>>> defaults write com.apple.systempreferences
>>>> TMShowUnsupportedNetworkVolumes 1
>>>>
>>>>
>>>> regards
>>>>
>>>> franz schmalzl
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss@lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>
>

From cdl at asgaard.org  Wed Apr 30 20:14:59 2008
From: cdl at asgaard.org (Christopher LILJENSTOLPE)
Date: Wed Apr 30 20:11:52 2008
Subject: [zfs-discuss] Can't export a ZFS FS via AFP
Message-ID: <E868F846-19B7-4887-92A3-F76BD50B70CA@asgaard.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Greetings,

	I have a tank where I've created a number of zfs file systems under  
10.5.2 server with build 111.  Something like

tank/parent/child.  /parent and /parent/child are mounted in / 
Volumes.  tank is not.  One interesting datapoint is that these don't  
show up as mounted file systems or drives in finder.  And since / 
Volumes don't show up in finder, normally, I have to do a goto to get  
to them via the finder.  That's a side issue, however.

In server admin, I can't get the sharing manager to see /Volumes/ 
parent/child to make it a sharepoint under afp.  I can do so using the  
sharing cli.

If I make parent/child a sharepoint using sharing, I can see it on a  
client, I am unable to browse to it, and a connect to server afp://server/child 
  (the sharepoint) fails.

On the sharing manager in serveradmin, once I've used sharing to share  
parent/child out, I can see child as a share, but any attempt to click  
on it is rejected (the next sharepoint is selected, instead).  Any  
ideas?

	Chris

- ---
???
Check my PGP key here:
http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B




-----BEGIN PGP SIGNATURE-----

iQEcBAEBAgAGBQJIGTWzAAoJEGmx2Mt/+Iw/ol4H/jSZiySzA5vns1+rwM6tVFB5
lF09Q/3+TqTXL8nirj1x9SI6wIP2wvWymQoKcsx+8xj2PSdglpMwq1CP53NaIwRe
CRVljCFcDNr3xstjP9wDOx9bkyGLs3H0KB4CG9s40EVEYl1ieuUUvoQvIy1f1EWv
uOWYk4LLn6wwr2767rhYSKIjR/CSmtO2mV+bhDIVc6zXe5CYycLFWH8hq3ZCcOkQ
zDnln303rjzjzgzwsyUcbsQMFJDPd7WqKDAnXayLZZm8cY2fCn75d9O9HcCJf5Mh
DlrUBdpU4r8fGU4fzf8Wo7xcrv+VAMR7vof8auXfbsBF8xnJaJG6W+gVn6mf3fU=
=eLOl
-----END PGP SIGNATURE-----
From Alexander_Niemeyer at mckinsey.com  Mon Apr 28 10:12:23 2008
From: Alexander_Niemeyer at mckinsey.com (Alexander_Niemeyer@mckinsey.com)
Date: Mon May  5 17:42:23 2008
Subject: [zfs-discuss] zfs and Finder and Sharing difficulties
Message-ID: <OF46F80977.258FE3FE-ON85257439.005D301A-85257439.005E852B@MCKINSEY.COM>

Hi all! Long-time lurker and always enjoy the good conversations.

I was wondering whether other folks had similar issues with zfs and the 
Finder as well as sharing.

Setup: zfs-111, one pool with 4 filesystems on it (RAIDZ of 4 hard drives)

Through the command line everything is smooth. Through the Finder, 
numerous troubles
- Finder does not recognize the path of the filesystems. E.g., filesystem 
tank/Backups will be shown as an alias in Finder view tank. Once clicking 
on Backups, the listed path just shows tank. Also, none of the filesystems 
can be moved into the Places column (as they are shown as aliases)
- Moving from filesystem to filesystem does not work. Everything gets 
copied fine, but Finder says the items are in use at the source and cannot 
be deleted
- Sharing this pool shows 5 separate shares (as seen from a Macbook): the 
pool tank and each of the 4 file systems as tank1, tank2, ... tank4, not 
the real names of the filesystems (e.g., Backups)

Since this topic has not shown up on the list, am I doing something 
obviously wrong?

Alex

+=========================================================+
This message may contain confidential and/or privileged
information.  If you are not the addressee or authorized to
receive this for the addressee, you must not use, copy,
disclose or take any action based on this message or any
information herein.  If you have received this message in
error, please advise the sender immediately by reply e-mail
and delete this message.  Thank you for your cooperation.
+=========================================================+
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080428/8507604c/attachment.html
From canadrian at electricteaparty.net  Fri Apr 25 17:22:26 2008
From: canadrian at electricteaparty.net (Thornton Adrian)
Date: Mon May  5 17:45:00 2008
Subject: [zfs-discuss] RE: Time machine with ZFS backend
In-Reply-To: <C72E059B-A80F-4C97-BA60-E4C975510B7E@spamfreemail.de>
References: <14AB7A2B-58DF-4A5C-8D14-A06DEDBE25CB@electricteaparty.net>
	<5D6879E9-C1DD-40C6-8536-A79FFB615306@spamfreemail.de>
	<938A6F82-AE62-425D-8D1B-9D13E260C11E@electricteaparty.net>
	<C72E059B-A80F-4C97-BA60-E4C975510B7E@spamfreemail.de>
Message-ID: <9757C1CF-18B3-4CD0-9A39-F98A134EBC76@electricteaparty.net>

Excellent, that did the trick. One side note is disk utility didn't  
want to create a 1TB sparsebundle on the OS drive, which only had 40gb  
free. I had to aim the "save as" window at the zfs pool, choose the  
disk image size, and then select a location on the OS drive after  
setting the size. But then I created it, copied it to the raidz, and  
bingo it works! I think I'll set up an automator app to initiate the  
ssh and mount the drive every time I reboot. :)

- Adrian

On 25-Apr-08, at 2:29 PM, ruebezahl wrote:

>
> On Apr 25, 2008, at 8:02 PM, Adrian Thornton wrote:
>
>> I tried this method, but now Time Machine is complaining it  
>> couldn't create the backup disk image. Any idea why this might be  
>> happening?
>
> jap, diskutil at the moment does not manage to create sparsebundles  
> on zfs volumes
> so when starting TM go to the volume you selected, write down the  
> name or make a screenshot ( be quick as the sparsbundle disappears)  
> then open up diskutility create the sparsebundle yourself and name  
> it the exact name TM did ( create a sparsbundle using GPT)
>
> save it to your desktop or something and copy it over to your zfs  
> volume manually..
>
> this way it works
>
> regards
>
> franz
>
>>
>> Thanks,
>> Adrian
>>
>> On Apr 20, 2008, at 01:03 , Franz Schmalzl wrote:
>>
>>>
>>> On Apr 19, 2008, at 8:49 PM, Adrian Thornton wrote:
>>>
>>>> Hey there, just wondering if anyone has anything else to say  
>>>> about the below-described method. Also, just curious why Time  
>>>> Machine would not work simply with a writable disk image on the  
>>>> ZFS pool, without going to all the AFS/SSH trouble described  
>>>> below? If Time Machine has a properly formatted disk image, why  
>>>> would it even care what filesystem that disk image is stored on?
>>>
>>> Time machine just won't let you choose a local sparsbundle....  
>>> that's why the afp/ssh thing is needed....
>>>
>>>
>>>>
>>>>
>>>> Thanks,
>>>> Adrian
>>>>
>>>>> [zfs-discuss] Time machine with ZFS backend
>>>>> Franz Schmalzl franzschmalzl at spamfreemail.de
>>>>> Sat Mar 22 06:17:42 PDT 2008
>>>>>
>>>>> Hi list!
>>>>>
>>>>> I just managed to get Time machine working on my external raidz...
>>>>> it's a bit dirty, but works
>>>>>
>>>>> Steps:
>>>>>
>>>>> Create a zfs volume
>>>>> Share over the network via afp
>>>>> open up ssh tunnel to localhost, since afp won't let you connect  
>>>>> to
>>>>> localhost
>>>>> (ssh user at localhost -L randomportnumber:localhost:548 )
>>>>> Command+K, apf://localhost:randomportnumber)
>>>>>
>>>>> (( of course randomportnumber hast to be a random port  
>>>>> number ;), like
>>>>> 1234 )
>>>>>
>>>>> Your "network" share should get mounted  and you can use it for  
>>>>> time
>>>>> machine witch creates i'ts sparsebundles...
>>>>>
>>>>> p.s.
>>>>>
>>>>> don't forget to apply
>>>>>
>>>>> defaults write com.apple.systempreferences
>>>>> TMShowUnsupportedNetworkVolumes 1
>>>>>
>>>>>
>>>>> regards
>>>>>
>>>>> franz schmalzl
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss@lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo/zfs-discuss
>>>
>>
>

