<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [zfs-discuss] There is something very wrong with the MacOS ZFS	documentation
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=Re%3A%20%5Bzfs-discuss%5D%20There%20is%20something%20very%20wrong%20with%20the%20MacOS%20ZFS%0A%09documentation&In-Reply-To=%3CD53B4558-F1DF-4E63-950F-5228492BCEC1%40halfpast.net%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001563.html">
   <LINK REL="Next"  HREF="001564.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[zfs-discuss] There is something very wrong with the MacOS ZFS	documentation</H1>
    <B>Jon Moog</B> 
    <A HREF="mailto:zfs-discuss%40lists.macosforge.org?Subject=Re%3A%20%5Bzfs-discuss%5D%20There%20is%20something%20very%20wrong%20with%20the%20MacOS%20ZFS%0A%09documentation&In-Reply-To=%3CD53B4558-F1DF-4E63-950F-5228492BCEC1%40halfpast.net%3E"
       TITLE="[zfs-discuss] There is something very wrong with the MacOS ZFS	documentation">jon at halfpast.net
       </A><BR>
    <I>Tue May  5 10:44:57 PDT 2009</I>
    <P><UL>
        <LI>Previous message: <A HREF="001563.html">[zfs-discuss] There is something very wrong with the MacOS ZFS	documentation
</A></li>
        <LI>Next message: <A HREF="001564.html">[zfs-discuss] There is something very wrong with the MacOS ZFS	documentation
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1556">[ date ]</a>
              <a href="thread.html#1556">[ thread ]</a>
              <a href="subject.html#1556">[ subject ]</a>
              <a href="author.html#1556">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>
On May 5, 2009, at 5:37 AM, Alex Bowden wrote:

&gt;<i>
</I>&gt;<i> Hi
</I>&gt;<i>
</I>&gt;<i> There is something very wrong with the MacOS ZFS documentation (and  
</I>&gt;<i> also to an extent software).
</I>
Unless you are using the read only version shipped with Leopard you  
are using beta unsupported software. It should be expected that beta  
software will not behave well in all conditions and documentation may  
be lacking. Caveat Emptor

&gt;<i>
</I>&gt;<i> I have been using ZFS under Solaris for a couple of years and know  
</I>&gt;<i> it to a superb facility.
</I>
Indeed, I would echo those sentiments.

&gt;<i>
</I>&gt;<i> Otherwise, my initial experience on the Mac would have caused me to  
</I>&gt;<i> bin it.
</I>&gt;<i>
</I>&gt;<i> Lets start with the Documentation.  The main ZFS documentation set  
</I>&gt;<i> documents ZFS so what I am looking for from the Mac side is simply  
</I>&gt;<i> to document the differences and issues in using the software under  
</I>&gt;<i> MacOS.   If fails dismally.
</I>
The documentation is largely the Sun documents and anecdotal  
suggestions relating to the variations and limitations in the OS X  
port. As with most software projects user documentation is delivered  
closer to the end of the development cycle.

&gt;<i> It fails so badly that I, and others, are loosing zpools  (Oh, God,  
</I>&gt;<i> I lost every thing after a reboot !!!).
</I>
In my personal experience no data has been lost regardless of the  
torture I've subjected the file system to. From the history of this  
mailing list it appears that other than a panic here and there over a  
missing pool the integrity of data has been quite high. Of course if  
you are expecting the polish of a commercial release you could be  
disappointed.

&gt;<i> Lets start with the &quot;getting started&quot;.
</I>&gt;<i>
</I>&gt;<i> Paragraph one.
</I>&gt;<i>
</I>&gt;<i> 	&quot;In all cases, the disks need to use the GUID Partition Table (GPT)  
</I>&gt;<i> and ZFS typically works best when it owns the entire disk due in  
</I>&gt;<i> part to how conservative it is with the write cache.&quot;
</I>&gt;<i>
</I>&gt;<i> Now it is well documented that ZFS works best when it owns the  
</I>&gt;<i> entire disk, partly I believe because it can then control the  
</I>&gt;<i> caching strategy for the disk.
</I>
Perhaps because it doesn't have to compete with other entities vying  
for access to the disk.

&gt;<i> The trouble is that under MacOS it seems to be essential that you  
</I>&gt;<i> DON'T give ZFS the whole disk.  If you do it will work fine until  
</I>&gt;<i> you reboot and then it'll trash your zpool.
</I>
I might expect the word essential to indicate data loss will occur.  
Failure of a pool to automounting does not jeopardize the data  
integrity and solutions are readily available.

&gt;<i> The examples work rather better than the stated advice.
</I>&gt;<i>
</I>&gt;<i> 	# diskutil partitiondisk /dev/disk2 GPTFormat ZFS %noformat% 100%
</I>&gt;<i> 	# zpool create puddle /dev/disk2s2
</I>&gt;<i>
</I>&gt;<i> OK,  that works fine BUT
</I>
If it works, use it.

&gt;<i>
</I>&gt;<i> A)	It does NOT give ZFS the whole disk.  It gives a single partition  
</I>&gt;<i> i.e.  slice 2 of the disk.   This is most of the disk but not the  
</I>&gt;<i> whole of it.
</I>
In this particular case I doubt there will be any performance issue  
and the loss of space should be negligible. Of course the full disk  
option exists for someone who wants it.

&gt;<i> B)	It silently introduces a new concept which is a partition of type  
</I>&gt;<i> ZFS which isn't even offered as an option in diskutil.app but which  
</I>&gt;<i> seems to be essential for a stable zpool.
</I>
I think the silent introduction of new features is a function of the  
developmental nature of this file system on OS X. This is an  
experimental implementation of ZFS on OS X and production quality and  
features are not going to be present.

&gt;<i> C)	Why introduce a new user to a single disk zpool.   Thats about as  
</I>&gt;<i> useful as a banking regulator.
</I>
We learn to walk before we learn to run. Many users with laptops would  
find creating a pool on the internal disk similar.

&gt;<i> And I am left trying to guess whether the Mac ZFS handles a 100% ZFS  
</I>&gt;<i> slice in a GPT partition as being the whole disk for caching  
</I>&gt;<i> purposes, or whether I end up with a degraded ZFS because it doesn't  
</I>&gt;<i> have the whole disk.
</I>
As I understand it the partition step simply identifies the disk as a  
ZFS slice. No ramification for performance should exist.

&gt;<i> What else don't you tell us.  Lets look ahead a little.
</I>&gt;<i>
</I>&gt;<i> Well how about the zfs command.  Anyone used to ZFS will know that  
</I>&gt;<i> the ZFS filesystem created by the zpool command is not generally  
</I>&gt;<i> used as a working filesystems but as an administrative bucket for  
</I>&gt;<i> the zpool.
</I>
The OS X implementation creates a file system automatically which is  
mounted in /Volumes/. This is a platform specific behavior.

&gt;<i> After creating the zpool it is then normal to add your ZFS  
</I>&gt;<i> filesystems using the zpool command.  This is completely  
</I>&gt;<i> unmentioned, which helps gloss over the problems mounting other ZFS  
</I>&gt;<i> file systems once created.
</I>
The man pages and Sun documentation provide ample examples of this.

&gt;<i> Then there is the fact that about one or two ZFS partition creation  
</I>&gt;<i> commands,  the disk system gets manically busy for an indeterminate  
</I>&gt;<i> period of time, &quot;diskutil list&quot; returns unstable results and the  
</I>&gt;<i> machine is likely to freeze.
</I>
I have not experienced this. I will reiterate that this is development  
quality software and as they say should not be used in a production  
environment.

&gt;<i> It seems to be best to build one ZFS partition at time.  Wait for  
</I>&gt;<i> all disk activity to stop, and then reboot, before building the next  
</I>&gt;<i> one.
</I>
Good advice and something others with a similar setup could benefit  
from. Specifically stating your setup would be helpful.

&gt;<i> So now lets look at the software.  The good news is that it is  
</I>&gt;<i> possible to build a working ZFS filesystem if you a) do that is  
</I>&gt;<i> expected rather than what the &quot;getting started&quot; says works best and  
</I>&gt;<i> b) build the ZFS partitions very gently.
</I>
The fact you have it working echos the experience of many on this list.

&gt;<i> But if instead you follow the worded instructions, ZFS manual,  
</I>&gt;<i> general ZFS documentation etc, and give it the whole disk
</I>&gt;<i>
</I>&gt;<i> 	zpool create lake raidz /dev/disk2 /dev/disk3 /dev/disk4 /dev/disk5
</I>&gt;<i>
</I>&gt;<i> You now have a whole pile of trouble.  loss of the whole zpool when  
</I>&gt;<i> you reboot is just the beginning.
</I>
I would argue the end as well.

&gt;<i> Because while you go off and start again trying to set up your  
</I>&gt;<i> system.  zfs gets clever and starts trying to recover the zpool.  I  
</I>&gt;<i> never succeeds, but it wont easily stop either.
</I>
If you import the pool all should be well in my experience.

&gt;<i> There is a very pretty situation in diskutil.app where all the  
</I>&gt;<i> partitions that were part of &quot;lake&quot; keep appearing and vanishing  
</I>&gt;<i> again, out of phase with each other, at about one second intervals.
</I>
Diskutil.app is not properly aware of ZFS. It is advisable to stick to  
the command line for ZFS at this point.

&gt;<i> Reformat the disks as MBR all free space and then back as GPT  
</I>&gt;<i> (usually clears anything), but no,  zfs still finds them again.
</I>
This type of disk trickery is likely to break the stability of the  
system and really isn't necessary.

&gt;<i> Documetation is very clear that you must delete the zpool using the  
</I>&gt;<i> zpool destroy command.
</I>&gt;<i>
</I>&gt;<i> But you can't do that when ZFS think that the pool doesn't exist.
</I>
You must manually import the pool if the disks don't have the proper  
partition format.

&gt;<i> In the end the only way I managed to move forward was to zero the  
</I>&gt;<i> disks with Ranish from a live linux cd.
</I>
If you have to, disabling the ZFS kext gets you to the same point.

&gt;<i> And this is all the consequence of following the standard advise in  
</I>&gt;<i> the MacOS ZFS &quot;getting started&quot;, e.g. that &quot;ZFS typically works best  
</I>&gt;<i> when it owns the entire disk&quot;.
</I>
Owning the entire disk does not in this case imply no partition.

&gt;<i> I note that on this list only last week Teng Yao had the same  
</I>&gt;<i> problem (Oh, God, I lost every thing after a reboot !!!)
</I>&gt;<i>
</I>&gt;<i> and Alex Blewitt helpfully replied that
</I>&gt;<i>
</I>&gt;<i> &quot;The documentation suggested /dev/disk0s2 would have been better,
</I>&gt;<i> rather than /dev/disk0, as otherwise ti doesn't mount on boot. That
</I>&gt;<i> sounds like what's happened here.&quot;
</I>&gt;<i>
</I>&gt;<i> Er - Well actually it doesn't.  That may be what it does in the  
</I>&gt;<i> example,  but it clearly advises that you use the whole disk.
</I>The definition of whole disk is getting in the way here.
&gt;<i> Sorry if I am ranting a little but this is a serious mess.
</I>At the risk of sounding contrary I've found the implementation of ZFS  
on OS X to be quite good and better than the implementation on  
everything short of Solaris. No&#235;l and company have done a great job on  
porting the file system and anyone with troubles should be reminded  
that as development software experimenting will be required and  
knowledge must be gathered from many sources above and beyond the  
normal documentation.
&gt;<i> 	<A HREF="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">Alex at designlifecycle.com</A>
</I>&gt;<i>
</I>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090505/e912b474/attachment-0001.html">http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20090505/e912b474/attachment-0001.html</A>&gt;
</PRE>



















<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001563.html">[zfs-discuss] There is something very wrong with the MacOS ZFS	documentation
</A></li>
	<LI>Next message: <A HREF="001564.html">[zfs-discuss] There is something very wrong with the MacOS ZFS	documentation
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1556">[ date ]</a>
              <a href="thread.html#1556">[ thread ]</a>
              <a href="subject.html#1556">[ subject ]</a>
              <a href="author.html#1556">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss">More information about the zfs-discuss
mailing list</a><br>
</body></html>
