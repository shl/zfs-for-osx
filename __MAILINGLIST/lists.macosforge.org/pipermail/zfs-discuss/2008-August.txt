From caronni at gmail.com  Sat Aug  2 00:25:55 2008
From: caronni at gmail.com (Germano Caronni)
Date: Sat, 2 Aug 2008 09:25:55 +0200
Subject: [zfs-discuss] build-119 still dying a silent death
In-Reply-To: <924BAD42-F7DA-47DE-84B0-3E53E1183B51@apple.com>
References: <327b821f0807221002w25756393gc4879426f1a9a06c@mail.gmail.com>
	<948DDB28-E0C0-41A2-BD4F-51016F7EB3A9@nrao.edu>
	<7CE95A44-0635-4003-B009-B7D93209DC59@apple.com>
	<B1BFF383-ECB9-4054-90A9-5A306AFCD47D@sun.com>
	<327b821f0807221337v766304aby5dc6a56a54c68455@mail.gmail.com>
	<4C84237B-12F7-4389-963C-85D481EB21FD@apple.com>
	<CC4E6330-D189-4184-8E00-3742938B1925@apple.com>
	<924BAD42-F7DA-47DE-84B0-3E53E1183B51@apple.com>
Message-ID: <327b821f0808020025m5c51b20am2777ff5a8aad96c7@mail.gmail.com>

Following Jonathan's suggestions, even a much smaller pool will do it.
So here the exact instructions I just used on my Mac (more data
below):

sudo zpool (to get the modules loaded)
mkfile 100M /Users/clear/pool1
zpool create z /Users/clear/pool1
zfs create z/crashfs
sudo rsync -aHPSv /dev/ /etc/ /Volumes/z/crashfs/1
[spinning ball]

When I got the spinning ball, I shortly depressed the power button.
*Maybe* this caused the following console entries:

02.08.08 08:57:29 com.apple.launchctl.System[2] BootCacheControl:
could not fetch history: Cannot allocate memory
02.08.08 08:57:29 com.apple.launchctl.System[2] BootCacheControl:
could not stop cache/fetch history: Cannot allocate memory
02.08.08 08:57:34 com.apple.quicklook[131] unrecognized rendering
intent `FICL:RI_to_be_removed'.
02.08.08 08:57:34 com.apple.quicklook[131] unrecognized rendering
intent `FICL:RI_to_be_removed'.

Then I power-cycled. Here as much system configuration as I can think
of. Anything more I can get you? Any way to get more debugging info
out of the system?

Germano

Model Name:	MacBook Pro
  Model Identifier:	MacBookPro4,1
  Processor Name:	Intel Core 2 Duo
  Processor Speed:	2.5 GHz
  Number Of Processors:	1
  Total Number Of Cores:	2
  L2 Cache:	6 MB
  Memory:	2 GB
  Bus Speed:	800 MHz
  Boot ROM Version:	MBP41.00C1.B03
  SMC Version:	1.27f1
  Serial Number:	W881...
  Sudden Motion Sensor:
  State:	Enabled

System Software Overview:

  System Version:	Mac OS X 10.5.4 (9E17)
  Kernel Version:	Darwin 9.4.0
  Boot Volume:	Mac
  Boot Mode:	Normal
  Computer Name:	mac

mac:~ ...$ kextstat
Index Refs Address    Size       Wired      Name (Version) <Linked Against>
    1    1 0x0        0x0        0x0        com.apple.kernel (9.4.0)
    2   45 0x0        0x0        0x0        com.apple.kpi.bsd (9.4.0)
    3    3 0x0        0x0        0x0        com.apple.kpi.dsep (9.4.0)
    4   64 0x0        0x0        0x0        com.apple.kpi.iokit (9.4.0)
    5   68 0x0        0x0        0x0        com.apple.kpi.libkern (9.4.0)
    6   63 0x0        0x0        0x0        com.apple.kpi.mach (9.4.0)
    7   32 0x0        0x0        0x0        com.apple.kpi.unsupported (9.4.0)
    8    1 0x0        0x0        0x0
com.apple.iokit.IONVRAMFamily (9.4.0)
    9    1 0x0        0x0        0x0        com.apple.driver.AppleNMI (9.4.0)
   10    1 0x0        0x0        0x0
com.apple.iokit.IOSystemManagementFamily (9.4.0)
   11    1 0x0        0x0        0x0
com.apple.iokit.ApplePlatformFamily (9.4.0)
   12   24 0x0        0x0        0x0        com.apple.kernel.6.0 (7.9.9)
   13    1 0x0        0x0        0x0        com.apple.kernel.bsd (7.9.9)
   14    1 0x0        0x0        0x0        com.apple.kernel.iokit (7.9.9)
   15    1 0x0        0x0        0x0        com.apple.kernel.libkern (7.9.9)
   16    1 0x0        0x0        0x0        com.apple.kernel.mach (7.9.9)
   17   18 0x5cd000   0x10000    0xf000
com.apple.iokit.IOPCIFamily (2.4.1) <7 6 5 4>
   18    9 0x635000   0x4000     0x3000
com.apple.iokit.IOACPIFamily (1.2.0) <12>
   19    3 0x706000   0x3d000    0x3c000
com.apple.driver.AppleACPIPlatform (1.2.1) <18 17 12 7 5 4>
   20    0 0x5c8000   0x5000     0x4000     com.apple.BootCache (30) <7 6 5 4 2>
   21    9 0x686000   0x18000    0x17000
com.apple.iokit.IOStorageFamily (1.5.2) <7 6 5 4 2>
   22    0 0x791000   0x10000    0xf000
com.apple.driver.DiskImages (194) <21 7 6 5 4 2>
   23    6 0x7fa000   0x39000    0x38000
com.apple.iokit.IOHIDFamily (1.5.2) <7 6 5 4 2>
   24    0 0xb51000   0x1a000    0x19000
com.apple.driver.AppleIntelCPUPowerManagement (9.39.0) <12 7 6 5 4 2>
   25    0 0x64f000   0x3000     0x2000
com.apple.security.TMSafetyNet (3) <7 6 5 3 2>
   26    0 0x7d5000   0x8000     0x7000
com.apple.nke.applicationfirewall (1.0.77) <7 6 5 4 2>
   27    0 0xa36000   0x18000    0x17000
com.apple.security.seatbelt (107.1) <7 6 5 3 2>
   28    0 0x7f7000   0x3000     0x2000     com.apple.driver.AppleAPIC
(1.2.0) <12>
   29    2 0x645000   0x3000     0x2000
com.apple.iokit.IOSMBusFamily (1.1) <6 5 4>
   30    0 0xb6b000   0x5000     0x4000
com.apple.driver.AppleACPIEC (1.2.1) <29 19 18 12>
   31    0 0x657000   0x4000     0x3000
com.apple.driver.AppleSMBIOS (1.0.14) <12>
   32    0 0x1087000  0x4000     0x3000
com.apple.driver.AppleACPIButtons (1.2.1) <23 19 18 7 6 5 4 2>
   33    0 0x743000   0x3000     0x2000
com.apple.driver.AppleACPIPCI (1.2.1) <19 18 17 12>
   34    0 0x785000   0x3000     0x2000     com.apple.driver.AppleHPET
(1.1) <18 7 6 5 4>
   35    0 0xa2d000   0x5000     0x4000     com.apple.driver.AppleRTC
(1.0.5) <18 6 5 4 2>
   36    1 0x846000   0x3000     0x2000
com.apple.driver.AppleEFIRuntime (1.1.0) <7 5 4>
   37    0 0x648000   0x7000     0x6000
com.apple.driver.AppleSmartBatteryManager (136.0.0) <29 6 5 4 2>
   38    0 0x849000   0x6000     0x5000
com.apple.driver.AppleEFINVRAM (1.1.0) <36 7 6 5 4>
   39    4 0x5dd000   0x16000    0x15000
com.apple.iokit.IONetworkingFamily (1.6.0) <7 6 5 4 2>
   40    1 0x866000   0x1f000    0x1e000
com.apple.iokit.IO80211Family (211.1) <39 7 6 5 4 2>
   41    0 0x885000   0x12d000   0x12c000
com.apple.driver.AirPortBrcm43xx (314.46.9) <40 39 17 7 6 5 4 2>
   42    0 0x5f3000   0x40000    0x3f000
com.apple.iokit.AppleYukon2 (3.1.10b2) <39 17 12 4 2>
   43   12 0x59e000   0x28000    0x27000
com.apple.iokit.IOUSBFamily (3.1.5) <7 6 5 4>
   44    0 0x7b3000   0xe000     0xd000
com.apple.driver.AppleUSBUHCI (3.1.5) <43 17 7 6 5 4>
   45    0 0x7a1000   0x12000    0x11000
com.apple.driver.AppleUSBEHCI (3.1.5) <43 17 7 6 5 4>
   47    2 0x9d2000   0xd000     0xc000
com.apple.iokit.IOATAFamily (1.7.3f1) <6 5 4 2>
   48    0 0xb2d000   0x9000     0x8000
com.apple.driver.AppleIntelPIIXATA (2.0.0) <47 17 6 5 4>
   49    2 0x6a7000   0x3f000    0x3e000
com.apple.iokit.IOFireWireFamily (3.4.4) <6 5 4 2>
   50    0 0x6e6000   0x20000    0x1f000
com.apple.driver.AppleFWOHCI (3.4.4) <49 17 7 6 5 4 2>
   51    0 0x5c6000   0x2000     0x1000
com.apple.iokit.IOUSBUserClient (3.0.7) <43 6 5 4>
   52    0 0xb75000   0xb000     0xa000
com.apple.driver.AppleUSBHub (3.1.5) <43 6 5 4>
   53    2 0xb41000   0x6000     0x5000
com.apple.iokit.IOAHCIFamily (1.0.4) <6 5 4 2>
   54    0 0xb47000   0xa000     0x9000
com.apple.driver.AppleAHCIPort (1.5.1) <53 17 6 5 4 2>
   55    0 0x108b000  0x10000    0xf000
com.apple.iokit.IOAHCIBlockStorage (1.1.0) <53 21 6 5 4 2>
   56    4 0x66d000   0x19000    0x18000
com.apple.iokit.IOSCSIArchitectureModelFamily (2.0.3) <6 5 4 2>
   57    0 0xa32000   0x4000     0x3000
com.apple.iokit.IOATAPIProtocolTransport (1.5.2) <56 47 12>
   59    0 0x9cd000   0x5000     0x4000
com.apple.driver.XsanFilter (2.7.91) <21 12>
   60    0 0x69e000   0x6000     0x5000
com.apple.iokit.SCSITaskUserClient (2.0.3) <56 21 6 5 4 2>
   61    3 0x746000   0x8000     0x7000
com.apple.iokit.IOCDStorageFamily (1.5) <21 6 5 4 2>
   62    2 0x74e000   0x6000     0x5000
com.apple.iokit.IODVDStorageFamily (1.5) <61 21 6 5 4 2>
   63    1 0x754000   0x5000     0x4000
com.apple.iokit.IOBDStorageFamily (1.5) <62 61 21 6 5 4 2>
   64    1 0x759000   0x16000    0x15000
com.apple.iokit.IOSCSIBlockCommandsDevice (2.0.3) <56 21 6 5 4 2>
   65    0 0x76f000   0x16000    0x15000
com.apple.iokit.IOSCSIMultimediaCommandsDevice (2.0.3) <64 63 62 61 56
21 6 5 4 2>
   66    0 0x633000   0x2000     0x1000
com.apple.driver.AppleUSBMergeNub (3.0.7) <43 5 4>
   67    0 0x7c1000   0x4000     0x3000
com.apple.driver.AppleUSBComposite (3.0.0) <43 5 4>
   68    4 0x833000   0x5000     0x4000
com.apple.iokit.IOUSBHIDDriver (3.1.0) <43 23 6 5 4>
   69    0 0xb8a000   0x4000     0x3000
com.apple.driver.AppleUSBTCKeyboard (1.6.5b3) <68 43 23 12>
   71    0 0x838000   0xb000     0xa000
com.apple.driver.AppleUSBMultitouch (183.28) <68 43 23 6 5 4 2>
   72    0 0x843000   0x3000     0x2000
com.apple.driver.AppleUSBTCButtons (1.6.5b3) <68 43 23 12>
   73    2 0x9ef000   0x37000    0x36000
com.apple.iokit.IOBluetoothFamily (2.1f17) <7 6 5 4 2>
   74    1 0xa26000   0x7000     0x6000
com.apple.driver.AppleUSBBluetoothHCIController (2.1f17) <73 43 12>
   75    0 0xb28000   0x5000     0x4000
com.apple.driver.BroadcomUSBBluetoothHCIController (2.1f17) <74 73 12>
   76    4 0xb8e000   0x1c000    0x1b000
com.apple.iokit.IOGraphicsFamily (1.5.2) <17 7 6 5 4>
   77    3 0xbaa000   0xe000     0xd000
com.apple.iokit.IONDRVSupport (1.5.2) <76 17 7 6 5 4>
   78    0 0x1083000  0x4000     0x3000
com.apple.driver.AppleBacklight (1.4.3) <77 76 17 12 6 5 4>
   79    4 0x639000   0x9000     0x8000     com.apple.driver.AppleSMC
(2.0.1d2) <18 7 6 5 4>
   80    1 0x7dd000   0xe000     0xd000
com.apple.driver.IOPlatformPluginFamily (3.1.6d0) <12>
   81    0 0x7eb000   0xc000     0xb000
com.apple.driver.ACPI_SMC_PlatformPlugin (3.1.6d0) <80 79 18 17 12 7 6
5 4>
   82    0 0xb3c000   0x5000     0x4000
com.apple.driver.AppleIRController (108) <68 43 23 12>
   83    0 0xb36000   0x6000     0x5000     com.apple.kext.AppleSMCLMU
(1.3.8d2) <79 6 5 4>
   84    1 0xa4e000   0x3000     0x2000
com.apple.kext.OSvKernDSPLib (1.1) <6 5>
   85    3 0xa51000   0x17000    0x16000
com.apple.iokit.IOAudioFamily (1.6.4b7) <84 6 5 4 2>
   86    0 0x107b000  0x4000     0x3000
com.apple.driver.AudioIPCDriver (1.0.4) <85 6 5 4 2>
   87    0 0x788000   0x9000     0x8000
com.apple.iokit.IOFireWireIP (1.7.1) <49 39 6 5 4 2>
   88    2 0x84f000   0x7000     0x6000
com.apple.iokit.IOHDAFamily (1.5.7a24) <6 5 4 2>
   89    0 0x856000   0xd000     0xc000
com.apple.driver.AppleHDAController (1.5.7a24) <88 17 6 5 4 2>
   90    2 0xbb8000   0x240000   0x23f000   com.apple.NVDAResman
(5.2.8) <77 76 17 12 6 5 4 2>
   91    0 0xdf8000   0x1f4000   0x1f3000   com.apple.nvidia.nv50hal
(5.2.8) <90 17 12>
   92    0 0x6a4000   0x3000     0x2000     com.apple.driver.AppleLPC
(1.2.3) <17 6 5 4>
   93    0 0x642000   0x3000     0x2000
com.apple.Dont_Steal_Mac_OS_X (6.0.2) <79 7 5 4 2>
   94    0 0x7cf000   0x6000     0x5000
com.apple.driver.SMCMotionSensor (2.0.6d2) <79 6 5 4>
   95    0 0xfec000   0x8f000    0x8e000    com.apple.GeForce (5.2.8)
<90 77 76 17 12 6 5 4 2>
   96    1 0xa68000   0x53000    0x52000
com.apple.driver.DspFuncLib (1.5.7a24) <85 6 5 4 2>
   97    0 0xabb000   0x62000    0x61000    com.apple.driver.AppleHDA
(1.5.7a24) <96 88 85 18 6 5 4 2>
   99    0 0xb1d000   0xb000     0xa000
com.apple.filesystems.autofs (2.0.1) <7 6 5 4 2>
  100    0 0x9b2000   0x1b000    0x1a000    com.apple.filesystems.ntfs
(2.1) <7 6 5 2>
  101    1 0x65b000   0x9000     0x8000
com.apple.iokit.IOSerialFamily (9.1) <7 6 5 4 2>
  102    0 0x664000   0x9000     0x8000
com.apple.iokit.IOBluetoothSerialManager (2.1f17) <101 12>
  104    0 0x652000   0x5000     0x4000
com.apple.driver.AppleHWSensor (1.7.1d0) <12>
  105    0 0x34a70000 0xcc000    0xcb000    com.apple.filesystems.zfs
(8.0) <7 6 5 2>

rsync -v
rsync  version 2.6.3  protocol version 28
Copyright (C) 1996-2004 by Andrew Tridgell and others
<http://rsync.samba.org/>
Capabilities: 64-bit files, socketpairs, hard links, symlinks, batchfiles,
              inplace, IPv6, 32-bit system inums, 64-bit internal inums

$ ls -al /System/Library/Extensions/zfs.kext/Contents/MacOS/zfs
-rw-r--r--@ 1 root  wheel  2898140 22 Jul 19:45
/System/Library/Extensions/zfs.kext/Contents/MacOS/zfs
$ md5 /System/Library/Extensions/zfs.kext/Contents/MacOS/zfs
MD5 (/System/Library/Extensions/zfs.kext/Contents/MacOS/zfs) =
075a5491bca49cd1a3aca3473c3c0914





On Thu, Jul 31, 2008 at 23:46, No?l Dellofano <ndellofano at apple.com> wrote:
> Hey all,
> I need some help.  Neither Don nor I are able to recreate this bug, with
> both of us trying different scenarios on different machines.
> We're using Germano's instructions:
> mkfile 20g /foo
> zpool create z /foo
> zfs create z/crashfs
> sudo rsync -aHPSv /dev /etc /usr /Volumes/z/crashfs/1
> [watch the spinning ball of death]
>
> I also tried using a disk drive instead of a file as a vdev.  Additionally
> tried rsyncing into a top level pool, as well as child filesystems like the
> one above.  I'm running  Leopard 9E17 on my Mac Pro, with zfs-119.  Is that
> what everyone else is running that is seeing this?  I can't get a hang.
>
> ideas or suggestions?  Am i missing a step?
> Noel
>

From andy at aligature.com  Sat Aug  2 08:02:28 2008
From: andy at aligature.com (Andrew Webber)
Date: Sat, 2 Aug 2008 11:02:28 -0400
Subject: [zfs-discuss] build-119 still dying a silent death
In-Reply-To: <924BAD42-F7DA-47DE-84B0-3E53E1183B51@apple.com>
References: <327b821f0807221002w25756393gc4879426f1a9a06c@mail.gmail.com>
	<948DDB28-E0C0-41A2-BD4F-51016F7EB3A9@nrao.edu>
	<7CE95A44-0635-4003-B009-B7D93209DC59@apple.com>
	<B1BFF383-ECB9-4054-90A9-5A306AFCD47D@sun.com>
	<327b821f0807221337v766304aby5dc6a56a54c68455@mail.gmail.com>
	<4C84237B-12F7-4389-963C-85D481EB21FD@apple.com>
	<CC4E6330-D189-4184-8E00-3742938B1925@apple.com>
	<924BAD42-F7DA-47DE-84B0-3E53E1183B51@apple.com>
Message-ID: <60b50dc10808020802h771dc267taf2e263c9142cec3@mail.gmail.com>

I have yet to have a serious problem in my day-to-day use of ZFS.  However,
running the listed procedure did cause my system to hang indefinitely.  I
don't have an actual keyboard and mouse hooked up to the machine, but the
VNC behavior that I saw indicated that the machine was frozen.  Also, it
stopped responding to pings after a little while.  After a reboot everything
was ok, but I have yet to try the procedure again.

Here are my stats:
Mac pro running 10.5.4
1x quad core xeon 2.8 GHz
4Gb ram 800 MHz DDR2
2x SATA drives in a mirror in my pool

/dev/disk1
   #:                       TYPE NAME                    SIZE
IDENTIFIER
   0:      GUID_partition_scheme                        *698.6 Gi   disk1
   1:                        EFI                         200.0 Mi   disk1s1
   2:                        ZFS lake                    698.3 Gi   disk1s2
/dev/disk2
   #:                       TYPE NAME                    SIZE
IDENTIFIER
   0:      GUID_partition_scheme                        *698.6 Gi   disk2
   1:                        EFI                         200.0 Mi   disk2s1
   2:                        ZFS lake                    698.3 Gi   disk2s2

Would any other information be helpful?  If you absolutely can't recreate
the problem, it could be possible to setup a remote ssh to my machine for a
test, if you could get some diagnostics.  Also, I would be happy to get you
any output of any test that I'm capable of running.



On Thu, Jul 31, 2008 at 5:46 PM, No?l Dellofano <ndellofano at apple.com>wrote:

> Hey all,
> I need some help.  Neither Don nor I are able to recreate this bug, with
> both of us trying different scenarios on different machines.
>
> We're using Germano's instructions:
>
> mkfile 20g /foo
> zpool create z /foo
> zfs create z/crashfs
> sudo rsync -aHPSv /dev /etc /usr /Volumes/z/crashfs/1
> [watch the spinning ball of death]
>
>
> I also tried using a disk drive instead of a file as a vdev.  Additionally
> tried rsyncing into a top level pool, as well as child filesystems like the
> one above.  I'm running  Leopard 9E17 on my Mac Pro, with zfs-119.  Is that
> what everyone else is running that is seeing this?  I can't get a hang.
>
>
> ideas or suggestions?  Am i missing a step?
>
> Noel
>
>
>
> On Jul 22, 2008, at 4:37 PM, No?l Dellofano wrote:
>
> For those interested or keeping track, I just filed this bug in Radar to
> track Germano's bug:
> 6094713:    rsync /dev to a ZFS volume hangs
>
> I'll try and get it fixed asap.  Thanks for finding this Germano!
>
> Noel
>
> On Jul 22, 2008, at 4:29 PM, No?l Dellofano wrote:
>
> So currently there is no way for me to sync the public bug tracker with
> Radar, our main bug track facility in house.  I have to go through the bugs
> manually and file them into our internal system which I just don't have the
> time or resources to do.  Hence the best way to alert me of an issue is just
> send it out on the list like this.  I try and keep up as much as possible,
> then I can just file the bug into radar as it comes up.  And I also have a
> contact for you if I need help reproducing it :)
> So, while it's not the best system, it works and is all I have.  So just
> send an email, or if you have private information just send me an email.
>  However, when at all possible, send it to the list so then everyone is
> alerted to it like below.
>
> thanks!
> Noel
>
> On Jul 22, 2008, at 1:37 PM, Germano Caronni wrote:
>
> Thank you for validating this issue.  ;-)
>
> For bugtracking, there is http://zfs.macosforge.org/trac/report -- but I
> am not sure if / how this is used.
>
> Germano
>
> On Tue, Jul 22, 2008 at 13:27, Jonathan Edwards <Jonathan.Edwards at sun.com>wrote:
>
>> i was able to reproduce a hard hang on a much smaller system config ..
>> just 4 x 64MB files here for my test pool .. compression doesn't matter:
>>
>> bash-3.2# zpool status -v
>>   pool: tpool
>>  state: ONLINE
>>  scrub: none requested
>> config:
>>
>>        NAME              STATE     READ WRITE CKSUM
>>        tpool             ONLINE       0     0     0
>>          /var/tmp/file1  ONLINE       0     0     0
>>          /var/tmp/file2  ONLINE       0     0     0
>>          /var/tmp/file3  ONLINE       0     0     0
>>          /var/tmp/file4  ONLINE       0     0     0
>>
>> errors: No known data errors
>> bash-3.2# rsync --version
>> rsync  version 2.6.3  protocol version 28
>> Copyright (C) 1996-2004 by Andrew Tridgell and others
>> <http://rsync.samba.org/>
>> Capabilities: 64-bit files, socketpairs, hard links, symlinks,
>> batchfiles,
>>               inplace, IPv6, 32-bit system inums, 64-bit internal inums
>>
>> ---
>> i'll have to figure out how to do a kdb equiv to get a backtrace here
>>
>> of note .. i also saw a vfs error last night on shutdown that also
>> resulted in a hard hang:
>>
>> Tue Jul 22 01:17:08 2008
>> panic(cpu 1 caller 0x001DBC3D): "vnode_put(0x529dc70): iocount < 1"@/
>> SourceCache/xnu/xnu-1228.5.20/bsd/vfs/vfs_subr.c:3581
>> Backtrace, Format - Frame : Return Address (4 potential args on stack)
>> 0x475dfd78 : 0x12b0fa (0x4592a4 0x475dfdac 0x133243 0x0)
>> 0x475dfdc8 : 0x1dbc3d (0x467410 0x529dc70 0x475dfe08 0x4792d532)
>> 0x475dfde8 : 0x1dbcee (0x529dc70 0x6f17220 0xf9bdf8ae 0x1064f140)
>> 0x475dfe08 : 0x478fd30b (0x529dc70 0x479602e4 0x0 0x0)
>> 0x475dff58 : 0x478e60a2 (0x1064f000 0xc81d 0x0 0x0)
>> 0x475dffc8 : 0x19ebdc (0x1023d200 0x0 0x1a20b5 0x5a8f3c8)
>> Backtrace terminated-invalid frame pointer 0
>>       Kernel loadable modules in backtrace (with dependencies):
>>          com.apple.filesystems.zfs(8.0)@0x478b5000->0x47980fff
>>
>> BSD process name corresponding to current thread: kernel_task
>>
>> Mac OS version:
>> 9E17
>>
>> Kernel version:
>> Darwin Kernel Version 9.4.0: Mon Jun  9 19:30:53 PDT 2008;
>> root:xnu-1228.5.20~1/RELEASE_I386
>> System model name: MacBookPro2,2 (Mac-F42187C8)
>>
>> ----
>> also could we up the version number in the zfs module plist to track
>> version problems a little better?
>> methinks (8.0) is the default template for a kext
>>
>> would also be nice to get some sort of web bugtraq in place so we can
>> just file 'em
>>
>> ---
>> .je
>>
>> On Jul 22, 2008, at 3:23 PM, No?l Dellofano wrote:
>>
>> > Neat!  I haven't seen this issue since fixing the memory scaling bug a
>> > while ago.  Sorry to hear it's giving you troubles.
>> >
>> > Germano,
>> > if it reproduces, then the snapshot would be awesome!  As my biggest
>> > issue is getting stuff like that to reproduce in house.  So you're
>> > just using standard leopard shipped rsync right?  If so, since he's
>> > the fatty that tries to shove everything in memory I wonder if we're
>> > stuck sleeping waiting for someone to give up some memory possibly and
>> > are wedged...
>> >
>> > as usual you guys are the best test team a girl can have :)  If your
>> > snapshot works send it over.  Otherwise I'll do my best to mimic your
>> > system and see if I can get a recreate..
>> >
>> > thanks!
>> > Noel
>> >
>> > On Jul 22, 2008, at 12:08 PM, Boyd Waters wrote:
>> >
>> >>
>> >> On Jul 22, 2008, at 11:02 AM, Germano Caronni wrote:
>> >>
>> >>> If I do a recursive diff on some of these trees, or a find and a
>> >>> following md5 sum on, say, 100'000+ files, the machine will die the
>> >>> same silent death as reported for excessive rsync before
>> >>
>> >> Interesting!
>> >>
>> >> I'm not seeing that problem any more, but I've been using rsync
>> >> 3.0.4,
>> >> built from source, 64-bit... it's dramatically improved over the
>> >> version that ships with Leopard.
>> >>
>> >> I wonder if there's a memory leak in the VFS layer.  (wow, that
>> >> almost
>> >> sounds like I know what I'm talking about, but really I don't)
>> >>
>> >>
>> >> _______________________________________________
>> >> zfs-discuss mailing list
>> >> zfs-discuss at lists.macosforge.org
>> >> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>> >
>> > _______________________________________________
>> > zfs-discuss mailing list
>> > zfs-discuss at lists.macosforge.org
>> > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080802/9d8611b5/attachment-0001.html 

From andy at aligature.com  Sat Aug  2 08:04:55 2008
From: andy at aligature.com (Andrew Webber)
Date: Sat, 2 Aug 2008 11:04:55 -0400
Subject: [zfs-discuss] build-119 still dying a silent death
In-Reply-To: <60b50dc10808020802h771dc267taf2e263c9142cec3@mail.gmail.com>
References: <327b821f0807221002w25756393gc4879426f1a9a06c@mail.gmail.com>
	<948DDB28-E0C0-41A2-BD4F-51016F7EB3A9@nrao.edu>
	<7CE95A44-0635-4003-B009-B7D93209DC59@apple.com>
	<B1BFF383-ECB9-4054-90A9-5A306AFCD47D@sun.com>
	<327b821f0807221337v766304aby5dc6a56a54c68455@mail.gmail.com>
	<4C84237B-12F7-4389-963C-85D481EB21FD@apple.com>
	<CC4E6330-D189-4184-8E00-3742938B1925@apple.com>
	<924BAD42-F7DA-47DE-84B0-3E53E1183B51@apple.com>
	<60b50dc10808020802h771dc267taf2e263c9142cec3@mail.gmail.com>
Message-ID: <60b50dc10808020804q4ab60037yd77d8d0877b2cf53@mail.gmail.com>

Actually, I want to be clear about something.  I shouldn't have put any
information about my two ZFS mirror drives in my previous posting.  The test
that causes the hang ran on my Apple supplied HFS+ drive.


On Sat, Aug 2, 2008 at 11:02 AM, Andrew Webber <andy at aligature.com> wrote:

> I have yet to have a serious problem in my day-to-day use of ZFS.  However,
> running the listed procedure did cause my system to hang indefinitely.  I
> don't have an actual keyboard and mouse hooked up to the machine, but the
> VNC behavior that I saw indicated that the machine was frozen.  Also, it
> stopped responding to pings after a little while.  After a reboot everything
> was ok, but I have yet to try the procedure again.
>
> Here are my stats:
> Mac pro running 10.5.4
> 1x quad core xeon 2.8 GHz
> 4Gb ram 800 MHz DDR2
> 2x SATA drives in a mirror in my pool
>
> /dev/disk1
>    #:                       TYPE NAME                    SIZE
> IDENTIFIER
>    0:      GUID_partition_scheme                        *698.6 Gi   disk1
>    1:                        EFI                         200.0 Mi   disk1s1
>    2:                        ZFS lake                    698.3 Gi   disk1s2
> /dev/disk2
>    #:                       TYPE NAME                    SIZE
> IDENTIFIER
>    0:      GUID_partition_scheme                        *698.6 Gi   disk2
>    1:                        EFI                         200.0 Mi   disk2s1
>    2:                        ZFS lake                    698.3 Gi   disk2s2
>
> Would any other information be helpful?  If you absolutely can't recreate
> the problem, it could be possible to setup a remote ssh to my machine for a
> test, if you could get some diagnostics.  Also, I would be happy to get you
> any output of any test that I'm capable of running.
>
>
>
>
> On Thu, Jul 31, 2008 at 5:46 PM, No?l Dellofano <ndellofano at apple.com>wrote:
>
>> Hey all,
>> I need some help.  Neither Don nor I are able to recreate this bug, with
>> both of us trying different scenarios on different machines.
>>
>> We're using Germano's instructions:
>>
>> mkfile 20g /foo
>> zpool create z /foo
>> zfs create z/crashfs
>> sudo rsync -aHPSv /dev /etc /usr /Volumes/z/crashfs/1
>> [watch the spinning ball of death]
>>
>>
>> I also tried using a disk drive instead of a file as a vdev.  Additionally
>> tried rsyncing into a top level pool, as well as child filesystems like the
>> one above.  I'm running  Leopard 9E17 on my Mac Pro, with zfs-119.  Is that
>> what everyone else is running that is seeing this?  I can't get a hang.
>>
>>
>> ideas or suggestions?  Am i missing a step?
>>
>> Noel
>>
>>
>>
>> On Jul 22, 2008, at 4:37 PM, No?l Dellofano wrote:
>>
>> For those interested or keeping track, I just filed this bug in Radar to
>> track Germano's bug:
>> 6094713:    rsync /dev to a ZFS volume hangs
>>
>> I'll try and get it fixed asap.  Thanks for finding this Germano!
>>
>> Noel
>>
>> On Jul 22, 2008, at 4:29 PM, No?l Dellofano wrote:
>>
>> So currently there is no way for me to sync the public bug tracker with
>> Radar, our main bug track facility in house.  I have to go through the bugs
>> manually and file them into our internal system which I just don't have the
>> time or resources to do.  Hence the best way to alert me of an issue is just
>> send it out on the list like this.  I try and keep up as much as possible,
>> then I can just file the bug into radar as it comes up.  And I also have a
>> contact for you if I need help reproducing it :)
>> So, while it's not the best system, it works and is all I have.  So just
>> send an email, or if you have private information just send me an email.
>>  However, when at all possible, send it to the list so then everyone is
>> alerted to it like below.
>>
>> thanks!
>> Noel
>>
>> On Jul 22, 2008, at 1:37 PM, Germano Caronni wrote:
>>
>> Thank you for validating this issue.  ;-)
>>
>> For bugtracking, there is http://zfs.macosforge.org/trac/report -- but I
>> am not sure if / how this is used.
>>
>> Germano
>>
>> On Tue, Jul 22, 2008 at 13:27, Jonathan Edwards <Jonathan.Edwards at sun.com
>> > wrote:
>>
>>> i was able to reproduce a hard hang on a much smaller system config ..
>>> just 4 x 64MB files here for my test pool .. compression doesn't matter:
>>>
>>> bash-3.2# zpool status -v
>>>   pool: tpool
>>>  state: ONLINE
>>>  scrub: none requested
>>> config:
>>>
>>>        NAME              STATE     READ WRITE CKSUM
>>>        tpool             ONLINE       0     0     0
>>>          /var/tmp/file1  ONLINE       0     0     0
>>>          /var/tmp/file2  ONLINE       0     0     0
>>>          /var/tmp/file3  ONLINE       0     0     0
>>>          /var/tmp/file4  ONLINE       0     0     0
>>>
>>> errors: No known data errors
>>> bash-3.2# rsync --version
>>> rsync  version 2.6.3  protocol version 28
>>> Copyright (C) 1996-2004 by Andrew Tridgell and others
>>> <http://rsync.samba.org/>
>>> Capabilities: 64-bit files, socketpairs, hard links, symlinks,
>>> batchfiles,
>>>               inplace, IPv6, 32-bit system inums, 64-bit internal inums
>>>
>>> ---
>>> i'll have to figure out how to do a kdb equiv to get a backtrace here
>>>
>>> of note .. i also saw a vfs error last night on shutdown that also
>>> resulted in a hard hang:
>>>
>>> Tue Jul 22 01:17:08 2008
>>> panic(cpu 1 caller 0x001DBC3D): "vnode_put(0x529dc70): iocount < 1"@/
>>> SourceCache/xnu/xnu-1228.5.20/bsd/vfs/vfs_subr.c:3581
>>> Backtrace, Format - Frame : Return Address (4 potential args on stack)
>>> 0x475dfd78 : 0x12b0fa (0x4592a4 0x475dfdac 0x133243 0x0)
>>> 0x475dfdc8 : 0x1dbc3d (0x467410 0x529dc70 0x475dfe08 0x4792d532)
>>> 0x475dfde8 : 0x1dbcee (0x529dc70 0x6f17220 0xf9bdf8ae 0x1064f140)
>>> 0x475dfe08 : 0x478fd30b (0x529dc70 0x479602e4 0x0 0x0)
>>> 0x475dff58 : 0x478e60a2 (0x1064f000 0xc81d 0x0 0x0)
>>> 0x475dffc8 : 0x19ebdc (0x1023d200 0x0 0x1a20b5 0x5a8f3c8)
>>> Backtrace terminated-invalid frame pointer 0
>>>       Kernel loadable modules in backtrace (with dependencies):
>>>          com.apple.filesystems.zfs(8.0)@0x478b5000->0x47980fff
>>>
>>> BSD process name corresponding to current thread: kernel_task
>>>
>>> Mac OS version:
>>> 9E17
>>>
>>> Kernel version:
>>> Darwin Kernel Version 9.4.0: Mon Jun  9 19:30:53 PDT 2008;
>>> root:xnu-1228.5.20~1/RELEASE_I386
>>> System model name: MacBookPro2,2 (Mac-F42187C8)
>>>
>>> ----
>>> also could we up the version number in the zfs module plist to track
>>> version problems a little better?
>>> methinks (8.0) is the default template for a kext
>>>
>>> would also be nice to get some sort of web bugtraq in place so we can
>>> just file 'em
>>>
>>> ---
>>> .je
>>>
>>> On Jul 22, 2008, at 3:23 PM, No?l Dellofano wrote:
>>>
>>> > Neat!  I haven't seen this issue since fixing the memory scaling bug a
>>> > while ago.  Sorry to hear it's giving you troubles.
>>> >
>>> > Germano,
>>> > if it reproduces, then the snapshot would be awesome!  As my biggest
>>> > issue is getting stuff like that to reproduce in house.  So you're
>>> > just using standard leopard shipped rsync right?  If so, since he's
>>> > the fatty that tries to shove everything in memory I wonder if we're
>>> > stuck sleeping waiting for someone to give up some memory possibly and
>>> > are wedged...
>>> >
>>> > as usual you guys are the best test team a girl can have :)  If your
>>> > snapshot works send it over.  Otherwise I'll do my best to mimic your
>>> > system and see if I can get a recreate..
>>> >
>>> > thanks!
>>> > Noel
>>> >
>>> > On Jul 22, 2008, at 12:08 PM, Boyd Waters wrote:
>>> >
>>> >>
>>> >> On Jul 22, 2008, at 11:02 AM, Germano Caronni wrote:
>>> >>
>>> >>> If I do a recursive diff on some of these trees, or a find and a
>>> >>> following md5 sum on, say, 100'000+ files, the machine will die the
>>> >>> same silent death as reported for excessive rsync before
>>> >>
>>> >> Interesting!
>>> >>
>>> >> I'm not seeing that problem any more, but I've been using rsync
>>> >> 3.0.4,
>>> >> built from source, 64-bit... it's dramatically improved over the
>>> >> version that ships with Leopard.
>>> >>
>>> >> I wonder if there's a memory leak in the VFS layer.  (wow, that
>>> >> almost
>>> >> sounds like I know what I'm talking about, but really I don't)
>>> >>
>>> >>
>>> >> _______________________________________________
>>> >> zfs-discuss mailing list
>>> >> zfs-discuss at lists.macosforge.org
>>> >> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>> >
>>> > _______________________________________________
>>> > zfs-discuss mailing list
>>> > zfs-discuss at lists.macosforge.org
>>> > http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>>
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080802/aa991c4b/attachment.html 

From caronni at gmail.com  Mon Aug  4 06:55:20 2008
From: caronni at gmail.com (Germano Caronni)
Date: Mon, 4 Aug 2008 15:55:20 +0200
Subject: [zfs-discuss] build-119 still dying a silent death
In-Reply-To: <327b821f0808020025m5c51b20am2777ff5a8aad96c7@mail.gmail.com>
References: <327b821f0807221002w25756393gc4879426f1a9a06c@mail.gmail.com>
	<948DDB28-E0C0-41A2-BD4F-51016F7EB3A9@nrao.edu>
	<7CE95A44-0635-4003-B009-B7D93209DC59@apple.com>
	<B1BFF383-ECB9-4054-90A9-5A306AFCD47D@sun.com>
	<327b821f0807221337v766304aby5dc6a56a54c68455@mail.gmail.com>
	<4C84237B-12F7-4389-963C-85D481EB21FD@apple.com>
	<CC4E6330-D189-4184-8E00-3742938B1925@apple.com>
	<924BAD42-F7DA-47DE-84B0-3E53E1183B51@apple.com>
	<327b821f0808020025m5c51b20am2777ff5a8aad96c7@mail.gmail.com>
Message-ID: <327b821f0808040655i5242aba2q827d91cf8be83728@mail.gmail.com>

I've now recompiled zfs-119 from source, re-intalled, and can still
reproduce the issue.
Would you have a dtrace program that might give you more useful information?

Now the funny part. I was hoping to see where rsync gets stuck on by
using dtruss.
When running
sudo dtruss -f -l rsync -aHPSvvvvvvvvvvvvvvvv /dev/  /Volumes/z/1/
via an ssh login, it completes. The machine only dies about 1-3 seconds later.

So, rsync does not per se hang, but causes something else (your guess
is as good as mine) to break. Now, if I only had any clue on what
kernel symbols to look for or how to debug leopard. MacOS is opaque to
me ;-)

Germano

From geerds at bago.net  Mon Aug  4 08:09:52 2008
From: geerds at bago.net (Joergen Geerds)
Date: Mon, 4 Aug 2008 11:09:52 -0400
Subject: [zfs-discuss] data loss with ZFS as scratch disk
Message-ID: <B1A6E1B2-9705-4A2D-96CF-3BB1F2A93F13@bago.net>

I installed the latest build of ZFS (July 16, 2008), and partinioned  
my external FW800 case (2 sata disks) as a ZFS raid pool.

First bug:
On my first attempt, I did follow the "Getting started" steps:
zpool create bigraid raidz disk1s2 disk2s2

This resulted in a raidz pool that was only reporting 230GB instead of  
460GB to the OS (but zpool iostat reported 460GB in size)
So I redid the setup, created the pool with just one disk, and then  
"zpool add bigraid raidz  disk2s2"
This gave me a pool with 460 GB. It would be great if you could look  
into this.

Second bug:
photoshop seems to love the additional speed of the ZFS scratch disk,  
it felt really faster than usual (no hard data to back it up) and no  
problems.
So I decided to do a quick test with PTgui 8b6 (using the ZFS bigraid  
as PTgui temp folder, but reading/writing all input/output from the  
internal HFS+). The output is a layered PSB file of about 1GB in this  
case. The PSB was shredded.
See http://nypano.com/clients/ptgui8b6-scratchdisk-error.jpg
I was able to verify that it is indeed the ZFS scratchdisk that is the  
culprit (moving the ptgui temp folder to the internal disk results in  
normal output, but PTgui 7.8 with the ZFS as temp folder shreds the  
PSB also).

Joost from PTgui claims that system & ZFS is at fault, not PTgui. Any  
ideas what went wrong?


System info:
Imac 2.16GHz, 10.5.4, 3GB, 500GB internal HFS+, 2x250GB ext FW800

pool: bigraid
state: ONLINE
scrub: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	bigraid     ONLINE       0     0     0
	  disk1s2   ONLINE       0     0     0
	  disk2s2   ONLINE       0     0     0

errors: No known data errors

History for 'bigraid':
2008-08-03.18:42:02 zpool create bigraid /dev/disk1s2
2008-08-03.18:46:03 zpool upgrade bigraid 8
2008-08-03.18:47:10 zpool add bigraid disk2s2



Joergen Geerds
http://luminous-newyork.com
http://newyorkpanorama.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080804/93faec2d/attachment.html 

From caronni at gmail.com  Mon Aug  4 09:01:28 2008
From: caronni at gmail.com (Germano Caronni)
Date: Mon, 4 Aug 2008 18:01:28 +0200
Subject: [zfs-discuss] build-119 still dying a silent death
In-Reply-To: <327b821f0808040655i5242aba2q827d91cf8be83728@mail.gmail.com>
References: <327b821f0807221002w25756393gc4879426f1a9a06c@mail.gmail.com>
	<948DDB28-E0C0-41A2-BD4F-51016F7EB3A9@nrao.edu>
	<7CE95A44-0635-4003-B009-B7D93209DC59@apple.com>
	<B1BFF383-ECB9-4054-90A9-5A306AFCD47D@sun.com>
	<327b821f0807221337v766304aby5dc6a56a54c68455@mail.gmail.com>
	<4C84237B-12F7-4389-963C-85D481EB21FD@apple.com>
	<CC4E6330-D189-4184-8E00-3742938B1925@apple.com>
	<924BAD42-F7DA-47DE-84B0-3E53E1183B51@apple.com>
	<327b821f0808020025m5c51b20am2777ff5a8aad96c7@mail.gmail.com>
	<327b821f0808040655i5242aba2q827d91cf8be83728@mail.gmail.com>
Message-ID: <327b821f0808040901t2b990795hafd71595d3c7ff4e@mail.gmail.com>

Here some specifics on the silent death. This should be a lot of fun for you
@ Apple ;-)

rsync 3.0.3 also fails. Having interspersed lots of sync() and sleep() in
that one, I finally got to see a partially copied dev tree. Funnily enough,
the major numbers of all devices on zfs are zero. That certainly is a
problem, and probably / hopefully it is related to the deadlock. The machine
dies at disk0s2 (my root volume). Trying to reproduce this manually:

bash-3.2# pwd
/Volumes/z
bash-3.2# mknod disk0 b 14 0
bash-3.2# sync
bash-3.2# ls -al
total 80
[...]
brw-r--r--  1 root     staff    0,   0 Aug  4 17:03 disk0
bash-3.2# sync
bash-3.2# mknod disk0s1 b 14 1
bash-3.2# sync
bash-3.2# ls -al
total 81
[...]
brw-r--r--  1 root     staff    0,   0 Aug  4 17:03 disk0
brw-r--r--  1 root     staff    0,   1 Aug  4 17:04 disk0s1
bash-3.2# sync
bash-3.2# mknod disk0s2 b 14 2
bash-3.2# sync
[the world comes to an end]


The output of 'stat' agrees that the major is indeed 0. Checking back on my
big repository confirms that all devices (char and block) in it now have a
major of '0'. I'm so happy that I still have the originals ;-)


I hope this helps. I looked a bit at the source code to zfs_mknode and
friends, but did not feel like poking randomly. Also, a pity that there are
no zfs dtrace probes around (ie dtrace -l does not list any). Is there a
good way to get them into the kernel? I'm guessing it would just involve
using the 'Debug' target and copying things to the right location -- but
maybe you can tell me? Obviously, what I wanted to do was dtrace on
zfs_mknode and then print out the arguments to the function to see if the
problem is above or below.

Germano
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080804/660a8c2f/attachment.html 

From zorg at sogeeky.net  Mon Aug  4 09:03:03 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Mon, 4 Aug 2008 09:03:03 -0700
Subject: [zfs-discuss] data loss with ZFS as scratch disk
In-Reply-To: <B1A6E1B2-9705-4A2D-96CF-3BB1F2A93F13@bago.net>
References: <B1A6E1B2-9705-4A2D-96CF-3BB1F2A93F13@bago.net>
Message-ID: <39CBC861-A2A0-4866-A3B1-039E0B6614FD@sogeeky.net>

230GB is correct. In a raidz configuration you only get n-1 disks  
worth of storage since one disk is reserved for parity. It really  
doesn't make sense to use less than three disks for raidz, with just  
two you may as well use a mirror configuration.

It is possible that the corruption is a result of you adding disk2s2  
to the pool a second time. It may have been using it both as a data  
disk and a parity disk resulting in scrambled data. Of course, it  
should not have allowed that.

I don't have any spare disks, can anyone else verify this behavior?

On Aug 4, 2008, at 8:09 AM, Joergen Geerds <geerds at bago.net> wrote:

> I installed the latest build of ZFS (July 16, 2008), and partinioned  
> my external FW800 case (2 sata disks) as a ZFS raid pool.
>
> First bug:
> On my first attempt, I did follow the "Getting started" steps:
> zpool create bigraid raidz disk1s2 disk2s2
>
> This resulted in a raidz pool that was only reporting 230GB instead  
> of 460GB to the OS (but zpool iostat reported 460GB in size)
> So I redid the setup, created the pool with just one disk, and then  
> "zpool add bigraid raidz  disk2s2"
> This gave me a pool with 460 GB. It would be great if you could look  
> into this.
>
> Second bug:
> photoshop seems to love the additional speed of the ZFS scratch  
> disk, it felt really faster than usual (no hard data to back it up)  
> and no problems.
> So I decided to do a quick test with PTgui 8b6 (using the ZFS  
> bigraid as PTgui temp folder, but reading/writing all input/output  
> from the internal HFS+). The output is a layered PSB file of about  
> 1GB in this case. The PSB was shredded.
> See http://nypano.com/clients/ptgui8b6-scratchdisk-error.jpg
> I was able to verify that it is indeed the ZFS scratchdisk that is  
> the culprit (moving the ptgui temp folder to the internal disk  
> results in normal output, but PTgui 7.8 with the ZFS as temp folder  
> shreds the PSB also).
>
> Joost from PTgui claims that system & ZFS is at fault, not PTgui.  
> Any ideas what went wrong?
>
>
> System info:
> Imac 2.16GHz, 10.5.4, 3GB, 500GB internal HFS+, 2x250GB ext FW800
>
> pool: bigraid
> state: ONLINE
> scrub: none requested
> config:
>
> 	NAME        STATE     READ WRITE CKSUM
> 	bigraid     ONLINE       0     0     0
> 	  disk1s2   ONLINE       0     0     0
> 	  disk2s2   ONLINE       0     0     0
>
> errors: No known data errors
>
> History for 'bigraid':
> 2008-08-03.18:42:02 zpool create bigraid /dev/disk1s2
> 2008-08-03.18:46:03 zpool upgrade bigraid 8
> 2008-08-03.18:47:10 zpool add bigraid disk2s2
>
>
>
> Joergen Geerds
> http://luminous-newyork.com
> http://newyorkpanorama.com
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080804/130fd188/attachment-0001.html 

From tnorton at apple.com  Mon Aug  4 09:56:36 2008
From: tnorton at apple.com (Thane Norton III)
Date: Mon, 4 Aug 2008 09:56:36 -0700
Subject: [zfs-discuss] data loss with ZFS as scratch disk
In-Reply-To: <39CBC861-A2A0-4866-A3B1-039E0B6614FD@sogeeky.net>
References: <B1A6E1B2-9705-4A2D-96CF-3BB1F2A93F13@bago.net>
	<39CBC861-A2A0-4866-A3B1-039E0B6614FD@sogeeky.net>
Message-ID: <82AC4DE0-50D4-48DE-B006-66D866A5C7D6@apple.com>

Unless you are setting up for future expansion, right?

On Aug 4, 2008, at 9:03 AM, Mr. Zorg wrote:

> 230GB is correct. In a raidz configuration you only get n-1 disks  
> worth of storage since one disk is reserved for parity. It really  
> doesn't make sense to use less than three disks for raidz, with just  
> two you may as well use a mirror configuration.
>

-
Thane Norton
tnorton at apple.com

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 2156 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080804/621ffb6c/attachment.bin 

From geerds at bago.net  Mon Aug  4 10:35:03 2008
From: geerds at bago.net (Joergen Geerds)
Date: Mon, 4 Aug 2008 13:35:03 -0400
Subject: [zfs-discuss] data loss with ZFS as scratch disk
In-Reply-To: <39CBC861-A2A0-4866-A3B1-039E0B6614FD@sogeeky.net>
References: <B1A6E1B2-9705-4A2D-96CF-3BB1F2A93F13@bago.net>
	<39CBC861-A2A0-4866-A3B1-039E0B6614FD@sogeeky.net>
Message-ID: <E6CA0AFA-C8AC-429A-BE96-97CCF4DDF718@bago.net>

I added the history of the pool on the bottom. after the first setup  
gave me only 230GB, I properly destroyed the pool, and redid it with  
one disk, and then adding the second to create a raid (i do assume  
that it would do some sort of stripping). if this is not the proper  
way to go, then the FAQ needs some work (i.e. "you can't build a raid  
from 2 disks"). maybe the FAQ should say that the raidz is something  
like a RAID5, not a RAID0, and therefore needs 3 disks minimum.

can anyone shed a bit more light on this?

Joergen Geerds
http://luminous-newyork.com
http://newyorkpanorama.com

On Aug 4, 2008, at 12:03 , Mr. Zorg wrote:

> 230GB is correct. In a raidz configuration you only get n-1 disks  
> worth of storage since one disk is reserved for parity. It really  
> doesn't make sense to use less than three disks for raidz, with just  
> two you may as well use a mirror configuration.
>
> It is possible that the corruption is a result of you adding disk2s2  
> to the pool a second time. It may have been using it both as a data  
> disk and a parity disk resulting in scrambled data. Of course, it  
> should not have allowed that.
>
> I don't have any spare disks, can anyone else verify this behavior?
>
> On Aug 4, 2008, at 8:09 AM, Joergen Geerds <geerds at bago.net> wrote:
>
>> I installed the latest build of ZFS (July 16, 2008), and  
>> partinioned my external FW800 case (2 sata disks) as a ZFS raid pool.
>>
>> First bug:
>> On my first attempt, I did follow the "Getting started" steps:
>> zpool create bigraid raidz disk1s2 disk2s2
>>
>> This resulted in a raidz pool that was only reporting 230GB instead  
>> of 460GB to the OS (but zpool iostat reported 460GB in size)
>> So I redid the setup, created the pool with just one disk, and then  
>> "zpool add bigraid raidz  disk2s2"
>> This gave me a pool with 460 GB. It would be great if you could  
>> look into this.
>>
>> Second bug:
>> photoshop seems to love the additional speed of the ZFS scratch  
>> disk, it felt really faster than usual (no hard data to back it up)  
>> and no problems.
>> So I decided to do a quick test with PTgui 8b6 (using the ZFS  
>> bigraid as PTgui temp folder, but reading/writing all input/output  
>> from the internal HFS+). The output is a layered PSB file of about  
>> 1GB in this case. The PSB was shredded.
>> See http://nypano.com/clients/ptgui8b6-scratchdisk-error.jpg
>> I was able to verify that it is indeed the ZFS scratchdisk that is  
>> the culprit (moving the ptgui temp folder to the internal disk  
>> results in normal output, but PTgui 7.8 with the ZFS as temp folder  
>> shreds the PSB also).
>>
>> Joost from PTgui claims that system & ZFS is at fault, not PTgui.  
>> Any ideas what went wrong?
>>
>>
>> System info:
>> Imac 2.16GHz, 10.5.4, 3GB, 500GB internal HFS+, 2x250GB ext FW800
>>
>> pool: bigraid
>> state: ONLINE
>> scrub: none requested
>> config:
>>
>> 	NAME        STATE     READ WRITE CKSUM
>> 	bigraid     ONLINE       0     0     0
>> 	  disk1s2   ONLINE       0     0     0
>> 	  disk2s2   ONLINE       0     0     0
>>
>> errors: No known data errors
>>
>> History for 'bigraid':
>> 2008-08-03.18:42:02 zpool create bigraid /dev/disk1s2
>> 2008-08-03.18:46:03 zpool upgrade bigraid 8
>> 2008-08-03.18:47:10 zpool add bigraid disk2s2
>>
>>
>>
>> Joergen Geerds
>> http://luminous-newyork.com
>> http://newyorkpanorama.com
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080804/74011935/attachment.html 

From zorg at sogeeky.net  Mon Aug  4 10:41:34 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Mon, 4 Aug 2008 10:41:34 -0700
Subject: [zfs-discuss] data loss with ZFS as scratch disk
In-Reply-To: <82AC4DE0-50D4-48DE-B006-66D866A5C7D6@apple.com>
References: <B1A6E1B2-9705-4A2D-96CF-3BB1F2A93F13@bago.net>
	<39CBC861-A2A0-4866-A3B1-039E0B6614FD@sogeeky.net>
	<82AC4DE0-50D4-48DE-B006-66D866A5C7D6@apple.com>
Message-ID: <DC0DA20D-E814-41EF-B837-00642B89F6A4@sogeeky.net>

Nope. Despite what it looks like, from the FAQ, usage, etc, it IS NOT  
possible to expand a raidz at this time. Not even in solaris. You can  
add to the pool, but not to the raidz member of the pool.

On Aug 4, 2008, at 9:56 AM, Thane Norton III <tnorton at apple.com> wrote:

> Unless you are setting up for future expansion, right?
>
> On Aug 4, 2008, at 9:03 AM, Mr. Zorg wrote:
>
>> 230GB is correct. In a raidz configuration you only get n-1 disks  
>> worth of storage since one disk is reserved for parity. It really  
>> doesn't make sense to use less than three disks for raidz, with  
>> just two you may as well use a mirror configuration.
>>
>
> -
> Thane Norton
> tnorton at apple.com
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From ndellofano at apple.com  Mon Aug  4 10:41:50 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon, 4 Aug 2008 10:41:50 -0700
Subject: [zfs-discuss] data loss with ZFS as scratch disk
In-Reply-To: <82AC4DE0-50D4-48DE-B006-66D866A5C7D6@apple.com>
References: <B1A6E1B2-9705-4A2D-96CF-3BB1F2A93F13@bago.net>
	<39CBC861-A2A0-4866-A3B1-039E0B6614FD@sogeeky.net>
	<82AC4DE0-50D4-48DE-B006-66D866A5C7D6@apple.com>
Message-ID: <D73DB2E7-35E2-4450-8810-6FEA706E6CFA@apple.com>

Nope, not even then.  You can NOT add just one disk to a raidz set, or  
change a mirror set to a raidz set.  No dice.  You can add another  
grouping of raidz disks to your pool(i.e. if you have 14 drives, your  
pool would be likely made up of 3 or 4 raidz groupings),  but you  
can't add a single disk to an existing raidz grouping.

This is a pretty common confusion, so maybe it's best if I add it to  
the FAQ when I update it next :)


Noel

On Aug 4, 2008, at 9:56 AM, Thane Norton III wrote:

> Unless you are setting up for future expansion, right?
>
> On Aug 4, 2008, at 9:03 AM, Mr. Zorg wrote:
>
>> 230GB is correct. In a raidz configuration you only get n-1 disks  
>> worth of storage since one disk is reserved for parity. It really  
>> doesn't make sense to use less than three disks for raidz, with  
>> just two you may as well use a mirror configuration.
>>
>
> -
> Thane Norton
> tnorton at apple.com
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From caronni at gmail.com  Mon Aug  4 10:42:16 2008
From: caronni at gmail.com (Germano Caronni)
Date: Mon, 4 Aug 2008 19:42:16 +0200
Subject: [zfs-discuss] data loss with ZFS as scratch disk
In-Reply-To: <82AC4DE0-50D4-48DE-B006-66D866A5C7D6@apple.com>
References: <B1A6E1B2-9705-4A2D-96CF-3BB1F2A93F13@bago.net>
	<39CBC861-A2A0-4866-A3B1-039E0B6614FD@sogeeky.net>
	<82AC4DE0-50D4-48DE-B006-66D866A5C7D6@apple.com>
Message-ID: <327b821f0808041042pe635becw5b0e6e4004ff33eb@mail.gmail.com>

Sadly, raidz* can not be shrunk (or expanded). You are not supposed to be
able to add a disk to a raidz, only replace drives.

There is hope, but it is a ways off:

Disparate raidz devices (wow, how I wish this was actually in production
code!)
http://mail.opensolaris.org/pipermail/zfs-code/2007-August/000583.html

Encouragement to work on this:
http://blogs.sun.com/ahl/date/20080407

Germano

p.s. see also bug 6718209 at bugs.opensolaris.org for the feature request

On Mon, Aug 4, 2008 at 18:56, Thane Norton III <tnorton at apple.com> wrote:

> Unless you are setting up for future expansion, right?
>
> On Aug 4, 2008, at 9:03 AM, Mr. Zorg wrote:
>
>  230GB is correct. In a raidz configuration you only get n-1 disks worth of
>> storage since one disk is reserved for parity. It really doesn't make sense
>> to use less than three disks for raidz, with just two you may as well use a
>> mirror configuration.
>>
>>
> -
> Thane Norton
> tnorton at apple.com
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080804/89ea2115/attachment.html 

From zorg at sogeeky.net  Mon Aug  4 10:43:41 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Mon, 4 Aug 2008 10:43:41 -0700
Subject: [zfs-discuss] data loss with ZFS as scratch disk
In-Reply-To: <E6CA0AFA-C8AC-429A-BE96-97CCF4DDF718@bago.net>
References: <B1A6E1B2-9705-4A2D-96CF-3BB1F2A93F13@bago.net>
	<39CBC861-A2A0-4866-A3B1-039E0B6614FD@sogeeky.net>
	<E6CA0AFA-C8AC-429A-BE96-97CCF4DDF718@bago.net>
Message-ID: <8A2A1846-5F5B-40CE-A57C-291B7D8BDB28@sogeeky.net>

That would be good, yes. It is very much like raid5. You CAN use two  
disks, it's just pointless.

On Aug 4, 2008, at 10:35 AM, Joergen Geerds <geerds at bago.net> wrote:

> I added the history of the pool on the bottom. after the first setup  
> gave me only 230GB, I properly destroyed the pool, and redid it with  
> one disk, and then adding the second to create a raid (i do assume  
> that it would do some sort of stripping). if this is not the proper  
> way to go, then the FAQ needs some work (i.e. "you can't build a  
> raid from 2 disks"). maybe the FAQ should say that the raidz is  
> something like a RAID5, not a RAID0, and therefore needs 3 disks  
> minimum.
>
> can anyone shed a bit more light on this?
>
> Joergen Geerds
> http://luminous-newyork.com
> http://newyorkpanorama.com
>
> On Aug 4, 2008, at 12:03 , Mr. Zorg wrote:
>
>> 230GB is correct. In a raidz configuration you only get n-1 disks  
>> worth of storage since one disk is reserved for parity. It really  
>> doesn't make sense to use less than three disks for raidz, with  
>> just two you may as well use a mirror configuration.
>>
>> It is possible that the corruption is a result of you adding  
>> disk2s2 to the pool a second time. It may have been using it both  
>> as a data disk and a parity disk resulting in scrambled data. Of  
>> course, it should not have allowed that.
>>
>> I don't have any spare disks, can anyone else verify this behavior?
>>
>> On Aug 4, 2008, at 8:09 AM, Joergen Geerds <geerds at bago.net> wrote:
>>
>>> I installed the latest build of ZFS (July 16, 2008), and  
>>> partinioned my external FW800 case (2 sata disks) as a ZFS raid  
>>> pool.
>>>
>>> First bug:
>>> On my first attempt, I did follow the "Getting started" steps:
>>> zpool create bigraid raidz disk1s2 disk2s2
>>>
>>> This resulted in a raidz pool that was only reporting 230GB  
>>> instead of 460GB to the OS (but zpool iostat reported 460GB in size)
>>> So I redid the setup, created the pool with just one disk, and  
>>> then "zpool add bigraid raidz  disk2s2"
>>> This gave me a pool with 460 GB. It would be great if you could  
>>> look into this.
>>>
>>> Second bug:
>>> photoshop seems to love the additional speed of the ZFS scratch  
>>> disk, it felt really faster than usual (no hard data to back it  
>>> up) and no problems.
>>> So I decided to do a quick test with PTgui 8b6 (using the ZFS  
>>> bigraid as PTgui temp folder, but reading/writing all input/output  
>>> from the internal HFS+). The output is a layered PSB file of about  
>>> 1GB in this case. The PSB was shredded.
>>> See http://nypano.com/clients/ptgui8b6-scratchdisk-error.jpg
>>> I was able to verify that it is indeed the ZFS scratchdisk that is  
>>> the culprit (moving the ptgui temp folder to the internal disk  
>>> results in normal output, but PTgui 7.8 with the ZFS as temp  
>>> folder shreds the PSB also).
>>>
>>> Joost from PTgui claims that system & ZFS is at fault, not PTgui.  
>>> Any ideas what went wrong?
>>>
>>>
>>> System info:
>>> Imac 2.16GHz, 10.5.4, 3GB, 500GB internal HFS+, 2x250GB ext FW800
>>>
>>> pool: bigraid
>>> state: ONLINE
>>> scrub: none requested
>>> config:
>>>
>>> 	NAME        STATE     READ WRITE CKSUM
>>> 	bigraid     ONLINE       0     0     0
>>> 	  disk1s2   ONLINE       0     0     0
>>> 	  disk2s2   ONLINE       0     0     0
>>>
>>> errors: No known data errors
>>>
>>> History for 'bigraid':
>>> 2008-08-03.18:42:02 zpool create bigraid /dev/disk1s2
>>> 2008-08-03.18:46:03 zpool upgrade bigraid 8
>>> 2008-08-03.18:47:10 zpool add bigraid disk2s2
>>>
>>>
>>>
>>> Joergen Geerds
>>> http://luminous-newyork.com
>>> http://newyorkpanorama.com
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080804/0477c9a6/attachment.html 

From ndellofano at apple.com  Mon Aug  4 10:47:18 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon, 4 Aug 2008 10:47:18 -0700
Subject: [zfs-discuss] data loss with ZFS as scratch disk
In-Reply-To: <E6CA0AFA-C8AC-429A-BE96-97CCF4DDF718@bago.net>
References: <B1A6E1B2-9705-4A2D-96CF-3BB1F2A93F13@bago.net>
	<39CBC861-A2A0-4866-A3B1-039E0B6614FD@sogeeky.net>
	<E6CA0AFA-C8AC-429A-BE96-97CCF4DDF718@bago.net>
Message-ID: <1906055D-2D78-4754-8995-BCD9B02C3FC9@apple.com>

So the best info on all this configuration information can be found on  
the Open Solaris site for ZFS:

http://opensolaris.org/os/community/zfs/docs/
http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide#

Since this info is all the same, I didn't repost it on the mac forge  
website, only added links to that information on the opensolaris  
documentation site.  On there you'll find a Best Practices guide, a  
ZFS admin guide, as well as your standard man page info for zfs and  
zpool.

I'll try and make the link more clear by adding it to the FAQ list.  
Currently it's only under the "Links" and information section.

Noel

On Aug 4, 2008, at 10:35 AM, Joergen Geerds wrote:

> I added the history of the pool on the bottom. after the first setup  
> gave me only 230GB, I properly destroyed the pool, and redid it with  
> one disk, and then adding the second to create a raid (i do assume  
> that it would do some sort of stripping). if this is not the proper  
> way to go, then the FAQ needs some work (i.e. "you can't build a  
> raid from 2 disks"). maybe the FAQ should say that the raidz is  
> something like a RAID5, not a RAID0, and therefore needs 3 disks  
> minimum.
>
> can anyone shed a bit more light on this?
>
> Joergen Geerds
> http://luminous-newyork.com
> http://newyorkpanorama.com
>
> On Aug 4, 2008, at 12:03 , Mr. Zorg wrote:
>
>> 230GB is correct. In a raidz configuration you only get n-1 disks  
>> worth of storage since one disk is reserved for parity. It really  
>> doesn't make sense to use less than three disks for raidz, with  
>> just two you may as well use a mirror configuration.
>>
>> It is possible that the corruption is a result of you adding  
>> disk2s2 to the pool a second time. It may have been using it both  
>> as a data disk and a parity disk resulting in scrambled data. Of  
>> course, it should not have allowed that.
>>
>> I don't have any spare disks, can anyone else verify this behavior?
>>
>> On Aug 4, 2008, at 8:09 AM, Joergen Geerds <geerds at bago.net> wrote:
>>
>>> I installed the latest build of ZFS (July 16, 2008), and  
>>> partinioned my external FW800 case (2 sata disks) as a ZFS raid  
>>> pool.
>>>
>>> First bug:
>>> On my first attempt, I did follow the "Getting started" steps:
>>> zpool create bigraid raidz disk1s2 disk2s2
>>>
>>> This resulted in a raidz pool that was only reporting 230GB  
>>> instead of 460GB to the OS (but zpool iostat reported 460GB in size)
>>> So I redid the setup, created the pool with just one disk, and  
>>> then "zpool add bigraid raidz  disk2s2"
>>> This gave me a pool with 460 GB. It would be great if you could  
>>> look into this.
>>>
>>> Second bug:
>>> photoshop seems to love the additional speed of the ZFS scratch  
>>> disk, it felt really faster than usual (no hard data to back it  
>>> up) and no problems.
>>> So I decided to do a quick test with PTgui 8b6 (using the ZFS  
>>> bigraid as PTgui temp folder, but reading/writing all input/output  
>>> from the internal HFS+). The output is a layered PSB file of about  
>>> 1GB in this case. The PSB was shredded.
>>> See http://nypano.com/clients/ptgui8b6-scratchdisk-error.jpg
>>> I was able to verify that it is indeed the ZFS scratchdisk that is  
>>> the culprit (moving the ptgui temp folder to the internal disk  
>>> results in normal output, but PTgui 7.8 with the ZFS as temp  
>>> folder shreds the PSB also).
>>>
>>> Joost from PTgui claims that system & ZFS is at fault, not PTgui.  
>>> Any ideas what went wrong?
>>>
>>>
>>> System info:
>>> Imac 2.16GHz, 10.5.4, 3GB, 500GB internal HFS+, 2x250GB ext FW800
>>>
>>> pool: bigraid
>>> state: ONLINE
>>> scrub: none requested
>>> config:
>>>
>>> 	NAME        STATE     READ WRITE CKSUM
>>> 	bigraid     ONLINE       0     0     0
>>> 	  disk1s2   ONLINE       0     0     0
>>> 	  disk2s2   ONLINE       0     0     0
>>>
>>> errors: No known data errors
>>>
>>> History for 'bigraid':
>>> 2008-08-03.18:42:02 zpool create bigraid /dev/disk1s2
>>> 2008-08-03.18:46:03 zpool upgrade bigraid 8
>>> 2008-08-03.18:47:10 zpool add bigraid disk2s2
>>>
>>>
>>>
>>> Joergen Geerds
>>> http://luminous-newyork.com
>>> http://newyorkpanorama.com
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080804/af172acf/attachment-0001.html 

From geerds at bago.net  Mon Aug  4 12:12:12 2008
From: geerds at bago.net (Joergen Geerds)
Date: Mon, 4 Aug 2008 15:12:12 -0400
Subject: [zfs-discuss] data loss with ZFS as scratch disk
In-Reply-To: <8A2A1846-5F5B-40CE-A57C-291B7D8BDB28@sogeeky.net>
References: <B1A6E1B2-9705-4A2D-96CF-3BB1F2A93F13@bago.net>
	<39CBC861-A2A0-4866-A3B1-039E0B6614FD@sogeeky.net>
	<E6CA0AFA-C8AC-429A-BE96-97CCF4DDF718@bago.net>
	<8A2A1846-5F5B-40CE-A57C-291B7D8BDB28@sogeeky.net>
Message-ID: <BB4982DC-6385-4112-AE99-1BCDC294619F@bago.net>

I did another test, with 3 250GB disks, and created a clean, working  
raidz set that reported 460GB space:
pool: bigraid
state: ONLINE
scrub: none requested
config:

	NAME         STATE     READ WRITE CKSUM
	bigraid      ONLINE       0     0     0
	  raidz1     ONLINE       0     0     0
	    disk3s2  ONLINE       0     0     0
	    disk2s2  ONLINE       0     0     0
	    disk1s2  ONLINE       0     0     0

errors: No known data errors

and yet, PTgui (using the ZFS as temp folder) still has some serious  
issues with data corruption when writing the PSB from it's temp files:
http://nypano.com/clients/ptgui8b6-scratchdisk-error-2.jpg

photoshop, on the other hand, has no issues using it for temp files.

any ideas? are there any non-kosher ways to read/write files to a ZFS  
that could destroy data along the way?

Joergen Geerds
http://luminous-newyork.com
http://newyorkpanorama.com

On Aug 4, 2008, at 13:43 , Mr. Zorg wrote:

> That would be good, yes. It is very much like raid5. You CAN use two  
> disks, it's just pointless.
>
> On Aug 4, 2008, at 10:35 AM, Joergen Geerds <geerds at bago.net> wrote:
>
>> I added the history of the pool on the bottom. after the first  
>> setup gave me only 230GB, I properly destroyed the pool, and redid  
>> it with one disk, and then adding the second to create a raid (i do  
>> assume that it would do some sort of stripping). if this is not the  
>> proper way to go, then the FAQ needs some work (i.e. "you can't  
>> build a raid from 2 disks"). maybe the FAQ should say that the  
>> raidz is something like a RAID5, not a RAID0, and therefore needs 3  
>> disks minimum.
>>
>> can anyone shed a bit more light on this?
>>
>> Joergen Geerds
>> http://luminous-newyork.com
>> http://newyorkpanorama.com
>>
>> On Aug 4, 2008, at 12:03 , Mr. Zorg wrote:
>>
>>> 230GB is correct. In a raidz configuration you only get n-1 disks  
>>> worth of storage since one disk is reserved for parity. It really  
>>> doesn't make sense to use less than three disks for raidz, with  
>>> just two you may as well use a mirror configuration.
>>>
>>> It is possible that the corruption is a result of you adding  
>>> disk2s2 to the pool a second time. It may have been using it both  
>>> as a data disk and a parity disk resulting in scrambled data. Of  
>>> course, it should not have allowed that.
>>>
>>> I don't have any spare disks, can anyone else verify this behavior?
>>>
>>> On Aug 4, 2008, at 8:09 AM, Joergen Geerds <geerds at bago.net> wrote:
>>>
>>>> I installed the latest build of ZFS (July 16, 2008), and  
>>>> partinioned my external FW800 case (2 sata disks) as a ZFS raid  
>>>> pool.
>>>>
>>>> First bug:
>>>> On my first attempt, I did follow the "Getting started" steps:
>>>> zpool create bigraid raidz disk1s2 disk2s2
>>>>
>>>> This resulted in a raidz pool that was only reporting 230GB  
>>>> instead of 460GB to the OS (but zpool iostat reported 460GB in  
>>>> size)
>>>> So I redid the setup, created the pool with just one disk, and  
>>>> then "zpool add bigraid raidz  disk2s2"
>>>> This gave me a pool with 460 GB. It would be great if you could  
>>>> look into this.
>>>>
>>>> Second bug:
>>>> photoshop seems to love the additional speed of the ZFS scratch  
>>>> disk, it felt really faster than usual (no hard data to back it  
>>>> up) and no problems.
>>>> So I decided to do a quick test with PTgui 8b6 (using the ZFS  
>>>> bigraid as PTgui temp folder, but reading/writing all input/ 
>>>> output from the internal HFS+). The output is a layered PSB file  
>>>> of about 1GB in this case. The PSB was shredded.
>>>> See http://nypano.com/clients/ptgui8b6-scratchdisk-error.jpg
>>>> I was able to verify that it is indeed the ZFS scratchdisk that  
>>>> is the culprit (moving the ptgui temp folder to the internal disk  
>>>> results in normal output, but PTgui 7.8 with the ZFS as temp  
>>>> folder shreds the PSB also).
>>>>
>>>> Joost from PTgui claims that system & ZFS is at fault, not PTgui.  
>>>> Any ideas what went wrong?
>>>>
>>>>
>>>> System info:
>>>> Imac 2.16GHz, 10.5.4, 3GB, 500GB internal HFS+, 2x250GB ext FW800
>>>>
>>>> pool: bigraid
>>>> state: ONLINE
>>>> scrub: none requested
>>>> config:
>>>>
>>>> 	NAME        STATE     READ WRITE CKSUM
>>>> 	bigraid     ONLINE       0     0     0
>>>> 	  disk1s2   ONLINE       0     0     0
>>>> 	  disk2s2   ONLINE       0     0     0
>>>>
>>>> errors: No known data errors
>>>>
>>>> History for 'bigraid':
>>>> 2008-08-03.18:42:02 zpool create bigraid /dev/disk1s2
>>>> 2008-08-03.18:46:03 zpool upgrade bigraid 8
>>>> 2008-08-03.18:47:10 zpool add bigraid disk2s2
>>>>
>>>>
>>>>
>>>> Joergen Geerds
>>>> http://luminous-newyork.com
>>>> http://newyorkpanorama.com
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080804/13d77d34/attachment.html 

From geerds at bago.net  Mon Aug  4 19:26:35 2008
From: geerds at bago.net (Joergen Geerds)
Date: Mon, 4 Aug 2008 22:26:35 -0400
Subject: [zfs-discuss] data loss with ZFS as scratch disk
In-Reply-To: <BB4982DC-6385-4112-AE99-1BCDC294619F@bago.net>
References: <B1A6E1B2-9705-4A2D-96CF-3BB1F2A93F13@bago.net>
	<39CBC861-A2A0-4866-A3B1-039E0B6614FD@sogeeky.net>
	<E6CA0AFA-C8AC-429A-BE96-97CCF4DDF718@bago.net>
	<8A2A1846-5F5B-40CE-A57C-291B7D8BDB28@sogeeky.net>
	<BB4982DC-6385-4112-AE99-1BCDC294619F@bago.net>
Message-ID: <010BED38-B836-4B21-95DC-83BF43A4F46F@bago.net>

Joost from PTgui told me that he's using mmap, mflush and munmap. This  
may be the reason why the problem only appears with PTGui.
I hope this helps a bit.

Joergen Geerds
http://luminous-newyork.com
http://newyorkpanorama.com

On Aug 4, 2008, at 15:12 , Joergen Geerds wrote:

> I did another test, with 3 250GB disks, and created a clean, working  
> raidz set that reported 460GB space:
> pool: bigraid
> state: ONLINE
> scrub: none requested
> config:
>
> 	NAME         STATE     READ WRITE CKSUM
> 	bigraid      ONLINE       0     0     0
> 	  raidz1     ONLINE       0     0     0
> 	    disk3s2  ONLINE       0     0     0
> 	    disk2s2  ONLINE       0     0     0
> 	    disk1s2  ONLINE       0     0     0
>
> errors: No known data errors
>
> and yet, PTgui (using the ZFS as temp folder) still has some serious  
> issues with data corruption when writing the PSB from it's temp files:
> http://nypano.com/clients/ptgui8b6-scratchdisk-error-2.jpg
>
> photoshop, on the other hand, has no issues using it for temp files.
>
> any ideas? are there any non-kosher ways to read/write files to a  
> ZFS that could destroy data along the way?
>
> Joergen Geerds
> http://luminous-newyork.com
> http://newyorkpanorama.com
>
> On Aug 4, 2008, at 13:43 , Mr. Zorg wrote:
>
>> That would be good, yes. It is very much like raid5. You CAN use  
>> two disks, it's just pointless.
>>
>> On Aug 4, 2008, at 10:35 AM, Joergen Geerds <geerds at bago.net> wrote:
>>
>>> I added the history of the pool on the bottom. after the first  
>>> setup gave me only 230GB, I properly destroyed the pool, and redid  
>>> it with one disk, and then adding the second to create a raid (i  
>>> do assume that it would do some sort of stripping). if this is not  
>>> the proper way to go, then the FAQ needs some work (i.e. "you  
>>> can't build a raid from 2 disks"). maybe the FAQ should say that  
>>> the raidz is something like a RAID5, not a RAID0, and therefore  
>>> needs 3 disks minimum.
>>>
>>> can anyone shed a bit more light on this?
>>>
>>> Joergen Geerds
>>> http://luminous-newyork.com
>>> http://newyorkpanorama.com
>>>
>>> On Aug 4, 2008, at 12:03 , Mr. Zorg wrote:
>>>
>>>> 230GB is correct. In a raidz configuration you only get n-1 disks  
>>>> worth of storage since one disk is reserved for parity. It really  
>>>> doesn't make sense to use less than three disks for raidz, with  
>>>> just two you may as well use a mirror configuration.
>>>>
>>>> It is possible that the corruption is a result of you adding  
>>>> disk2s2 to the pool a second time. It may have been using it both  
>>>> as a data disk and a parity disk resulting in scrambled data. Of  
>>>> course, it should not have allowed that.
>>>>
>>>> I don't have any spare disks, can anyone else verify this behavior?
>>>>
>>>> On Aug 4, 2008, at 8:09 AM, Joergen Geerds <geerds at bago.net> wrote:
>>>>
>>>>> I installed the latest build of ZFS (July 16, 2008), and  
>>>>> partinioned my external FW800 case (2 sata disks) as a ZFS raid  
>>>>> pool.
>>>>>
>>>>> First bug:
>>>>> On my first attempt, I did follow the "Getting started" steps:
>>>>> zpool create bigraid raidz disk1s2 disk2s2
>>>>>
>>>>> This resulted in a raidz pool that was only reporting 230GB  
>>>>> instead of 460GB to the OS (but zpool iostat reported 460GB in  
>>>>> size)
>>>>> So I redid the setup, created the pool with just one disk, and  
>>>>> then "zpool add bigraid raidz  disk2s2"
>>>>> This gave me a pool with 460 GB. It would be great if you could  
>>>>> look into this.
>>>>>
>>>>> Second bug:
>>>>> photoshop seems to love the additional speed of the ZFS scratch  
>>>>> disk, it felt really faster than usual (no hard data to back it  
>>>>> up) and no problems.
>>>>> So I decided to do a quick test with PTgui 8b6 (using the ZFS  
>>>>> bigraid as PTgui temp folder, but reading/writing all input/ 
>>>>> output from the internal HFS+). The output is a layered PSB file  
>>>>> of about 1GB in this case. The PSB was shredded.
>>>>> See http://nypano.com/clients/ptgui8b6-scratchdisk-error.jpg
>>>>> I was able to verify that it is indeed the ZFS scratchdisk that  
>>>>> is the culprit (moving the ptgui temp folder to the internal  
>>>>> disk results in normal output, but PTgui 7.8 with the ZFS as  
>>>>> temp folder shreds the PSB also).
>>>>>
>>>>> Joost from PTgui claims that system & ZFS is at fault, not  
>>>>> PTgui. Any ideas what went wrong?
>>>>>
>>>>>
>>>>> System info:
>>>>> Imac 2.16GHz, 10.5.4, 3GB, 500GB internal HFS+, 2x250GB ext FW800
>>>>>
>>>>> pool: bigraid
>>>>> state: ONLINE
>>>>> scrub: none requested
>>>>> config:
>>>>>
>>>>> 	NAME        STATE     READ WRITE CKSUM
>>>>> 	bigraid     ONLINE       0     0     0
>>>>> 	  disk1s2   ONLINE       0     0     0
>>>>> 	  disk2s2   ONLINE       0     0     0
>>>>>
>>>>> errors: No known data errors
>>>>>
>>>>> History for 'bigraid':
>>>>> 2008-08-03.18:42:02 zpool create bigraid /dev/disk1s2
>>>>> 2008-08-03.18:46:03 zpool upgrade bigraid 8
>>>>> 2008-08-03.18:47:10 zpool add bigraid disk2s2
>>>>>
>>>>>
>>>>>
>>>>> Joergen Geerds
>>>>> http://luminous-newyork.com
>>>>> http://newyorkpanorama.com
>>>>>
>>>>> _______________________________________________
>>>>> zfs-discuss mailing list
>>>>> zfs-discuss at lists.macosforge.org
>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080804/41ca5404/attachment-0001.html 

From zorg at sogeeky.net  Mon Aug  4 20:09:53 2008
From: zorg at sogeeky.net (Mr. Zorg)
Date: Mon, 4 Aug 2008 20:09:53 -0700
Subject: [zfs-discuss] data loss with ZFS as scratch disk
In-Reply-To: <010BED38-B836-4B21-95DC-83BF43A4F46F@bago.net>
References: <B1A6E1B2-9705-4A2D-96CF-3BB1F2A93F13@bago.net>
	<39CBC861-A2A0-4866-A3B1-039E0B6614FD@sogeeky.net>
	<E6CA0AFA-C8AC-429A-BE96-97CCF4DDF718@bago.net>
	<8A2A1846-5F5B-40CE-A57C-291B7D8BDB28@sogeeky.net>
	<BB4982DC-6385-4112-AE99-1BCDC294619F@bago.net>
	<010BED38-B836-4B21-95DC-83BF43A4F46F@bago.net>
Message-ID: <D61A2215-F2E9-4865-8A37-53CC2CCB66F7@sogeeky.net>

I know there were issues with mmap coherency at one point, but that  
was supposedly fixed in zfs-111...  Needs another look perhaps, N?el?

On Aug 4, 2008, at 7:26 PM, Joergen Geerds <geerds at bago.net> wrote:

> Joost from PTgui told me that he's using mmap, mflush and munmap.  
> This may be the reason why the problem only appears with PTGui.
> I hope this helps a bit.
>
> Joergen Geerds
> http://luminous-newyork.com
> http://newyorkpanorama.com
>
> On Aug 4, 2008, at 15:12 , Joergen Geerds wrote:
>
>> I did another test, with 3 250GB disks, and created a clean,  
>> working raidz set that reported 460GB space:
>> pool: bigraid
>> state: ONLINE
>> scrub: none requested
>> config:
>>
>> 	NAME         STATE     READ WRITE CKSUM
>> 	bigraid      ONLINE       0     0     0
>> 	  raidz1     ONLINE       0     0     0
>> 	    disk3s2  ONLINE       0     0     0
>> 	    disk2s2  ONLINE       0     0     0
>> 	    disk1s2  ONLINE       0     0     0
>>
>> errors: No known data errors
>>
>> and yet, PTgui (using the ZFS as temp folder) still has some  
>> serious issues with data corruption when writing the PSB from it's  
>> temp files:
>> http://nypano.com/clients/ptgui8b6-scratchdisk-error-2.jpg
>>
>> photoshop, on the other hand, has no issues using it for temp files.
>>
>> any ideas? are there any non-kosher ways to read/write files to a  
>> ZFS that could destroy data along the way?
>>
>> Joergen Geerds
>> http://luminous-newyork.com
>> http://newyorkpanorama.com
>>
>> On Aug 4, 2008, at 13:43 , Mr. Zorg wrote:
>>
>>> That would be good, yes. It is very much like raid5. You CAN use  
>>> two disks, it's just pointless.
>>>
>>> On Aug 4, 2008, at 10:35 AM, Joergen Geerds <geerds at bago.net> wrote:
>>>
>>>> I added the history of the pool on the bottom. after the first  
>>>> setup gave me only 230GB, I properly destroyed the pool, and  
>>>> redid it with one disk, and then adding the second to create a  
>>>> raid (i do assume that it would do some sort of stripping). if  
>>>> this is not the proper way to go, then the FAQ needs some work  
>>>> (i.e. "you can't build a raid from 2 disks"). maybe the FAQ  
>>>> should say that the raidz is something like a RAID5, not a RAID0,  
>>>> and therefore needs 3 disks minimum.
>>>>
>>>> can anyone shed a bit more light on this?
>>>>
>>>> Joergen Geerds
>>>> http://luminous-newyork.com
>>>> http://newyorkpanorama.com
>>>>
>>>> On Aug 4, 2008, at 12:03 , Mr. Zorg wrote:
>>>>
>>>>> 230GB is correct. In a raidz configuration you only get n-1  
>>>>> disks worth of storage since one disk is reserved for parity. It  
>>>>> really doesn't make sense to use less than three disks for  
>>>>> raidz, with just two you may as well use a mirror configuration.
>>>>>
>>>>> It is possible that the corruption is a result of you adding  
>>>>> disk2s2 to the pool a second time. It may have been using it  
>>>>> both as a data disk and a parity disk resulting in scrambled  
>>>>> data. Of course, it should not have allowed that.
>>>>>
>>>>> I don't have any spare disks, can anyone else verify this  
>>>>> behavior?
>>>>>
>>>>> On Aug 4, 2008, at 8:09 AM, Joergen Geerds <geerds at bago.net>  
>>>>> wrote:
>>>>>
>>>>>> I installed the latest build of ZFS (July 16, 2008), and  
>>>>>> partinioned my external FW800 case (2 sata disks) as a ZFS raid  
>>>>>> pool.
>>>>>>
>>>>>> First bug:
>>>>>> On my first attempt, I did follow the "Getting started" steps:
>>>>>> zpool create bigraid raidz disk1s2 disk2s2
>>>>>>
>>>>>> This resulted in a raidz pool that was only reporting 230GB  
>>>>>> instead of 460GB to the OS (but zpool iostat reported 460GB in  
>>>>>> size)
>>>>>> So I redid the setup, created the pool with just one disk, and  
>>>>>> then "zpool add bigraid raidz  disk2s2"
>>>>>> This gave me a pool with 460 GB. It would be great if you could  
>>>>>> look into this.
>>>>>>
>>>>>> Second bug:
>>>>>> photoshop seems to love the additional speed of the ZFS scratch  
>>>>>> disk, it felt really faster than usual (no hard data to back it  
>>>>>> up) and no problems.
>>>>>> So I decided to do a quick test with PTgui 8b6 (using the ZFS  
>>>>>> bigraid as PTgui temp folder, but reading/writing all input/ 
>>>>>> output from the internal HFS+). The output is a layered PSB  
>>>>>> file of about 1GB in this case. The PSB was shredded.
>>>>>> See http://nypano.com/clients/ptgui8b6-scratchdisk-error.jpg
>>>>>> I was able to verify that it is indeed the ZFS scratchdisk that  
>>>>>> is the culprit (moving the ptgui temp folder to the internal  
>>>>>> disk results in normal output, but PTgui 7.8 with the ZFS as  
>>>>>> temp folder shreds the PSB also).
>>>>>>
>>>>>> Joost from PTgui claims that system & ZFS is at fault, not  
>>>>>> PTgui. Any ideas what went wrong?
>>>>>>
>>>>>>
>>>>>> System info:
>>>>>> Imac 2.16GHz, 10.5.4, 3GB, 500GB internal HFS+, 2x250GB ext FW800
>>>>>>
>>>>>> pool: bigraid
>>>>>> state: ONLINE
>>>>>> scrub: none requested
>>>>>> config:
>>>>>>
>>>>>> 	NAME        STATE     READ WRITE CKSUM
>>>>>> 	bigraid     ONLINE       0     0     0
>>>>>> 	  disk1s2   ONLINE       0     0     0
>>>>>> 	  disk2s2   ONLINE       0     0     0
>>>>>>
>>>>>> errors: No known data errors
>>>>>>
>>>>>> History for 'bigraid':
>>>>>> 2008-08-03.18:42:02 zpool create bigraid /dev/disk1s2
>>>>>> 2008-08-03.18:46:03 zpool upgrade bigraid 8
>>>>>> 2008-08-03.18:47:10 zpool add bigraid disk2s2
>>>>>>
>>>>>>
>>>>>>
>>>>>> Joergen Geerds
>>>>>> http://luminous-newyork.com
>>>>>> http://newyorkpanorama.com
>>>>>>
>>>>>> _______________________________________________
>>>>>> zfs-discuss mailing list
>>>>>> zfs-discuss at lists.macosforge.org
>>>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>>>
>>>> _______________________________________________
>>>> zfs-discuss mailing list
>>>> zfs-discuss at lists.macosforge.org
>>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> /div>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080804/105b280d/attachment-0001.html 

From ndellofano at apple.com  Tue Aug  5 12:37:06 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue, 5 Aug 2008 12:37:06 -0700
Subject: [zfs-discuss] build-119 still dying a silent death
In-Reply-To: <327b821f0808040901t2b990795hafd71595d3c7ff4e@mail.gmail.com>
References: <327b821f0807221002w25756393gc4879426f1a9a06c@mail.gmail.com>
	<948DDB28-E0C0-41A2-BD4F-51016F7EB3A9@nrao.edu>
	<7CE95A44-0635-4003-B009-B7D93209DC59@apple.com>
	<B1BFF383-ECB9-4054-90A9-5A306AFCD47D@sun.com>
	<327b821f0807221337v766304aby5dc6a56a54c68455@mail.gmail.com>
	<4C84237B-12F7-4389-963C-85D481EB21FD@apple.com>
	<CC4E6330-D189-4184-8E00-3742938B1925@apple.com>
	<924BAD42-F7DA-47DE-84B0-3E53E1183B51@apple.com>
	<327b821f0808020025m5c51b20am2777ff5a8aad96c7@mail.gmail.com>
	<327b821f0808040655i5242aba2q827d91cf8be83728@mail.gmail.com>
	<327b821f0808040901t2b990795hafd71595d3c7ff4e@mail.gmail.com>
Message-ID: <F110148A-D906-454F-ABF7-509B90728EE6@apple.com>

Sadly Dtrace doesn't help us here because it doesn't work on kexts.   
So that's why no probes are listed and none are available even to add  
statically.

Debugging remotely is also a problem since you have to have symbols  
and another machine and boot args and some magical incantations to get  
the symbols for zfs loaded.

So first it would be easier and really helpful if you could get a  
sample of the rsync process.  Then I can get some idea of what's  
running.  Can you do

#sudo sample <pid> 10 10 -file /tmp/mysample

where <pid> is the pid of the rsync process?

Also, Andrew did you say you get this problem on your HFS+ drive as  
well?  Or did I just misread that... I"m still working on getting a  
reproduce and narrowing the scope... for whatever reason, I have no  
problems doing thins on my mac pros and it won't reproduce....

Thanks for all your help and info!  I'm trying to pin it down :)

Noel

On Aug 4, 2008, at 9:01 AM, Germano Caronni wrote:

> Here some specifics on the silent death. This should be a lot of fun  
> for you @ Apple ;-)
>
> rsync 3.0.3 also fails. Having interspersed lots of sync() and  
> sleep() in that one, I finally got to see a partially copied dev  
> tree. Funnily enough, the major numbers of all devices on zfs are  
> zero. That certainly is a problem, and probably / hopefully it is  
> related to the deadlock. The machine dies at disk0s2 (my root  
> volume). Trying to reproduce this manually:
>
> bash-3.2# pwd
> /Volumes/z
> bash-3.2# mknod disk0 b 14 0
> bash-3.2# sync
> bash-3.2# ls -al
> total 80
> [...]
> brw-r--r--  1 root     staff    0,   0 Aug  4 17:03 disk0
> bash-3.2# sync
> bash-3.2# mknod disk0s1 b 14 1
> bash-3.2# sync
> bash-3.2# ls -al
> total 81
> [...]
> brw-r--r--  1 root     staff    0,   0 Aug  4 17:03 disk0
> brw-r--r--  1 root     staff    0,   1 Aug  4 17:04 disk0s1
> bash-3.2# sync
> bash-3.2# mknod disk0s2 b 14 2
> bash-3.2# sync
> [the world comes to an end]
>
>
> The output of 'stat' agrees that the major is indeed 0. Checking  
> back on my big repository confirms that all devices (char and block)  
> in it now have a major of '0'. I'm so happy that I still have the  
> originals ;-)
>
>
> I hope this helps. I looked a bit at the source code to zfs_mknode  
> and friends, but did not feel like poking randomly. Also, a pity  
> that there are no zfs dtrace probes around (ie dtrace -l does not  
> list any). Is there a good way to get them into the kernel? I'm  
> guessing it would just involve using the 'Debug' target and copying  
> things to the right location -- but maybe you can tell me?  
> Obviously, what I wanted to do was dtrace on zfs_mknode and then  
> print out the arguments to the function to see if the problem is  
> above or below.
>
> Germano

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080805/83707a7d/attachment.html 

From caronni at gmail.com  Tue Aug  5 12:47:15 2008
From: caronni at gmail.com (Germano Caronni)
Date: Tue, 5 Aug 2008 21:47:15 +0200
Subject: [zfs-discuss] another failure mode for zfs
Message-ID: <327b821f0808051247hcd371cfo542e026056d54d47@mail.gmail.com>

Hi Noel, all,

here another way to make a mac misbehave (besides the mknod thing in the
other thread):
I have a pool that if imported at the time of a restart or shutdown will
hang the machine. You get the spinning sunburst symbol forever.
A copy of the pool is found at http://olymp.dreamhosters.com/pool1.bz2

An alternate way to create this pool:
mkfile 100M /pool1
sudo zpool
zpool create z /pool1
zfs set compression=on z
zfs mount z
copy in the attached c file
export CFLAGS -g -Wall
make maketree
mkdir a
cd a
../maketree 276000
(actually i did a maketree 1000000 but had to ^C it because the pool was
getting full)
The 'foo' file should not matter.

Now, irrespective of 'z' being mounted or not, as long as it is imported,
the machine won't shut down.
Once you manually export it before the shutdown, everything goes as
expected.

please tell me if you can repro, either with pool1 or with a freshly created
pool.

Germano

p.s. the source is obviously write-only code. use at your own risk, yadda
yadda.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080805/601a706b/attachment.html 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: maketree.c
Url: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080805/601a706b/attachment.c 

From andy at aligature.com  Tue Aug  5 12:48:14 2008
From: andy at aligature.com (Andrew Webber)
Date: Tue, 5 Aug 2008 15:48:14 -0400
Subject: [zfs-discuss] build-119 still dying a silent death
In-Reply-To: <F110148A-D906-454F-ABF7-509B90728EE6@apple.com>
References: <327b821f0807221002w25756393gc4879426f1a9a06c@mail.gmail.com>
	<B1BFF383-ECB9-4054-90A9-5A306AFCD47D@sun.com>
	<327b821f0807221337v766304aby5dc6a56a54c68455@mail.gmail.com>
	<4C84237B-12F7-4389-963C-85D481EB21FD@apple.com>
	<CC4E6330-D189-4184-8E00-3742938B1925@apple.com>
	<924BAD42-F7DA-47DE-84B0-3E53E1183B51@apple.com>
	<327b821f0808020025m5c51b20am2777ff5a8aad96c7@mail.gmail.com>
	<327b821f0808040655i5242aba2q827d91cf8be83728@mail.gmail.com>
	<327b821f0808040901t2b990795hafd71595d3c7ff4e@mail.gmail.com>
	<F110148A-D906-454F-ABF7-509B90728EE6@apple.com>
Message-ID: <60b50dc10808051248q18caaf40p2ae1d955f140e201@mail.gmail.com>

Sorry if I wasn't clear.  In your post on 7/31 you listed out the
theoretical instructions to reproduce the spinning beach ball.

The "mkfile 20g /foo" portion creates a file on my HFS+ drive.  Then the
"zpool create z /foo" portion creates a ZFS pool using that file.  So in the
end I am running an rsync on a ZFS filesystem on a ZFS pool created out of a
file on an HFS+ drive.

Perhaps I misinterpreted the instructions.  What that the result that you
were expecting?


On Tue, Aug 5, 2008 at 3:37 PM, No?l Dellofano <ndellofano at apple.com> wrote:

> Sadly Dtrace doesn't help us here because it doesn't work on kexts.  So
> that's why no probes are listed and none are available even to add
> statically.
> Debugging remotely is also a problem since you have to have symbols and
> another machine and boot args and some magical incantations to get the
> symbols for zfs loaded.
>
> So first it would be easier and really helpful if you could get a sample of
> the rsync process.  Then I can get some idea of what's running.  Can you do
>
> #sudo sample <pid> 10 10 -file /tmp/mysample
>
> where <pid> is the pid of the rsync process?
>
> Also, Andrew did you say you get this problem on your HFS+ drive as well?
>  Or did I just misread that... I"m still working on getting a reproduce
> and narrowing the scope... for whatever reason, I have no problems doing
> thins on my mac pros and it won't reproduce....
>
> Thanks for all your help and info!  I'm trying to pin it down :)
>
> Noel
>
> On Aug 4, 2008, at 9:01 AM, Germano Caronni wrote:
>
> Here some specifics on the silent death. This should be a lot of fun for
> you @ Apple ;-)
>
> rsync 3.0.3 also fails. Having interspersed lots of sync() and sleep() in
> that one, I finally got to see a partially copied dev tree. Funnily enough,
> the major numbers of all devices on zfs are zero. That certainly is a
> problem, and probably / hopefully it is related to the deadlock. The machine
> dies at disk0s2 (my root volume). Trying to reproduce this manually:
>
> bash-3.2# pwd
> /Volumes/z
> bash-3.2# mknod disk0 b 14 0
> bash-3.2# sync
> bash-3.2# ls -al
> total 80
> [...]
> brw-r--r--  1 root     staff    0,   0 Aug  4 17:03 disk0
> bash-3.2# sync
> bash-3.2# mknod disk0s1 b 14 1
> bash-3.2# sync
> bash-3.2# ls -al
> total 81
> [...]
> brw-r--r--  1 root     staff    0,   0 Aug  4 17:03 disk0
> brw-r--r--  1 root     staff    0,   1 Aug  4 17:04 disk0s1
> bash-3.2# sync
> bash-3.2# mknod disk0s2 b 14 2
> bash-3.2# sync
> [the world comes to an end]
>
>
> The output of 'stat' agrees that the major is indeed 0. Checking back on my
> big repository confirms that all devices (char and block) in it now have a
> major of '0'. I'm so happy that I still have the originals ;-)
>
>
> I hope this helps. I looked a bit at the source code to zfs_mknode and
> friends, but did not feel like poking randomly. Also, a pity that there are
> no zfs dtrace probes around (ie dtrace -l does not list any). Is there a
> good way to get them into the kernel? I'm guessing it would just involve
> using the 'Debug' target and copying things to the right location -- but
> maybe you can tell me? Obviously, what I wanted to do was dtrace on
> zfs_mknode and then print out the arguments to the function to see if the
> problem is above or below.
>
> Germano
>
>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080805/47e9f33b/attachment-0001.html 

From caronni at gmail.com  Tue Aug  5 12:54:01 2008
From: caronni at gmail.com (Germano Caronni)
Date: Tue, 5 Aug 2008 21:54:01 +0200
Subject: [zfs-discuss] build-119 still dying a silent death
In-Reply-To: <60b50dc10808051248q18caaf40p2ae1d955f140e201@mail.gmail.com>
References: <327b821f0807221002w25756393gc4879426f1a9a06c@mail.gmail.com>
	<327b821f0807221337v766304aby5dc6a56a54c68455@mail.gmail.com>
	<4C84237B-12F7-4389-963C-85D481EB21FD@apple.com>
	<CC4E6330-D189-4184-8E00-3742938B1925@apple.com>
	<924BAD42-F7DA-47DE-84B0-3E53E1183B51@apple.com>
	<327b821f0808020025m5c51b20am2777ff5a8aad96c7@mail.gmail.com>
	<327b821f0808040655i5242aba2q827d91cf8be83728@mail.gmail.com>
	<327b821f0808040901t2b990795hafd71595d3c7ff4e@mail.gmail.com>
	<F110148A-D906-454F-ABF7-509B90728EE6@apple.com>
	<60b50dc10808051248q18caaf40p2ae1d955f140e201@mail.gmail.com>
Message-ID: <327b821f0808051254s5fdd1ba7j4da2768f6e6cb498@mail.gmail.com>

Hi Andrew,

your description of what you did matches what I did.
Does your system die when you do 'mknod disk0s2 b 14 2' in zfs (regardless
of which pool you use, assuming that's the device node for your root
partition? For me, that line suffices to give me a spinning ball.

Germano

On Tue, Aug 5, 2008 at 21:48, Andrew Webber <andy at aligature.com> wrote:

> Sorry if I wasn't clear.  In your post on 7/31 you listed out the
> theoretical instructions to reproduce the spinning beach ball.
>
> The "mkfile 20g /foo" portion creates a file on my HFS+ drive.  Then the
> "zpool create z /foo" portion creates a ZFS pool using that file.  So in the
> end I am running an rsync on a ZFS filesystem on a ZFS pool created out of a
> file on an HFS+ drive.
>
> Perhaps I misinterpreted the instructions.  What that the result that you
> were expecting?
>
>
> On Tue, Aug 5, 2008 at 3:37 PM, No?l Dellofano <ndellofano at apple.com>wrote:
>
>> Sadly Dtrace doesn't help us here because it doesn't work on kexts.  So
>> that's why no probes are listed and none are available even to add
>> statically.
>> Debugging remotely is also a problem since you have to have symbols and
>> another machine and boot args and some magical incantations to get the
>> symbols for zfs loaded.
>>
>> So first it would be easier and really helpful if you could get a sample
>> of the rsync process.  Then I can get some idea of what's running.  Can you
>> do
>>
>> #sudo sample <pid> 10 10 -file /tmp/mysample
>>
>> where <pid> is the pid of the rsync process?
>>
>> Also, Andrew did you say you get this problem on your HFS+ drive as well?
>>  Or did I just misread that... I"m still working on getting a reproduce
>> and narrowing the scope... for whatever reason, I have no problems doing
>> thins on my mac pros and it won't reproduce....
>>
>> Thanks for all your help and info!  I'm trying to pin it down :)
>>
>> Noel
>>
>> On Aug 4, 2008, at 9:01 AM, Germano Caronni wrote:
>>
>> Here some specifics on the silent death. This should be a lot of fun for
>> you @ Apple ;-)
>>
>> rsync 3.0.3 also fails. Having interspersed lots of sync() and sleep() in
>> that one, I finally got to see a partially copied dev tree. Funnily enough,
>> the major numbers of all devices on zfs are zero. That certainly is a
>> problem, and probably / hopefully it is related to the deadlock. The machine
>> dies at disk0s2 (my root volume). Trying to reproduce this manually:
>>
>> bash-3.2# pwd
>> /Volumes/z
>> bash-3.2# mknod disk0 b 14 0
>> bash-3.2# sync
>> bash-3.2# ls -al
>> total 80
>> [...]
>> brw-r--r--  1 root     staff    0,   0 Aug  4 17:03 disk0
>> bash-3.2# sync
>> bash-3.2# mknod disk0s1 b 14 1
>> bash-3.2# sync
>> bash-3.2# ls -al
>> total 81
>> [...]
>> brw-r--r--  1 root     staff    0,   0 Aug  4 17:03 disk0
>> brw-r--r--  1 root     staff    0,   1 Aug  4 17:04 disk0s1
>> bash-3.2# sync
>> bash-3.2# mknod disk0s2 b 14 2
>> bash-3.2# sync
>> [the world comes to an end]
>>
>>
>> The output of 'stat' agrees that the major is indeed 0. Checking back on
>> my big repository confirms that all devices (char and block) in it now have
>> a major of '0'. I'm so happy that I still have the originals ;-)
>>
>>
>> I hope this helps. I looked a bit at the source code to zfs_mknode and
>> friends, but did not feel like poking randomly. Also, a pity that there are
>> no zfs dtrace probes around (ie dtrace -l does not list any). Is there a
>> good way to get them into the kernel? I'm guessing it would just involve
>> using the 'Debug' target and copying things to the right location -- but
>> maybe you can tell me? Obviously, what I wanted to do was dtrace on
>> zfs_mknode and then print out the arguments to the function to see if the
>> problem is above or below.
>>
>> Germano
>>
>>
>>
>> _______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080805/2eb27a2a/attachment.html 

From Jonathan.Edwards at Sun.COM  Tue Aug  5 12:55:56 2008
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Tue, 05 Aug 2008 15:55:56 -0400
Subject: [zfs-discuss] build-119 still dying a silent death
In-Reply-To: <F110148A-D906-454F-ABF7-509B90728EE6@apple.com>
References: <327b821f0807221002w25756393gc4879426f1a9a06c@mail.gmail.com>
	<948DDB28-E0C0-41A2-BD4F-51016F7EB3A9@nrao.edu>
	<7CE95A44-0635-4003-B009-B7D93209DC59@apple.com>
	<B1BFF383-ECB9-4054-90A9-5A306AFCD47D@sun.com>
	<327b821f0807221337v766304aby5dc6a56a54c68455@mail.gmail.com>
	<4C84237B-12F7-4389-963C-85D481EB21FD@apple.com>
	<CC4E6330-D189-4184-8E00-3742938B1925@apple.com>
	<924BAD42-F7DA-47DE-84B0-3E53E1183B51@apple.com>
	<327b821f0808020025m5c51b20am2777ff5a8aad96c7@mail.gmail.com>
	<327b821f0808040655i5242aba2q827d91cf8be83728@mail.gmail.com>
	<327b821f0808040901t2b990795hafd71595d3c7ff4e@mail.gmail.com>
	<F110148A-D906-454F-ABF7-509B90728EE6@apple.com>
Message-ID: <707B7901-DC89-47BF-BB0B-0010BFB7C272@Sun.COM>

odd - i was able to reproduce this about 3 more times, but now nothing  
as soon as i started tracing with dtruss

if you have XCode installed - you could get a bit of stack backtrace  
on the process with a "sudo dtruss -fs <cmd>" .. use script and see if  
the file saves after you hang (i lost a couple of them on an HFS+) ..  
was guessing at interactions with mdimportd or fseventd and friends,  
but it could also be a perm thing .. (i know we saw some issues with  
mknod on NFSv4)

On Aug 5, 2008, at 3:37 PM, No?l Dellofano wrote:

> Sadly Dtrace doesn't help us here because it doesn't work on kexts.   
> So that's why no probes are listed and none are available even to  
> add statically.
>
> Debugging remotely is also a problem since you have to have symbols  
> and another machine and boot args and some magical incantations to  
> get the symbols for zfs loaded.
>
> So first it would be easier and really helpful if you could get a  
> sample of the rsync process.  Then I can get some idea of what's  
> running.  Can you do
>
> #sudo sample <pid> 10 10 -file /tmp/mysample
>
> where <pid> is the pid of the rsync process?
>
> Also, Andrew did you say you get this problem on your HFS+ drive as  
> well?  Or did I just misread that... I"m still working on getting a  
> reproduce and narrowing the scope... for whatever reason, I have no  
> problems doing thins on my mac pros and it won't reproduce....
>
> Thanks for all your help and info!  I'm trying to pin it down :)
>
> Noel
>
> On Aug 4, 2008, at 9:01 AM, Germano Caronni wrote:
>
>> Here some specifics on the silent death. This should be a lot of  
>> fun for you @ Apple ;-)
>>
>> rsync 3.0.3 also fails. Having interspersed lots of sync() and  
>> sleep() in that one, I finally got to see a partially copied dev  
>> tree. Funnily enough, the major numbers of all devices on zfs are  
>> zero. That certainly is a problem, and probably / hopefully it is  
>> related to the deadlock. The machine dies at disk0s2 (my root  
>> volume). Trying to reproduce this manually:
>>
>> bash-3.2# pwd
>> /Volumes/z
>> bash-3.2# mknod disk0 b 14 0
>> bash-3.2# sync
>> bash-3.2# ls -al
>> total 80
>> [...]
>> brw-r--r--  1 root     staff    0,   0 Aug  4 17:03 disk0
>> bash-3.2# sync
>> bash-3.2# mknod disk0s1 b 14 1
>> bash-3.2# sync
>> bash-3.2# ls -al
>> total 81
>> [...]
>> brw-r--r--  1 root     staff    0,   0 Aug  4 17:03 disk0
>> brw-r--r--  1 root     staff    0,   1 Aug  4 17:04 disk0s1
>> bash-3.2# sync
>> bash-3.2# mknod disk0s2 b 14 2
>> bash-3.2# sync
>> [the world comes to an end]
>>
>>
>> The output of 'stat' agrees that the major is indeed 0. Checking  
>> back on my big repository confirms that all devices (char and  
>> block) in it now have a major of '0'. I'm so happy that I still  
>> have the originals ;-)
>>
>>
>> I hope this helps. I looked a bit at the source code to zfs_mknode  
>> and friends, but did not feel like poking randomly. Also, a pity  
>> that there are no zfs dtrace probes around (ie dtrace -l does not  
>> list any). Is there a good way to get them into the kernel? I'm  
>> guessing it would just involve using the 'Debug' target and copying  
>> things to the right location -- but maybe you can tell me?  
>> Obviously, what I wanted to do was dtrace on zfs_mknode and then  
>> print out the arguments to the function to see if the problem is  
>> above or below.
>>
>> Germano
>


From franzschmalzl at spamfreemail.de  Wed Aug  6 10:21:34 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Wed, 6 Aug 2008 19:21:34 +0200
Subject: [zfs-discuss] hf
Message-ID: <9FA8BB02-5CE2-46B5-A1E5-2383673A079D@spamfreemail.de>





moved homefolder back to zfs again... fingers crossed





From XTTurner at mbbc.edu  Thu Aug  7 15:31:34 2008
From: XTTurner at mbbc.edu (Timothy Turner)
Date: Thu, 7 Aug 2008 17:31:34 -0500
Subject: [zfs-discuss] nas
Message-ID: <46F0E75BFC7CEC43ABE183E4EA4C4D277FB15F0339@mailbox.mbbc.edu>

Hello has anyone been setup a raidz nas with one of the many prosumer/home use nas devices?

Tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080807/dfbf9862/attachment.html 

From jim at netgate.com  Thu Aug  7 17:44:01 2008
From: jim at netgate.com (Jim Thompson)
Date: Thu, 7 Aug 2008 14:44:01 -1000
Subject: [zfs-discuss] nas
In-Reply-To: <46F0E75BFC7CEC43ABE183E4EA4C4D277FB15F0339@mailbox.mbbc.edu>
References: <46F0E75BFC7CEC43ABE183E4EA4C4D277FB15F0339@mailbox.mbbc.edu>
Message-ID: <62B49BAC-3296-4568-88CC-60B6EA1A0E8C@netgate.com>

Find one with a 64-bit CPU, and we'll talk.

On Aug 7, 2008, at 12:31 PM, Timothy Turner <XTTurner at mbbc.edu> wrote:

> Hello has anyone been setup a raidz nas with one of the many  
> prosumer/home use nas devices?
>
>
>
> Tim
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

From sah.list at gmail.com  Thu Aug  7 22:41:28 2008
From: sah.list at gmail.com (Sean Hafeez)
Date: Thu, 7 Aug 2008 22:41:28 -0700
Subject: [zfs-discuss] Mounts in Finder for tank/foo?
Message-ID: <53026DCD-2682-4E3C-BF8B-9B820E4B142C@gmail.com>

Is this a known issue?

data2          647G   268G   188G  /Volumes/data2
data2/iTunes   459G   268G   459G  /Volumes/data2/iTunes

Let's say I want to do:

zfs set mountpoint=/Volumes/iTunes data/iTunes

Which gives me:

data2           454Gi  188Gi  266Gi    42%    /Volumes/data2
data2/iTunes    727Gi  461Gi  266Gi    64%    /Volumes/iTunes

Okay, so now I have this in the Volumes directory. Great, however I  
cannot see it in Finder. Go -> Folder /Volumes show it however since I  
do not have show hidden files on (and I do not want to leave them on)  
I cannot tell iTunes (in iTunes prefs) its directory as this does not  
show up on the desktop as a mount.

Hope that makes sense.

Thanks,
Sean


From franzschmalzl at spamfreemail.de  Fri Aug  8 17:18:52 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Sat, 9 Aug 2008 02:18:52 +0200
Subject: [zfs-discuss] snapshot
Message-ID: <56FC3220-43DA-4634-8CD0-3BC8A026D72C@spamfreemail.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1





hey list!

i'm having a little understanding issue here

look at this


raidtank/ruebebackup     116G   280G   116G  /Volumes/raidtank/ 
ruebebackup
raidtank/ruebebackup at 1  42,6K      -   116G  -


let's imagine i used the snapshots for a while to make my backups and  
my harddrive started filling up

so i would have ruebebackup at 1,2,3,4,5,6,7 and so on

of course i could start to delete some old snapshots but what if the  
ruebebackup itself got so outdated i want to delete it and only keep  
the latest state ( snapshot )
zfs tells my i can't do that

basically i want to simulate time machine behaviour

how would i do that ?


regards

ruebezahl





-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.8 (Darwin)

iQEcBAEBAgAGBQJInOJsAAoJEP8ZopU3BhmtTzwH/R0JpPc4wMMo6s8rwfTbzTsU
xJCIY0IpgRJaygHdQUTc/uBxLpvZaj/VGCnhZcrsObdw2bCfOJtI0mlLtdxeEOsJ
iSOWsokxP4la65fKORUrEBfcvAFO0cH1CWAhlWEfKLk92RHCTqIqhBI/z1c/L0G+
fnkJLpQercNsb+mpUXpAkkDbW+cFb827wSLpjhJ8rR20uYvft/alJxSHPBjOGOWc
J4nrrU7AHL8kP287f7aKjdpz/2j1vivap5P5m+UluKAaQjs/xW623hU3An/2LQfg
cqTtPbumYRmcq54c4QZqnruXTujPx/pHCa+gPkm/TDrTUHPajS0mubyVvdLSGwA=
=mIbD
-----END PGP SIGNATURE-----

From ndellofano at apple.com  Fri Aug  8 18:28:37 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 8 Aug 2008 18:28:37 -0700
Subject: [zfs-discuss] snapshot
In-Reply-To: <56FC3220-43DA-4634-8CD0-3BC8A026D72C@spamfreemail.de>
References: <56FC3220-43DA-4634-8CD0-3BC8A026D72C@spamfreemail.de>
Message-ID: <6E8BFFA6-C0A7-4338-8C5E-5652C50EBBBB@apple.com>

So if I'm understanding you correctly, you have a bunch of snapshots  
of your filesystem, and you want to delete your filesystem  
alltogether  and only keep your last snapshot of your filesystem  
around for backup purposes incase you want something from it at some  
point?

If the above is the case, then just use 'zfs send' to send your  
snapshot to a file you can store anywhere on your machine, or another  
drive.  And no, wherever you save that file, it doesn't have to be a  
ZFS filesystem.  So if you had an external firewire drive with an HFS+  
partition on it, you could do
'zfs send raidtank/ruebebackup at 7 > ruebebackup.out'

This will send a complete instantiation of your filesystem to a file.   
Then you can just delete the filesystem and all the snapshots.  Then  
if you want to access it, you can use 'zfs recv', note that in order  
to receive you do have to receive into a ZFS pool, since ZFS will  
rebuild your filesystem just as you left it, options and all.

For more related functionality of snapshots you can also look at 'zfs  
clone', which given a snapshot will recreate a writeable version of  
that snapshot.  You can also use 'zfs promote' if you decide at some  
point that you want to promote your clone to your current 'parent'  
filesystem.


Is the above information what you're looking for? Or do you have any  
other questions?

Noel

On Aug 8, 2008, at 5:18 PM, ruebezahl wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
>
>
>
>
> hey list!
>
> i'm having a little understanding issue here
>
> look at this
>
>
> raidtank/ruebebackup     116G   280G   116G  /Volumes/raidtank/
> ruebebackup
> raidtank/ruebebackup at 1  42,6K      -   116G  -
>
>
> let's imagine i used the snapshots for a while to make my backups and
> my harddrive started filling up
>
> so i would have ruebebackup at 1,2,3,4,5,6,7 and so on
>
> of course i could start to delete some old snapshots but what if the
> ruebebackup itself got so outdated i want to delete it and only keep
> the latest state ( snapshot )
> zfs tells my i can't do that
>
> basically i want to simulate time machine behaviour
>
> how would i do that ?
>
>
> regards
>
> ruebezahl
>
>
>
>
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.8 (Darwin)
>
> iQEcBAEBAgAGBQJInOJsAAoJEP8ZopU3BhmtTzwH/R0JpPc4wMMo6s8rwfTbzTsU
> xJCIY0IpgRJaygHdQUTc/uBxLpvZaj/VGCnhZcrsObdw2bCfOJtI0mlLtdxeEOsJ
> iSOWsokxP4la65fKORUrEBfcvAFO0cH1CWAhlWEfKLk92RHCTqIqhBI/z1c/L0G+
> fnkJLpQercNsb+mpUXpAkkDbW+cFb827wSLpjhJ8rR20uYvft/alJxSHPBjOGOWc
> J4nrrU7AHL8kP287f7aKjdpz/2j1vivap5P5m+UluKAaQjs/xW623hU3An/2LQfg
> cqTtPbumYRmcq54c4QZqnruXTujPx/pHCa+gPkm/TDrTUHPajS0mubyVvdLSGwA=
> =mIbD
> -----END PGP SIGNATURE-----
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From caronni at gmail.com  Sat Aug  9 02:10:55 2008
From: caronni at gmail.com (Germano Caronni)
Date: Sat, 9 Aug 2008 11:10:55 +0200
Subject: [zfs-discuss] another failure mode for zfs
In-Reply-To: <327b821f0808051247hcd371cfo542e026056d54d47@mail.gmail.com>
References: <327b821f0808051247hcd371cfo542e026056d54d47@mail.gmail.com>
Message-ID: <327b821f0808090210y29ffadbcg422aa7bd2586dcad@mail.gmail.com>

Did anybody have a chance to reproduce this issue? Or is this another one of
the 'it only happens to me' instances?

Germano

On Tue, Aug 5, 2008 at 21:47, Germano Caronni <caronni at gmail.com> wrote:

> Hi Noel, all,
>
> here another way to make a mac misbehave (besides the mknod thing in the
> other thread):
> I have a pool that if imported at the time of a restart or shutdown will
> hang the machine. You get the spinning sunburst symbol forever.
> A copy of the pool is found at http://olymp.dreamhosters.com/pool1.bz2
>
> An alternate way to create this pool:
> mkfile 100M /pool1
> sudo zpool
> zpool create z /pool1
> zfs set compression=on z
> zfs mount z
> copy in the attached c file
> export CFLAGS -g -Wall
> make maketree
> mkdir a
> cd a
> ../maketree 276000
> (actually i did a maketree 1000000 but had to ^C it because the pool was
> getting full)
> The 'foo' file should not matter.
>
> Now, irrespective of 'z' being mounted or not, as long as it is imported,
> the machine won't shut down.
> Once you manually export it before the shutdown, everything goes as
> expected.
>
> please tell me if you can repro, either with pool1 or with a freshly
> created pool.
>
> Germano
>
> p.s. the source is obviously write-only code. use at your own risk, yadda
> yadda.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080809/905104ee/attachment.html 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: maketree.c
Url: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080809/905104ee/attachment.c 

From franzschmalzl at spamfreemail.de  Sun Aug 10 12:43:12 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Sun, 10 Aug 2008 21:43:12 +0200
Subject: [zfs-discuss] snapshot
References: <017EA3B6-BB3D-477C-820B-ADFA586F5EA5@spamfreemail.de>
Message-ID: <CBFE2D23-436C-4F4F-80E9-1249202BBA2E@spamfreemail.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1


- -----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1





hi noel!

thank you for your rapid response...

so i read my mail to you again this morning and noticed there was no  
way whatsoever you could understand what i want to ask :)
sorry, i was tired, let me try again :)


let my show you a zfs list


NAME                     USED  AVAIL  REFER  MOUNTPOINT
hometank                 117G   297G   796K  /Volumes/hometank
hometank/meister        54,0M   297G  54,0M  /Volumes/hometank/meister
hometank/ruebezahl       117G   297G   116G  /Volumes/hometank/ruebezahl
hometank/ruebezahl at 1    75,1M      -   116G  -
raidtank                 635G   280G  57,1M  /Volumes/raidtank
raidtank/installers      161G   280G   161G  /Volumes/raidtank/ 
installers
raidtank/meisterbackup  54,7M   280G  54,7M  /Volumes/raidtank/ 
meisterbackup
raidtank/movies          347G   280G   347G  /Volumes/raidtank/movies
raidtank/ruebebackup     116G   280G   116G  /Volumes/raidtank/ 
ruebebackup
raidtank/ruebebackup at 1  42,6K      -   116G  -
raidtank/sysbackup      10,9G   280G  10,9G  /Volumes/raidtank/backup


my homefolders are on hometank
my backups are on raidtank together with several other things...


i used zfs send hometank/ruebezahl at 1 > blafile
zfs recv raidtank/ruebebackup < blafile
and then some incremental sending

let's state i have done this for a while and my raidtank decided to  
fill up

so it would look like this

raidtank/ruebebackup
raidtank/ruebebackup at 1
raidtank/ruebebackup at 2
raidtank/ruebebackup at 3
raidtank/ruebebackup at 4
raidtank/ruebebackup at 5
.........

so at this point i could start to delete @1 and @2 and @3 and maybe  
free up about 3 or 4  gigs
but imagine the original initial backup ruebebackup is so old it does  
not make sense to keep it anymore

how would i manage to promote ruebebackup at 5 to the latest filesystem  
and still be able to resume my incremental backups without having to  
delete everything and start with a new initial backup?


if i still didn't manage to make a point here:

imagine time machine

backs up initial and then incremental
harddrive fills up
deletes oldes backup
and "rotates" the backups trough time without the need of a now  
initial backup


i want to imitate this behaviour


best whateverwouldbeappropriateinenglish

ruebezahl :)



ps.
thanks for your time
>
>
>
>
>
>
>
>
>
>
>
>> So if I'm understanding you correctly, you have a bunch of  
>> snapshots of your filesystem, and you want to delete your  
>> filesystem alltogether  and only keep your last snapshot of your  
>> filesystem around for backup purposes incase you want something  
>> from it at some point?
>>
>> If the above is the case, then just use 'zfs send' to send your  
>> snapshot to a file you can store anywhere on your machine, or  
>> another drive.  And no, wherever you save that file, it doesn't  
>> have to be a ZFS filesystem.  So if you had an external firewire  
>> drive with an HFS+ partition on it, you could do
>> 'zfs send raidtank/ruebebackup at 7 > ruebebackup.out'
>>
>> This will send a complete instantiation of your filesystem to a  
>> file.  Then you can just delete the filesystem and all the  
>> snapshots.  Then if you want to access it, you can use 'zfs recv',  
>> note that in order to receive you do have to receive into a ZFS  
>> pool, since ZFS will rebuild your filesystem just as you left it,  
>> options and all.
>>
>> For more related functionality of snapshots you can also look at  
>> 'zfs clone', which given a snapshot will recreate a writeable  
>> version of that snapshot.  You can also use 'zfs promote' if you  
>> decide at some point that you want to promote your clone to your  
>> current 'parent' filesystem.
>>
>>
>> Is the above information what you're looking for? Or do you have  
>> any other questions?
>>
>> Noel
>>
>> On Aug 8, 2008, at 5:18 PM, ruebezahl wrote:
>>
>>> -----BEGIN PGP SIGNED MESSAGE-----
>>> Hash: SHA1
>>>
>>>
>>>
>>>
>>>
>>> hey list!
>>>
>>> i'm having a little understanding issue here
>>>
>>> look at this
>>>
>>>
>>> raidtank/ruebebackup     116G   280G   116G  /Volumes/raidtank/
>>> ruebebackup
>>> raidtank/ruebebackup at 1  42,6K      -   116G  -
>>>
>>>
>>> let's imagine i used the snapshots for a while to make my backups  
>>> and
>>> my harddrive started filling up
>>>
>>> so i would have ruebebackup at 1,2,3,4,5,6,7 and so on
>>>
>>> of course i could start to delete some old snapshots but what if the
>>> ruebebackup itself got so outdated i want to delete it and only keep
>>> the latest state ( snapshot )
>>> zfs tells my i can't do that
>>>
>>> basically i want to simulate time machine behaviour
>>>
>>> how would i do that ?
>>>
>>>
>>> regards
>>>
>>> ruebezahl
>>>
>>>
>>>
>>>
>>>
>>> -----BEGIN PGP SIGNATURE-----
>>> Version: GnuPG v1.4.8 (Darwin)
>>>
>>> iQEcBAEBAgAGBQJInOJsAAoJEP8ZopU3BhmtTzwH/R0JpPc4wMMo6s8rwfTbzTsU
>>> xJCIY0IpgRJaygHdQUTc/uBxLpvZaj/VGCnhZcrsObdw2bCfOJtI0mlLtdxeEOsJ
>>> iSOWsokxP4la65fKORUrEBfcvAFO0cH1CWAhlWEfKLk92RHCTqIqhBI/z1c/L0G+
>>> fnkJLpQercNsb+mpUXpAkkDbW+cFb827wSLpjhJ8rR20uYvft/alJxSHPBjOGOWc
>>> J4nrrU7AHL8kP287f7aKjdpz/2j1vivap5P5m+UluKAaQjs/xW623hU3An/2LQfg
>>> cqTtPbumYRmcq54c4QZqnruXTujPx/pHCa+gPkm/TDrTUHPajS0mubyVvdLSGwA=
>>> =mIbD
>>> -----END PGP SIGNATURE-----
>>> _______________________________________________
>>> zfs-discuss mailing list
>>> zfs-discuss at lists.macosforge.org
>>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>>
>
>
>
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.8 (Darwin)
>
> iQEcBAEBAgAGBQJInUR+AAoJEP8ZopU3BhmtGbIIAKZklNU8QTQGVPFiTUo/dEyR
> YWagZ67eg0dDdjnzUk6edI3enRmriLwPtuR7Cs4Vr481j3kdKg7+Dy6afUIW4o+L
> YWTNAkn3gcadWHy++ChLdyUSK4sAuVv6qkvEt6n5rfKcvsOEPqx7IUabIgxjpFYS
> vt8LblUFXrEao34L/HEaPaqLUT7iErFyZefdti1cbKYjIPC0HC/EEJ0miVojY+HR
> IpKpYa7ZbNRq2xY0Ws5PbxLrOjhm3DaOCroJFiVn3UwZ0M02Skh+fTtp7SZprEuH
> Syq2hGbkIzu4xoU3d0SpUr0Eka7pa43JnEgjGIX3p6cI08F6T8eHHyLgQ2NetMk=
> =DYQF
> -----END PGP SIGNATURE-----

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.8 (Darwin)

iQEcBAEBAgAGBQJIn0TQAAoJEP8ZopU3Bhmt8UIH/0wqRfSdZjVp+Qag9eRyUt3y
BJT2NkLs1kHAbtPy9yXMnEKebHSoUxcxFl2GBO8vhD4NdYCPTyH1A8DkcWlaNFnK
2aSgAyWA0OGOYColL3PadGKSL4aA09bFVMgu5Vky12mJ3JlRblrkV3LJ0vTHXajN
6KFesNQWpHmYJd3Cg8aNF9vB/kxf8PiYGqtqZeBgN1DtA3GV3J+mdGaDIbRMv+XS
vvvHUjrfrqrjL8T6t/u96ZIHRvSNuZTubxTKeXQY8pYExPc0JPrds0JScbpzr9VQ
p5oICZTvgOvPjekpvKPHWH70iQ8PKtlDXGTkBC/1zRH1FXTitYfZ0MzKXxz09vI=
=CFek
-----END PGP SIGNATURE-----

From caronni at gmail.com  Sun Aug 10 13:34:32 2008
From: caronni at gmail.com (Germano Caronni)
Date: Sun, 10 Aug 2008 22:34:32 +0200
Subject: [zfs-discuss] snapshot
In-Reply-To: <CBFE2D23-436C-4F4F-80E9-1249202BBA2E@spamfreemail.de>
References: <017EA3B6-BB3D-477C-820B-ADFA586F5EA5@spamfreemail.de>
	<CBFE2D23-436C-4F4F-80E9-1249202BBA2E@spamfreemail.de>
Message-ID: <327b821f0808101334x8355a5dk4cb036fdc9fafb61@mail.gmail.com>

Paint me naive. But if you destroy all snapshots from ruebebackup, what you
are left with is the latest backup you made. All the space referenced by the
old snapshots gets freed when you destroy them. So you loose the ability to
go back in time, but in return you get the space back that is only
referenced in the deleted snapshots.

That's exactly what you want, no?

Germano


On Sun, Aug 10, 2008 at 21:43, ruebezahl <franzschmalzl at spamfreemail.de>wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
>
> - -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
>
>
>
>
> hi noel!
>
> thank you for your rapid response...
>
> so i read my mail to you again this morning and noticed there was no
> way whatsoever you could understand what i want to ask :)
> sorry, i was tired, let me try again :)
>
>
> let my show you a zfs list
>
>
> NAME                     USED  AVAIL  REFER  MOUNTPOINT
> hometank                 117G   297G   796K  /Volumes/hometank
> hometank/meister        54,0M   297G  54,0M  /Volumes/hometank/meister
> hometank/ruebezahl       117G   297G   116G  /Volumes/hometank/ruebezahl
> hometank/ruebezahl at 1    75,1M      -   116G  -
> raidtank                 635G   280G  57,1M  /Volumes/raidtank
> raidtank/installers      161G   280G   161G  /Volumes/raidtank/
> installers
> raidtank/meisterbackup  54,7M   280G  54,7M  /Volumes/raidtank/
> meisterbackup
> raidtank/movies          347G   280G   347G  /Volumes/raidtank/movies
> raidtank/ruebebackup     116G   280G   116G  /Volumes/raidtank/
> ruebebackup
> raidtank/ruebebackup at 1  42,6K      -   116G  -
> raidtank/sysbackup      10,9G   280G  10,9G  /Volumes/raidtank/backup
>
>
> my homefolders are on hometank
> my backups are on raidtank together with several other things...
>
>
> i used zfs send hometank/ruebezahl at 1 > blafile
> zfs recv raidtank/ruebebackup < blafile
> and then some incremental sending
>
> let's state i have done this for a while and my raidtank decided to
> fill up
>
> so it would look like this
>
> raidtank/ruebebackup
> raidtank/ruebebackup at 1
> raidtank/ruebebackup at 2
> raidtank/ruebebackup at 3
> raidtank/ruebebackup at 4
> raidtank/ruebebackup at 5
> .........
>
> so at this point i could start to delete @1 and @2 and @3 and maybe
> free up about 3 or 4  gigs
> but imagine the original initial backup ruebebackup is so old it does
> not make sense to keep it anymore
>
> how would i manage to promote ruebebackup at 5 to the latest filesystem
> and still be able to resume my incremental backups without having to
> delete everything and start with a new initial backup?
>
>
> if i still didn't manage to make a point here:
>
> imagine time machine
>
> backs up initial and then incremental
> harddrive fills up
> deletes oldes backup
> and "rotates" the backups trough time without the need of a now
> initial backup
>
>
> i want to imitate this behaviour
>
>
> best whateverwouldbeappropriateinenglish
>
> ruebezahl :)
>
>
>
> ps.
> thanks for your time
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >> So if I'm understanding you correctly, you have a bunch of
> >> snapshots of your filesystem, and you want to delete your
> >> filesystem alltogether  and only keep your last snapshot of your
> >> filesystem around for backup purposes incase you want something
> >> from it at some point?
> >>
> >> If the above is the case, then just use 'zfs send' to send your
> >> snapshot to a file you can store anywhere on your machine, or
> >> another drive.  And no, wherever you save that file, it doesn't
> >> have to be a ZFS filesystem.  So if you had an external firewire
> >> drive with an HFS+ partition on it, you could do
> >> 'zfs send raidtank/ruebebackup at 7 > ruebebackup.out'
> >>
> >> This will send a complete instantiation of your filesystem to a
> >> file.  Then you can just delete the filesystem and all the
> >> snapshots.  Then if you want to access it, you can use 'zfs recv',
> >> note that in order to receive you do have to receive into a ZFS
> >> pool, since ZFS will rebuild your filesystem just as you left it,
> >> options and all.
> >>
> >> For more related functionality of snapshots you can also look at
> >> 'zfs clone', which given a snapshot will recreate a writeable
> >> version of that snapshot.  You can also use 'zfs promote' if you
> >> decide at some point that you want to promote your clone to your
> >> current 'parent' filesystem.
> >>
> >>
> >> Is the above information what you're looking for? Or do you have
> >> any other questions?
> >>
> >> Noel
> >>
> >> On Aug 8, 2008, at 5:18 PM, ruebezahl wrote:
> >>
> >>> -----BEGIN PGP SIGNED MESSAGE-----
> >>> Hash: SHA1
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> hey list!
> >>>
> >>> i'm having a little understanding issue here
> >>>
> >>> look at this
> >>>
> >>>
> >>> raidtank/ruebebackup     116G   280G   116G  /Volumes/raidtank/
> >>> ruebebackup
> >>> raidtank/ruebebackup at 1  42,6K      -   116G  -
> >>>
> >>>
> >>> let's imagine i used the snapshots for a while to make my backups
> >>> and
> >>> my harddrive started filling up
> >>>
> >>> so i would have ruebebackup at 1,2,3,4,5,6,7 and so on
> >>>
> >>> of course i could start to delete some old snapshots but what if the
> >>> ruebebackup itself got so outdated i want to delete it and only keep
> >>> the latest state ( snapshot )
> >>> zfs tells my i can't do that
> >>>
> >>> basically i want to simulate time machine behaviour
> >>>
> >>> how would i do that ?
> >>>
> >>>
> >>> regards
> >>>
> >>> ruebezahl
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> -----BEGIN PGP SIGNATURE-----
> >>> Version: GnuPG v1.4.8 (Darwin)
> >>>
> >>> iQEcBAEBAgAGBQJInOJsAAoJEP8ZopU3BhmtTzwH/R0JpPc4wMMo6s8rwfTbzTsU
> >>> xJCIY0IpgRJaygHdQUTc/uBxLpvZaj/VGCnhZcrsObdw2bCfOJtI0mlLtdxeEOsJ
> >>> iSOWsokxP4la65fKORUrEBfcvAFO0cH1CWAhlWEfKLk92RHCTqIqhBI/z1c/L0G+
> >>> fnkJLpQercNsb+mpUXpAkkDbW+cFb827wSLpjhJ8rR20uYvft/alJxSHPBjOGOWc
> >>> J4nrrU7AHL8kP287f7aKjdpz/2j1vivap5P5m+UluKAaQjs/xW623hU3An/2LQfg
> >>> cqTtPbumYRmcq54c4QZqnruXTujPx/pHCa+gPkm/TDrTUHPajS0mubyVvdLSGwA=
> >>> =mIbD
> >>> -----END PGP SIGNATURE-----
> >>> _______________________________________________
> >>> zfs-discuss mailing list
> >>> zfs-discuss at lists.macosforge.org
> >>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >>
> >
> >
> >
> >
> > -----BEGIN PGP SIGNATURE-----
> > Version: GnuPG v1.4.8 (Darwin)
> >
> > iQEcBAEBAgAGBQJInUR+AAoJEP8ZopU3BhmtGbIIAKZklNU8QTQGVPFiTUo/dEyR
> > YWagZ67eg0dDdjnzUk6edI3enRmriLwPtuR7Cs4Vr481j3kdKg7+Dy6afUIW4o+L
> > YWTNAkn3gcadWHy++ChLdyUSK4sAuVv6qkvEt6n5rfKcvsOEPqx7IUabIgxjpFYS
> > vt8LblUFXrEao34L/HEaPaqLUT7iErFyZefdti1cbKYjIPC0HC/EEJ0miVojY+HR
> > IpKpYa7ZbNRq2xY0Ws5PbxLrOjhm3DaOCroJFiVn3UwZ0M02Skh+fTtp7SZprEuH
> > Syq2hGbkIzu4xoU3d0SpUr0Eka7pa43JnEgjGIX3p6cI08F6T8eHHyLgQ2NetMk=
> > =DYQF
> > -----END PGP SIGNATURE-----
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.8 (Darwin)
>
> iQEcBAEBAgAGBQJIn0TQAAoJEP8ZopU3Bhmt8UIH/0wqRfSdZjVp+Qag9eRyUt3y
> BJT2NkLs1kHAbtPy9yXMnEKebHSoUxcxFl2GBO8vhD4NdYCPTyH1A8DkcWlaNFnK
> 2aSgAyWA0OGOYColL3PadGKSL4aA09bFVMgu5Vky12mJ3JlRblrkV3LJ0vTHXajN
> 6KFesNQWpHmYJd3Cg8aNF9vB/kxf8PiYGqtqZeBgN1DtA3GV3J+mdGaDIbRMv+XS
> vvvHUjrfrqrjL8T6t/u96ZIHRvSNuZTubxTKeXQY8pYExPc0JPrds0JScbpzr9VQ
> p5oICZTvgOvPjekpvKPHWH70iQ8PKtlDXGTkBC/1zRH1FXTitYfZ0MzKXxz09vI=
> =CFek
> -----END PGP SIGNATURE-----
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080810/7a6fd240/attachment.html 

From franzschmalzl at spamfreemail.de  Sun Aug 10 13:41:07 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Sun, 10 Aug 2008 22:41:07 +0200
Subject: [zfs-discuss] snapshot
In-Reply-To: <327b821f0808101334x8355a5dk4cb036fdc9fafb61@mail.gmail.com>
References: <017EA3B6-BB3D-477C-820B-ADFA586F5EA5@spamfreemail.de>
	<CBFE2D23-436C-4F4F-80E9-1249202BBA2E@spamfreemail.de>
	<327b821f0808101334x8355a5dk4cb036fdc9fafb61@mail.gmail.com>
Message-ID: <C6DB3D97-4FD8-4B86-A4E2-334C0129EB99@spamfreemail.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1



sort of, the latest snapshot should stay and all the others disappear  
yes.
but the original filesystem stays ( ruebebackup ) and has very old  
data in it which is not needed anymore
since the latest snapshot is a child of that filesystem i cannot  
delete it

by promoting the latest snapshot to a regular filesystem i loose the  
ability to continue my redundant backups don't i ?


thanks


ruebezahl




> Paint me naive. But if you destroy all snapshots from ruebebackup,  
> what you are left with is the latest backup you made. All the space  
> referenced by the old snapshots gets freed when you destroy them. So  
> you loose the ability to go back in time, but in return you get the  
> space back that is only referenced in the deleted snapshots.
>
> That's exactly what you want, no?
>
> Germano
>
>
> On Sun, Aug 10, 2008 at 21:43, ruebezahl <franzschmalzl at spamfreemail.de 
> > wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
>
> - -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
>
>
>
>
> hi noel!
>
> thank you for your rapid response...
>
> so i read my mail to you again this morning and noticed there was no
> way whatsoever you could understand what i want to ask :)
> sorry, i was tired, let me try again :)
>
>
> let my show you a zfs list
>
>
> NAME                     USED  AVAIL  REFER  MOUNTPOINT
> hometank                 117G   297G   796K  /Volumes/hometank
> hometank/meister        54,0M   297G  54,0M  /Volumes/hometank/meister
> hometank/ruebezahl       117G   297G   116G  /Volumes/hometank/ 
> ruebezahl
> hometank/ruebezahl at 1    75,1M      -   116G  -
> raidtank                 635G   280G  57,1M  /Volumes/raidtank
> raidtank/installers      161G   280G   161G  /Volumes/raidtank/
> installers
> raidtank/meisterbackup  54,7M   280G  54,7M  /Volumes/raidtank/
> meisterbackup
> raidtank/movies          347G   280G   347G  /Volumes/raidtank/movies
> raidtank/ruebebackup     116G   280G   116G  /Volumes/raidtank/
> ruebebackup
> raidtank/ruebebackup at 1  42,6K      -   116G  -
> raidtank/sysbackup      10,9G   280G  10,9G  /Volumes/raidtank/backup
>
>
> my homefolders are on hometank
> my backups are on raidtank together with several other things...
>
>
> i used zfs send hometank/ruebezahl at 1 > blafile
> zfs recv raidtank/ruebebackup < blafile
> and then some incremental sending
>
> let's state i have done this for a while and my raidtank decided to
> fill up
>
> so it would look like this
>
> raidtank/ruebebackup
> raidtank/ruebebackup at 1
> raidtank/ruebebackup at 2
> raidtank/ruebebackup at 3
> raidtank/ruebebackup at 4
> raidtank/ruebebackup at 5
> .........
>
> so at this point i could start to delete @1 and @2 and @3 and maybe
> free up about 3 or 4  gigs
> but imagine the original initial backup ruebebackup is so old it does
> not make sense to keep it anymore
>
> how would i manage to promote ruebebackup at 5 to the latest filesystem
> and still be able to resume my incremental backups without having to
> delete everything and start with a new initial backup?
>
>
> if i still didn't manage to make a point here:
>
> imagine time machine
>
> backs up initial and then incremental
> harddrive fills up
> deletes oldes backup
> and "rotates" the backups trough time without the need of a now
> initial backup
>
>
> i want to imitate this behaviour
>
>
> best whateverwouldbeappropriateinenglish
>
> ruebezahl :)
>
>
>
> ps.
> thanks for your time
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >> So if I'm understanding you correctly, you have a bunch of
> >> snapshots of your filesystem, and you want to delete your
> >> filesystem alltogether  and only keep your last snapshot of your
> >> filesystem around for backup purposes incase you want something
> >> from it at some point?
> >>
> >> If the above is the case, then just use 'zfs send' to send your
> >> snapshot to a file you can store anywhere on your machine, or
> >> another drive.  And no, wherever you save that file, it doesn't
> >> have to be a ZFS filesystem.  So if you had an external firewire
> >> drive with an HFS+ partition on it, you could do
> >> 'zfs send raidtank/ruebebackup at 7 > ruebebackup.out'
> >>
> >> This will send a complete instantiation of your filesystem to a
> >> file.  Then you can just delete the filesystem and all the
> >> snapshots.  Then if you want to access it, you can use 'zfs recv',
> >> note that in order to receive you do have to receive into a ZFS
> >> pool, since ZFS will rebuild your filesystem just as you left it,
> >> options and all.
> >>
> >> For more related functionality of snapshots you can also look at
> >> 'zfs clone', which given a snapshot will recreate a writeable
> >> version of that snapshot.  You can also use 'zfs promote' if you
> >> decide at some point that you want to promote your clone to your
> >> current 'parent' filesystem.
> >>
> >>
> >> Is the above information what you're looking for? Or do you have
> >> any other questions?
> >>
> >> Noel
> >>
> >> On Aug 8, 2008, at 5:18 PM, ruebezahl wrote:
> >>
> >>> -----BEGIN PGP SIGNED MESSAGE-----
> >>> Hash: SHA1
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> hey list!
> >>>
> >>> i'm having a little understanding issue here
> >>>
> >>> look at this
> >>>
> >>>
> >>> raidtank/ruebebackup     116G   280G   116G  /Volumes/raidtank/
> >>> ruebebackup
> >>> raidtank/ruebebackup at 1  42,6K      -   116G  -
> >>>
> >>>
> >>> let's imagine i used the snapshots for a while to make my backups
> >>> and
> >>> my harddrive started filling up
> >>>
> >>> so i would have ruebebackup at 1,2,3,4,5,6,7 and so on
> >>>
> >>> of course i could start to delete some old snapshots but what if  
> the
> >>> ruebebackup itself got so outdated i want to delete it and only  
> keep
> >>> the latest state ( snapshot )
> >>> zfs tells my i can't do that
> >>>
> >>> basically i want to simulate time machine behaviour
> >>>
> >>> how would i do that ?
> >>>
> >>>
> >>> regards
> >>>
> >>> ruebezahl
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> -----BEGIN PGP SIGNATURE-----
> >>> Version: GnuPG v1.4.8 (Darwin)
> >>>
> >>> iQEcBAEBAgAGBQJInOJsAAoJEP8ZopU3BhmtTzwH/R0JpPc4wMMo6s8rwfTbzTsU
> >>> xJCIY0IpgRJaygHdQUTc/uBxLpvZaj/VGCnhZcrsObdw2bCfOJtI0mlLtdxeEOsJ
> >>> iSOWsokxP4la65fKORUrEBfcvAFO0cH1CWAhlWEfKLk92RHCTqIqhBI/z1c/L0G+
> >>> fnkJLpQercNsb+mpUXpAkkDbW+cFb827wSLpjhJ8rR20uYvft/alJxSHPBjOGOWc
> >>> J4nrrU7AHL8kP287f7aKjdpz/2j1vivap5P5m+UluKAaQjs/xW623hU3An/2LQfg
> >>> cqTtPbumYRmcq54c4QZqnruXTujPx/pHCa+gPkm/TDrTUHPajS0mubyVvdLSGwA=
> >>> =mIbD
> >>> -----END PGP SIGNATURE-----
> >>> _______________________________________________
> >>> zfs-discuss mailing list
> >>> zfs-discuss at lists.macosforge.org
> >>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
> >>
> >
> >
> >
> >
> > -----BEGIN PGP SIGNATURE-----
> > Version: GnuPG v1.4.8 (Darwin)
> >
> > iQEcBAEBAgAGBQJInUR+AAoJEP8ZopU3BhmtGbIIAKZklNU8QTQGVPFiTUo/dEyR
> > YWagZ67eg0dDdjnzUk6edI3enRmriLwPtuR7Cs4Vr481j3kdKg7+Dy6afUIW4o+L
> > YWTNAkn3gcadWHy++ChLdyUSK4sAuVv6qkvEt6n5rfKcvsOEPqx7IUabIgxjpFYS
> > vt8LblUFXrEao34L/HEaPaqLUT7iErFyZefdti1cbKYjIPC0HC/EEJ0miVojY+HR
> > IpKpYa7ZbNRq2xY0Ws5PbxLrOjhm3DaOCroJFiVn3UwZ0M02Skh+fTtp7SZprEuH
> > Syq2hGbkIzu4xoU3d0SpUr0Eka7pa43JnEgjGIX3p6cI08F6T8eHHyLgQ2NetMk=
> > =DYQF
> > -----END PGP SIGNATURE-----
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.8 (Darwin)
>
> iQEcBAEBAgAGBQJIn0TQAAoJEP8ZopU3Bhmt8UIH/0wqRfSdZjVp+Qag9eRyUt3y
> BJT2NkLs1kHAbtPy9yXMnEKebHSoUxcxFl2GBO8vhD4NdYCPTyH1A8DkcWlaNFnK
> 2aSgAyWA0OGOYColL3PadGKSL4aA09bFVMgu5Vky12mJ3JlRblrkV3LJ0vTHXajN
> 6KFesNQWpHmYJd3Cg8aNF9vB/kxf8PiYGqtqZeBgN1DtA3GV3J+mdGaDIbRMv+XS
> vvvHUjrfrqrjL8T6t/u96ZIHRvSNuZTubxTKeXQY8pYExPc0JPrds0JScbpzr9VQ
> p5oICZTvgOvPjekpvKPHWH70iQ8PKtlDXGTkBC/1zRH1FXTitYfZ0MzKXxz09vI=
> =CFek
> -----END PGP SIGNATURE-----
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.8 (Darwin)

iQEcBAEBAgAGBQJIn1JkAAoJEP8ZopU3BhmtrTcIAInomzZkrpNjyL/P3Scv37uF
c9tQoGmE7gNq+U0sApzSmGKhgqghS7CWYAe/Kzdawwi37ziEf3a3rlCwifCRzBK9
xczNIWA9xZl+F1ybiQfcRFn4ChosjVFwnS4WR5L4FnYCOXj3OGpAJOMtcIj4QBy8
lUs/dRqOOQr9NQhgMt/xQU3iOhj4FpBUn/mplmibqq0RWl6FHxLQ5AWcn8Aa7gNg
+59YkXrAoEacP7Jq16On/AAFf08pd9BBcCwpDJuaY2PXFakKLXRGhbqU9H2zgSWZ
oml+s/fC5b9RjURsTKHem0zrX2BQcnwuTf6rfwAnkJofOWsLbu5Oxcj7wMRny8I=
=Eyg+
-----END PGP SIGNATURE-----

From franzschmalzl at spamfreemail.de  Sun Aug 10 13:45:12 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Sun, 10 Aug 2008 22:45:12 +0200
Subject: [zfs-discuss] language
Message-ID: <E00F7658-CE21-4337-AA35-F30ADA5E1067@spamfreemail.de>

after re-reading my mail i'm always thinking " what the fuck do i want  
to say"

i think im facing some linguistic  barriers here, so please be  
patient :)

excuse me, my english went a bit rusty these days

ruebezahl




From franzschmalzl at spamfreemail.de  Sun Aug 10 13:56:05 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Sun, 10 Aug 2008 22:56:05 +0200
Subject: [zfs-discuss] snapshot
In-Reply-To: <327b821f0808101348w6dc6e3b3mf24dda4ce6758721@mail.gmail.com>
References: <017EA3B6-BB3D-477C-820B-ADFA586F5EA5@spamfreemail.de>
	<CBFE2D23-436C-4F4F-80E9-1249202BBA2E@spamfreemail.de>
	<327b821f0808101334x8355a5dk4cb036fdc9fafb61@mail.gmail.com>
	<C6DB3D97-4FD8-4B86-A4E2-334C0129EB99@spamfreemail.de>
	<327b821f0808101348w6dc6e3b3mf24dda4ce6758721@mail.gmail.com>
Message-ID: <B6155961-E1E8-4982-914B-3C402B394637@spamfreemail.de>

ruebbackup is the latest snapshot ?

i thought ruebebackup@<latestnumber>  was the latest one...



i think i might actually get something fundamental wrong here,

i'll play around a bit more and see if i can get some illumination to  
my mind  :)


ruebezahl




On 10.08.2008, at 22:48, Germano Caronni wrote:

> no no, ruebebackup is not the original filesystem. it is the latest  
> snapshot plus whatever changes you made since then. or do you  
> actually see a different behaviour on your system?
>
> Germano
>


From info at martin-hauser.net  Sun Aug 10 23:47:27 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Mon, 11 Aug 2008 08:47:27 +0200
Subject: [zfs-discuss] snapshot
In-Reply-To: <B6155961-E1E8-4982-914B-3C402B394637@spamfreemail.de>
References: <017EA3B6-BB3D-477C-820B-ADFA586F5EA5@spamfreemail.de>
	<CBFE2D23-436C-4F4F-80E9-1249202BBA2E@spamfreemail.de>
	<327b821f0808101334x8355a5dk4cb036fdc9fafb61@mail.gmail.com>
	<C6DB3D97-4FD8-4B86-A4E2-334C0129EB99@spamfreemail.de>
	<327b821f0808101348w6dc6e3b3mf24dda4ce6758721@mail.gmail.com>
	<B6155961-E1E8-4982-914B-3C402B394637@spamfreemail.de>
Message-ID: <D59BDE5B-1F07-4BBE-85DC-6ACCEAC4A424@martin-hauser.net>

Hey there,

well, I guess the problem here is as follows. You are copying  
snapshots in incremental order from filesystem a to filesystem b  
(where b is your ruebebackup). Therefore, unlike it would be with only  
snapshots and no zfs send/recv magic, your original filesystem is the  
_first_ snapshot you copied over (at least as far as I understand it,  
could be wrong if you go, as stated, with send -i ). This could be  
easily verified by looking at the files available and visible at the  
directory.

Kind regards

Martin


On Aug 10, 2008, at 22:56 PM, ruebezahl wrote:

> ruebbackup is the latest snapshot ?
>
> i thought ruebebackup@<latestnumber>  was the latest one...
>
>
>
> i think i might actually get something fundamental wrong here,
>
> i'll play around a bit more and see if i can get some illumination to
> my mind  :)
>
>
> ruebezahl
>
>
>
>
> On 10.08.2008, at 22:48, Germano Caronni wrote:
>
>> no no, ruebebackup is not the original filesystem. it is the latest
>> snapshot plus whatever changes you made since then. or do you
>> actually see a different behaviour on your system?
>>
>> Germano
>>
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 2272 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080811/26efe376/attachment.bin 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080811/26efe376/attachment-0001.bin 

From franzschmalzl at spamfreemail.de  Mon Aug 11 02:42:30 2008
From: franzschmalzl at spamfreemail.de (ruebezahl)
Date: Mon, 11 Aug 2008 11:42:30 +0200
Subject: [zfs-discuss] snapshot
In-Reply-To: <D59BDE5B-1F07-4BBE-85DC-6ACCEAC4A424@martin-hauser.net>
References: <017EA3B6-BB3D-477C-820B-ADFA586F5EA5@spamfreemail.de>
	<CBFE2D23-436C-4F4F-80E9-1249202BBA2E@spamfreemail.de>
	<327b821f0808101334x8355a5dk4cb036fdc9fafb61@mail.gmail.com>
	<C6DB3D97-4FD8-4B86-A4E2-334C0129EB99@spamfreemail.de>
	<327b821f0808101348w6dc6e3b3mf24dda4ce6758721@mail.gmail.com>
	<B6155961-E1E8-4982-914B-3C402B394637@spamfreemail.de>
	<D59BDE5B-1F07-4BBE-85DC-6ACCEAC4A424@martin-hauser.net>
Message-ID: <C56A6950-6329-44D5-921B-76A7A751DB56@spamfreemail.de>

Hey Guys!


Forget about my snapshot issues, solved them.
I think i didn't quite get the concept of snapshot based backups.


Thanks for all your help.
I appreciate it.


ruebezahl




From hdfdisk at gmail.com  Mon Aug 11 09:02:29 2008
From: hdfdisk at gmail.com (Freeleader Phoenix)
Date: Tue, 12 Aug 2008 00:02:29 +0800
Subject: [zfs-discuss] Panic when using zpool status
Message-ID: <3A601651-651E-461F-96A2-2F2AF008748D@gmail.com>

Hey all
      I just wrote a huge single file (5gb) To my zpool, and when I'm  
reading that file I found my HDD is always lighting,so I reboot and  
boot into single user mode, using zpool import to import my pool. Then  
I got:
      zpool status:

     status: One or more device is corrupted.......(I forgot the  
following)

   But All CKSUM is 0.

  Then I got kernel Panic

  I rollback the snapshot which I made 1 weeks ago, and run zpool  
status again, the result is the same

Anyone has any idea about it?

Sincerely
Bjartskular

From johnm at tekserve.com  Tue Aug 12 13:12:52 2008
From: johnm at tekserve.com (John McAdams)
Date: Tue, 12 Aug 2008 16:12:52 -0400
Subject: [zfs-discuss] zfs filesystems
Message-ID: <20080812201252.52120225@imap.tekserve.com>

I'm trying to grasp the implementation of multiple filesystems in OS X.

For example <http://flux.org.uk/howto/solaris/zfs_tutorial_02> creates a bunch of home folders for users:

# zfs create salmon/kent
# zfs create salmon/dennisr
# zfs create salmon/billj

If I attempt something similar in OS X nothing happens. But what would the result be?

When the pool is created

zpool create tank mirror disk2s2 disk3s2 

I end up with a /Volumes/mirror and mirror on the Desktop. If I run zfs create mirror/user1 wouldn't that create, or try to create, a volume at /Volumes/mirror/user1?

It would be great to create a filesystem for every user and then put quotas on them. Is that possible with the current (119) version of ZFS on Mac OS?

Thanks,
-- jmca

From caronni at gmail.com  Tue Aug  5 13:55:09 2008
From: caronni at gmail.com (Germano Caronni)
Date: Tue, 5 Aug 2008 22:55:09 +0200
Subject: [zfs-discuss] another failure mode for zfs
In-Reply-To: <327b821f0808051247hcd371cfo542e026056d54d47@mail.gmail.com>
References: <327b821f0808051247hcd371cfo542e026056d54d47@mail.gmail.com>
Message-ID: <327b821f0808051355ta0df0c9y22c7505b1b4a1599@mail.gmail.com>

This may or may not help. After some more operations with my laptop I just
tried to unmount the pool (which worked fine several times so far)

So I typed 'diskutil unmount /Volumes/z'. The screen became grey, with a
nice message in the middle, telling me to power down my machine. After
restart, the following information was provided:

Tue Aug  5 22:22:34 2008
panic(cpu 1 caller 0x34A72BD7): "[ZFS]: assertion failed in
/Users/.../src/zfs-119/zfs_kext/zfs/dnode_sync.c line 397: pass <
100"@/Users/.../src/zfs-119/zfs_kext/zfs/dnode_sync.c:397
Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x35ab7b28 : 0x12b0fa (0x4592a4 0x35ab7b5c 0x133243 0x0)
0x35ab7b78 : 0x34a72bd7 (0x34ada700 0x34ada0b4 0x18d 0x34ada6f4)
0x35ab7c98 : 0x34a6b65d (0x67ba908 0x0 0x34addd74 0x0)
0x35ab7cd8 : 0x34a48f9b (0x472b950 0x0 0x0 0x2eccf96b)
0x35ab7d28 : 0x1f178b (0x4da3930 0x0 0x61ffb44 0x1d3f24)
0x35ab7d58 : 0x1dffa4 (0x4da3930 0x0 0x61ffb44 0x1f60b3)
0x35ab7da8 : 0x1e0278 (0x4da3930 0x0 0x1 0x61ffb44)
0x35ab7dd8 : 0x1e041d (0x4da3930 0x0 0x61ffb44 0x0)
0x35ab7f78 : 0x3ddde2 (0x448d250 0x61ffa40 0x61ffa84 0x0)
0x35ab7fc8 : 0x19f2c3 (0x5c699e0 0x0 0x1a20b5 0x5c699e0)
No mapping exists for frame pointer
Backtrace terminated-invalid frame pointer 0xbffffe98
      Kernel loadable modules in backtrace (with dependencies):
         com.apple.filesystems.zfs(8.0)@0x34a47000->0x34b06fff

BSD process name corresponding to current thread: umount

Mac OS version:
9E17

Kernel version:
Darwin Kernel Version 9.4.0: Mon Jun  9 19:30:53 PDT 2008;
root:xnu-1228.5.20~1/RELEASE_I386
System model name: MacBookPro4,1 (Mac-F42C89C8)

As I've recompiled the kext, I've attached the build with dsyms to this
mail. It's as of yet unmodified, just a local compile. Maybe it helps...

Germano

2008/8/5 Germano Caronni <caronni at gmail.com>

> Hi Noel, all,
>
> here another way to make a mac misbehave (besides the mknod thing in the
> other thread):
> I have a pool that if imported at the time of a restart or shutdown will
> hang the machine. You get the spinning sunburst symbol forever.
> A copy of the pool is found at http://olymp.dreamhosters.com/pool1.bz2
>
> An alternate way to create this pool:
> mkfile 100M /pool1
> sudo zpool
> zpool create z /pool1
> zfs set compression=on z
> zfs mount z
> copy in the attached c file
> export CFLAGS -g -Wall
> make maketree
> mkdir a
> cd a
> ../maketree 276000
> (actually i did a maketree 1000000 but had to ^C it because the pool was
> getting full)
> The 'foo' file should not matter.
>
> Now, irrespective of 'z' being mounted or not, as long as it is imported,
> the machine won't shut down.
> Once you manually export it before the shutdown, everything goes as
> expected.
>
> please tell me if you can repro, either with pool1 or with a freshly
> created pool.
>
> Germano
>
> p.s. the source is obviously write-only code. use at your own risk, yadda
> yadda.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080805/c1c9d4ad/attachment-0001.html 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Leopard_Release.tar.bz2
Type: application/x-bzip2
Size: 2428112 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080805/c1c9d4ad/attachment-0001.bin 

From caronni at gmail.com  Tue Aug 12 16:44:36 2008
From: caronni at gmail.com (Germano Caronni)
Date: Wed, 13 Aug 2008 01:44:36 +0200
Subject: [zfs-discuss] zfs filesystems
In-Reply-To: <20080812201252.52120225@imap.tekserve.com>
References: <20080812201252.52120225@imap.tekserve.com>
Message-ID: <327b821f0808121644x57868f8fy40dffcec735b8d35@mail.gmail.com>

the filesystem user1 will appear as a directory inside the mounted device
'mirror'.
you can see its path with 'zfs list', and it should be mounted automatically
when you create it, otherwise try 'zfs mount mirror/user1'
i believe quotas work on zfs-119. 'zfs set quota=10g mirror/user1'. You can
try it out, and see if it does what you expect ;-)
Germano


On Tue, Aug 12, 2008 at 22:12, John McAdams <johnm at tekserve.com> wrote:

> I'm trying to grasp the implementation of multiple filesystems in OS X.
>
> For example <http://flux.org.uk/howto/solaris/zfs_tutorial_02> creates a
> bunch of home folders for users:
>
> # zfs create salmon/kent
> # zfs create salmon/dennisr
> # zfs create salmon/billj
>
> If I attempt something similar in OS X nothing happens. But what would the
> result be?
>
> When the pool is created
>
> zpool create tank mirror disk2s2 disk3s2
>
> I end up with a /Volumes/mirror and mirror on the Desktop. If I run zfs
> create mirror/user1 wouldn't that create, or try to create, a volume at
> /Volumes/mirror/user1?
>
> It would be great to create a filesystem for every user and then put quotas
> on them. Is that possible with the current (119) version of ZFS on Mac OS?
>
> Thanks,
> -- jmca
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080813/9a78a54e/attachment.html 

From ndellofano at apple.com  Tue Aug 12 17:07:19 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Tue, 12 Aug 2008 17:07:19 -0700
Subject: [zfs-discuss] zfs filesystems
In-Reply-To: <327b821f0808121644x57868f8fy40dffcec735b8d35@mail.gmail.com>
References: <20080812201252.52120225@imap.tekserve.com>
	<327b821f0808121644x57868f8fy40dffcec735b8d35@mail.gmail.com>
Message-ID: <3F3F2042-002F-499B-B94F-E9F662082E78@apple.com>

Also note that you need to be root or use sudo to create a zfs  
filesystem, else the filesystem will be created but won't mount.  I  
have a bug open on this and will fix it up with a handy error messsage:

<5014318> check for root permission before trying to create a zfs  
filesystem

Quotas, reservations, and all the filesystem goodness works fine, so  
you should be able to easily set up multiple filesystems and set  
quotas and such on them.

Noel


On Aug 12, 2008, at 4:44 PM, Germano Caronni wrote:

> the filesystem user1 will appear as a directory inside the mounted  
> device 'mirror'.
> you can see its path with 'zfs list', and it should be mounted  
> automatically when you create it, otherwise try 'zfs mount mirror/ 
> user1'
> i believe quotas work on zfs-119. 'zfs set quota=10g mirror/user1'.  
> You can try it out, and see if it does what you expect ;-)
> Germano
>
>
> On Tue, Aug 12, 2008 at 22:12, John McAdams <johnm at tekserve.com>  
> wrote:
> I'm trying to grasp the implementation of multiple filesystems in OS  
> X.
>
> For example <http://flux.org.uk/howto/solaris/zfs_tutorial_02>  
> creates a bunch of home folders for users:
>
> # zfs create salmon/kent
> # zfs create salmon/dennisr
> # zfs create salmon/billj
>
> If I attempt something similar in OS X nothing happens. But what  
> would the result be?
>
> When the pool is created
>
> zpool create tank mirror disk2s2 disk3s2
>
> I end up with a /Volumes/mirror and mirror on the Desktop. If I run  
> zfs create mirror/user1 wouldn't that create, or try to create, a  
> volume at /Volumes/mirror/user1?
>
> It would be great to create a filesystem for every user and then put  
> quotas on them. Is that possible with the current (119) version of  
> ZFS on Mac OS?
>
> Thanks,
> -- jmca
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080812/23654797/attachment.html 

From macrbg at mac.com  Wed Aug 13 18:33:43 2008
From: macrbg at mac.com (Robert Gordon)
Date: Wed, 13 Aug 2008 20:33:43 -0500
Subject: [zfs-discuss] another failure mode for zfs
In-Reply-To: <327b821f0808090210y29ffadbcg422aa7bd2586dcad@mail.gmail.com>
References: <327b821f0808051247hcd371cfo542e026056d54d47@mail.gmail.com>
	<327b821f0808090210y29ffadbcg422aa7bd2586dcad@mail.gmail.com>
Message-ID: <31EF3B70-B04F-4140-AC00-F623A39932B6@mac.com>


On Aug 9, 2008, at 4:10 AM, Germano Caronni wrote:

> Did anybody have a chance to reproduce this issue? Or is this  
> another one of the 'it only happens to me' instances?
>
> Germano

I was able to reproduce the issue.

To unmount i used the following command:

sudo zfs umount -f drip/test


Robert.

From lopez.on.the.lists at yellowspace.net  Fri Aug 15 13:10:12 2008
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Fri, 15 Aug 2008 22:10:12 +0200
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
Message-ID: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>

Hi list,

I was just wondering wether implementing an OSX-specific option for
case-insensitivity on certain filesystems would be a mess to do?

Just a thought... all those Adobe apps don't like case-insensitive
HFS, and maybe some others too, so zfs is a no-go for them right now.

Alright, after all, Adobe could start linking libraries
while preserving case..

;)

Regards && kudos,


Lorenzo



From macrbg at mac.com  Fri Aug 15 14:51:25 2008
From: macrbg at mac.com (Robert Gordon)
Date: Fri, 15 Aug 2008 16:51:25 -0500
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
Message-ID: <D2FBA292-84D8-4CD9-9642-F1D434977A1D@mac.com>


On Aug 15, 2008, at 3:10 PM, Lorenzo Perone wrote:

> Hi list,
>
> I was just wondering wether implementing an OSX-specific option for
> case-insensitivity on certain filesystems would be a mess to do?
>
> Just a thought... all those Adobe apps don't like case-insensitive
> HFS, and maybe some others too, so zfs is a no-go for them right now.
>
> Alright, after all, Adobe could start linking libraries
> while preserving case..

Presumably the Apple Z-team will refresh the ZFS bits and pick-up
http://br.opensolaris.org/os/community/arc/caselog/2007/244

This is available in build 77 of open solaris; where IIRC the current
published level is 72.

Robert.

From lists at loveturtle.net  Fri Aug 15 15:06:09 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Fri, 15 Aug 2008 18:06:09 -0400
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <D2FBA292-84D8-4CD9-9642-F1D434977A1D@mac.com>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
	<D2FBA292-84D8-4CD9-9642-F1D434977A1D@mac.com>
Message-ID: <48A5FDD1.4020008@loveturtle.net>

Yeah, This has already existed for a while now

Our sxce installs at work have this

Robert Gordon wrote:
> On Aug 15, 2008, at 3:10 PM, Lorenzo Perone wrote:
>
>    
>> Hi list,
>>
>> I was just wondering wether implementing an OSX-specific option for
>> case-insensitivity on certain filesystems would be a mess to do?
>>
>> Just a thought... all those Adobe apps don't like case-insensitive
>> HFS, and maybe some others too, so zfs is a no-go for them right now.
>>
>> Alright, after all, Adobe could start linking libraries
>> while preserving case..
>>      
>
> Presumably the Apple Z-team will refresh the ZFS bits and pick-up
> http://br.opensolaris.org/os/community/arc/caselog/2007/244
>
> This is available in build 77 of open solaris; where IIRC the current
> published level is 72.
>
> Robert.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>    


From nathan.stocks at gmail.com  Fri Aug 15 15:38:15 2008
From: nathan.stocks at gmail.com (Nathan)
Date: Fri, 15 Aug 2008 16:38:15 -0600
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
Message-ID: <96c9d6a80808151538m77ad6e27s3697c230676a7e86@mail.gmail.com>

On Fri, Aug 15, 2008 at 2:10 PM, Lorenzo Perone
<lopez.on.the.lists at yellowspace.net> wrote:
> Hi list,
>
> I was just wondering wether implementing an OSX-specific option for
> case-insensitivity on certain filesystems would be a mess to do?
>
> Just a thought... all those Adobe apps don't like case-insensitive
> HFS, and maybe some others too, so zfs is a no-go for them right now.
>
> Alright, after all, Adobe could start linking libraries
> while preserving case..

(Resending with Reply-to-all)

<opinion>

I would much rather Apple just used the regular case-sensitivity in
ZFS, AND remove case-insensitivity from HFS+.  And make ZFS the
default filesystem in Snow Leopard.

Then everybody would just fix their case-insensitivity issues for the
new OS release and Windows would be the last remaining modern
operating system that doesn't have real case sensitivity.

</opinion>

~ Nathan

From ndellofano at apple.com  Fri Aug 15 16:53:24 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 15 Aug 2008 16:53:24 -0700
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <D2FBA292-84D8-4CD9-9642-F1D434977A1D@mac.com>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
	<D2FBA292-84D8-4CD9-9642-F1D434977A1D@mac.com>
Message-ID: <F2067D82-5A2B-4610-8369-9D02633F0CE1@apple.com>

Robert is correct.  We've synced ZFS on Mac to Solaris build 94, which  
picks up the case insensitivity stuff and a bunch of other goodness.   
Currently we're testing and debugging the synced code.  I'll release  
it on macosforge as soon as I deem it "stable".  (oh yeah, love those  
quote marks..)

Noel :)

On Aug 15, 2008, at 2:51 PM, Robert Gordon wrote:

>
> On Aug 15, 2008, at 3:10 PM, Lorenzo Perone wrote:
>
>> Hi list,
>>
>> I was just wondering wether implementing an OSX-specific option for
>> case-insensitivity on certain filesystems would be a mess to do?
>>
>> Just a thought... all those Adobe apps don't like case-insensitive
>> HFS, and maybe some others too, so zfs is a no-go for them right now.
>>
>> Alright, after all, Adobe could start linking libraries
>> while preserving case..
>
> Presumably the Apple Z-team will refresh the ZFS bits and pick-up
> http://br.opensolaris.org/os/community/arc/caselog/2007/244
>
> This is available in build 77 of open solaris; where IIRC the current
> published level is 72.
>
> Robert.
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From ndellofano at apple.com  Fri Aug 15 16:59:40 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Fri, 15 Aug 2008 16:59:40 -0700
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <96c9d6a80808151538m77ad6e27s3697c230676a7e86@mail.gmail.com>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
	<96c9d6a80808151538m77ad6e27s3697c230676a7e86@mail.gmail.com>
Message-ID: <40DC6309-8D70-42C7-A689-1AF7CB337EF9@apple.com>

ZFS will be case sensitive by default (i.e. it's current normal  
behavior), it will just have an option for case insensitivity.

And yes (chime in), it would be great if people would get with the  
times and fix their third party apps to not rely on case  
insensitivity.  However in the real world scenario, i feel that in all  
probability this will take a while and most likely a lot of pain, many  
beatings, and some bribes until it happens.... (see well known Adobe  
example below...)

Noel

On Aug 15, 2008, at 3:38 PM, Nathan wrote:

> On Fri, Aug 15, 2008 at 2:10 PM, Lorenzo Perone
> <lopez.on.the.lists at yellowspace.net> wrote:
>> Hi list,
>>
>> I was just wondering wether implementing an OSX-specific option for
>> case-insensitivity on certain filesystems would be a mess to do?
>>
>> Just a thought... all those Adobe apps don't like case-insensitive
>> HFS, and maybe some others too, so zfs is a no-go for them right now.
>>
>> Alright, after all, Adobe could start linking libraries
>> while preserving case..
>
> (Resending with Reply-to-all)
>
> <opinion>
>
> I would much rather Apple just used the regular case-sensitivity in
> ZFS, AND remove case-insensitivity from HFS+.  And make ZFS the
> default filesystem in Snow Leopard.
>
> Then everybody would just fix their case-insensitivity issues for the
> new OS release and Windows would be the last remaining modern
> operating system that doesn't have real case sensitivity.
>
> </opinion>
>
> ~ Nathan
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From nathan.stocks at gmail.com  Fri Aug 15 17:37:22 2008
From: nathan.stocks at gmail.com (Nathan)
Date: Fri, 15 Aug 2008 18:37:22 -0600
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <40DC6309-8D70-42C7-A689-1AF7CB337EF9@apple.com>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
	<96c9d6a80808151538m77ad6e27s3697c230676a7e86@mail.gmail.com>
	<40DC6309-8D70-42C7-A689-1AF7CB337EF9@apple.com>
Message-ID: <96c9d6a80808151737y7cbf25dtc40c634e80887660@mail.gmail.com>

On Fri, Aug 15, 2008 at 5:59 PM, No?l Dellofano <ndellofano at apple.com> wrote:
> ZFS will be case sensitive by default (i.e. it's current normal behavior),
> it will just have an option for case insensitivity.
>
> And yes (chime in), it would be great if people would get with the times and
> fix their third party apps to not rely on case insensitivity.  However in
> the real world scenario, i feel that in all probability this will take a
> while and most likely a lot of pain, many beatings, and some bribes until it
> happens.... (see well known Adobe example below...)

Ah, but just imagine this scenario!

1) Snow Leopard includes deprecation warnings for case-insensitive filesystems.

2) WWDC '09: Stevenote "We've taken filesystems to the next level.
[iFS], based on ZFS.  Folks, it's gonna be _huge_.  Available in Snow
Leopard as on option.  Default in [Snow Leopard + 1].  To fix those
pesky case-sensitivity issues we've introduced a new tool called
[XTherapist] which fully integrates into Xcode.  Launch it.  _Boom_,
case sensitivity problems in your application projects are solved.
Folks, it's _amazing_.  Now I'd like to have [John Smith] from Adobe
take the stage.  We called Adobe two weeks ago and flew out a few of
their developers to see what they could do to fix their case
sensitivity problems in two weeks..."

3) Happiness and bliss.

Doesn't it just make you smile?

~ Nathan

From swpalmer at gmail.com  Fri Aug 15 18:18:23 2008
From: swpalmer at gmail.com (Scott Palmer)
Date: Fri, 15 Aug 2008 21:18:23 -0400
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <96c9d6a80808151538m77ad6e27s3697c230676a7e86@mail.gmail.com>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
	<96c9d6a80808151538m77ad6e27s3697c230676a7e86@mail.gmail.com>
Message-ID: <5268B236-F9B6-406D-B920-8EF8FE87BCF9@gmail.com>


On 15-Aug-08, at 6:38 PM, Nathan wrote:

> On Fri, Aug 15, 2008 at 2:10 PM, Lorenzo Perone
> <lopez.on.the.lists at yellowspace.net> wrote:
>> Hi list,
>>
>> I was just wondering wether implementing an OSX-specific option for
>> case-insensitivity on certain filesystems would be a mess to do?
>>
>> Just a thought... all those Adobe apps don't like case-insensitive
>> HFS, and maybe some others too, so zfs is a no-go for them right now.
>>
>> Alright, after all, Adobe could start linking libraries
>> while preserving case..
>
> (Resending with Reply-to-all)
>
> <opinion>
>
> I would much rather Apple just used the regular case-sensitivity in
> ZFS, AND remove case-insensitivity from HFS+.  And make ZFS the
> default filesystem in Snow Leopard.
>
> Then everybody would just fix their case-insensitivity issues for the
> new OS release and Windows would be the last remaining modern
> operating system that doesn't have real case sensitivity.
>
> </opinion>
>
> ~ Nathan

<Another_opinion>

Case-insensitive filesystems SUCK. They are VERY user unfriendly.  If  
I name a file "green"  I shouldn't have to explain the case of the  
letters I used. The file's name is "green" or "Green" or "GREEN"  
because all of that spells green.

When you tell people you're name is Nathan do you tell them to spell  
it with a capital N or you won't know that they are talking referring  
to you?  It's not a perfect example because we know the rules for  
proper names are to use a capital letter, but it still makes the  
point.  Distinguishing files by the case of the letters is a very  
computer-ish, almost primitive thing to do.  The only reason case  
sensitive filesystems exist is because people didn't put the effort or  
CPU cycles towards making the machine respond intelligently.  We are  
beyond that now.  Filenames are not to be treated the same as variable  
names in C code ... they are intended for the end user, not tech-heads.

Scott

</Another_opinion>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 1937 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080815/b166e795/attachment-0001.bin 

From cdl at asgaard.org  Fri Aug 15 18:58:59 2008
From: cdl at asgaard.org (Christopher LILJENSTOLPE)
Date: Fri, 15 Aug 2008 18:58:59 -0700
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <96c9d6a80808151538m77ad6e27s3697c230676a7e86@mail.gmail.com>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
	<96c9d6a80808151538m77ad6e27s3697c230676a7e86@mail.gmail.com>
Message-ID: <CD1F6169-5668-4E4B-BFBA-563FAE0791E2@asgaard.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Here Here!

	Chris

On 15 Aug 2008, at 15.38, Nathan wrote:

> On Fri, Aug 15, 2008 at 2:10 PM, Lorenzo Perone
> <lopez.on.the.lists at yellowspace.net> wrote:
>> Hi list,
>>
>> I was just wondering wether implementing an OSX-specific option for
>> case-insensitivity on certain filesystems would be a mess to do?
>>
>> Just a thought... all those Adobe apps don't like case-insensitive
>> HFS, and maybe some others too, so zfs is a no-go for them right now.
>>
>> Alright, after all, Adobe could start linking libraries
>> while preserving case..
>
> (Resending with Reply-to-all)
>
> <opinion>
>
> I would much rather Apple just used the regular case-sensitivity in
> ZFS, AND remove case-insensitivity from HFS+.  And make ZFS the
> default filesystem in Snow Leopard.
>
> Then everybody would just fix their case-insensitivity issues for the
> new OS release and Windows would be the last remaining modern
> operating system that doesn't have real case sensitivity.
>
> </opinion>
>
> ~ Nathan
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

- ---
???
Check my PGP key here:
http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B




-----BEGIN PGP SIGNATURE-----

iQEcBAEBAgAGBQJIpjRjAAoJEGmx2Mt/+Iw/bw4IAIyFAMPOvxWaPE2QArOlA/NM
3I1sC/i7PEkFJuBKP9o7epDS6EC9P3QjltIfEEvOf5VVxHjEko/U+LIu9D690wzv
zvmLITRDRtaxjRdF+2ZYHNO44R/WLYBP1QP2SBGAL2f6JE9Q2wKjuTk+7W3v2F7Z
QOtuaxtMNz+a4ZyPxIU9QSANPvvm3f68PMjy21tO2phXktdVHoCxTGw9tF7F2UY9
xmWocroIskrZuKoEmeCa4EUFHzEgIYUuLfxNusf7LQ+69dLMc+xAPsxRMHNTUSd6
QYa9F4PR14nOTFlR42nQzQCu6LEHJHVcaX3J/9LJSgw5AHNdHaDydzwEgCQSw8w=
=7Olc
-----END PGP SIGNATURE-----

From jkh at apple.com  Fri Aug 15 19:30:30 2008
From: jkh at apple.com (Jordan K. Hubbard)
Date: Fri, 15 Aug 2008 19:30:30 -0700
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <5268B236-F9B6-406D-B920-8EF8FE87BCF9@gmail.com>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
	<96c9d6a80808151538m77ad6e27s3697c230676a7e86@mail.gmail.com>
	<5268B236-F9B6-406D-B920-8EF8FE87BCF9@gmail.com>
Message-ID: <ABFE6936-F41B-4CBF-8814-B398998E7519@apple.com>


On Aug 15, 2008, at 6:18 PM, Scott Palmer wrote:

> Case-insensitive filesystems SUCK. They are VERY user unfriendly.   
> If I name a file "green"  I shouldn't have to explain the case of  
> the letters I used. The file's name is "green" or "Green" or "GREEN"  
> because all of that spells green.

Umm.  Wait, what?  Those are all the attributes of a case-insensitive  
filesystem, which you say SUCK.  I think you are contradicting  
yourself here.

In any case, neither this or any blue-sky scenarios where developers  
just magically fix all their stuff (or, for that matter, HFS is  
ditched and ZFS becomes the default filesystem for Mom) are  
particularly great discussion material for this list since it's one  
who's ostensible focus is on the technical details of making ZFS  
better for its intended purpose, whatever that intention may be at any  
given time, and right now the stated intention is to focus on server  
scenarios.

The whole marketing and "MacOSX futures" discussions would probably be  
better targeted at those mailing lists devoted to such topics and also  
populated by the folks who make those sorts of decisions.

I'm sorry - I'm getting something in my earpiece...  Oh, those mailing  
lists don't exist?  Huh.  What a shame. ;-)

- Jordan





From swpalmer at gmail.com  Fri Aug 15 19:33:45 2008
From: swpalmer at gmail.com (Scott Palmer)
Date: Fri, 15 Aug 2008 22:33:45 -0400
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <610AFC7B-5738-4FF2-B667-CAEFEB186BF7@asgaard.org>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
	<96c9d6a80808151538m77ad6e27s3697c230676a7e86@mail.gmail.com>
	<5268B236-F9B6-406D-B920-8EF8FE87BCF9@gmail.com>
	<610AFC7B-5738-4FF2-B667-CAEFEB186BF7@asgaard.org>
Message-ID: <C1396904-EB5E-4A0D-A45E-97854A54ABCC@gmail.com>

Oops yes.. bad typo on my part.

I will  expand on my rant in an effort to see if I can state my case  
in a way that will make someone "tend to agree with me" :-)

Case SENSITIVE filesystems are brain-dead poo.
Next we will force other computer-isms on the end users for example:  
you can't start a file name with a number, only letters, underscores  
or a dollar sign,  no punctuation in them, etc. -- oh and just forget  
about putting SPACES in them - that's just nuts!  It just doesn't make  
any sense.  If you used a physical filing cabinet you wouldn't expect  
to label the file folders with case sensitivity.

I think case-sensitive filesystems are ridiculous - and perhaps more  
to the point, very un-mac-like.  These things are supposed to be easy  
to use, allowing different files with a name that differs only by case  
is unnatural for people.  It is natural for the machine of course  
which leads to the only people that prefer it are people that are  
thinking like programmers.

The Mac platform is already ahead of others by allowing things like  
slashes in the filenames, it would be silly to take a step BACKWARDS  
and force the nuisance of case-sensitivity on the users.  Just as all  
displayable characters should be allowed in filenames - including the  
path separators (which programmers should deal with using escape  
sequences - just like when using a command line you have to escape  
spaces or put quotes your filename)

In other words everybody should fix their case-sensitivity issues  
(e.g. geeks that use capital .C for C++ files and lowercase .c for C  
files, programmers that sort names/files as if 'a' comes after 'Z',  
etc.).  That's what the regular folk want.   Case-sensitivity in this  
context only makes sense to the sort of people that would be  
subscribed to this list.. not your grandma.

(I should of course make sure to clarify that I want a case- 
preserving, case-insensitve filesystem. not some through-back to MS-DOS)

Sorry Jordan - I had all the above written before your post came  
through... if anyone cares they can reply off-list.  For now I'm just  
glad that ZFS will have the option and I hope Apple does the right  
thing and makes case-insensitivity the default when they ship RW ZFS.

Regards,

Scott

On 15-Aug-08, at 10:02 PM, Christopher LILJENSTOLPE wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> I think, then, you mean that case-SENSITIVE file systems suck, in  
> your opinion.  You want the filesystem to not care what the case is  
> in the naming, correct?  That is case-insensitivity, which is what  
> you have today with HFS+, FAT, FAT32, and NTFS.....
>
> I don't tend to agree with you, however.....
>
> 	Chris
>
> On 15 Aug 2008, at 18.18, Scott Palmer wrote:
>
>>
>> On 15-Aug-08, at 6:38 PM, Nathan wrote:
>>
>>> On Fri, Aug 15, 2008 at 2:10 PM, Lorenzo Perone
>>> <lopez.on.the.lists at yellowspace.net> wrote:
>>>> Hi list,
>>>>
>>>> I was just wondering wether implementing an OSX-specific option for
>>>> case-insensitivity on certain filesystems would be a mess to do?
>>>>
>>>> Just a thought... all those Adobe apps don't like case-insensitive
>>>> HFS, and maybe some others too, so zfs is a no-go for them right  
>>>> now.
>>>>
>>>> Alright, after all, Adobe could start linking libraries
>>>> while preserving case..
>>>
>>> (Resending with Reply-to-all)
>>>
>>> <opinion>
>>>
>>> I would much rather Apple just used the regular case-sensitivity in
>>> ZFS, AND remove case-insensitivity from HFS+.  And make ZFS the
>>> default filesystem in Snow Leopard.
>>>
>>> Then everybody would just fix their case-insensitivity issues for  
>>> the
>>> new OS release and Windows would be the last remaining modern
>>> operating system that doesn't have real case sensitivity.
>>>
>>> </opinion>
>>>
>>> ~ Nathan
>>
>> <Another_opinion>
>>
>> Case-insensitive filesystems SUCK. They are VERY user unfriendly.   
>> If I name a file "green"  I shouldn't have to explain the case of  
>> the letters I used. The file's name is "green" or "Green" or  
>> "GREEN" because all of that spells green.
>>
>> When you tell people you're name is Nathan do you tell them to  
>> spell it with a capital N or you won't know that they are talking  
>> referring to you?  It's not a perfect example because we know the  
>> rules for proper names are to use a capital letter, but it still  
>> makes the point.  Distinguishing files by the case of the letters  
>> is a very computer-ish, almost primitive thing to do.  The only  
>> reason case sensitive filesystems exist is because people didn't  
>> put the effort or CPU cycles towards making the machine respond  
>> intelligently.  We are beyond that now.  Filenames are not to be  
>> treated the same as variable names in C code ... they are intended  
>> for the end user, not tech-heads.
>>
>> Scott
>>
>> </Another_opinion>_______________________________________________
>> zfs-discuss mailing list
>> zfs-discuss at lists.macosforge.org
>> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>
> - ---
> ???
> Check my PGP key here:
> http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B
>
>
>
>
> -----BEGIN PGP SIGNATURE-----
>
> iQEcBAEBAgAGBQJIpjUfAAoJEGmx2Mt/+Iw/cZsH/2gd4LysTuto0KtfurObDX5C
> n80RTurjAjDC2EByaH5KaVYcLyo+ug04/fsbUMGia3itOXpd6fHs1WSWdYiX/lvc
> NGFhGWT9JmgKMDlK7jeZ2FJH/sOrYBTZm5GpCj79gEqyqvfgSY0dhq5+an3a8jhD
> soo8Q8viB8QXwXpE+HQ+j10r4OJtD1eemiJ1c5xOCmNO/r3pwhi92U/tAVZqKrCA
> 9xGwmghEhvhYKJ9x3jVeF3yJbVgTu3nJpWMUprWEl3Crp8QGqOjdUmM4PpbZFz0Z
> YodX5z7wAO/sGIJpJrrb2wNeXs01G2S9Bf+TZo0NljjOyShBE4UDDWkFqP1TQr0=
> =hwBo
> -----END PGP SIGNATURE-----

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 1937 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080815/4762be8e/attachment.bin 

From nathan.stocks at gmail.com  Fri Aug 15 19:40:22 2008
From: nathan.stocks at gmail.com (Nathan)
Date: Fri, 15 Aug 2008 20:40:22 -0600
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <DC1982A8-6DCC-489F-BD85-DEC07FBDC1CC@electricteaparty.net>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
	<96c9d6a80808151538m77ad6e27s3697c230676a7e86@mail.gmail.com>
	<40DC6309-8D70-42C7-A689-1AF7CB337EF9@apple.com>
	<96c9d6a80808151737y7cbf25dtc40c634e80887660@mail.gmail.com>
	<DC1982A8-6DCC-489F-BD85-DEC07FBDC1CC@electricteaparty.net>
Message-ID: <96c9d6a80808151940v7f94b462n36017b53b335f9c2@mail.gmail.com>

On Fri, Aug 15, 2008 at 6:47 PM, Adrian Thornton
<canadrian at electricteaparty.net> wrote:
> Nathan/No?l,
>
> A brief break from lurking, and I was wondering what the issue is with
> case-insensitivity? I mean, from a non-developer's point of view (mine), I'm
> not sure I would even want case-sensitivity. To the average end-user,
> FOLDER, folder, Folder, foldeR, folDer, ad nauseum, should all be the same
> folder. As a case example, imagine your grandmother is filing e-cards she
> has received at Christmas, and she created a folder in her Documents folder
> called "Christmas Cards." The next year she creates one called "Christmas
> cards" without looking first. When she looks back in one of those folders,
> she's suddenly in a panic that she can't find one year's e-cards, freaking
> out that she's "broken her computer", and next thing you know I have to go
> to Grandma's place to explain about case-sensitivity for like the eighteenth
> time. Doesn't this just add unnecessary complication?
>
> Just hoping to have it explained, really. From my brief forays into the *nix
> world, I've noticed that the average Joe wants computers to be more
> human-like, developers want them to be more technical, and neither party is
> particularly fond of the other. Isn't there some middle ground?

(Okay, posting a third time, this time manually adding zfs-discuss at ...
to the addresses)

Good question.  I'll refer you to a good and complete answer:

http://drewthaler.blogspot.com/2007/12/case-against-insensitivity.html

Note that your specific "grandmother" example (and Mr. Palmer's stated
example) is almost exactly replicated (and explained) under the 4th
subheading titled "Case-insensitivity is a layering violation."

My intention was not to start a flamewar... and I encourage Mr. Palmer
to read the linked post above...

~ Nathan

From canadrian at electricteaparty.net  Fri Aug 15 20:42:30 2008
From: canadrian at electricteaparty.net (Adrian Thornton)
Date: Fri, 15 Aug 2008 21:42:30 -0600
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <96c9d6a80808151940v7f94b462n36017b53b335f9c2@mail.gmail.com>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
	<96c9d6a80808151538m77ad6e27s3697c230676a7e86@mail.gmail.com>
	<40DC6309-8D70-42C7-A689-1AF7CB337EF9@apple.com>
	<96c9d6a80808151737y7cbf25dtc40c634e80887660@mail.gmail.com>
	<DC1982A8-6DCC-489F-BD85-DEC07FBDC1CC@electricteaparty.net>
	<96c9d6a80808151940v7f94b462n36017b53b335f9c2@mail.gmail.com>
Message-ID: <92FA8E1F-55BA-4D3B-AECE-0C6945F3816F@electricteaparty.net>

Sorry, I originally meant to post to the list, not directly to Nathan.  
But thanks for the cross-post to the list!

 From a developer's point of view, I'm sure  that article is very  
valid. The adaptation of case-INsensitivity to various languages is no  
doubt a monumental problem. The author of the article also makes a  
number of points about layering violations. Please allow me to clarify  
my point of view, however, with a couple quotations:

"Any sufficiently advanced technology is indistinguishable from  
magic." - Arthur C. Clarke
"Any technology distinguishable from magic is insufficiently  
advanced." - Gehm's Corollary

I would say that Gehm's Corollary is almost a mantra when it comes to  
Apple products. Apple makes products which are amazingly advanced, and  
therefore amazingly easy and almost pleasurable for the end-user to  
make use of - that's why many of us are Mac fans in the first place!  
As I mentioned, this sort of thing is no doubt a mammoth undertaking.   
Drew Thaler states in his article that "Case-insensitivity adds  
complexity and provides no actual benefit." Who says it provides no  
actual benefit? The end-user sees a benefit. Ok, so it's a "layering  
violation." Who defines these layers, and why are they so important?  
Did God send them down to us on stone tablets? What about FUSE? *nix  
geeks hate FUSE because it's a blatant "layering violation." And so  
what? Isn't the end-goal of computers to provide a function we desire?  
Surely there are no rules about how this HAS to be done. The end-goal  
is to provide a user experience that will accomplish the job, and  
hopefully not be too painful in doing so. ZFS itself does away with  
such outdated paradigms as the separation of volumes and filesystems.  
Sometimes we need to ignore the "rules" when the spirit behind them is  
lost; rules only exist to lead the way to a better world for the  
populace. When they interfere with that, what purpose do they hold?

To re-hash some things I've already mentioned: I'm not a developer. I  
don't FULLY understand the nuts and bolts of these things. Also, I do  
sympathize with the difficulties of writing a lot of code in a  
language computers understand, in order to shift the UI closer to  
something that HUMANS understand. I also understand from the countless  
days I've spent learning how to use various *nix systems that the  
developer intelligentsia doesn't have a lot of sympathy for us n00bs -  
ask a question, and they just say RTFM. One soon finds that "TFM" is  
written BY codies FOR codies, and is of little use to someone not in  
the know. Now, is the answer REALLY to tell end-users, "no, we're not  
going to spend the time making a system that's easy to use; it's your  
responsibility to learn the technical arcana yourself"? I don't think  
so. I think we should take a page from the Humane Interface folks and  
try to make computers interact with humans on a human level. It really  
is true that "Any technology distinguishable from magic is  
insufficiently advanced."

I'm sorry, developer folks. I know it's a tough pill to swallow. All I  
can say is that I really appreciate the work you do, and I hope you  
don't get too worn down trying to make these machines act the way that  
real humans do.

- Adrian

On 15-Aug-08, at 8:40 PM, Nathan wrote:
> On Fri, Aug 15, 2008 at 6:47 PM, Adrian Thornton
> <canadrian at electricteaparty.net> wrote:
>> Nathan/No?l,
>>
>> A brief break from lurking, and I was wondering what the issue is  
>> with
>> case-insensitivity? I mean, from a non-developer's point of view  
>> (mine), I'm
>> not sure I would even want case-sensitivity. To the average end-user,
>> FOLDER, folder, Folder, foldeR, folDer, ad nauseum, should all be  
>> the same
>> folder. As a case example, imagine your grandmother is filing e- 
>> cards she
>> has received at Christmas, and she created a folder in her  
>> Documents folder
>> called "Christmas Cards." The next year she creates one called  
>> "Christmas
>> cards" without looking first. When she looks back in one of those  
>> folders,
>> she's suddenly in a panic that she can't find one year's e-cards,  
>> freaking
>> out that she's "broken her computer", and next thing you know I  
>> have to go
>> to Grandma's place to explain about case-sensitivity for like the  
>> eighteenth
>> time. Doesn't this just add unnecessary complication?
>>
>> Just hoping to have it explained, really. From my brief forays into  
>> the *nix
>> world, I've noticed that the average Joe wants computers to be more
>> human-like, developers want them to be more technical, and neither  
>> party is
>> particularly fond of the other. Isn't there some middle ground?
>
> (Okay, posting a third time, this time manually adding zfs-discuss at ...
> to the addresses)
>
> Good question.  I'll refer you to a good and complete answer:
>
> http://drewthaler.blogspot.com/2007/12/case-against-insensitivity.html
>
> Note that your specific "grandmother" example (and Mr. Palmer's stated
> example) is almost exactly replicated (and explained) under the 4th
> subheading titled "Case-insensitivity is a layering violation."
>
> My intention was not to start a flamewar... and I encourage Mr. Palmer
> to read the linked post above...
>
> ~ Nathan


From cdl at asgaard.org  Fri Aug 15 21:15:15 2008
From: cdl at asgaard.org (Christopher LILJENSTOLPE)
Date: Fri, 15 Aug 2008 21:15:15 -0700
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <92FA8E1F-55BA-4D3B-AECE-0C6945F3816F@electricteaparty.net>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
	<96c9d6a80808151538m77ad6e27s3697c230676a7e86@mail.gmail.com>
	<40DC6309-8D70-42C7-A689-1AF7CB337EF9@apple.com>
	<96c9d6a80808151737y7cbf25dtc40c634e80887660@mail.gmail.com>
	<DC1982A8-6DCC-489F-BD85-DEC07FBDC1CC@electricteaparty.net>
	<96c9d6a80808151940v7f94b462n36017b53b335f9c2@mail.gmail.com>
	<92FA8E1F-55BA-4D3B-AECE-0C6945F3816F@electricteaparty.net>
Message-ID: <4AC7A977-EEDD-4B0B-84E1-7B4EEC390F5D@asgaard.org>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Adrian,

	You hit the nail on the head - if you think that case insensitivity  
is a UI improvement, then do it in the UI, NOT in the filesystem.  The  
filesystem should maintain as much state as possible, and keep that  
state as relevant as possible - once you throw something away, you  
can't get it back.  It also means that a lot of FS activity which the  
system depends on (but not the user) will be burdened by the  
translations you want done.   If there is a win to what you want, do  
it at the UI layer, and only take that burden for user filesystem  
interactions (which are a small percentage of the overall filesystem  
burden).

	Also, if you make a file system case insensitive, then those of us  
who want case sensitivity are screwed, as you have already turfed the  
data that case sensitivity requires.  If it's a UI function, then the  
data is there, and you can decide if you want it mangled, or straight  
(i.e. a preference).

	Chris

On 15 Aug 2008, at 20.42, Adrian Thornton wrote:

> Sorry, I originally meant to post to the list, not directly to Nathan.
> But thanks for the cross-post to the list!
>
> From a developer's point of view, I'm sure  that article is very
> valid. The adaptation of case-INsensitivity to various languages is no
> doubt a monumental problem. The author of the article also makes a
> number of points about layering violations. Please allow me to clarify
> my point of view, however, with a couple quotations:
>
> "Any sufficiently advanced technology is indistinguishable from
> magic." - Arthur C. Clarke
> "Any technology distinguishable from magic is insufficiently
> advanced." - Gehm's Corollary
>
> I would say that Gehm's Corollary is almost a mantra when it comes to
> Apple products. Apple makes products which are amazingly advanced, and
> therefore amazingly easy and almost pleasurable for the end-user to
> make use of - that's why many of us are Mac fans in the first place!
> As I mentioned, this sort of thing is no doubt a mammoth undertaking.
> Drew Thaler states in his article that "Case-insensitivity adds
> complexity and provides no actual benefit." Who says it provides no
> actual benefit? The end-user sees a benefit. Ok, so it's a "layering
> violation." Who defines these layers, and why are they so important?
> Did God send them down to us on stone tablets? What about FUSE? *nix
> geeks hate FUSE because it's a blatant "layering violation." And so
> what? Isn't the end-goal of computers to provide a function we desire?
> Surely there are no rules about how this HAS to be done. The end-goal
> is to provide a user experience that will accomplish the job, and
> hopefully not be too painful in doing so. ZFS itself does away with
> such outdated paradigms as the separation of volumes and filesystems.
> Sometimes we need to ignore the "rules" when the spirit behind them is
> lost; rules only exist to lead the way to a better world for the
> populace. When they interfere with that, what purpose do they hold?
>
> To re-hash some things I've already mentioned: I'm not a developer. I
> don't FULLY understand the nuts and bolts of these things. Also, I do
> sympathize with the difficulties of writing a lot of code in a
> language computers understand, in order to shift the UI closer to
> something that HUMANS understand. I also understand from the countless
> days I've spent learning how to use various *nix systems that the
> developer intelligentsia doesn't have a lot of sympathy for us n00bs -
> ask a question, and they just say RTFM. One soon finds that "TFM" is
> written BY codies FOR codies, and is of little use to someone not in
> the know. Now, is the answer REALLY to tell end-users, "no, we're not
> going to spend the time making a system that's easy to use; it's your
> responsibility to learn the technical arcana yourself"? I don't think
> so. I think we should take a page from the Humane Interface folks and
> try to make computers interact with humans on a human level. It really
> is true that "Any technology distinguishable from magic is
> insufficiently advanced."
>
> I'm sorry, developer folks. I know it's a tough pill to swallow. All I
> can say is that I really appreciate the work you do, and I hope you
> don't get too worn down trying to make these machines act the way that
> real humans do.
>
> - Adrian
>
> On 15-Aug-08, at 8:40 PM, Nathan wrote:
>> On Fri, Aug 15, 2008 at 6:47 PM, Adrian Thornton
>> <canadrian at electricteaparty.net> wrote:
>>> Nathan/No?l,
>>>
>>> A brief break from lurking, and I was wondering what the issue is
>>> with
>>> case-insensitivity? I mean, from a non-developer's point of view
>>> (mine), I'm
>>> not sure I would even want case-sensitivity. To the average end- 
>>> user,
>>> FOLDER, folder, Folder, foldeR, folDer, ad nauseum, should all be
>>> the same
>>> folder. As a case example, imagine your grandmother is filing e-
>>> cards she
>>> has received at Christmas, and she created a folder in her
>>> Documents folder
>>> called "Christmas Cards." The next year she creates one called
>>> "Christmas
>>> cards" without looking first. When she looks back in one of those
>>> folders,
>>> she's suddenly in a panic that she can't find one year's e-cards,
>>> freaking
>>> out that she's "broken her computer", and next thing you know I
>>> have to go
>>> to Grandma's place to explain about case-sensitivity for like the
>>> eighteenth
>>> time. Doesn't this just add unnecessary complication?
>>>
>>> Just hoping to have it explained, really. From my brief forays into
>>> the *nix
>>> world, I've noticed that the average Joe wants computers to be more
>>> human-like, developers want them to be more technical, and neither
>>> party is
>>> particularly fond of the other. Isn't there some middle ground?
>>
>> (Okay, posting a third time, this time manually adding zfs- 
>> discuss at ...
>> to the addresses)
>>
>> Good question.  I'll refer you to a good and complete answer:
>>
>> http://drewthaler.blogspot.com/2007/12/case-against- 
>> insensitivity.html
>>
>> Note that your specific "grandmother" example (and Mr. Palmer's  
>> stated
>> example) is almost exactly replicated (and explained) under the 4th
>> subheading titled "Case-insensitivity is a layering violation."
>>
>> My intention was not to start a flamewar... and I encourage Mr.  
>> Palmer
>> to read the linked post above...
>>
>> ~ Nathan
>
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss
>

- ---
???
Check my PGP key here:
http://pgp.mit.edu:11371/pks/lookup?op=get&search=0xCB67593B




-----BEGIN PGP SIGNATURE-----

iQEcBAEBAgAGBQJIplRWAAoJEGmx2Mt/+Iw/SHQIAIdH7lstxoTz1rm9sPU8BjIk
/rUFlhwhHwHc8jSunj6Gl4JVfgFJPh76pCaX6im2N2WKMJZEPHAhZfk+C46NGyqn
rWY4WPy5FnxrDQHS2hnu6UrLuMjZQz6aXZVoAyshvJdg44iBQj9b/RP55SeV/Rju
Exq4G1O4Q59JVl0m8xCPo6j8hT7gNJvXIDrk0ws6iZEQXpkfj4frz2yuKmT7/j47
SQVTVqFwDjJ/zVSH6UAoaMSzNL/MYgfneLR/x2v5EaPClN8d0cW6+3uZmJryCmED
8cLmOgIazeH0v2rvFRDVSnqC5ucc7DUMYPfEwLkG3z34lG/YjVdSf/Qd+Aip4lE=
=Nzr3
-----END PGP SIGNATURE-----

From lists at loveturtle.net  Sat Aug 16 06:06:24 2008
From: lists at loveturtle.net (Dillon Kass)
Date: Sat, 16 Aug 2008 09:06:24 -0400
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <CD1F6169-5668-4E4B-BFBA-563FAE0791E2@asgaard.org>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>	<96c9d6a80808151538m77ad6e27s3697c230676a7e86@mail.gmail.com>
	<CD1F6169-5668-4E4B-BFBA-563FAE0791E2@asgaard.org>
Message-ID: <48A6D0D0.3080601@loveturtle.net>

It blows my mind that anyone actually wants case insensitivity..
I don't think grandma is going to be opening the terminal and creating 
any zpools or running 10.6 server on her imac anytime in the near future 
so I don't see this as an issue.

The case sensitivity dataset is important so that I can use ZFS & crappy 
applications that fail at case sensitivity at the same time and without 
wasting space on a second partition. That sure would be nice....I'm 
looking at you Blizzard.

At the end of the day the point is moot, regardless of your preference 
zfs lets you do it both ways, and since it's not going to be the default 
fs in client (or probably even available at all outside of the command 
line) worrying about grandma is just a waste of time.......so...why are 
we all still talking about it? :-)

One last point, The idea of just changing to case sensitivity to force 
developers to make applications that don't suck isn't realiastic. Users 
are stupid, users will say "this program worked in 10.5 and now it 
doesn't work in 10.6, i'm mad and 10.6 sucks." a quick google search 
will return lots of other broken apps that don't work and then "10.6 
sucks" will become a new theme. Soon microsoft will be running "pc, mac" 
commercials where mac guy starts talking about users upgrading to an 
older version to run photoshop and WoW...



From johannes at meyerle.org  Sun Aug 17 09:27:00 2008
From: johannes at meyerle.org (Johannes Meyerle)
Date: Sun, 17 Aug 2008 18:27:00 +0200
Subject: [zfs-discuss] Deleting zfs.readonly.kext if zfs.kext does not exist?
Message-ID: <6C9EEAA4-B218-47F9-B833-F0A8507F7A7D@meyerle.org>

On the Zfs download page I delete all files and installed all new files?

On thing is not clear.

In my Leopard installation I can?t find the file /System/Library/ 
Extensions/zfs.kext but the file /System/Library/Extensions/ 
zfs.readonly.kext so I delete this file after I had safed a copy in an  
other folder.

Is this so correct or shoud I copy this file back to the old folder

Is this file needet for reading the file system?

A question for a beginner I gues!!!

Johannes Meyerle
Dohlenfelsenstrasse 2
91809 Wellheim
johannes at meyerle.org

NOTICE:

Diese Nachricht bzw. ein beigef?gter Anhang kann mit einer  
qualifizierten elektronischen Signatur nach dem Signaturgesetz  
versehen sein. Diese Signatur ersetzt gem. ? 126a BGB die sonst  
notwendige Unterschrift. Zur Pr?fung der Digitalen Signatur wenden Sie  
sich bitte an Ihren Systembetreuer / Administrator.

This message, together with any attachments, may contain privileged  
and/or confidential information. If you have received this e-mail in  
error or are not an intended recipient, you may not use, reproduce,  
disseminate or distribute it; do not open nor save any attachments,  
delete it immediately from your system and notify the sender promptly  
by e-mail that you have done so. Thank you.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080817/0908570b/attachment.html 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 2569 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080817/0908570b/attachment.bin 

From nathan.stocks at gmail.com  Sun Aug 17 09:31:35 2008
From: nathan.stocks at gmail.com (Nathan)
Date: Sun, 17 Aug 2008 10:31:35 -0600
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <4AC7A977-EEDD-4B0B-84E1-7B4EEC390F5D@asgaard.org>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
	<96c9d6a80808151538m77ad6e27s3697c230676a7e86@mail.gmail.com>
	<40DC6309-8D70-42C7-A689-1AF7CB337EF9@apple.com>
	<96c9d6a80808151737y7cbf25dtc40c634e80887660@mail.gmail.com>
	<DC1982A8-6DCC-489F-BD85-DEC07FBDC1CC@electricteaparty.net>
	<96c9d6a80808151940v7f94b462n36017b53b335f9c2@mail.gmail.com>
	<92FA8E1F-55BA-4D3B-AECE-0C6945F3816F@electricteaparty.net>
	<4AC7A977-EEDD-4B0B-84E1-7B4EEC390F5D@asgaard.org>
Message-ID: <96c9d6a80808170931w4210a408q960b2d30a3b462ed@mail.gmail.com>

On Fri, Aug 15, 2008 at 10:15 PM, Christopher LILJENSTOLPE
<cdl at asgaard.org> wrote:

>        You hit the nail on the head - if you think that case insensitivity
> is a UI improvement, then do it in the UI, NOT in the filesystem.

Amen.  Thanks for boiling it down to the core issue, Chris.  That's
worth repeating.  (UI = User Interface)

Case insensitivity is a UI improvement.

No one's arguing that there's not good places for case insensitivity.
Just not in the filesystem.  For the non-developers here, the "Finder"
is UI.  Open/Save dialogs are UI.  "Programs" are UI.  "Applications"
are UI.  MS Word is UI.  Even the Terminal (bash, etc) is UI.  Having
a case-sensitive _filesystem_ won't prevent anything that wants or
needs to be case-insensitive from doing so.

Case insensitivity is a UI improvement.

~ Nathan

From info at martin-hauser.net  Sun Aug 17 12:01:26 2008
From: info at martin-hauser.net (Martin Hauser)
Date: Sun, 17 Aug 2008 21:01:26 +0200
Subject: [zfs-discuss] Deleting zfs.readonly.kext if zfs.kext does not
	exist?
In-Reply-To: <6C9EEAA4-B218-47F9-B833-F0A8507F7A7D@meyerle.org>
References: <6C9EEAA4-B218-47F9-B833-F0A8507F7A7D@meyerle.org>
Message-ID: <2BFD36C6-3F8C-4346-842D-22ED89F2D3E6@martin-hauser.net>

The zfs.readonly.kext does not need to be deleted. It is the read only  
support that ships with Mac Os X leopard and will be placed there by  
the next upgrade. The read-write zfs.kext will automatically disable  
the zfs.readonly.kext while it is loaded.

The instructions on the site are meant for both upgrade and first time  
installations... rm -rf ing a .kext that doesn't even exist won't even  
give you an error.

Regards

Martin

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 2272 bytes
Desc: not available
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080817/745fa00c/attachment.bin 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 186 bytes
Desc: This is a digitally signed message part
Url : http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080817/745fa00c/attachment-0001.bin 

From caronni at gmail.com  Mon Aug 18 00:36:37 2008
From: caronni at gmail.com (Germano Caronni)
Date: Mon, 18 Aug 2008 09:36:37 +0200
Subject: [zfs-discuss] another failure mode for zfs
In-Reply-To: <31EF3B70-B04F-4140-AC00-F623A39932B6@mac.com>
References: <327b821f0808051247hcd371cfo542e026056d54d47@mail.gmail.com>
	<327b821f0808090210y29ffadbcg422aa7bd2586dcad@mail.gmail.com>
	<31EF3B70-B04F-4140-AC00-F623A39932B6@mac.com>
Message-ID: <327b821f0808180036y67452370m7be35d4a536321f6@mail.gmail.com>

Hey Robert,

thanks for validating my bug-claim ;-) Did you test with a downloaded pool1,
or a freshly generated directory tree?

Germano

On Thu, Aug 14, 2008 at 03:33, Robert Gordon <macrbg at mac.com> wrote:

>
> On Aug 9, 2008, at 4:10 AM, Germano Caronni wrote:
>
>  Did anybody have a chance to reproduce this issue? Or is this another one
>> of the 'it only happens to me' instances?
>>
>> Germano
>>
>
> I was able to reproduce the issue.
>
> To unmount i used the following command:
>
> sudo zfs umount -f drip/test
>
>
> Robert.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080818/5390249d/attachment.html 

From brouce at gmx.net  Mon Aug 18 01:23:03 2008
From: brouce at gmx.net (wishi)
Date: Mon, 18 Aug 2008 10:23:03 +0200
Subject: [zfs-discuss] zfs write as user
Message-ID: <48A93167.7070805@gmx.net>

Hi!


I installed zfs write support and created a pool with 9 GB for backups.

zpool status
  pool: backups_wegups
 state: ONLINE
status: The pool is formatted using an older on-disk format.  The pool can
	still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
	pool will no longer be accessible on older software versions.
 scrub: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	backups_wegups  ONLINE       0     0     0
	  disk0s8   ONLINE       0     0     0

errors: No known data errors


Okay, it's not the newest version. But I'm fine with that as long as it
works on my MacOS 10.5.4 system.
Are there any known issues with updateing?


wishi at dawn ~
% zfs get all
NAME            PROPERTY       VALUE                    SOURCE
backups_wegups  type           filesystem               -
backups_wegups  creation       So Aug 17 19:50 2008     -
backups_wegups  used           386M                     -
backups_wegups  available      8,67G                    -
backups_wegups  referenced     386M                     -
backups_wegups  compressratio  1.01x                    -
backups_wegups  mounted        yes                      -
backups_wegups  quota          none                     default
backups_wegups  reservation    none                     default
backups_wegups  recordsize     128K                     default
backups_wegups  mountpoint     /Volumes/backups_wegups  default
backups_wegups  sharenfs       off                      default
backups_wegups  checksum       on                       default
backups_wegups  compression    on                       local
backups_wegups  atime          off                      local
backups_wegups  devices        on                       default
backups_wegups  exec           on                       default
backups_wegups  setuid         off                      local
backups_wegups  readonly       off                      default
backups_wegups  zoned          off                      default
backups_wegups  snapdir        hidden                   default
backups_wegups  aclmode        groupmask                default
backups_wegups  aclinherit     secure                   default
backups_wegups  canmount       on                       default
backups_wegups  shareiscsi     off                      default
backups_wegups  xattr          on                       default
backups_wegups  copies         1                        default
backups_wegups  version        1                        -



Can someone tell me how I allow my local users to write on the pool. At
the moment I need sudo... which isn't very compfortable I think.


Thanks,
wishi

From macrbg at mac.com  Mon Aug 18 08:46:50 2008
From: macrbg at mac.com (Robert Gordon)
Date: Mon, 18 Aug 2008 10:46:50 -0500
Subject: [zfs-discuss] another failure mode for zfs
In-Reply-To: <327b821f0808180036y67452370m7be35d4a536321f6@mail.gmail.com>
References: <327b821f0808051247hcd371cfo542e026056d54d47@mail.gmail.com>
	<327b821f0808090210y29ffadbcg422aa7bd2586dcad@mail.gmail.com>
	<31EF3B70-B04F-4140-AC00-F623A39932B6@mac.com>
	<327b821f0808180036y67452370m7be35d4a536321f6@mail.gmail.com>
Message-ID: <2F7F64C6-4B55-4310-90F6-D2B7A7412D55@mac.com>


On Aug 18, 2008, at 2:36 AM, Germano Caronni wrote:

> Hey Robert,
>
> thanks for validating my bug-claim ;-) Did you test with a  
> downloaded pool1, or a freshly generated directory tree?
>
> Germano

I have a small Solaris ONNV (build 90) box (http://blogs.sun.com/macrbg/date/20070508 
) where i created several zvols and 'shared' them as iscsi targets.

Using  the sns iscsi initiator (http://www.studionetworksolutions.com/support/faq.php?pi=11&fi=51 
) I initialized one of them with a GUID partition table and created a  
zpool,
and ran your test-case program....  so it was a nice new pool to go  
swim in :)

Robert.. 

From macrbg at mac.com  Mon Aug 18 08:59:43 2008
From: macrbg at mac.com (Robert Gordon)
Date: Mon, 18 Aug 2008 10:59:43 -0500
Subject: [zfs-discuss] zfs write as user
In-Reply-To: <48A93167.7070805@gmx.net>
References: <48A93167.7070805@gmx.net>
Message-ID: <E333E758-B273-44BF-8ABA-8EB9F248E0F0@mac.com>


On Aug 18, 2008, at 3:23 AM, wishi wrote:
{snip}
> Are there any known issues with updateing?

Not that i have seen thus far..

>
> Can someone tell me how I allow my local users to write on the pool.

If your create a file system for each user you can then change the  
owner of the mount point for the intended user.

> At the moment I need sudo... which isn't very compfortable I think.

It's not convenient :) -- Maybe the Apple Z-Team will use zfs  
delegation to help solve this issue ??

Robert..

From ndellofano at apple.com  Mon Aug 18 10:29:32 2008
From: ndellofano at apple.com (=?ISO-8859-1?Q?No=EBl_Dellofano?=)
Date: Mon, 18 Aug 2008 10:29:32 -0700
Subject: [zfs-discuss] zfs write as user
In-Reply-To: <E333E758-B273-44BF-8ABA-8EB9F248E0F0@mac.com>
References: <48A93167.7070805@gmx.net>
	<E333E758-B273-44BF-8ABA-8EB9F248E0F0@mac.com>
Message-ID: <8A46746D-9818-422C-A85A-A904931A2CC1@apple.com>

The only reason we currently create version 6 pools (when the latest  
is version 8), is to be compatible with the original zfs readonly  
version released in the original version of Leopard.  See the second  
item of:

http://zfs.macosforge.org/trac/wiki/faq

So if you don't intend to need to mount your pool readonly on a  
Leopard system that only has the old readonly zfs code on it, upgrade  
away.

>> At the moment I need sudo... which isn't very compfortable I think.
>
> It's not convenient :) -- Maybe the Apple Z-Team will use zfs
> delegation to help solve this issue ??

You can also use Finder to change the permissions on your pool if that  
is easier for you.  But yeah, we're still  working on a proper  
presentation model.  Solaris' delegated admin model is cool, but in  
Mac OSX we do all our access and permissions checking in the VFS  
layer, so this may not make the most sense for the OSX model... hence  
we're still figuring out ...

Noel



On Aug 18, 2008, at 8:59 AM, Robert Gordon wrote:

>
> On Aug 18, 2008, at 3:23 AM, wishi wrote:
> {snip}
>> Are there any known issues with updateing?
>
> Not that i have seen thus far..
>
>>
>> Can someone tell me how I allow my local users to write on the pool.
>
> If your create a file system for each user you can then change the
> owner of the mount point for the intended user.
>
>> At the moment I need sudo... which isn't very compfortable I think.
>
> It's not convenient :) -- Maybe the Apple Z-Team will use zfs
> delegation to help solve this issue ??
>
> Robert..
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From allenw at iobm.com  Fri Aug 15 20:43:00 2008
From: allenw at iobm.com (Allen Wittenauer)
Date: Fri, 15 Aug 2008 20:43:00 -0700
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <40DC6309-8D70-42C7-A689-1AF7CB337EF9@apple.com>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
	<96c9d6a80808151538m77ad6e27s3697c230676a7e86@mail.gmail.com>
	<40DC6309-8D70-42C7-A689-1AF7CB337EF9@apple.com>
Message-ID: <0C474E3D-6227-4058-8221-6E43ECDC1147@iobm.com>


On Aug 15, 2008, at 4:59 PM, No?l Dellofano wrote:
> And yes (chime in), it would be great if people would get with the
> times and fix their third party apps to not rely on case
> insensitivity.  However in the real world scenario, i feel that in all
> probability this will take a while and most likely a lot of pain, many
> beatings, and some bribes until it happens.... (see well known Adobe
> example below...)

	(slightly off topic)

	It would be great to have an option (or library shim or whatever)  
that could flag an application as "broken" and have that shim force  
the case of the app to either upper or lower or whatever by changing  
the open(), etc, system calls.

	This way, legacy apps might have a better chance of working on case  
sensitive file systems.  [I suspect in many cases they'll work fine,  
but I don't know how prevalent file name comparisons are amongst  
apps.  "Did the user open 'File' or 'file'?"-kind of decision trees.]

From sah.list at gmail.com  Mon Aug 18 15:30:50 2008
From: sah.list at gmail.com (Sean Hafeez)
Date: Mon, 18 Aug 2008 15:30:50 -0700
Subject: [zfs-discuss] An interesting crash..
Message-ID: <D98A0218-3197-4D94-B712-041A4A35EF7C@gmail.com>

Mon Aug 18 14:02:34 2008
panic(cpu 0 caller 0x00AD3D21): "ZFS: I/O failure (write on <unknown>  
off 0: zio 0x5192000 [L0 ZFS directory] 1000L/400P  
DVA[0]=<0:62002ea400:400> DVA[1]=<0:16002d1400:400> fletcher4 lzjb LE  
contiguous birth=14120 fill=1 cksum=3498ec9a6a: 
248c486680be:d8d42a10f95cc:389c946baa00d4b): error " "6"@/Volumes/ 
pixie_dust/home/ndellofano/zfs-work/zfs-119/zfs_kext/zfs/zio.c:918
Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x50593e38 : 0x12b0fa (0x4592a4 0x50593e6c 0x133243 0x0)
0x50593e88 : 0xad3d21 (0xb436b0 0xb436a4 0xb3f400 0xb70670)
0x50593f08 : 0xad03fb (0x5192000 0x374 0x50593f28 0xb1718a)
0x50593f48 : 0xb2f4a6 (0x5192000 0x5102500 0x647c7 0xb585f4)
0x50593fc8 : 0x19ebdc (0x4eb64a0 0x0 0x1a20b5 0x798fb58)
Backtrace terminated-invalid frame pointer 0
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0xabc000->0xb87fff

BSD process name corresponding to current thread: kernel_task

Mac OS version:
9E17

Kernel version:
Darwin Kernel Version 9.4.0: Mon Jun  9 19:30:53 PDT 2008;  
root:xnu-1228.5.20~1/RELEASE_I386
System model name: MacBookPro3,1 (Mac-F4238BC8)

So we has some power issues here. The HD is external and formated ZFS.  
The HD would go down but not the laptop. When the power comes back on  
the HD spins up and the Mac would hard look requiring a reboot.

Thanks,
Sean


From lopez.on.the.lists at yellowspace.net  Wed Aug 20 04:34:51 2008
From: lopez.on.the.lists at yellowspace.net (Lorenzo Perone)
Date: Wed, 20 Aug 2008 13:34:51 +0200
Subject: [zfs-discuss] case-insensitivity as a filesystem option?
In-Reply-To: <F2067D82-5A2B-4610-8369-9D02633F0CE1@apple.com>
References: <5027CC75-317A-450A-BCA3-68E4FE14EC8C@yellowspace.net>
	<D2FBA292-84D8-4CD9-9642-F1D434977A1D@mac.com>
	<F2067D82-5A2B-4610-8369-9D02633F0CE1@apple.com>
Message-ID: <74A4114D-B81C-4FD2-9560-13C4EEB5BF5D@yellowspace.net>


On 16.08.2008, at 01:53, No?l Dellofano wrote:

> Robert is correct.  We've synced ZFS on Mac to Solaris build 94,  
> which picks up the case insensitivity stuff and a bunch of other  
> goodness.  Currently we're testing and debugging the synced code.   
> I'll release it on macosforge as soon as I deem it "stable".  (oh  
> yeah, love those quote marks..)

That sounds great. Choice is often better than no choice,
and for my part, it was just a practical point of view,
not a philosophical/theoretical issue...

Lookin' forward to installing the next pack of bits,
hopefully soon with all pending fixes, and strawberries
and, most importantly, zfs create-beer and
zfs create-coffee :)

[ Didn't mean to start those sensitivity-wars though...]

Regards,

Lorenzo



From sah.list at gmail.com  Wed Aug 20 13:33:56 2008
From: sah.list at gmail.com (Sean Hafeez)
Date: Wed, 20 Aug 2008 13:33:56 -0700
Subject: [zfs-discuss] KP after plugging in drive.
Message-ID: <D4112B84-4EB0-4CC9-AA44-51FAFD4A6780@gmail.com>

Plugged in and powered on external ZFS volume. System Kernel Panic.


Wed Aug 20 13:21:16 2008
panic(cpu 0 caller 0x354F8D21): "ZFS: I/O failure (write on <unknown>  
off 0: zio 0x784ddd8 [L0 persistent error log] 4000L/400P  
DVA[0]=<0:6200064c00:400> DVA[1]=<0:1600054c00:400>  
DVA[2]=<0:3100036800:400> fletcher4 lzjb LE contiguous birth=17307  
fill=1 cksum=5dba230ace:3ee5e235caf5:16deb8a9804f64:5e70846d445e8c8):  
error " "6"@/Volumes/pixie_dust/home/ndellofano/zfs-work/zfs-119/ 
zfs_kext/zfs/zio.c:918
Backtrace, Format - Frame : Return Address (4 potential args on stack)
0x352cfe38 : 0x12b0fa (0x4592a4 0x352cfe6c 0x133243 0x0)
0x352cfe88 : 0x354f8d21 (0x355686b0 0x355686a4 0x35564400 0x35595670)
0x352cff08 : 0x354f53fb (0x784ddd8 0x2f0 0x352cff28 0x3553c18a)
0x352cff48 : 0x355544a6 (0x784ddd8 0x507c080 0x5573e4 0x3557d5f4)
0x352cffc8 : 0x19ebdc (0x616e620 0x0 0x1a20b5 0x434e8b8)
Backtrace terminated-invalid frame pointer 0
       Kernel loadable modules in backtrace (with dependencies):
          com.apple.filesystems.zfs(8.0)@0x354e1000->0x355acfff

BSD process name corresponding to current thread: kernel_task

Mac OS version:
9E17

Kernel version:
Darwin Kernel Version 9.4.0: Mon Jun  9 19:30:53 PDT 2008;  
root:xnu-1228.5.20~1/RELEASE_I386
System model name: MacBookPro3,1 (Mac-F4238BC8)

From slord at perforce.com  Wed Aug 27 13:45:31 2008
From: slord at perforce.com (Stewart Lord)
Date: Wed, 27 Aug 2008 13:45:31 -0700
Subject: [zfs-discuss] Setting Tunables
Message-ID: <3CDA50AF-9134-49FC-8A50-E59DAB4CEFD0@perforce.com>

Hi Folks,

I am wondering how to set ZFS tunables under OS X. On solaris one  
would use mdb or /etc/system. Does anyone know the OS X equivalent?

Thanks!
Stewart

From qapf at qapf.com  Wed Aug 27 17:19:51 2008
From: qapf at qapf.com (Alex P. Frangis)
Date: Wed, 27 Aug 2008 17:19:51 -0700
Subject: [zfs-discuss] ZFS and Samba do not get along
Message-ID: <C4DB3D37.6F24%qapf@qapf.com>

Using the built in Samba sharing in OSX I have a folder shared on a 3 Drive Raid-Z connected via firewire, and when accessing the samba share from windows hosts I can not copy files, all file copies end with a host is unreachable error after transfering 99% of their content. You can open files, you can view files in notepad, you can mount ISO's across the network, but copying a file from point A to point B fails.

Anyone have any idea how to fix this one?

Thanks,
Alex


10.5.4 running ZFS 119
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080827/d8bc2ce4/attachment.html 

From marko.mili at gmail.com  Fri Aug 29 15:18:50 2008
From: marko.mili at gmail.com (Marko Milisavljevic)
Date: Fri, 29 Aug 2008 15:18:50 -0700
Subject: [zfs-discuss] ZFS and Samba do not get along
In-Reply-To: <C4DB3D37.6F24%qapf@qapf.com>
References: <C4DB3D37.6F24%qapf@qapf.com>
Message-ID: <ca3c5ef60808291518g7ddec061ib7921a16df82d097@mail.gmail.com>

No idea how to fix it, but I can confirm the same problem form Vista.

On Wed, Aug 27, 2008 at 5:19 PM, Alex P. Frangis <qapf at qapf.com> wrote:
> Using the built in Samba sharing in OSX I have a folder shared on a 3 Drive
> Raid-Z connected via firewire, and when accessing the samba share from
> windows hosts I can not copy files, all file copies end with a host is
> unreachable error after transfering 99% of their content. You can open
> files, you can view files in notepad, you can mount ISO's across the
> network, but copying a file from point A to point B fails.
>
> Anyone have any idea how to fix this one?

From Jonathan.Edwards at Sun.COM  Sat Aug 30 04:08:35 2008
From: Jonathan.Edwards at Sun.COM (Jonathan Edwards)
Date: Sat, 30 Aug 2008 07:08:35 -0400
Subject: [zfs-discuss] ZFS and Samba do not get along
In-Reply-To: <ca3c5ef60808291518g7ddec061ib7921a16df82d097@mail.gmail.com>
References: <C4DB3D37.6F24%qapf@qapf.com>
	<ca3c5ef60808291518g7ddec061ib7921a16df82d097@mail.gmail.com>
Message-ID: <3FDE7512-BE6D-4E66-97A2-7A303A258914@sun.com>

you need a more recent version of Samba, minimally 3.0.26 along with  
the right build parameters to get the ZFS ACL support (ZFS uses a  
variant of NFSv4 style ACLs)  .. we put this in around snv_70 in  
opensolaris last year see:
http://mail.opensolaris.org/pipermail/zfs-discuss/2007-September/042552.html

you can find all sun's patches (ones that haven't been integrated into  
the Samba base) against the current GPLv2 version of samba (3.0.31)  
here:
http://src.opensolaris.org/source/xref/sfw/usr/src/cmd/samba/

or optionally i guess we (apple/sun/someone) could start a side  
project to attempt a port of the CIFS server which has integral  
support in the newer ZFS bits:
http://opensolaris.org/os/project/cifs-server/

hth
Jonathan

On Aug 29, 2008, at 6:18 PM, Marko Milisavljevic wrote:

> No idea how to fix it, but I can confirm the same problem form Vista.
>
> On Wed, Aug 27, 2008 at 5:19 PM, Alex P. Frangis <qapf at qapf.com>  
> wrote:
>> Using the built in Samba sharing in OSX I have a folder shared on a  
>> 3 Drive
>> Raid-Z connected via firewire, and when accessing the samba share  
>> from
>> windows hosts I can not copy files, all file copies end with a host  
>> is
>> unreachable error after transfering 99% of their content. You can  
>> open
>> files, you can view files in notepad, you can mount ISO's across the
>> network, but copying a file from point A to point B fails.
>>
>> Anyone have any idea how to fix this one?
> _______________________________________________
> zfs-discuss mailing list
> zfs-discuss at lists.macosforge.org
> http://lists.macosforge.org/mailman/listinfo.cgi/zfs-discuss


From craig at mainstream.net  Sat Aug 30 11:38:54 2008
From: craig at mainstream.net (Craig Peterson)
Date: Sat, 30 Aug 2008 14:38:54 -0400
Subject: [zfs-discuss] Problems with ZFS on Intel Mac Pro Copying Hundreds
	of Files
Message-ID: <2CBFBA1A-A57B-45BC-93E8-B91FD08A67CF@mainstream.net>

I'm running build 119 on an up-to-date Mac Pro.  I've set up a three- 
disk zfs raid-z:

$ zpool status
   pool: Internal
  state: ONLINE
status: The pool is formatted using an older on-disk format.  The pool  
can
         still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
         pool will no longer be accessible on older software versions.
  scrub: none requested
config:

         NAME         STATE     READ WRITE CKSUM
         Internal     ONLINE       0     0     0
           raidz1     ONLINE       0     0     0
             disk1s2  ONLINE       0     0     0
             disk2s2  ONLINE       0     0     0
             disk3s2  ONLINE       0     0     0

errors: No known data errors

I'm copying my music collection over to the pool, and it locks up  
trying to write to the zfs partition after an indeterminate amount of  
time.  At this point I have to reboot the machine by physically power  
cycling it in order to get back at the zfs partition and anything that  
tries to write to it gets locked up:

iTunes/iTunes Music/Dr. Jeffrey D. Thompson/Music for Brainwave  
Massage CD2/
iTunes/iTunes Music/Dr. Jeffrey D. Thompson/Music for Brainwave  
Massage CD2/03 Awakened Focus.m4a
    138083516 100%   70.69MB/s    0:00:01 (xfer#55201, to- 
check=12475/116940)
iTunes/iTunes Music/Dr. Jeffrey D. Thompson/Music for Brainwave  
Massage CD2/04 Tranquil Awareness.m4a
^C
rsync error: received SIGINT, SIGTERM, or SIGHUP (code 20) at / 
SourceCache/rsync/rsync-35.2/rsync/rsync.c(244) [sender=2.6.9]
rsync: writefd_unbuffered failed to write 133 bytes [generator]:  
Broken pipe (32)
bash-3.2# rsync -av --progress iTunes /Volumes/Internal/
building file list ...
116941 files to consider
host-10-129-1-069:~ craig$

I've only tried using rsync to copy the files, and rebooting the  
machine every hour or so isn't much of an option :-)

Suggestions?  I found a bug report of something similar that was said  
to be fixed in build 119...

Thanks,

Craig.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: http://lists.macosforge.org/pipermail/zfs-discuss/attachments/20080830/6e3d594e/attachment.html 

